DOI,CodeAvailability
10.1038/sdata20142,MISSING
10.1038/sdata20141,MISSING
10.1038/sdata20146,MISSING
10.1038/sdata20147,MISSING
10.1038/sdata20144,MISSING
10.1038/sdata20148,MISSING
10.1038/sdata20145,MISSING
10.1038/sdata20143,MISSING
10.1038/sdata20149,MISSING
10.1038/sdata201412,MISSING
10.1038/sdata201411,MISSING
10.1038/sdata201415,MISSING
10.1038/sdata201413,MISSING
10.1038/sdata201417,MISSING
10.1038/sdata201418,MISSING
10.1038/sdata201419,MISSING
10.1038/sdata201414,MISSING
10.1038/sdata201423,MISSING
10.1038/sdata201424,MISSING
10.1038/sdata201431,MISSING
10.1038/sdata201429,MISSING
10.1038/sdata201436,MISSING
10.1038/sdata201435,MISSING
10.1038/sdata201434,MISSING
10.1038/sdata201433,MISSING
10.1038/sdata201437,MISSING
10.1038/sdata201440,MISSING
10.1038/sdata201442,MISSING
10.1038/sdata201439,MISSING
10.1038/sdata201443,MISSING
10.1038/sdata201438,MISSING
10.1038/sdata201447,MISSING
10.1038/sdata201445,MISSING
10.1038/sdata201441,MISSING
10.1038/sdata201448,MISSING
10.1038/sdata201449,MISSING
10.1038/sdata201450,MISSING
10.1038/sdata201453,MISSING
10.1038/sdata201451,MISSING
10.1038/sdata201452,MISSING
10.1038/sdata201446,MISSING
10.1038/sdata201456,MISSING
10.1038/sdata201454,MISSING
10.1038/sdata20151,MISSING
10.1038/sdata20152,MISSING
10.1038/sdata20153,MISSING
10.1038/sdata20156,MISSING
10.1038/sdata20155,MISSING
10.1038/sdata20158,MISSING
10.1038/sdata20159,MISSING
10.1038/sdata201511,MISSING
10.1038/sdata201513,MISSING
10.1038/sdata20157,MISSING
10.1038/sdata201510,MISSING
10.1038/sdata201514,MISSING
10.1038/sdata201516,MISSING
10.1038/sdata201517,MISSING
10.1038/sdata201515,MISSING
10.1038/sdata201521,MISSING
10.1038/sdata201518,MISSING
10.1038/sdata201520,MISSING
10.1038/sdata201527,MISSING
10.1038/sdata201523,MISSING
10.1038/sdata201519,MISSING
10.1038/sdata201524,MISSING
10.1038/sdata201525,MISSING
10.1038/sdata201526,MISSING
10.1038/sdata201528,MISSING
10.1038/sdata201529,MISSING
10.1038/sdata201522,MISSING
10.1038/sdata201530,MISSING
10.1038/sdata201533,MISSING
10.1038/sdata201531,MISSING
10.1038/sdata201535,MISSING
10.1038/sdata201532,MISSING
10.1038/sdata201536,MISSING
10.1038/sdata201538,MISSING
10.1038/sdata201540,MISSING
10.1038/sdata201512,MISSING
10.1038/sdata201541,MISSING
10.1038/sdata201542,MISSING
10.1038/sdata201544,MISSING
10.1038/sdata201546,MISSING
10.1038/sdata201545,MISSING
10.1038/sdata201547,MISSING
10.1038/sdata201548,MISSING
10.1038/sdata201549,MISSING
10.1038/sdata201552,MISSING
10.1038/sdata201553,MISSING
10.1038/sdata201551,MISSING
10.1038/sdata201550,MISSING
10.1038/sdata201543,MISSING
10.1038/sdata201554,MISSING
10.1038/sdata201555,MISSING
10.1038/sdata201558,MISSING
10.1038/sdata201557,MISSING
10.1038/sdata201556,MISSING
10.1038/sdata201559,MISSING
10.1038/sdata201563,MISSING
10.1038/sdata201561,MISSING
10.1038/sdata201562,MISSING
10.1038/sdata201569,MISSING
10.1038/sdata201567,MISSING
10.1038/sdata201565,MISSING
10.1038/sdata201570,MISSING
10.1038/sdata201564,MISSING
10.1038/sdata201560,MISSING
10.1038/sdata201574,MISSING
10.1038/sdata201572,MISSING
10.1038/sdata201568,MISSING
10.1038/sdata201566,MISSING
10.1038/sdata201573,MISSING
10.1038/sdata201576,MISSING
10.1038/sdata201571,MISSING
10.1038/sdata201575,MISSING
10.1038/sdata201577,MISSING
10.1038/sdata20162,MISSING
10.1038/sdata201578,MISSING
10.1038/sdata20161,MISSING
10.1038/sdata20164,MISSING
10.1038/sdata20163,MISSING
10.1038/sdata20168,MISSING
10.1038/sdata20169,MISSING
10.1038/sdata201610,MISSING
10.1038/sdata20167,MISSING
10.1038/sdata20165,MISSING
10.1038/sdata201613,MISSING
10.1038/sdata201614,MISSING
10.1038/sdata201612,MISSING
10.1038/sdata20166,MISSING
10.1038/sdata201611,MISSING
10.1038/sdata201616,MISSING
10.1038/sdata201619,MISSING
10.1038/sdata201615,MISSING
10.1038/sdata201617,MISSING
10.1038/sdata201621,MISSING
10.1038/sdata201624,MISSING
10.1038/sdata201623,MISSING
10.1038/sdata201622,MISSING
10.1038/sdata201629,MISSING
10.1038/sdata201627,MISSING
10.1038/sdata201631,MISSING
10.1038/sdata201626,MISSING
10.1038/sdata201628,MISSING
10.1038/sdata201632,MISSING
10.1038/sdata201630,MISSING
10.1038/sdata201635,MISSING
10.1038/sdata201620,MISSING
10.1038/sdata201636,MISSING
10.1038/sdata201638,MISSING
10.1038/sdata201637,MISSING
10.1038/sdata201634,MISSING
10.1038/sdata201625,MISSING
10.1038/sdata201641,MISSING
10.1038/sdata201640,MISSING
10.1038/sdata201644,MISSING
10.1038/sdata201643,MISSING
10.1038/sdata201639,MISSING
10.1038/sdata201646,MISSING
10.1038/sdata201642,MISSING
10.1038/sdata201645,MISSING
10.1038/sdata201652,MISSING
10.1038/sdata201648,MISSING
10.1038/sdata201650,MISSING
10.1038/sdata201651,MISSING
10.1038/sdata201647,MISSING
10.1038/sdata201654,MISSING
10.1038/sdata201653,MISSING
10.1038/sdata201664,MISSING
10.1038/sdata201662,MISSING
10.1038/sdata201656,MISSING
10.1038/sdata201657,MISSING
10.1038/sdata201649,MISSING
10.1038/sdata201658,MISSING
10.1038/sdata201660,MISSING
10.1038/sdata201661,MISSING
10.1038/sdata201663,MISSING
10.1038/sdata201668,MISSING
10.1038/sdata201666,MISSING
10.1038/sdata201665,MISSING
10.1038/sdata201669,MISSING
10.1038/sdata201655,MISSING
10.1038/sdata201667,MISSING
10.1038/sdata201672,MISSING
10.1038/sdata201671,MISSING
10.1038/sdata201674,MISSING
10.1038/sdata201670,MISSING
10.1038/sdata201673,MISSING
10.1038/sdata201675,MISSING
10.1038/sdata201680,MISSING
10.1038/sdata201676,MISSING
10.1038/sdata201679,MISSING
10.1038/sdata201683,MISSING
10.1038/sdata201684,MISSING
10.1038/sdata201685,MISSING
10.1038/sdata201681,MISSING
10.1038/sdata201686,MISSING
10.1038/sdata201691,MISSING
10.1038/sdata201682,MISSING
10.1038/sdata201687,MISSING
10.1038/sdata201688,MISSING
10.1038/sdata201689,MISSING
10.1038/sdata201690,MISSING
10.1038/sdata201693,MISSING
10.1038/sdata201696,MISSING
10.1038/sdata201692,MISSING
10.1038/sdata201698,MISSING
10.1038/sdata201697,MISSING
10.1038/sdata2016101,MISSING
10.1038/sdata201694,MISSING
10.1038/sdata2016100,MISSING
10.1038/sdata201699,MISSING
10.1038/sdata201695,MISSING
10.1038/sdata2016104,MISSING
10.1038/sdata2016103,MISSING
10.1038/sdata2016105,MISSING
10.1038/sdata2016106,MISSING
10.1038/sdata2016108,MISSING
10.1038/sdata2016107,MISSING
10.1038/sdata2016102,MISSING
10.1038/sdata2016110,MISSING
10.1038/sdata2016112,MISSING
10.1038/sdata2016118,MISSING
10.1038/sdata2016109,MISSING
10.1038/sdata2016116,MISSING
10.1038/sdata2016114,MISSING
10.1038/sdata2016119,MISSING
10.1038/sdata2016117,MISSING
10.1038/sdata2016115,MISSING
10.1038/sdata2016122,MISSING
10.1038/sdata2016124,MISSING
10.1038/sdata2016123,MISSING
10.1038/sdata2016130,MISSING
10.1038/sdata2016127,MISSING
10.1038/sdata2016132,MISSING
10.1038/sdata2016128,MISSING
10.1038/sdata2016129,MISSING
10.1038/sdata2016134,MISSING
10.1038/sdata2016126,MISSING
10.1038/sdata20171,MISSING
10.1038/sdata2016133,MISSING
10.1038/sdata20172,MISSING
10.1038/sdata2016125,MISSING
10.1038/sdata2016120,MISSING
10.1038/sdata2016131,MISSING
10.1038/sdata20175,MISSING
10.1038/sdata201717,MISSING
10.1038/sdata201715,MISSING
10.1038/sdata201716,MISSING
10.1038/sdata201711,MISSING
10.1038/sdata201712,MISSING
10.1038/sdata201721,MISSING
10.1038/sdata201722,MISSING
10.1038/sdata201718,MISSING
10.1038/sdata20178,MISSING
10.1038/sdata201723,MISSING
10.1038/sdata20179,MISSING
10.1038/sdata20177,MISSING
10.1038/sdata201720,MISSING
10.1038/sdata201727,MISSING
10.1038/sdata201731,MISSING
10.1038/sdata201729,MISSING
10.1038/sdata201710,MISSING
10.1038/sdata201724,MISSING
10.1038/sdata201726,MISSING
10.1038/sdata201713,MISSING
10.1038/sdata201714,MISSING
10.1038/sdata201732,MISSING
10.1038/sdata20173,MISSING
10.1038/sdata201734,MISSING
10.1038/sdata201736,MISSING
10.1038/sdata201733,MISSING
10.1038/sdata201737,MISSING
10.1038/sdata201738,MISSING
10.1038/sdata2016121,MISSING
10.1038/sdata201730,MISSING
10.1038/sdata201741,MISSING
10.1038/sdata201745,MISSING
10.1038/sdata201749,MISSING
10.1038/sdata201748,MISSING
10.1038/sdata201743,MISSING
10.1038/sdata201739,MISSING
10.1038/sdata201735,MISSING
10.1038/sdata201750,MISSING
10.1038/sdata201747,MISSING
10.1038/sdata201740,MISSING
10.1038/sdata201746,MISSING
10.1038/sdata201752,MISSING
10.1038/sdata201753,MISSING
10.1038/sdata201756,MISSING
10.1038/sdata201755,MISSING
10.1038/sdata201754,MISSING
10.1038/sdata201744,MISSING
10.1038/sdata201761,MISSING
10.1038/sdata201757,MISSING
10.1038/sdata201760,MISSING
10.1038/sdata201770,MISSING
10.1038/sdata201766,MISSING
10.1038/sdata201764,MISSING
10.1038/sdata201765,MISSING
10.1038/sdata201769,MISSING
10.1038/sdata201751,MISSING
10.1038/sdata201763,MISSING
10.1038/sdata201768,MISSING
10.1038/sdata201773,MISSING
10.1038/sdata201772,MISSING
10.1038/sdata201774,MISSING
10.1038/sdata201776,MISSING
10.1038/sdata201759,MISSING
10.1038/sdata201775,MISSING
10.1038/sdata201767,MISSING
10.1038/sdata201778,MISSING
10.1038/sdata201781,MISSING
10.1038/sdata201779,MISSING
10.1038/sdata201783,MISSING
10.1038/sdata201787,MISSING
10.1038/sdata201785,MISSING
10.1038/sdata201777,MISSING
10.1038/sdata201788,MISSING
10.1038/sdata201790,MISSING
10.1038/sdata201784,MISSING
10.1038/sdata201789,MISSING
10.1038/sdata201791,MISSING
10.1038/sdata201792,MISSING
10.1038/sdata201795,MISSING
10.1038/sdata201796,MISSING
10.1038/sdata201794,MISSING
10.1038/sdata201797,MISSING
10.1038/sdata201793,MISSING
10.1038/sdata2017100,MISSING
10.1038/sdata2017101,MISSING
10.1038/sdata201798,MISSING
10.1038/sdata2017110,MISSING
10.1038/sdata2017108,MISSING
10.1038/sdata2017111,MISSING
10.1038/sdata2017103,MISSING
10.1038/sdata2017116,MISSING
10.1038/sdata2017109,MISSING
10.1038/sdata2017118,MISSING
10.1038/sdata2017112,MISSING
10.1038/sdata2017106,MISSING
10.1038/sdata2017105,MISSING
10.1038/sdata2017107,MISSING
10.1038/sdata2017123,MISSING
10.1038/sdata2017117,MISSING
10.1038/sdata2017120,MISSING
10.1038/sdata2017121,MISSING
10.1038/sdata2017122,MISSING
10.1038/sdata2017128,MISSING
10.1038/sdata2017104,MISSING
10.1038/sdata2017129,MISSING
10.1038/sdata2017130,MISSING
10.1038/sdata2017126,MISSING
10.1038/sdata2017127,MISSING
10.1038/sdata2017132,MISSING
10.1038/sdata2017125,MISSING
10.1038/sdata2017124,MISSING
10.1038/sdata2017115,MISSING
10.1038/sdata2017136,MISSING
10.1038/sdata2017134,MISSING
10.1038/sdata2017137,MISSING
10.1038/sdata2017141,MISSING
10.1038/sdata2017143,MISSING
10.1038/sdata2017142,MISSING
10.1038/sdata2017145,MISSING
10.1038/sdata2017147,MISSING
10.1038/sdata2017139,MISSING
10.1038/sdata2017149,MISSING
10.1038/sdata2017151,MISSING
10.1038/sdata2017135,MISSING
10.1038/sdata2017148,MISSING
10.1038/sdata2017144,MISSING
10.1038/sdata2017140,MISSING
10.1038/sdata2017155,MISSING
10.1038/sdata2017152,MISSING
10.1038/sdata2017153,MISSING
10.1038/sdata2017150,MISSING
10.1038/sdata2017156,MISSING
10.1038/sdata2017161,MISSING
10.1038/sdata2017146,MISSING
10.1038/sdata2017157,MISSING
10.1038/sdata2017165,MISSING
10.1038/sdata2017162,MISSING
10.1038/sdata2017167,MISSING
10.1038/sdata2017163,MISSING
10.1038/sdata2017159,MISSING
10.1038/sdata2017160,MISSING
10.1038/sdata2017164,MISSING
10.1038/sdata2017168,MISSING
10.1038/sdata2017166,MISSING
10.1038/sdata2017169,MISSING
10.1038/sdata2017172,MISSING
10.1038/sdata2017170,MISSING
10.1038/sdata2017171,MISSING
10.1038/sdata2017175,MISSING
10.1038/sdata2017173,MISSING
10.1038/sdata2017180,MISSING
10.1038/sdata2017176,MISSING
10.1038/sdata2017178,MISSING
10.1038/sdata2017185,MISSING
10.1038/sdata2017188,MISSING
10.1038/sdata2017186,MISSING
10.1038/sdata2017182,MISSING
10.1038/sdata2017189,MISSING
10.1038/sdata2017187,MISSING
10.1038/sdata2017177,MISSING
10.1038/sdata2017193,MISSING
10.1038/sdata2017195,MISSING
10.1038/sdata2017194,MISSING
10.1038/sdata2017184,MISSING
10.1038/sdata2017192,MISSING
10.1038/sdata2017181,MISSING
10.1038/sdata2017179,MISSING
10.1038/sdata2017190,MISSING
10.1038/sdata2017198,MISSING
10.1038/sdata2017196,MISSING
10.1038/sdata2017191,MISSING
10.1038/sdata2017199,MISSING
10.1038/sdata2017200,MISSING
10.1038/sdata2017197,MISSING
10.1038/sdata2017203,MISSING
10.1038/sdata2017201,MISSING
10.1038/sdata2017205,MISSING
10.1038/sdata2017207,MISSING
10.1038/sdata2017204,MISSING
10.1038/sdata2017202,MISSING
10.1038/sdata2017206,MISSING
10.1038/sdata20181,MISSING
10.1038/sdata20185,MISSING
10.1038/sdata20184,MISSING
10.1038/sdata20189,MISSING
10.1038/sdata20183,MISSING
10.1038/sdata201812,MISSING
10.1038/sdata201810,MISSING
10.1038/sdata20188,MISSING
10.1038/sdata201813,MISSING
10.1038/sdata201818,MISSING
10.1038/sdata201816,MISSING
10.1038/sdata201814,MISSING
10.1038/sdata201811,MISSING
10.1038/sdata201819,MISSING
10.1038/sdata201817,MISSING
10.1038/sdata201815,MISSING
10.1038/sdata201823,MISSING
10.1038/sdata20187,MISSING
10.1038/sdata201820,MISSING
10.1038/sdata201824,MISSING
10.1038/sdata201826,MISSING
10.1038/sdata20186,MISSING
10.1038/sdata201833,MISSING
10.1038/sdata201825,MISSING
10.1038/sdata201821,MISSING
10.1038/sdata201830,MISSING
10.1038/sdata201831,MISSING
10.1038/sdata201838,MISSING
10.1038/sdata201835,MISSING
10.1038/sdata201836,MISSING
10.1038/sdata201822,MISSING
10.1038/sdata201828,MISSING
10.1038/sdata201837,MISSING
10.1038/sdata201847,MISSING
10.1038/sdata201846,MISSING
10.1038/sdata201840,MISSING
10.1038/sdata201834,MISSING
10.1038/sdata201843,MISSING
10.1038/sdata201852,MISSING
10.1038/sdata201848,MISSING
10.1038/sdata201844,MISSING
10.1038/sdata201842,MISSING
10.1038/sdata201849,MISSING
10.1038/sdata201853,MISSING
10.1038/sdata201856,MISSING
10.1038/sdata201851,MISSING
10.1038/sdata201859,MISSING
10.1038/sdata201855,MISSING
10.1038/sdata201864,MISSING
10.1038/sdata201862,MISSING
10.1038/sdata201858,MISSING
10.1038/sdata201857,MISSING
10.1038/sdata201854,MISSING
10.1038/sdata201861,MISSING
10.1038/sdata201866,MISSING
10.1038/sdata201860,MISSING
10.1038/sdata201863,MISSING
10.1038/sdata201875,MISSING
10.1038/sdata201873,MISSING
10.1038/sdata201867,MISSING
10.1038/sdata201874,MISSING
10.1038/sdata201868,MISSING
10.1038/sdata201877,MISSING
10.1038/sdata201871,MISSING
10.1038/sdata201865,MISSING
10.1038/sdata201876,MISSING
10.1038/sdata201879,MISSING
10.1038/sdata201885,MISSING
10.1038/sdata201829,MISSING
10.1038/sdata201882,MISSING
10.1038/sdata201872,MISSING
10.1038/sdata201889,MISSING
10.1038/sdata201881,MISSING
10.1038/sdata201884,MISSING
10.1038/sdata201891,MISSING
10.1038/sdata201883,MISSING
10.1038/sdata201869,MISSING
10.1038/sdata201896,MISSING
10.1038/sdata201890,MISSING
10.1038/sdata201886,MISSING
10.1038/sdata201897,MISSING
10.1038/sdata2018100,MISSING
10.1038/sdata201899,MISSING
10.1038/sdata2018102,MISSING
10.1038/sdata201888,MISSING
10.1038/sdata2018101,MISSING
10.1038/sdata201845,MISSING
10.1038/sdata2018103,MISSING
10.1038/sdata2018108,MISSING
10.1038/sdata201893,MISSING
10.1038/sdata2018105,MISSING
10.1038/sdata2018106,MISSING
10.1038/sdata2018104,MISSING
10.1038/sdata2018107,MISSING
10.1038/sdata2018119,MISSING
10.1038/sdata2018117,MISSING
10.1038/sdata2018111,MISSING
10.1038/sdata2018115,MISSING
10.1038/sdata2018123,MISSING
10.1038/sdata2018116,MISSING
10.1038/sdata2018124,MISSING
10.1038/sdata2018120,MISSING
10.1038/sdata2018122,MISSING
10.1038/sdata2018113,MISSING
10.1038/sdata201887,MISSING
10.1038/sdata2018126,MISSING
10.1038/sdata2018112,MISSING
10.1038/sdata2018114,MISSING
10.1038/sdata201880,MISSING
10.1038/sdata2018132,MISSING
10.1038/sdata2018135,MISSING
10.1038/sdata2018133,MISSING
10.1038/sdata2018125,MISSING
10.1038/sdata2018131,MISSING
10.1038/sdata2018127,MISSING
10.1038/sdata2018134,MISSING
10.1038/sdata2018139,MISSING
10.1038/sdata2018129,MISSING
10.1038/sdata2018128,MISSING
10.1038/sdata2018138,MISSING
10.1038/sdata2018130,MISSING
10.1038/sdata2018140,MISSING
10.1038/sdata2018147,MISSING
10.1038/sdata2018136,MISSING
10.1038/sdata2018145,MISSING
10.1038/sdata2018141,MISSING
10.1038/sdata2018148,MISSING
10.1038/sdata2018146,MISSING
10.1038/sdata2018153,MISSING
10.1038/sdata2018149,MISSING
10.1038/sdata2018151,MISSING
10.1038/sdata2018156,MISSING
10.1038/sdata2018150,MISSING
10.1038/sdata2018155,MISSING
10.1038/sdata2018157,MISSING
10.1038/sdata2018142,MISSING
10.1038/sdata2018158,MISSING
10.1038/sdata2018161,MISSING
10.1038/sdata2018163,MISSING
10.1038/sdata2018159,MISSING
10.1038/sdata2018162,MISSING
10.1038/sdata2018169,MISSING
10.1038/sdata2018168,MISSING
10.1038/sdata2018170,MISSING
10.1038/sdata2018166,MISSING
10.1038/sdata2018171,MISSING
10.1038/sdata2018160,MISSING
10.1038/sdata2018167,MISSING
10.1038/sdata2018109,MISSING
10.1038/sdata2018172,MISSING
10.1038/sdata2018165,MISSING
10.1038/sdata2018179,MISSING
10.1038/sdata2018181,MISSING
10.1038/sdata2018177,MISSING
10.1038/sdata2018175,MISSING
10.1038/sdata2018173,MISSING
10.1038/sdata2018176,MISSING
10.1038/sdata2018154,MISSING
10.1038/sdata2018183,MISSING
10.1038/sdata2018180,MISSING
10.1038/sdata2018164,MISSING
10.1038/sdata2018182,MISSING
10.1038/sdata2018178,MISSING
10.1038/sdata2018185,MISSING
10.1038/sdata2018192,MISSING
10.1038/sdata2018174,MISSING
10.1038/sdata2018191,MISSING
10.1038/sdata2018186,MISSING
10.1038/sdata2018187,MISSING
10.1038/sdata2018193,MISSING
10.1038/sdata2018188,MISSING
10.1038/sdata2018184,MISSING
10.1038/sdata2018194,MISSING
10.1038/sdata2018199,MISSING
10.1038/sdata2018201,MISSING
10.1038/sdata2018198,MISSING
10.1038/sdata2018196,MISSING
10.1038/sdata2018195,MISSING
10.1038/sdata2018200,MISSING
10.1038/sdata2018197,MISSING
10.1038/sdata2018189,MISSING
10.1038/sdata2018190,MISSING
10.1038/sdata2018223,MISSING
10.1038/sdata2018202,MISSING
10.1038/sdata2018210,MISSING
10.1038/sdata2018207,MISSING
10.1038/sdata2018212,MISSING
10.1038/sdata2018205,MISSING
10.1038/sdata2018211,MISSING
10.1038/sdata2018208,MISSING
10.1038/sdata2018234,MISSING
10.1038/sdata2018229,MISSING
10.1038/sdata2018239,MISSING
10.1038/sdata2018225,MISSING
10.1038/sdata2018217,MISSING
10.1038/sdata2018226,MISSING
10.1038/sdata2018230,MISSING
10.1038/sdata2018203,MISSING
10.1038/sdata2018231,MISSING
10.1038/sdata2018209,MISSING
10.1038/sdata2018233,MISSING
10.1038/sdata2018214,MISSING
10.1038/sdata2018228,MISSING
10.1038/sdata2018236,MISSING
10.1038/sdata2018238,MISSING
10.1038/sdata2018227,MISSING
10.1038/sdata2018206,MISSING
10.1038/sdata2018232,MISSING
10.1038/sdata2018213,MISSING
10.1038/sdata2018240,MISSING
10.1038/sdata2018220,MISSING
10.1038/sdata2018222,MISSING
10.1038/sdata2018244,MISSING
10.1038/sdata2018235,MISSING
10.1038/sdata2018245,MISSING
10.1038/sdata2018224,MISSING
10.1038/sdata2018246,MISSING
10.1038/sdata2018219,MISSING
10.1038/sdata2018256,MISSING
10.1038/sdata2018247,MISSING
10.1038/sdata2018242,MISSING
10.1038/sdata2018252,MISSING
10.1038/sdata2018254,MISSING
10.1038/sdata2018248,MISSING
10.1038/sdata2018241,MISSING
10.1038/sdata2018218,MISSING
10.1038/sdata2018250,MISSING
10.1038/sdata2018237,MISSING
10.1038/sdata2018262,MISSING
10.1038/sdata2018249,MISSING
10.1038/sdata2018255,MISSING
10.1038/sdata2018258,MISSING
10.1038/sdata2018251,MISSING
10.1038/sdata2018257,MISSING
10.1038/sdata2018259,MISSING
10.1038/sdata2018263,MISSING
10.1038/sdata2018264,MISSING
10.1038/sdata2018273,MISSING
10.1038/sdata2018269,MISSING
10.1038/sdata2018267,MISSING
10.1038/sdata2018268,MISSING
10.1038/sdata2018270,MISSING
10.1038/sdata2018266,MISSING
10.1038/sdata2018271,MISSING
10.1038/sdata2018260,MISSING
10.1038/sdata2018272,MISSING
10.1038/sdata2018276,MISSING
10.1038/sdata2018277,MISSING
10.1038/sdata2018275,MISSING
10.1038/sdata2018281,MISSING
10.1038/sdata2018279,MISSING
10.1038/sdata2018278,MISSING
10.1038/sdata2018287,MISSING
10.1038/sdata2018285,MISSING
10.1038/sdata2018291,MISSING
10.1038/sdata2018292,MISSING
10.1038/sdata2018274,MISSING
10.1038/sdata2018293,MISSING
10.1038/sdata2018283,MISSING
10.1038/sdata2018265,MISSING
10.1038/sdata2018290,MISSING
10.1038/sdata2018284,MISSING
10.1038/sdata2018243,MISSING
10.1038/sdata2018295,MISSING
10.1038/sdata2018282,MISSING
10.1038/sdata2018288,MISSING
10.1038/sdata2018289,MISSING
10.1038/sdata2018261,MISSING
10.1038/sdata2018294,MISSING
10.1038/sdata2018303,MISSING
10.1038/sdata2018298,MISSING
10.1038/sdata2018296,MISSING
10.1038/sdata2018306,MISSING
10.1038/sdata2018305,MISSING
10.1038/sdata2018297,MISSING
10.1038/sdata2018280,MISSING
10.1038/sdata2018299,MISSING
10.1038/sdata2018300,MISSING
10.1038/sdata2018309,MISSING
10.1038/sdata2018304,MISSING
10.1038/sdata2018301,MISSING
10.1038/sdata2018302,MISSING
10.1038/sdata2018310,MISSING
10.1038/sdata20193,MISSING
10.1038/sdata20191,MISSING
10.1038/sdata20194,MISSING
10.1038/sdata20192,MISSING
10.1038/sdata201910,MISSING
10.1038/sdata20199,MISSING
10.1038/sdata20197,MISSING
10.1038/sdata20195,MISSING
10.1038/sdata20196,MISSING
10.1038/sdata20198,MISSING
10.1038/sdata201914,MISSING
10.1038/sdata2018308,MISSING
10.1038/sdata201913,MISSING
10.1038/sdata201917,MISSING
10.1038/sdata2018307,MISSING
10.1038/sdata201912,MISSING
10.1038/sdata201911,MISSING
10.1038/sdata201922,MISSING
10.1038/sdata201916,MISSING
10.1038/sdata201919,MISSING
10.1038/sdata201921,MISSING
10.1038/sdata201915,MISSING
10.1038/sdata201918,MISSING
10.1038/sdata201920,MISSING
10.1038/sdata201923,MISSING
10.1038/sdata201924,MISSING
10.1038/sdata201926,MISSING
10.1038/sdata201932,MISSING
10.1038/sdata201925,MISSING
10.1038/sdata201934,MISSING
10.1038/sdata201930,MISSING
10.1038/sdata201929,MISSING
10.1038/sdata201928,MISSING
10.1038/sdata201927,MISSING
10.1038/sdata201931,MISSING
10.1038/sdata201940,MISSING
10.1038/sdata201939,MISSING
10.1038/sdata201933,MISSING
10.1038/sdata201936,MISSING
10.1038/sdata201938,MISSING
10.1038/sdata201935,MISSING
10.1038/sdata201937,MISSING
10.1038/s41597-019-0013-x,MISSING
10.1038/s41597-019-0014-9,MISSING
10.1038/s41597-019-0019-4,MISSING
10.1038/s41597-019-0012-y,MISSING
10.1038/s41597-019-0011-z,Custom codes are available from the corresponding authors on reasonable request.
10.1038/s41597-019-0017-6,MISSING
10.1038/s41597-019-0028-3,The LC-MS feature detection software (MassHunter®) used in this work is commercially available from Agilent®.
10.1038/s41597-019-0010-0,Data were processed with CrystFEL 0.6.3. CrystFEL 0.6.3 is a free open source software under the GNU Public License version 3 and can be downloaded from http://www.desy.de/~twhite/crystfel/. CASS is publicly available on GitLab ( https://gitlab.gwdg.de/p.lfoucar/cass ). The script used to optimize detector geometry is publicly available on GitHub ( https://github.com/tbarends/pygeom ).
10.1038/s41597-019-0024-7,MISSING
10.1038/s41597-019-0020-y,"The custom written code (shell scripts and MATLAB scripts to implement steps 1-to organize the data according to the BIDS standard is included in the data release. The data2bids MATLAB-function is part of FieldTrip, and is available from https://github.com/fieldtrip/fieldtrip , the bids-tools are available from https://github.com/robertoostenveld/bids-tools , the bids validator is available from https://github.com/bids-standard/bids-validator , and the DICOM to NIfTI converter is available from https://github.com/rordenlab/dcm2niix ."
10.1038/s41597-019-0030-9,MISSING
10.1038/s41597-019-0022-9,"A number of in-house scripts are commonly used for the processing of the raw FASTQ files before alignment as well as for correction of the CAGE specific sequencing bias mentioned above (and described in more detail in supplementary note 3-e of Carninci et al . 20 ). A brief description of these scripts follows: splitByBarcode is used to split multiplexed sequences into constituent sample FASTQ files and can be found in the MOIRAI system 29 ; rRNAdust removes all sequences that match to known rRNA sequences with two or fewer errors and is freely available through the FANTOM5 website ( http://fantom.gsc.riken.jp/5/suppl/rRNAdust/ ); starbam2gcorrectedctss is a shell script used to convert BAM files to CTSS bed files, correcting for any additional Gs at the 5′ end, and is available upon request."
10.1038/s41597-019-0027-4,"Readers can access tutorial and codes in our raw and pre-processed datasets at the figshare.com. Of them, a 59-page tutorial named “Tutorial Data Analysis for Multi-channel EEG Recordings during a Sustained-attention Driving Task.pdf” is provided for researchers to pre-process and analyse multi-channel EEG data acquired during a sustained-attention driving task. Furthermore, MATLAB codes named “Code-availability.zip” for EEG pre-processing and data analysis can also be found."
10.1038/s41597-019-0032-7,"Figures 1 , 3 and 4 are generated with R ( https://www.r-project.org/ ). The written codes to generate these figures are publically accessible via a fixed repository 25 . This repository contains both scripts and data files to allow users to fully comprehend the provided figures. The bathymetry raster used in Figs 1 and 3 are based on the EMODnet bathymetry maps 26 ."
10.1038/s41597-019-0021-x,MISSING
10.1038/s41597-019-0015-8,"A custom code was written in perl programming language to parse and filter the phosphoproteomics data automatically, presented in this study. The code is provided as supplementary material (Supplementary File 1 )."
10.1038/s41597-019-0031-8,MISSING
10.1038/s41597-019-0016-7,"The MyHeart Counts iOS app ( https://github.com/ResearchKit/MyHeartCounts ) was built using Apple’s ResearchKit framework ( http://researchkit.org/ ), which is open source and available on GitHub ( https://github.com/researchkit/researchkit ). It leverages AppCore ( https://github.com/ResearchKit/AppCore ), a layer built on top of ResearchKit that was shared among the five initial ResearchKit apps. The Bridge iOS SDK ( https://github.com/Sage-Bionetworks/Bridge-iOS-SDK ) provides integration with Sage Bionetworks’ Bridge Server, a back-end data service designed for collection of participant donated study data ( https://developer.sagebridge.org/ ). The MyHeart Counts app can be downloaded on the Apple App Store at ( https://itunes.apple.com/us/app/myheart-counts/id972189947?mt=8 )."
10.1038/s41597-019-0036-3,JavaScript code used to generate the paddy rice map is available from the figshare repository 32 .
10.1038/s41597-019-0035-4,The MRIQC Web API is available under the Apache-2.0 license. The source code is accessible through GitHub ( https://github.com/poldracklab/mriqcwebapi ). MRIQC is one possible client to generate IQMs and submit rating feedback. It is available under the BSD 3-clause license. The source code is publicly accessible through GitHub ( https://github.com/poldracklab/mriqc ).
10.1038/s41597-019-0037-2,"All code used to import the raw data was custom-written in Matlab®, version R2014a. Code is available upon request."
10.1038/s41597-019-0034-5,The image registration tools used in this work are freely available in the NiftyReg toolkit. The vessel segmentation method is not freely available but can be replaced by any freely available software tools such as the Vascular Modeling Toolkit.
10.1038/s41597-019-0038-1,"SDSM version 4.2, freely available ( https://sdsm.org.uk/software.html ), is used to statically downscale the projection from the second generation Canadian Earth System Model (CanESM2). The predictors derived from CanESM2 and the NCEP reanalysis data 32 are exported into SDSM directory for model calibration and projection. The CanESM2 is one of the GCMs used in the Coupled Model Inter-comparison Project Phase 5 (CMIP5). A free code written in R (mean-R.txt) is provided to compute the ensembles mean for a single predictand."
10.1038/s41597-019-0029-2,"All the R scripts for calculating the reliability of the scales used in the current datasets were available on OSF 39 . We licensed the data under an Attribution-NonCommercial 4.0 International Creative Commons (CC BY-NC 4.0) license. The R packages used for processing the data were version 3.4.0 (2017-04-21). The packages used in generating current results as following: “tidyverse” ( https://www.tidyverse.org/ ), “psych” ( https://personality-project.org/r/psych/ ), and “foreign” ( https://cran.r-project.org/package=foreign )."
10.1038/s41597-019-0045-2,The algorithm used in this work to convert impedance data to permittivity is described in Eq. 2 .
10.1038/s41597-019-0033-6,MISSING
10.1038/s41597-019-0048-z,"The R code developed for preparing the inputs to the SLEUTH model and integrating the modelling results into global maps is publicly and freely available 17 . The code consists of two R programming language scripts (version R 3.4.3; https://www.r-project.org/ ), which prepare simulation inputs and integrate outputs. The script is internally documented to assist understanding and customisation for further use. We have also shared the modified SLEUTH model, as well as the scenario.jinja file in the Python package sleuth-automation 17 (version 1.0.2; https://pypi.org/project/sleuth-automation/ )."
10.1038/s41597-019-0041-6,Our GitHub archive 15 contains the scripts used in the sequence analysis (file “sequencingscripts.sh” in the folder “scripts”) and in downstream analysis and figure plotting (under the folder “scripts”).
10.1038/s41597-019-0043-4,R code for building dendrograms with bar plots is freely available on Gitlab at: https://gitlab.com/oncobox/watermelon_multisection/blob/master/utils/gallow_plot.R .
10.1038/s41597-019-0042-5,MISSING
10.1038/s41597-019-0044-3,"The datasets included in this paper were manipulated using ESRI ArcGIS v10.2 and later versions. All the ArcMap Tools referred to in the Data Usage section are available in version 10.2 up to the current version, 10.6. The digital shoreline analysis was performed using the open access R-package, AMBUR 39 . LiDAR point cloud manipulation was performed using the open access software, lasizp ( https://laszip.org/ )."
10.1038/s41597-019-0051-4,The pipeline used to analyze RNA sequence data is available on figshare 21 . The script used to remove batch effects from RNA sequencing data is available on figshare 22 .
10.1038/s41597-019-0056-z,All code for data characterization has been written in Python using the Networkx package. The code is open source at https://github.com/bansallab/asnr/ .
10.1038/s41597-019-0053-2,Codes used for image processing and data acquisition for the three types of experiments have been made publically available and are included in the JRC data catalogue 22 .
10.1038/s41597-019-0040-7,MISSING
10.1038/s41597-019-0052-3,"The complete set of Psychtoolbox Matlab scripts for running this study are available for download at BOLD5000.org . The Psychtoolbox Version 3 Matlab toolbox and documentation are both available for download at http://psychtoolbox.org . As per convention in the field, the complete set of images used as stimuli are available for download at BOLD5000.org ."
10.1038/s41597-019-0023-8,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at https://github.com/RDCEP/ggcmi/tree/phase1 .
10.1038/s41597-019-0061-2,No custom computer code was used to generate the data described in the manuscript.
10.1038/s41597-019-0055-0,The source code for generating BioWordVec is freely available at https://github.com/ncbi-nlp/BioWordVec . The PubMed data are available from https://www.nlm.nih.gov/databases/download/pubmed_medline.html . The MeSH RDF data are available from https://www.nlm.nih.gov/databases/download/mesh.html .
10.1038/s41597-019-0059-9,Code for the technical validation can be found archived in the Zenodo repository as “Test_validation” 23 .
10.1038/s41597-019-0069-7,MISSING
10.1038/s41597-019-0064-z,"The R (version 3.3.1; https://www.r-project.org ) code developed for production of the expenditure datasets are publicly and freely available through Figshare 39 . The script is internally documented to both explain its purpose and, when required, guiding the user through its customisation."
10.1038/s41597-019-0062-1,MISSING
10.1038/s41597-019-0057-y,The laser profilometry technique algorithm was developed on Matlab 2014b. Further details on the geometry 3D reconstruction and the algorithm structure are given by Rifai et al . 21 and Rifai 14 .
10.1038/s41597-019-0068-8,The code that support the findings of this study are available on request from the corresponding author [RN].
10.1038/s41597-019-0067-9,"The execution of this work involved using many software tools, whose settings and parameters are described below. ( 1 ) GCE : version1.0.0, parameters: -H 1; ( 2 ) SOAPdenovo : version2, k-mer size of 59; ( 3 ) GAPcloser : version1.12, parameters: -l 150 -p 31; ( 4 ) SSPACE : version3.0, default parameters; ( 5 ) RepeatMasker : Repeat Masker-open-4-0-6, parameters: -a -nolow -no_is -norna -parallel 1; ( 6 ) RepeatModeler : RepeatModeler-open-1.0.11, parameters: -database genome -engine ncbi -pa 15; ( 7 ) Tandem Repeats Finder : TRF-407b, parameters: 2 7 7 80 10 50 2000 -d -h; ( 8 ) TBLASTn : blast-2.2.26, parameters: -p tblastn -e 1e-05 -F T -m 8 -d; ( 9 ) GeneWise : version2.4.1, parameters: -tfor -genesf -gff; ( 10 ) Augustus : version3.2.3, parameters: –uniqueGeneId = true–noInFrameStop = true–gff3 = on–genemodel = complete–strand = both; ( 11 ) GlimmerHMM : version3.0.1, parameters: -g -f; ( 12 ) SNAP : snap-2013-11-29, default parameters; ( 13 ) Trinity : trinityrnaseq-2.1.1, parameters: –seqType fq-CPU 20–max_memory 200G–normalize_reads–full_cleanup–min_glue 2–min_kmer_cov 2–KMER_SIZE 25; ( 14 ) PASA : PASA_r20140417, default parameters; ( 15 ) InterPro : version29.0, perl-based version4.8, default parameters; ( 16 ) tRNAscan-SE : tRNAscan-SE-1.3.1, default parameters; ( 17 ) INFERNAL : version1.1rc4 (June 2013); ( 18 ) BLASTp : blast-2.2.26, parameters: -p blastn -e 1e-10 -v 10000 -b 10000; ( 19 ) BWA : bwa-0.7.8, parameters: mem -k 32 -w 10 -B 3 -O 11 -E 4 -t 10; ( 20 ) SAMtools : samtools-0.1.19, parameters: mpileup mpileup -m 2 -u; ( 21 ) EVM : VidenceModeler-1.1.1, parameters: –segmentSize 200000–overlapSize 20000; ( 22 ) Tophat : tophat-2.0.13, parameters: -p 6–max-intron-length 500000 -m 2–library-type fr-unstranded; ( 23 ) Cufflinks : cufflinks-2.1.1, parameters: -I 500000 -p 1–library-type fr-unstranded -L CUFF; ( 24 ) BUSCO : version3.0.2, OrthoDBv9_vertebrata."
10.1038/s41597-019-0060-3,"We cannot provide access to the raw source data due to their proprietary nature. As mentioned in Step 1 of the Methods section, the source data mainly contain a total of 3.01 billion GPS trajectory samples produced by more than 12,000 taxis during 45 days. As stated by Poulis et al . 26 , the publication of the trajectories of personal movement could lead to identity disclosure, even if directly identifying information (e.g., names of taxi drivers and passengers) is not published. Moreover, existing trajectory anonymization techniques 26 , 27 cannot be used in our research because existing techniques do not care about travel speeds in trajectories and do not need the information of taxi status. However, to obtain the travel speed dataset accurately, we have to use the information of taxi status (as described in Step 3) to indicate when each taxi picks up or drops off passengers. Python (version 2.7.12) is used to produce the link travel dataset in this research. We have not shared the code because the code is dedicatedly designed for our raw source data and researchers cannot benefit from the code without the source data. Meanwhile, the code might reveal the identity of taxi drivers and raw real-time trajectory information of taxis in the road network. However, the code is straightforward, and its steps have been described in detail in the section of ‘Methods’. It is easy for a third party to exactly repeat the method."
10.1038/s41597-019-0066-x,"As mentioned above, some catchment delineation steps can be programmed using Python scripts which was provided in our data set. It contains two scripts, one is for single-lake catchment delineation named “Single-lake catchment.py” and the other is for tandem-lake catchment and mixed-lake catchment delineation named “Tandem-lake and Mixed-lake catchment.py”. Before running these two scripts, it needs to install ArcGIS 10.1 or above version and install the python function carried by it in the Windows system. Then, the scripts can be opened by the IDLE (python GUI). As for “Single-lake catchment.py”, line 9 is for setting a workspace for program running. It should put the initial peripheral boundary shapefile named “bd.shp” and lake shapefile named “lake.shp” in the workspace folder. Line 10 is for the original DEM grid file setting. By finishing all the above steps, it will automatically generates all the catchment units named “basin.shp”. As for “Tandem-lake and Mixed-lake catchment.py”, add the “river.kmz” file which is the river depicted from Google Earth into the workspace folder, the rest setting is the same as the “Single-lake catchment.py”."
10.1038/s41597-019-0058-x,"Perl scripts we used to calculate residues’ environment stickiness and different nDp versions are provided (Scripts.tar.gz 15 ). Accordingly, the scripts archive contains two folders with demonstration input and output files for each script. A wrapper allows to run all calculations from a PDB file. The PyMol 31 script we used to visualize results on protein structures is also provided. All scripts were extensively commented and made easily readable to facilitate re-use and adaptation (Table 5 ). Table 5 Overview of the scripts archive content. Full size table An installation of the software FreeSASA 32 is required to run environment stickiness calculations. FreeSASA provides ASA calculations our script relies on. FreeSASA is available under MIT license and v2.0.3 is included in the scripts archive. The software AnAnaS 33 , 34 (Analytical Analyzer of Symmetries) is used to detect symmetry order and symmetry axes positions required to run nDp calculations. AnAnaS is free for academic use and included in the scripts archive as a binary file."
10.1038/s41597-019-0072-z,The code was written and annotated in R 3.4.1 54 and is available from Figshare 52 . The key package for implementing Random Forests was randomforestSRC 2.7.0 55 .
10.1038/s41597-019-0082-x,SMRT Link 5.1 pipeline: http://www.pacb.com/products-and-services/analytical-software/smrt-analysis/ . CD-HIT: http://www.bioinformatics.org/cd-hit/ (version 4.6.6). Blast: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ (version 2.2.31).
10.1038/s41597-019-0071-0,"The R codes used for correlation analysis, identification of tissue-specific chromatin accessible regions, and tissue-specific TFs are available in the supplementary materials (Supplementary File 1 ). A repository list containing the chromatin accessibility raw count matrix and the motif enrichment matrix is available online 32 ."
10.1038/s41597-019-0079-5,MISSING
10.1038/s41597-019-0078-6,"Several tools have been implemented in the data analyses, whose versions, settings and parameters are described below. (1) SOAPnuke: version 1.5.3, parameters used were -n 0.1 -l 20 -q 0.4 -d -M 1 -Q 2 -i -G–seqType 1; (2) Platanus: version 1.2.4, parameters used were: contig step: -k 32 -u 0.1 -d 0.5 -c 2 -t 30 -s 10 -m 300G; scaffold step: -t 30 –u 0.1; gapclose step: default parameters; (3) GapCloser: version 1.12, parameters used were –l 150 –p 25 –t 30; (4) SSPACE: version 1.1, default parameters; (5) BUSCO: version 3.0.2; (6) TRF: version 4.07b, default parameters; (7) Repbase: version 21.01; (8) RepeatModeler: version 1.0.4, default parameters; (9) RepeatMasker: open-4-0-6, default parameters; (10) Blast: version 2.2.26, parameters used were -F F -m 8 -p tblastn -e 1e-05 -a 5; (11) Genewise: version 2.4.1, default parameters; (12) Hisat: version 2-2.0.1-beta, parameters used were -p 4–max-intronlen 50000–sensitive–dta–dta-cufflinks–phred64–no-discordant–no-mixed; (13) Stringtie: version 1.2.2, default parameters; (14) InterPro: version 5.16–55.0; (15) GO: version 20141201; (16) KEGG: version 84.0; (17) Swissprot: version release-2017-09; (18) TrEMBL: version release-2017-09; (19) BWA-mem: version 0.7.15, default parameters; (20) Picard: version 1.118, default parameters."
10.1038/s41597-019-0073-y,MISSING
10.1038/s41597-019-0070-1,Matlab code for data analysis of location correction and mobility network construction can be obtained freely from Supplemental File 1 with no restrictions to access.
10.1038/s41597-019-0077-7,No custom codes were used in generating or processing of the dataset described herein.
10.1038/s41597-019-0076-8,Python code to synthesise the results and to generate the figures of FLUXCOM results can be obtained through the public repository at https://git.bgc-jena.mpg.de/skoirala/fluxcom_ef_figures . MATLAB code for generating the flux products and ensemble estimates is available on request to Martin Jung (mjung@bgc-jena.mpg.de) for the sake of reproducibility. The collaborative nature of the FLUXCOM initiative and the demanding computing resulted in complex and large amounts of code that was customized to the HPC and file system of MPI-BGC and is therefore challenging to use. Code for processing MODIS satellite data is available on request to Kazuhito Ichii (ichii@chiba-u.jp).
10.1038/s41597-019-0074-x,Code is available upon request.
10.1038/s41597-019-0081-y,"All code developed for the Catalysis-Hub platform is made available open source from the SUNCAT Center’s GitHub repository at https://github.com/SUNCAT-Center , which includes the database backend 52 , frontend 50 and the CatHub python API 39 . The Python script used for plotting the data shown in Fig. 3 , using the CatHub API, is made available as a tutorial at https://github.com/SUNCAT-Center/CatHub/tree/master/tutorials/1_bimetallic_alloys/heatmaps.py ."
10.1038/s41597-019-0080-z,"The CatHub python API and the CatKit software packages are available open-source from the GitHub repository at https://github.com/SUNCAT-Center/ . In addition, the latest stable version of the CatHub module is available from the Zenodo repository 27 . The Python scripts used for plotting the data shown in Fig. 2 is made available as a tutorials at https://github.com/SUNCAT-Center/CatHub/tree/master/tutorials/1_bimetallic_alloys/ . The code used to classify the adsorption sites is made available at https://github.com/SUNCAT-Center/CatHub/tree/master/cathub/classification.py ."
10.1038/s41597-019-0083-9,All data are available as NetCDF files. The calibration process described in this paper and the production of the NetCDF files were undertaken using Matlab scripts written for this purpose. This code is available from the corresponding author upon request.
10.1038/s41597-019-0075-9,"The source-code for Google-Earth-Engine procedure as well as R-codes for the calculation of modified Mann-Kendall-trend test and the NDVI difference images are freely available at: https://github.com/fabianfassnacht/Tibet_Landsat . The trend products were calculated using R version 3.5.1 and packages “trend (1.1.1)”, “raster (2.6–7)”, “extraDistr (1.8.10)”, “doParallel (1.0.14)”, “foreach (1.4.4)”, “matrixStats (0.53.1)”, “quantreg (5.38)”, “MCMCpack (1.4–4)”, “mcmc (0.9–5)”, “Matrix-Models 0.4–1”, “coda (0.19–2)”, “SparseM (1.77)”, “forecast (8.5)”, “stinepack (1.3)”, “rgdal (1.2–16)” and “HKProcess (0.0–2)”."
10.1038/s41597-019-0091-9,"The custom code used to develop the CFTI5Med web-interface is entirely open and based on HTML language. As such the code can be reutilised by whoever is interested in replicating elsewhere our experience. The server-side procedures were developed in PHP open-source language. The client-side procedures were developed in JavaScript language, using specific features of jQuery ( http://jquery.com/ ) and Google Maps API ( https://developers.google.com/maps/ ) to provide a reliable and fast geographic interface. The Boxer computer code v. 3.3 used to derive earthquake source parameters is available online 21 ."
10.1038/s41597-019-0090-x,The source code for PathoPhenoDB is freely available at https://github.com/bio-ontology-research-group/pathophenodb .
10.1038/s41597-019-0086-6,"The posterior probability density functions presented in this paper were sampled using the PyCBC Inference software. The PyCBC Inference toolkit uses the Bayesian inference methodology described in this paper; a more detailed description of the toolkit is presented in ref. 11 . The source code and documentation of PyCBC Inference is available as part of the PyCBC software package at http://pycbc.org . The results in this paper were generated with the PyCBC version 1.12.3 release. In the data release repository for this work 34 we provide scripts and configuration files for replicating our analysis. The scripts document our command line calls to the pycbc_inference executable which performs the ensemble MCMC analyses. The command line call to pycbc_inference contains options for: the ensemble MCMC configuration, data conditioning, and locations of the configuration file and gravitational-wave detector data files. The configuration files included in the repository, and used as an input to pycbc_inference , specify the prior probability density functions used in the analyses, including sections for: initializing the distribution of Markov-chain positions in the ensemble MCMC, declaring transformations between the parameters that define the prior and the parameters that the ensemble MCMC samples (eg. \(({m}_{1},{m}_{2})\to ({\mathscr{M}},q)\) ), and defining additional constraints to the prior probability density function 11 ."
10.1038/s41597-019-0088-4,"All analyses were performed using open sources software tools. Raw sequencing files were downloaded from Illumina BaseSpace using the Illumina Python Run Downloader 41 . Individual samples were initially divided across four lanes for sequencing, and these files were concatenated into one single end Fastq file using the UNIX cat command. cat < FN1 >.fastq < FN2 >.fastq < FN3 >.fastq < FN4 >.fastq > < COND_REP >.fastq The concatenated sequences were input to FastQC v.0.10.1 27 for quality control analysis using default parameters. fastqc < COND_REP >.fastq -o <FASTQC_DIRECTORY> Fastq files were input to Star 2.6 28 for alignment specifying BAM file format sorted by coordinate and requesting unmapped read files. STAR–runMode alignReads–outSAMtype BAM SortedByCoordinate –outSAMstrandField intronMotif–outReadsUnmapped Fastx–readFilesIn < COND_REP >.fastq.gz –outFileNamePrefix < COND_REP >–runThreadN 16–genomeDir Rnor_6.0 –readFilesCommand zcat Read counts for each sample were extracted using HTSeq. 0.10.0 29 . The reverse option was used to indicate strand orientation. Illumina’s TruSeq Stranded mRNA protocol was used to sequence the data and produces libraries where the first read is on the opposite strand to the RNA molecule. htseq-count -f bam–stranded=reverse–mode=intersection-nonempty -r name < COND_REP >/Aligned.sortedByCoord.out.bam Rattus_norvegicus.Rnor_6.0.93_PARSED.gtf >< COND_REP >/gene_counts_Reversed.htseq The raw counts were normalized using DESeq2’s 31 , 32 default procedure, relative log expression (RLE), using the estimateSizeFactors function. Detailed instructions can be found on the Bioconductor website for DESeq2 42 ."
10.1038/s41597-019-0085-7,"Each filled template file was converted into csv format and then parsed using a Python script titled ‘templates_0.2’ 33 . This script converts STAF data in template files into csv files with the same table format as present in data and mapping tables in YSTAFDB. The Python script produces the following csv files: cross_boundary_flows , cross_boundary_flows_citations , flows , flows_citations , processes , processes_citations . Therefore, this set of six csv files were produced ~60 times to develop the material cycles part of YSTAFDB, one for each filled template (i.e., for each material cycle publication). All csv files of the same type, e.g., cross_boundary_flows , were then merged using another Python script titled ‘merged_1.0’ 33 . This procedure yielded six merged csv files, one for each type: cross_boundary_flows , cross_boundary_flows_citations , flows , flows_citations , processes , and processes_citations . Some manual cleaning was then performed to correct any formatting errors identified. All other tables in YSTAFDB were manually produced directly from publications, including criticality , recycling , and their related tables."
10.1038/s41597-019-0097-3,"The code used in this work relies heavily on the open-source tools developed by Materials Project, in particular, Python Materials Genomics ( http://pymatgen.org/ ) and Atomate ( https://atomate.org ). The versions used in this work are pymatgen/4.7.3 and atomate/0.5.1 for all the calculations."
10.1038/s41597-019-0092-8,MISSING
10.1038/s41597-019-0087-5,"All analyses were performed using open sources software tools. Raw sequencing files were downloaded from Illumina BaseSpace using the Illumina Python Run Downloader 41 . Individual samples were initially divided across four lanes for sequencing, and these files were concatenated into one single end Fastq file using the UNIX cat command. cat < FN1 >.fastq < FN2 >.fastq < FN3 >.fastq < FN4 >.fastq> < COND_REP >.fastq The concatenated sequences were input to FastQC v.0.10.1 26 for quality control analysis using default parameters. fastqc < COND_REP >.fastq –o <FASTQC_DIRECTORY> Fastq files were input to Star 2.6 27 for alignment specifying BAM file format sorted by coordinate and requesting unmapped read files. STAR–runMode alignReads–outSAMtype BAM SortedByCoordinate –outSAMstrandField intronMotif–outReadsUnmapped Fastx–readFilesIn < COND_REP >.fastq.gz –outFileNamePrefix < COND_REP >–runThreadN 16–genomeDir Rnor_6.0 –readFilesCommand zcat Read counts for each sample were extracted using HTSeq 0.10.0 28 . The reverse option was used to indicate strand orientation. Illumina’s TruSeq Stranded mRNA protocol was used to sequence the data and produces libraries where the first read is on the opposite strand to the RNA molecule. htseq-count -f bam–stranded=reverse–mode=intersection-nonempty -r name < COND_REP >/Aligned.sortedByCoord.out.bam\Rattus_norvegicus.Rnor_6.0.93_PARSED.gtf> < COND_REP >/gene_counts_Reversed.htseq The raw counts were normalized using DESeq2’s 31 , 32 default procedure, relative log expression (RLE), using the estimateSizeFactors function. Detailed instructions can be found on the Bioconductor website for DESeq2 42 ."
10.1038/s41597-019-0099-1,MISSING
10.1038/s41597-019-0101-y,"SOAPnuke: https://github.com/BGI-flexlab/SOAPnuke . Version: v1.5.2. Parameters: -l 5 -q 0.51 -n 0.55 -i -Q 2–seqType 1. HISAT2: http://www.ccb.jhu.edu/software/hisat . Version:v2.0.4.Parameters:–phred64–sensitive–no-discordant–no-mixed -I 1 -X 1000. Bowtie2: http://bowtie-bio.sourceforge.net/Bowtie2 . Version: v2.2.5. Parameters: -q–phred64–sensitive–dpad 0–gbar 99999999–mp 1,1–np 1–score-min L,0, −0.1 -I 1 -X 1000–no-mixed–no-discordant -p 1 -k 200. RSEM: http://deweylab.biostat.wisc.edu/RSEM . Version: v1.2.12. Parameters: default."
10.1038/s41597-019-0094-6,The code for processing the data from raw sequencing reads to DEGs is available within figshare (CODE_for_RNA-seq.sh) 16 .
10.1038/s41597-019-0095-5,The code used to process the data and perform the quality control and visualization analysis can be found at: https://github.com/hbc/MouseKidneyFibrOmics and at Zenodo 41 .
10.1038/s41597-019-0098-2,MISSING
10.1038/s41597-019-0106-6,The computer codes used to generate spatiotemporal RNA expression data sets reported in this study and for automated cell segmentation and RNA spot detection were developed at Vanderbilt University and are intended solely for scientific research. The computer codes used for automated cell segmentation are available in the public dropbox repository: https://www.dropbox.com/sh/egb27tsgk6fpixf/AADaJ8DSjab_c0gU7N7ZF0Zba?dl=0 . The computer codes used to generate spatiotemporal RNA expression data sets reported in this study are available from the corresponding author on reasonable request.
10.1038/s41597-019-0093-7,"The code written and used for the development of the forest type, EVI2 and river flow data is available in the repositories specified in the references 53 , 54 , 62 . Programs used, including versions, are outlined in the repository descriptions."
10.1038/s41597-019-0103-9,The code to reproduce the results is available on Zenodo repository 22 .
10.1038/s41597-019-0089-3,"All estimation routines to produce the open-access estimated five-year migration flows for all pairs of 200 countries between 1990 and 2015 are in the migest R package available on CRAN 19 . The formatted data and R code for the application of the functions are publicly available on Figshare 21 . They are 1) a tidy version of the United Nations bilateral migrant stock data, demographic period measures for net migration, births and deaths and demographic population totals; 2) an R script that given the three imported data sets a) cleans each to find the countries with complete input data; b) calculates the native population for the diagonal elements of the migrant stock data required for some estimation methods; and c) estimates bilateral migration flows for each method and period combination."
10.1038/s41597-019-0107-5,"The following parameters were used to trim row reads with Trimmomatic (version 0.36) 15 : (i) LEADING and TRAILING = 3, removing bases from the two ends of the reads if below a threshold quality of 3; (ii) SLIDING WINDOW = 4:2, cutting the reads when the average quality within the window composed of 4 bases falls below a threshold equal to 2; (iii) MINLEN = 50, removing the reads shorter than 50 bp. CLC Genomics Workbench (version 7.0.3) was used with default parameters for alignment of reads. Default assembly parameters of Trinity (version 2.1.1) were used, with the addition of the “–jaccard_clip” function because a high gene density with overlapping of UnTranslated Region (UTR) was expected ( https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity ). Local BLAST+(version 2.3.0) 19 was used for BLASTx search against the NCBI non-redundant protein database (downloaded 10 January 2018) setting E-value cut off at 10 −3 . TransDecoder (version 2.1, http://transdecoder.github.io ) and BUSCO (version 1.2) 23 were used with default parameters."
10.1038/s41597-019-0100-z,The code used at each step were shown in the respective methods.
10.1038/s41597-019-0102-x,Analysis code is available on github ( https://github.com/rich-wood/CBCA ) and on Zenodo 16 .
10.1038/s41597-019-0084-8,"For DPI replication and integration of potential future data updates, we provide via figshare all Python scripts and spatial data necessary to replicate each DPI. Because these DPI scripts require input criteria data totaling 65 GBs in size, we bundled both scripts and data into four compressed files, DPI_InputsAndCode_Part01-04.zip 34 . These scripts use ArcPy, a Python module associated with ESRI’s ArcGIS Desktop software ( www.esri.com ) and require both the Advance license of this mapping software and an accompanying Spatial Analyst extension license. A README.pdf accompanies these zip files and provides all necessary instructions to setup the database and run any of the DPI scripts."
10.1038/s41597-019-0114-6,"The program code to produce the AHAH index and components is open source and available through either the GitHub repository( https://github.com/GDSL-UL/AHAH ) or figshare repository 22 . The code consists of two R script files containing the commands for building: a) the network distances between each postcode in GB and the nearest service using the Routino tool and the OSM transport network data and b) the scores of AHAH domains and the overall index. Each file is internally documented to explain purpose and, when required, to guide the user in the appropriate script customization. The R scripts, also make use of the FNN package 23 and its k-nearest neighbour search algorithm and the splitstackshape package 24 for reshaping wide data, even when the data are unbalanced."
10.1038/s41597-019-0113-7,"The code for the mixed gambles task were adopted from a previous study 18 . All codes were executed using Matlab version 2014b and the Psychtoolbox ( www.psychtoolbox.org ). Code is available on GitHub ( https://github.com/rotemb9/NARPS_scientific_data ). Preprocessed data included in the dataset were preprocessed with fMRIprep version 1.1.4 28 , available from fmriprep.org The quality assessment reports were generated with MRIQC version 0.14.2 45 , available from mriqc.org."
10.1038/s41597-019-0115-5,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-019-0122-6,"The latest version of the code and guide for the dataset reproduction are hosted at https://github.com/giacfalk/Electrification_SSA_data . The code is available in two files: a JavaScript file to be imported into Google Earth Engine (step 1), and an R script to be run after having run the JavaScript file in Earth Engine (step 2). In order to run the Earth Engine script, it is necessary to create a Google account and register for Earth Engine via https://signup.earthengine.google.com . All the input data required for the analysis are openly available, with the exception of the LandScan gridded population dataset. To obtain access to population data, it is possible to either apply for an account to use the LandScan data at https://landscan.ornl.gov/user/apply , or use the WorldPop data, which are directly accessible through running the script in the repository. Analysis was run in R 3.4.4 using packages raster (2.8–19), ncdf4 (1.16), RnetCDF (1.9–1), googledrive (0.1.3), data.table ( 1 .11.8), dplyr (0.7.8), plyr (1.8.4), ggplot2 (3.0.0), wbstats (0.2), ggrepel (0.8.0), sf (0.7–1), cowplot (0.9.3), reshape2 (1.4.3), rworldmap (1.3–6), rgdal (1.3–6), tidyr (0.8.3) and RColorBrewer (1.1–2). Furthermore, the checkpoint package (0.5.0) is used to provide a snapshot of and retrieve the required version of the packages and their dependencies. The average processing time for the Google Earth Engine Javascript code (to run and export assets to Google Drive) is about 10 minutes, while the R script takes around 10 minutes to install the required packages and dependencies, and an additional 10 minutes to generate the netCDF files and the figures and save them (although this depends on the specifications of the hardware used). Note that the running time of the R script is also sensitive to the internet connection speed, since the script retrieves the assets from Google Drive. The processing of satellite data and geospatial operations are performed at a 1000 m scale under a EPSG 4326 coordinate reference system."
10.1038/s41597-019-0049-y,"All code used to generate the insect egg dataset as well as reproduce the tables and plots shown here is made freely available. Python code used to compile the dataset and extract text information from text sources, as well as the R code used to convert the raw dataset to the final dataset and to generate the tables and figures shown here is available at https://github.com/shchurch/Insect_Egg_Evolution . Python code used to measure published images of eggs is available at https://github.com/sdonoughe/Insect_Egg_Image_Parser , and Python code to cross-reference the egg dataset with taxonomic tools is available at https://github.com/brunoasm/TaxReformer . Statistical analyses were performed using R version 3.4.2 33 ."
10.1038/s41597-019-0112-8,"This study did not use any computer codes to generate the dataset. Microsoft Excel was used to enter, store and quality check the collected data."
10.1038/s41597-019-0124-4,The custom Matlab code used to process data (see previous section) is freely available on the following repository: https://github.com/fmoissenet/CGA_Rehazenter_Toolbox/tree/article_ScientificData2019 . The Biomechanics ToolKit (BTK) is also freely available on the following repository: http://biomechanical-toolkit.github.io/ .
10.1038/s41597-019-0121-7,The QM9-G4MP2 database contains raw outputs and scripts to parse the data addressed in this paper. All scripts are released with the BSD license. Other details on the scripts are discussed in the Usage Notes.
10.1038/s41597-019-0116-4,"All the required code to replicate the feature characterization of GM12878 or GM18502 and the mixture, as well as all figures included in this document, are available in a public repository on GitHub at https://github.com/cailab-tamu/sciData-LCL ."
10.1038/s41597-019-0118-2,Custom code used for the processing of the described data can be accessed at figshare 34 .
10.1038/s41597-019-0123-5,MISSING
10.1038/s41597-019-0110-x,"The execution of this work involved many software tools, whose versions, settings and parameters are described below. (1) SOAPdenovo: version 3.0, default parameters; (2) FALCON: version 3.1, length_cutoff_pr = 5000, max_diff = 120, max_cov = 130; (3) HiCUP: version 0.5.10, (4) HiRise: Dovetail Genomics LLC, Santa Cruz, CA, USA; (5) BWA: version 0.7.8, default parameters; (6) Tandem Repeat Finder: version 409, default parameters; (7) RepeatMasker: version 4.0.5, default parameters; (8) Repbase: version 15.02; (9) RepeatModeler: version 1.0.11, default parameters; (10) RepeatScout: version 1.0.5, default parameters; (11) PILER: version 1.06, default parameters; (12) LTR_FINDER: version 1.0.7, default parameters; (13) Augustus: version 3.0.2, default parameters; (14) GENSCAN: version 1.0, default parameters; (15) geneid: version 1.4, default parameters; (16) GlimmerHMM: version 3.0.2, default parameters; (17) SNAP: version 11-29-2013; (18) BLAST: version 2.2.26, default parameters; (19) GeneWise: version 2.2.0, default parameters; (20) TopHat: version 2.0.8, default parameters; (21) Cufflinks: version 2.1.1, default parameters; (22) Trinity: version 2.4.0, default parameters; (23) PASA: version 2.3.3, default parameters; (24) EVidenceModeler: version 1.1.1, default parameters; (25) InterPro: version 5.16, default parameters; (26) Pfam database: version 03-30-2016; (27) InterProScan: version 4.8, default parameters; (28) NR database: version 08-10-2015; (29) KEGG database: version 08-31-2015; (30) SwissProt database: version 05-24-2016; (31) HMMER: version 3.1b1, default parameters; (32) tRNAscan-SE: version 1.3.1, default parameters; (33) BUSCO : version 3.0.2, Embryophyta Version odb9; (34) CEGMA: version 2.5."
10.1038/s41597-019-0119-1,The EddyPro software used to generate this dataset is publicly and freely available from the developers ( https://www.licor.com/env/support/EddyPro/home.html ).
10.1038/s41597-019-0117-3,"All the code used to process the genomic data is freely available as a part of the provided Zenodo repository 17 and the code is located in the folder named scripts . The scripts folder also contains dependencies listed in the file used_packages_and_their_versions.tsv and the used parameter values listed in used_parameters.tsv. Depending on the scripts’ functionality, they are separated into various folders: The folder rnaseq-subtype-classification contains scripts used for read mapping, gene expression quantification, and profile classification. The folder kraken/human-unmapped contains scripts to assign reads to bacterial taxa. The folder kraken/diff-expr-taxa contains scripts for differential analysis of bacterial species in CMS. The folder 16S-metabarcoding contains scripts for metabarcoding data analysis."
10.1038/s41597-019-0120-8,No specific codes were used to produce the data presented.
10.1038/s41597-019-0125-3,"We present custom codes with the MATLAB to determine ( G , h , I ITL ) from an individual temperature profile (see ‘Supplementary Material’). The main program is ‘ThermoclineMLD.m’, which contains four Matlab functions: ‘validata.m’ to filter out bad profiles, ‘getgradient.m’ to calculate the vertical gradients between z (0.1) and z (0.7) , ‘ELGCore.m’ to calculate the ELGs from z 1 to z (0.7) , and “Iindex.m’ to calculate the Identification index ( I ITL ) for the technical validation (see the Technical Validation Section). The code ‘getgradient.m’ can use temperature or potential density profiles to obtain the ITL and thermocline gradient or mixed layer and pycnocline gradient, but only the work with temperature profiles will be explained in this data descriptor. Interested readers may use our MATLAB codes to analyse the WOD 2013–14 CTD temperature profiles and to get the derived dataset ( G , h , SST , T m , H ITL , Q -Index, I -index)."
10.1038/s41597-019-0126-2,No custom code was created for the generation of the Seamap Australia V1 dataset.
10.1038/s41597-019-0134-2,MISSING
10.1038/s41597-019-0127-1,"All the bioinformatics tools/packages used in building of tea plant genome are described below along with their versions, settings and parameters. ( 1 ) Jellyfish : version 2.2.6, k-mer size of 23; ( 2 ) RS_Subreads protocol : minimum subread length (equal or larger than 2 kb); minimum polymerase read quality (equal or larger than 0.8); ( 3 ) Platanus : version 1.24, default parameters, minimum number of link was 4 for scaffolding; ( 4 ) Krskgf : version 1.2, default parameters; ( 5 ) GapCloser : version 1.12, default parameters; ( 6 ) Pbjelly : version 15.8.24, default parameters; ( 7 ) Haplomerger : version 20161205, default parameters, “–step = 20” to speed up the LASTZ process; “minOverlap = 99,999,999” to avoid mixing up haplotypes; ( 8 ) Augustus : version 3.0.3, parameters: –gff3 = on, –species = teaplant, –genemodel = partial, –UTR = off, –uniqueGeneId = true; ( 9 ) Blastall : version 2.2.26, parameters: -e 1e-5, -F F; ( 10 ) GeneWise : version 2.2.0, parameters: -genesf -quiet –pseudo; ( 11 ) PASA : version 2.0.2, parameters: -C; “-R” to run alignment/assembly pipeline; “-T” to indicate that transcripts were trimmed using the TGI seqclean tool; ( 12 ) StringTie : version 1.3.1b, default parameters; ( 13 ) MAKER : version 2.31.8, parameters: organism_type = eukaryotic, min_contig = 150, min_protein = 50, alt_splice = 1, correct_est_fusion = 1; ( 14 ) InterProScan : version 5.3–46.0, parameters:–disable-precalc,–goterms,–pathways; ( 15 ) RepeatModeler : version 1.0.11, parameters: -database teaplant, -e ncbi, -pa 30; 16 ) LTR_Finder : version 1.07, default parameters; ( 17 ) Repeatmasker : version 4.0.5, parameters: -e ncbi, -pa 30; ( 18 ) RepeatProteinMask : version 4.0.5, default parameters; ( 19 ) Tandem Repeats Finder : version 4.04, default parameters; ( 20 ) BUSCO : version 2.0, default parameters, the lineage dataset is: embryophyta_odb9 (Creation date: 2016-11-01, number of species: 30, number of BUSCOs: 1440); ( 21 ) Primer premier : version 5.0."
10.1038/s41597-019-0129-z,"Custom code written in MATLAB for the generation of the macaque connectome, including wrapper scripts for neuroimaging preprocessing and DWI tractography, is available upon request from the corresponding author. The C code for the Reduced Wong-Wang model in TVB is available at https://github.com/BrainModes/TVB_C ."
10.1038/s41597-019-0130-6,All tools used in this study were properly cited in the sections above. Settings and parameters were also clearly described.
10.1038/s41597-019-0128-0,"The data are published as raw, without modification or recoding, as downloaded from the Google App Engine Servers, with the exception of GPS coordinates (see Usage Notes). Data from all unique phone setups and different frequencies were concatenated in single tables, and in some cases variables have been relabeled for clarity and consistency. The Matlab routines used to develop the initial smartphone setups and the summary information of the phone setups are included as Supplementary Files 1 and 2 to this publication."
10.1038/s41597-019-0135-1,MISSING
10.1038/s41597-019-0133-3,"1. GBS demultiplexing https://github.com/timflutre 2. Filters FASTQ files with CASAVA 2.20 fastq_illumina_filter –keep N -v -v -o good_reads.fq raw_reads.fastq 3. Cutadapt (regular 3′ adapter) https://cutadapt.readthedocs.io/en/stable/guide.html cutadapt -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT -G CTCGGCATTCCTGCTGAACCGCTCTTCCGATCT -g ACACTCTTTCCCTACACGACGCTCTTCCGATCT -u7 -U7 -m10 4. Burrows-Wheeler Alignment – BWA-MEM http://bio-bwa.sourceforge.net/bwa.shtml bwa mem ref.fa read1.fastq.gz read2.fastq.gz > aligned.reads.sam with these options: -M Mark shorter split hits as secondary (for Picard compatibility) -R Complete read group header line with ‘\t’ used in STR to be converted to a TEB in the output SAM. An example is ‘@RG\tID:\tSM:\tPL:\tLB:’ 5. Picard tools https://broadinstitute.github.io/picard/ SortSam : java –jar picard.jar SortSam with these options: INPUT (BAM file), OUTPUT (BAM file), SORT_ORDER MarkDuplicates : java –jar picard.jar MarkDuplicates with these options: INPUT (BAM file), OUTPUT (BAM file), METRIC_FILE (file) BuildBamIndex : java –jar picard.jar BuildBamIndex with these options: INPUT (BAM file) 6. GATK tools HaplotypeCaller : java –jar GenomeAnalysisTK.jar –T HaplotypeCaller –R ref.fasta –I file.bam –genotyping_mode DISCOVERY –drf DuplicateRead –emitRefConfidence GVCF –o file.g.vcf https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php CombineGVCFs : java –jar GenomeAnalysisTK.jar –T CombineGVCFs –R ref.fasta –drf DuplicateRead –G Standard –G AS_Standard –variant sample1 to sample‘n’.g.vcf –o cohort_file.g.vcf https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_variantutils_CombineGVCFs.php GenotypeGVCFs : java –jar GenomeAnalysisTK.jar –T GenotypeGVCFs –R ref.fasta –drf DuplicateRead –G Standard –G AS_Standard –variant cohort_file.g.vcf –o final_file.vcf https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_variantutils_GenotypeGVCFs.php SelectVariants : java –jar GenomeAnalysisTK.jar –T SelectVariants –R ref.fasta –V final_file.vcf –selectType SNP –o file_snps.vcf https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_variantutils_SelectVariants.php VariantFiltration : java –jar GenomeAnalysisTK –T VariantFiltration –R ref.fasta –V file_snps.vcf–filterExpression « QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < −12.5 || ReadPosRankSum < −8.0 » –filteredName «FILTER » -o filtered_snps.vcf https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_filters_VariantFiltration.php 7. VCF filtering vcftools –vcf filtered_snps.vcf –remove-filtered-all –recode –out filteredFinal_snps.vcf 8. Falcon and Falcon_Unzip Assembly for SMRT sequencing https://github.com/PacificBiosciences/FALCON/wiki https://github.com/PacificBiosciences/FALCON_unzip/wiki Main parameters: length_cutoff = 5000, length_cutoff_pr = 5000 pa_HPCdaligner_option = -v -dal128 -e0.70 -M40 -l2500 -k17 -h500 -w7 -s100 ovlp_HPCdaligner_option = -v -dal128 -M40 -k19 -h500 -e.96 -l1500 -s100 pa_DBsplit_option = -a -x500 -s200 ovlp_DBsplit_option = -s200 falcon_sense_option = –output_multi –output_dformat –min_idt 0.80 –min_cov 4 max_n_read 400 –n_core 16 falcon_sense_skip_contained = False overlap_filtering_setting = –max_diff 120 –max_cov 120 –min_cov 4 –n_core 24 9. Purge Haplotigs https://bitbucket.org/mroachawri/purge_haplotigs/src/master/ purge_haplotigs readhist -b aligned.bam -g genome.fasta 10. Supernova Assembly for 10x Chromium sequencing https://support.10xgenomics.com/de-novo-assembly/software/overview/latest/welcome Option pseudohap2 style output 11. Scaffolding Falcon assembly with LINKS using Supernova outputs Assembly https://github.com/bcgsc/LINKS LINKS -f.fa -s fileofname.fofn -b cns1-linked_draft -d 5000 -t 100 -k 19 -l 5 -a 0.3 LINKS -f.fa -s fileofname.fofn -b cns2-linked_draft -d 6000 -t 80 -k 19 -l 15 -a 0.3 LINKS -f.fa -s fileofname.fofn -b cns3-linked_draft -d 7000 -t 60 -k 19 -l 20 -a 0.3 LINKS -f.fa -s fileofname.fofn -b cns4-linked_draft -d 10000 -t 30 -k 19 -l 20 -a 0.3 LINKS -f.fa -s fileofname.fofn -b cns5-linked_draft -d 15000 -t 30 -k 19 -l 20 -a 0.3 LINKS -f.fa -s fileofname.fofn -b cns6-linked_draft -d 50000 -t 30 -k 19 -l 30 -a 0.3 LINKS -f.fa -s fileofname,fofn -b cns7-linked_draft -d 75000 -t 30 -k 19 -l 40 -a 0.3 12. Improving quality with PILON and Illumina sequencing https://github.com/broadinstitute/pilon/wiki/Requirements-&-Usage 13. Allmaps pseudomolecules scaffolding https://github.com/tanghaibao/jcvi/wiki/ALLMAPS 14. Assembly evaluation with BUSCO v3 https://busco.ezlab.org/ 15. Vitis TE(s) Identification using RepeatMasker http://www.repeatmasker.org/ 16. Annotation with MAKER_P pipeline, SNAP and Augustus gene finder http://www.yandell-lab.org/publications/pdf/maker_current_protocols.pdf https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-5-59 https://github.com/Gaius-Augustus/Augustus DB (vitis) AND “Vitis”[porgn] from https://www.ncbi.nlm.nih.gov EST DB (vitis) AND “Vitis”[porgn] from https://www.ncbi.nlm.nih.gov - First run : rm_pass = 0, est2genome = 1 and protein2genome = 1 gff3_merge -d master_datastore_index.log maker2zff -c 0 -e 0 -o 0 -x 0.05 maker1.gff fathom -categorize 1000 genome.ann genome.dna fathom -export 1000 -plus uni.ann uni.dna forge export.ann export.dna hmm-assembler.pl RGM. >snap1.hmm - Second run: rm_pass = 1, est2genome = 0, protein2genome = 0, maker_gff = maker1.gff, snaphmm = snap1.hmm leading to a maker2.gff3 and a snap2.hmm files. gff3_merge -d master_datastore_index.log maker2zff -c 0 -e 0 -o 0 -x 0.05 maker2.gff fathom -categorize 1000 genome.ann genome.dna fathom -export 1000 -plus uni.ann uni.dna forge export.ann export.dna hmm-assembler.pl RGM. >snap2.hmm Run Augustus: zff2gff3.pl genome.ann | perl -plne ‘s/\t(\S+)$/\t\.\t$1/’ > genome.gff3 autoAug.pl –genome = ../pilon2.fasta –species = RGM18 –cdna = sequence_est_ncbi.fasta –trainingset = genome.gff3 –singleCPU –v –useexisting - Third run : rm_pass = 1, est2genome = 0, protein2genome = 0, maker_gff = maker2.gff, snaphmm = snap2.hmm, augustus_species = RGM18 leading to a maker3.gff3, maker3.transcripts.fasta and maker3.proteins.fasta structural prediction. gff3_merge -d master_datastore_index.log fasta_merge -d master_datastore_index.log 17. Interproscan functional annotation and putative gene function assignation Download protein DB from http://www.uniprot.org makeblastdb -in protein_db.fasta -input_type fasta -dbtype prot blastp -db protein_db.fasta -query maker3.proteins.fasta -out maker3.proteins.blastp -evalue 0.000001 -outfmt 6 -max_hsps 1 maker_functional_gff protein_db.fasta maker3.proteins.blastp maker3.gff3 ≫ maker3.putative.gff3 maker_functional_fasta protein_db.fasta maker3.proteins.blastp maker3.proteins.fasta ≫ maker3.putative.proteins.fa maker_functional_fasta protein_db.fasta maker3.proteins.blastp maker3.transcripts.fasta ≫ maker3.putative.transcripts.fa Run Interproscan interproscan.sh –iprlookup –goterms -f tsv -i maker3.putative.proteins.fa -pa -b RGM.annotated.proteins 18. Assembly validation using WBA mem bwa mem -M -t 20 VitRiparia.fasta reads_pe.R1.fastq reads_pe.R2.fastq > aln_pe_reads.sam samtools view -bS aln_pe_reads.sam -o aln_pe_reads.bam #| samtools sort - aln_reads.sorted.bam samtools sort -o aln_pe_reads.sorted.bam aln_pe_reads.bam bamtools stats -in aln_pe_reads.sorted.bam > bamstat_pe.reads"
10.1038/s41597-019-0111-9,The following software were used for data analysis: 1. Fastp was used for preprocessing for FastQ files. https://github.com/OpenGene/fastp . 2. FastQC was used for quality control. http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . 3. Trinity 2.4.0 was used to de novo transcriptome assembly. https://github.com/trinityrnaseq/trinityrnaseq . 4. Transrate v1.0.3 and BUSCO v3 were used for assessing assembly quality. http://hibberdlab.com/transrate/ and https://busco.ezlab.org/ . 5. Blast2GO was used for GO annotation. http://www.blast2go.com/b2ghome . 6. KEGG database was used for metabolic pathway annotation. http://www.genome.jp/kegg/ . 7. SAMtools was used for detecting SNPs. https://samtools.github.io/bcftools/howtos/variant-calling.html . 8. MISA was used to identify SSRs. http://pgrc.ipk-gatersleben.de/misa/misa.html . 9. PlantTFDB 4.0 database was used for predicting transcription factor. http://planttfdb.cbi.pku.edu.cn/prediction.php .
10.1038/s41597-019-0132-4,"Software versions and parameters used are as follows: BFC v r181 MEGAHIT v 1.1.2: –k-list 23, 43, 63, 83, 103, 123 SPAdes v 3.11.1: -m 2000, -k 33, 55, 77, 99, 127 –meta BBDuk v 38.08 for 16S, v 38.06 for 18S BBMap v 37.78 iTagger v 2.2 For 16S iTags: Cutadapt v 1.18: –interleaved -g GTGYCAGCMGCCGCGGTAA -G GGACTACNVGGGTWTCTAAT -m 275 –discard-untrimmed QIIME2 v 2018.6: qiime dada2 denoise-paired\ –p-trunc-len-f 210\ –p-trunc-len-r 181 For 18S itags: Cutadapt v 1.18: –interleaved -g CCAGCASCYGCGGTAATTCC -G ACTTTCGTTCTTGATYRA -m 275 –discard-untrimmed QIIME2 v2018.6: qiime dada2 denoise-paired\ –p-trunc-len-f 259\ –p-trunc-len-r 200"
10.1038/s41597-019-0109-3,"All custom R codes employed in this work have been shared online in a GitHub repository (10.5281/zenodo.2578182) 41 . Two short scripts are available online; “ script_from_long_to_wide.r ” and “ Clayton_analysis_code.r ”. The first one (named “ script_from_long_to_wide.r ”) is a short script designed for loading a genetic dataset (genotype calls) derived from OpenArray technology and transforming it into a handy-format file, which can be further imported into PLINK software. Basically, this script carries out a dataset manipulation and transformation from long to wide format. In order to run the script, users will need an input file derived from OpenArray technology containing information in the long format arranged into three columns (NCBI_SNP_Reference, Sample_ID and Genotype_Call). The second script shared (named “ Clayton_analysis_code.r ”) gathers functions and R commands required for the application of the X-chromosome specific statistical tests developed by Clayton and collaborators 34 , 36 (see section ‘High-Level Analysis: Statistical Analysis’ for further details)."
10.1038/s41597-019-0140-4,Codes used for RNA-seq data processing in the current study are available as supplementary material in Figshare at: https://doi.org/10.6084/m9.figshare.7755152.v3 32 (Codes used for RNA-seq data processing).
10.1038/s41597-019-0131-5,"Sequencing data were processed using SAMtools (version 0.1.19), Picard (version 2.18.9) and GATK (version 3.8-1-0). Clean reads were aligned to human reference genome (hg19) using GSNAP (version 2014-10-22). TraCeR (version 2015-10-21) was used to assemble the TCR sequences of single T cells. All downstream analyses were performed using open source R (version 3.5.0). A series of R package were utilized for data analyses including HTSeqGenie (version 4.8.0) for expression quantification, single-cell consensus clustering (SC3, version 1.7.2) for unsupervised clustering and Rtsne (version 0.13) for dimension reduction. Static visualizations of iSTARTRAC are rendered as Portable Document Format (PDF). Tables are generated with R package DT (version 0.5), which provides R interface to the JavaScript library DataTables and allows for data querying, selection and download. Other R packages used by iSTARTRAC includes ggplot2 (version 3.1.0) for plotting box plots, violin plots and volcano plots, ComplexHeatmap (version 1.18.1) for plotting heatmaps, limma (version 3.36.5) for detecting DEGs, ks (version 1.11.3) for plotting cell densities, Startrac (version 0.1.0) for obtaining indices of STARTRAC, RColorBrewer (version 1.1-2) for colour palettes and org.Hs.eg.db (version 3.6.0) for converting gene names etc. Code for preliminary data processing including size-factor normalization, dimensional reduction and clustering is available on Figshare ( https://doi.org/10.6084/m9.figshare.8204624.v1 ), and code for STARTRAC is available on GitHub ( https://github.com/Japrin/STARTRAC )."
10.1038/s41597-019-0139-x,No specific code were developed in this work. The data analysis were performed according to the manuals and protocols provided by the developer of the corresponding bioinformatics tools.
10.1038/s41597-019-0136-0,MISSING
10.1038/s41597-019-0142-2,"The data assembly was performed using MS Excel (Microsoft, Redmond, USA). The datasets included in this paper were manipulated using ESRI ArcGIS v10.5 and Google Earth V7.3.2.5776. Where maps were available, facility locations were digitized in ArcGIS, attribute information captured and converted to MS Excel."
10.1038/s41597-019-0138-y,"The proprietary VASP-code is primarily used in the DFPT calculations. The processing and modifications of the simulations were implemented using Pymatgen 20 and FireWorks 35 . Pymatgen (Python Materials Genomics) is an open-source Python library under Massachusetts Instrutute of Technology (MIT) license for materials analysis. The workflow shown in Fig. 1 is implemented using FireWorks in Atomate 36 , which stores, executes, and manages calculation workflows and is free to public through Atomate’s Github site under a modified GNU General Public License."
10.1038/s41597-019-0146-y,"The linear mixed models were solved using the restricted maximum likelihood (REML) algorithm as implemented in ASReml-R 34 . All statistical analysis were implemented in R environment (Version 2.15.3) 35 . Example R-scripts for outlier correction (“Outlier.correction.R”) and estimation of BLUEs (“Estimation.of.BLUEs.R”) are included together with the data files 24 in the e!DAL-Plant Genomics and Phenomics Research Data Repository (PGP) 23 . For applying the R-scripts it is necessary to download and save the data files and set the appropriate working directory in the scripts. The example script “Outlier.correction.R” runs for the analysis of TGW in winter wheat and produces the output files “Coefficient.of.variation.TGW.txt”, “Outliers.TGW.txt” and “Data.corrected.TGW.txt”, which correspond to the CV of the regeneration years, the removed outliers and the outlier corrected data, respectively. The script “Estimation.of.BLUEs.R” runs also for TGW in winter wheat based on the outlier corrected data and computes the BLUEs contained in the output “BLUEs.TGW.txt”. Both scripts can easily be adapted for the other traits and spring wheat following the annotations in scripts."
10.1038/s41597-019-0141-3,"All software used in the computational analysis described above was obtained from the Bioconda project 69 using the Conda package manager (https://conda.io) and the pipelines were executed through the Snakemake workflow engine 70 . Conda environment files, Snakemake pipeline files and the outputs of each analysis can be accessed through Open Science Framework 68 ."
10.1038/s41597-019-0143-1,"We did not use any custom coding in the process of producing the PF-AHF dataset. In this study, Microsoft Excel and ArcGIS software were employed to process all the data such as the future population density data and the gridded AHF intensity. ArcGIS software was also used to resample the PF-AHF data and draw the figures."
10.1038/s41597-019-0144-0,"The code was written using R software, R.3.4, to produce the data. The code is available online within Figshare 13 ."
10.1038/s41597-019-0145-z,All code for predicting the MS/MS spectra including model parameters and settings are available via http://sourceforge.net/projects/cfm-id . Additional scripts used to implement the prediction algorithm and query the compiled database are available on GitHub ( https://github.com/USEPA/CFM-ID_generation_of_CompTox_Chemicals_Dashboard_Structures_Paper ).
10.1038/s41597-019-0147-x,MISSING
10.1038/s41597-019-0151-1,The reproducibility of the curation algorithm can be verified by executing the provided scripts on Code Ocean 25 . The code has been developed and tested using Python 3.5 on Linux operating system and is available under the MIT license. The RDKit cheminformatics software is freely available under the BSD licence ( http://www.rdkit.org ). ALOGPS 2.1 used for reference value generation is freely available online ( http://www.vcclab.org/lab/alogps/ ).
10.1038/s41597-019-0150-2,"The MOD-LSP parameter sets were created via Python scripts (using the xarray package) 45 archived on GitHub 46 ( https://github.com/tbohn/VIC_Landcover_MODIS_NLCD_INEGI/releases/tag/v1.6 ). The NLCD_INEGI harmonized US-Mexico land cover classifications are available for download at Zenodo 47 , with scripts archived on GitHub 30 ( https://github.com/tbohn/NLCD_INEGI/releases/tag/v1.6 ). The L2015 parameter set was converted from ascii to VIC-5-compliant NetCDF format by the “tonic” tool ( https://github.com/UW-Hydro/tonic/releases/0.2 )."
10.1038/s41597-019-0148-9,The code used to register GPS positions via ICP can be downloaded from MathWorks’ file exchange 25 . The script and function used to load and process the data files can be downloaded from the Zenodo repository ( https://doi.org/10.5281/zenodo.1432702 ) 26 . The codes have been tested using MATLAB 2017 on a typical personal computer and can run using different MATLAB versions and computers.
10.1038/s41597-019-0155-x,The R script used for generating the GCN250 datasets is available for download 15 with instructions for code reuse. The code can be used for generating CNs for future Land cover datasets.
10.1038/s41597-019-0154-y,The code of DRAMP original source and of preprocessing to re-produced the analysis results has been uploaded to GitHub: https://github.com/CPUDRAMP/DRAMP2.0 .
10.1038/s41597-019-0156-9,"Source code and Docker distribution is freely available for download at https://github.com/dsi-icl/PlatformTM , implemented in C#, Javascript, MariaDB, MongoDB and supported on all platforms. Public demo instance available at: platformtm.cloud/app."
10.1038/s41597-019-0152-0,"Data integration and analysis were performed using R language for statistical computing 95 version 3.0.2 (2013-05-16), R version 3.3.1 (2016-06-21), R version 3.4.2 (2017-09-18). The case study was built using Python 3.6. The project repository is accessible via GitHub HENA repository and accompanied by DOI 116 , 117 ."
10.1038/s41597-019-0149-8,"A full description of our analysis pipeline, describing all of the programs and parameters used, is openly available at https://github.com/irobii/cmt . The Markdown file in the pipeline folder documents each step of the pipeline, as well as provides external links to relevant sources for further information."
10.1038/s41597-019-0137-z,"The custom code used to generate the surrogate network via the method outlined in this text was run on MATLAB version R2017b. The script and the required inputs can be accessed on the online repository 27 , along with usage notes and descriptions of relevant parameters."
10.1038/s41597-019-0157-8,"As mentioned above, three different software packages were utilized in this work. Psi4 v1.1 34 is freely available from its official website 38 Q-Chem v5.0 35 and FHI-AIMS 32 must be downloaded from their official sites 39 , 40 with a signed license."
10.1038/s41597-019-0159-6,"In addition to the description of our method here, we also provide code and instructions for reproducing the presented results and extending the developed model in Julia programming language 18 . In a first set of files we provide the contiguous programs that we use to generate the presented results and their validation; these are the files that end with “.jl”. They can be used to reproduce the generated results. In a second set of files that are written as Jupyter Notebook instruction we provide step by step explanations on how our model and the validation metric work. These files can be used to customize our method for individual modeling purposes and to better understand the modeling and validation steps; they end with “.ipynb”. The presented results are generated without indiviually performed model selections; they are produced with model parameters of e drive = 0.5, e dest = 2, p min = 0.1 and p max = 0.9 that are found to be good parameters for the validated cities. The results are further sampled with a total vehicle fleet size of 1,000 cars per city zone; this allows us to exploit the law of large numbers and converge towards realistic distributions of cars while keeping the calculation within the range of feasible computational time with moderate computational power."
10.1038/s41597-019-0162-y,MISSING
10.1038/s41597-019-0164-9,"The Google Earth Engine (GEE) 14 code and the shapefile “samplePlotsShapefile” (extensions.cpg, .dbf, .prj, .shp, .shx), used for computing the NDVI statistics are part of the data and files uploaded together with Globe-LFMC into figshare 12 . This code can only be run if the user has access to a Google account and to GEE."
10.1038/s41597-019-0163-x,MISSING
10.1038/s41597-019-0158-7,"The Matlab code used to generate this dataset is available from the corresponding authors upon request. The repository contains 26 different scripts due to the site dependent characteristics. These scripts read the WetNeph and DryNeph raw files as well as the T /RH sensor files. The sample RH is established for each particular case as explained in this text, and flags for size cut and valid/invalid measurements are also set taking into consideration each particular case. The user can find detailed explanations of the various corrections applied to the raw measurements in the literature as cited in this paper. The codes apply the corrections to the raw measurements, obtaining the resulting corrected quantities for σ sp (RH dry ), σ bsp (RH dry ), σ sp (RH wet ) and σ bsp (RH wet ). The user can use this dataset to apply their own methodology for obtaining f (RH) and/or applying empirical regressions to the measured humidograms."
10.1038/s41597-019-0160-0,"The following software version/script were used in the current manuscript. The RStudio software packages are available open-source from the repository at https://www.rstudio.com/ . SOAPdenovo-Trans (v.1.03) 12 . Trimmomatic (v.0.36) 13 . CD-HIT-EST (v.4.6.1) 14 . BlastP (v.2.2.30). The DESeq. 2 scripts were used for plotting the differential expression data. https://bioconductor.org/packages/release/bioc/vignettes/DESeq . 2/inst/doc/DESeq. 2.html. As an input we have used- (1) a table having RAW read counts and (2) metadata, that is, each line contains description of one of the samples. See example below: #SampleName Condition. C1_raw_read_count control. D1_raw_read_count tolerant."
10.1038/s41597-019-0161-z,"Software used for read preprocessing, genome and transcriptome assembly and annotation is described in the Methods section together with the versions used. Custom MATLAB code used for orthology analysis is deposited on figshare 31 ."
10.1038/s41597-019-0167-6,"Code is written in MATLAB (R2018b). Although MATLAB is a proprietary language, the.m files can be read with a text viewer."
10.1038/s41597-019-0166-7,MISSING
10.1038/s41597-019-0168-5,"We uploaded a selection of Python scripts to the public repository of the project . First, the read data main . py is in charge of taking a raw ZED recording file SVO and saving the left and right color images, and the corresponding depth map. It also saves the manifest JSON and the log files. This script is largely based on the ZED sample script and needs GPU capabilities to compute the depth maps. It also needs the ZED API v2.3.3. Then, the data . py is a sample script that takes a manifest JSON file and loads the sequence into python variables. We used the script named gps . py to connect to the GPS device. This script queries the current geolocalization in an infinite loop. When closed, it saves the data to a JSON file. Next, the script gps2csv . py takes a JSON file generated by the previous script and dumps the data to a comma-separated text file. This file is intended to be uploaded to a GPS Visualizer . in order to visually inspect the accuracy of the GPS records."
10.1038/s41597-019-0172-9,MISSING
10.1038/s41597-019-0170-y,"The guidelines and example codes provided in the data description present good practices for preprocessing and analysing spectroscopic (NIR and FTIR) and biomechanical signal. The NIR spectroscopic data is validated through multivariate modelling ( i . e ., partial least squares regression) and cross-validation. In addition, the means of calculating the amide (collagen) and proteoglycan images from FTIR images are presented. The custom codes presented were written using MATLAB R2015–2017 (Mathworks Inc., Natick, MA, USA). Examples codes are available in a public repository ( https://github.com/uef-bbc/sarin-scientific-data-2019 )."
10.1038/s41597-019-0173-8,The Undatable software was used to create age-depth models based on the age-depth constraints given in the “udinput” text files for each core. The software and accompanying source code can be downloaded from the Zenodo public archive ( https://doi.org/10.5281/zenodo.2527642 ).
10.1038/s41597-019-0174-7,"Our CWL workflows, a tutorial describing the installation and execution procedures for our workflows, and the code for our biomarker discovery analysis can be found on our Code Ocean capsule ( https://doi.org/10.24433/CO.7378111.v3 ) 29 ."
10.1038/s41597-019-0175-6,The custom Matlab code used to calculate joint angles is freely available on Zenodo 28 .
10.1038/s41597-019-0178-3,"The reported data were generated form experiments, and not relevant to any computer codes."
10.1038/s41597-019-0177-4,BioBB’s source code is available at GitHub. URLs for the code and documentation repositories and the alternative installation and execution options are summarized in Online-only Table 1 .
10.1038/s41597-019-0169-4,"All data elaborated from original sources and newly produced in this work were the result of custom-built codes written in Mathworks Matlab2018a on Windows PC. These consist of several independent and dependent scripts and functions to read and reorganize variables, perform calculations, and save intermediate and final data. Custom scripts were also developed to generate .PNG images, georeferenced .TIF maps, and georeferenced NetCDF4 .NC maps of both application rates and data quality. Due to the complexity and size of source data, which use several storage formats and occupy approximately 43 GB, scripts are not directly distributed but are available along with all source data upon request. The only custom code that is distributed with PEST-CHEMGRIDS is the script written in Matlab2018a to read any of the two data storage formats and redraw figures of application rates and their quality indices."
10.1038/s41597-019-0179-2,"The following software and versions were used for quality control and data analysis: 1. For protein identification from raw MS data, Proteome Discoverer software (Thermo) version 2.1 was used: https://tools.thermofisher.com/ with the Mascot 2.6.1 search engine. 2. For quantitative data analysis, MaxQuant software version 1.6.2.10 with the Andromeda search engine was used: http://www.coxdocs.org/docu.php?id=maxquant:start 3. For statistical proteomics analysis, Perseus software version 1.6.1.3 was used: ( http://www.perseus-framework.org ) 4. The UniProt human database was used (September 2018) as database for protein searches: https://www.uniprot.org/ 5. Functional analysis was performed with IPA software version 2.4: http://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis 6. Complexes analysis was performed using g:Profiler: https://biit.cs.ut.ee/gprofiler/gost 7. Statistical analyses were performed using R 3.5.2: www.r-project.org 8. The R script and the data used to create volcano plot are available at the Github repository https://github.com/LabMedMolGE/VolcanoPlot ."
10.1038/s41597-019-0184-5,Code for the FAIR Evaluator and each compliance test is available in GitHub ( https://github.com/FAIRMetrics/Metrics/tree/master/MetricsEvaluatorCode ). The public interface to the prototype Evaluator software is available for anyone wishing to execute evaluations ( https://w3id.org/AmIFAIR ). Detailed instructions regarding how to create and submit proposals for new MIs are available in our online documentation ( https://github.com/FAIRMetrics/Metrics ).
10.1038/s41597-019-0183-6,"Source code, parameters for standard tools, and genome reference information is accessible via a central repository on Synapse at ( http://CommonMind.org )."
10.1038/s41597-019-0190-7,"All analyses were performed using open source software tools, and the detailed parameters for each tool are shown in the relevant methods."
10.1038/s41597-019-0182-7,"The script used to compile pollen records, run Bacon, and select the best age estimations for each pollen record is available in Github repository https://github.com/yuewangpaleo/BaconAgeNeotoma . The script “BaconAgeCode.R” was run using R 27 . Necessary packages and repository for this code include: bulk-baconizing 32 , rbacon 31 , neotoma 26 , and Bchron 37 ."
10.1038/s41597-019-0187-2,MISSING
10.1038/s41597-019-0185-4,Scripts used in the RNA sequencing analyses are available at https://github.com/icnn/RNAseq-PIPELINE.git .
10.1038/s41597-019-0189-0,"The original data collection was done by entering trait information into an Excel spreadsheet (Microsoft Office 2013). No code is available for this step. The final PalmTraits 1.0 dataset was saved as tab-delimited text file 181 . Scripts to load the PalmTraits 1.0 dataset into R, to plot multi-variate trait variation and to combine it with phylogenetic and species distribution data are available in the accompanying dataset 181 . The scripts were developed in R version 3.5.0, and using the associated libraries as indicated in the scripts. There are no restrictions to use the provided code."
10.1038/s41597-019-0188-1,"Softwares used to store miniSEED files in EIDA and to create the DATALESS volumes are only partially established in the seismological community. For the purpose of storing in EIDA structure, we used a home-made procedure (written as bash shell script) based on the program qmerge ( http://www.ncedc.org/qug/software/ucb/ ). The procedure repackages the miniSEED files coming from the digitizers (or after a conversion from the raw proprietary format) into daily files organized as explained above (Supplementary Fig. 4 ), and it allows the user to update the miniSEED file header according to the parameters defined in the configuration file (e.g, the network and station code, the channel fields); this is necessary because some digitizers do not set automatically the values of these fields. The Incorporated Research Institutions for Seismology (IRIS, https://www.iris.edu/hq/ ) provides the graphical program pdcc ( https://ds.iris.edu/ds/nodes/dmc/software/downloads/pdcc/ ) to create the DATALESS files. This code allows to view, create and edit SEED metadata files by means of handy tools. The catalog from which the program retrieves the data concerning the instrument responses is the Nominal Response Library (NRL) maintained by IRIS 43 , whereas the coordinates, description and all the information regarding the stations must be entered by the user in appropriate text fields. Nevertheless, because the seismic instrumentation used by the network 3A is not entirely present in the IRIS NRL or some specific information have to be retrieved from the datasheets of the manufacturers, we prefer to use a home-made procedure (written as bash shell script) to create the DATALESS files. The use of a command-line code was very convenient, given the large number of stations. The procedure retrieves the information about the instrumentation (sensor responses given in terms of poles and zeros, filters of the digitizers, and so on) from text files organized in a kind of local catalog, and the parameters of the stations (coordinates, start and end dates, etc.) from a database that collects them. However, pdcc was useful for the technical validation of the obtained DATALESS files."
10.1038/s41597-019-0191-6,The specific commands used to analyse RNA-seq data and to draw the article figures are available at https://github.com/yuragal/fradians-rnaseq .
10.1038/s41597-019-0192-5,"Filtering before assembly was performed with an in-house Pythonv2.7 script, which is freely available ( https://github.com/YannickCogne/Qfiltering ). The script was automated with a bash script for each sample."
10.1038/s41597-019-0195-2,"The versions, settings and parameters of the software used in this work are as follows: Genome assembly: ( 1) Falcon: version 1.8.2; all parameters were set as default; ( 2) Quiver: version: 2.1.0; parameters: all parameters were set as default; ( 3) pilon : version:1.22; all parameters were set as default; ( 4) LACHESIS : parameters: RE_SITE_SEQ = AAGCTT, USE_REFERENCE = 0, DO_CLUSTERING = 1, DO_ORDERING = 1, DO_REPORTING = 1, CLUSTER_N = 24, CLUSTER_MIN_RE_SITES = 300, CLUSTER_MAX_LINK_DENSITY = 4, CLUSTER_NONINFORMATIVE_RATIO = 10, REPORT_EXCLUDED_GROUPS = −1; Genome annotation: ( 1) RepeatProteinMask : parameters: -noLowSimple -pvalue 0.0001 -engine wublast. ( 2) RepeatMasker : version: open-4.0.7; parameters: -a -nolow -no_is -norna -parallel 1. (3) LTR_FINDER : version:1.05; parameters: -C -w 2. ( 4) RepeatModeler : version: open-1.0.10; parameters:-database genome -engine ncbi -pa 15. ( 5) RepeatScout : version: 1.0.5; parameters: all parameters were set as default. ( 6) TRF : matching weight = 2, mismatching penalty = 7, INDEL penalty = 7, match probability = 80, INDEL probability = 10, minimum alignment score to report = 50, maximum period size to report = 2000, -d –h. ( 7) Augustus : version:3.1.2; parameters:–extrinsicCfgFile–uniqueGeneId = true–noInFrameStop = true–gff3 = on–genemodel = complete–strand = both. ( 8) GlimmerHMM : version:3.0.3; parameters: -f –g. ( 9) Genscan : -cds. ( 10) Geneid : version: 1.2; parameters: -P -v -G -p geneid. ( 11) Genewise : version: 2.4.0; parameters: -trev -genesf -gff –sum. ( 12) BLAST : version 2.7.1; parameters: -p tblastn -e 1e-05 -F T -m 8 -d. (13) EVidenceModeler : version: 1.1.1; parameters: G genome.fa -g denovo.gff3 –w weight_file -e transcript.gff3 -p protein.gff3–min_intron_length 20. ( 14) PASA : version: 2.3.3; parameters: all parameters were set as default. Gene family identification and phylogenetic analysis: ( 1) Blastp: parameters: -e 1e-7 -outfmt 6. ( 2) Orthomcl: parameters: all parameters were set as default. ( 3) MUSCLE: version 3.8.31; parameters: all parameters were set as default. ( 4) Gblocks: version: 0.91b; parameters: all parameters were set as default. ( 5) RAxML: version: 8.2.12; parameters: -n sp -m PROTGAMMAAUTO -T 20 -f a. ( 6) MCMCTREE: parameters: all parameters were set as default."
10.1038/s41597-019-0200-9,"The preprocessing and normalisation of the EMG and kinematics data, as well as the synchronisation between them were performed using custom Python code, which can be obtained from the authors upon request. The Python script used for the classification of movements (see the Technical Validation section) can be found in a folder called functions ."
10.1038/s41597-019-0197-0,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-019-0199-y,"All code is implemented in Python and published at https://github.com/oruhnau/when2heat under the open MIT license. The entire processing workflow is documented in a single Jupyter Notebook (processing.ipynb), which draws on custom functions that are structured in different Python scripts (download.py, read.py, preprocess.py, demand.py, cop.py, write.py, metadata.py, misc.py). While the download of weather and population data is automated, the input data from the EU Building Database, BGW 7 , and BDEW 8 , as well as the COP curve parameters are included in the code repository."
10.1038/s41597-019-0194-3,"The execution of this work involved using many software tools. To allow readers to repeat any steps involved in genome assembly and genome annotation, the settings and parameters were provided below: Genome assembly: (1) Falcon : all parameters were set to the defaults; ( 2) quiver : all parameters were set to the defaults; ( 3) pilon : all parameters were set to the defaults; ( 4) LACHESIS : RE_SITE_SEQ = AAGCTT, USE_REFERENCE = 0, DO_CLUSTERING = 1, DO_ORDERING = 1, DO_REPORTING = 1, CLUSTER_N = 24, CLUSTER_MIN_RE_SITES = 300, CLUSTER_MAX_LINK_DENSITY = 4, CLUSTER_NONINFORMATIVE_RATIO = 10, REPORT_EXCLUDED_GROUPS = −1; Genome annotation: (1) RepeatProteinMask : -noLowSimple -pvalue 0.0001 -engine wublast; ( 2) RepeatMasker : -a -nolow -no_is -norna -parallel 1; ( 3) LTR_FINDER : -C -w 2; ( 4) RepeatModeler : -database genome -engine ncbi -pa 15; ( 5) RepeatScout : all parameters were set to the defaults; ( 6) TRF : matching weight = 2, mismatching penalty = 7, INDEL penalty = 7, match probability = 80, INDEL probability = 10, minimum alignment score to report = 50, maximum period size to report = 2000, -d –h; ( 7) Augustus :–extrinsicCfgFile–uniqueGeneId = true–noInFrameStop = true–gff3 = on–genemodel = complete–strand = both; ( 8) GlimmerHMM : -f –g; ( 9) Genscan : -cds; ( 10) Geneid : -P -v -G -p geneid; ( 11) Genewise : -trev -genesf -gff –sum; ( 12) BLAST : -p tblastn -e 1e-05 -F T -m 8 -d; (1 3) EVidenceModeler : G genome.fa -g denovo.gff3 –w weight_file -e transcript.gff3 -p protein.gff3–min_intron_length 20 ( 14) PASA : all parameters were set to the defaults."
10.1038/s41597-019-0204-5,MISSING
10.1038/s41597-019-0208-1,MISSING
10.1038/s41597-019-0212-5,"The scripts used for pre-processing and visualization in Data Records and Usage Notes are available at our github ( https://github.com/ch-shin/ENERTALK-dataset ). Unfortunately, the codes for collecting and storing data are the private property of the enterprise (Encored Inc.) and cannot be opened."
10.1038/s41597-019-0207-2,"The R codes used to preprocess, merge, and correct for batch-effects for generation of all 11 cancer type-specific MMDs can be found in figshare ( https://doi.org/10.6084/m9.figshare.7878086 ) 22 . The exemplary R codes and metadata used to develop clinical predictive models using lung MMD 57 are described in our earlier works 5 , 6 , 58 ."
10.1038/s41597-019-0203-6,"Custom R and Fortran codes used to run the analyses, together with an example dataset are available at https://doi.org/10.6084/m9.figshare.7906739.v4 (ref. 42 )."
10.1038/s41597-019-0209-0,"The LabVIEW-based graphical code for the experiment and data acquisition is highly specific to the sensors and equipment used in our experiment. It has therefore not been made available with this dataset 31 , 32 . Nevertheless, readers who wish to replicate the experiment can contact the corresponding author for further assistance. For readers who want to reproduce the experiment, we hope the detailed description provided in this article will suffice. The data preprocessing steps outlined in the previous subsection were implemented in MATLAB 2014b. The linear interpolation was performed using the interp1 function. The raw log files and the data preprocessing code are available in the dataset 31 , 32 . Hence, readers who wish to reproduce and/or improve-upon our preprocessing pipeline can easily undertake the same."
10.1038/s41597-019-0210-7,MISSING
10.1038/s41597-019-0211-6,"The Matlab code used to detect periods of prosthesis wear/non-wear is included with the dataset on figshare 11 The inputs to this function are: (1) a 4 column matrix containing the epoch data from the monitor worn on the prosthesis (activity counts on the x, y, and z axes and the vector magnitude), (2) a 4 column matrix containing the epoch data from the monitor worn on the anatomically intact wrist (activity counts on the x, y, and z axes and the vector magnitude), and (3) the epoch size in minutes. Within the function, the prosthesis worn monitor data for each epoch is analysed, and the epoch marked as wear or non-wear. Full details on the algorithm used to determine whether each epoch should be categorised as wear/non-wear have been published previously as supplementary material to Chadwell et al . 6 and in the appendices of Chadwell’s thesis 2 . The algorithm involved comparison of each epoch to the previous epoch. If activity was recorded during a prosthesis non-wear period, or the vector magnitude was equal to 0 during a prosthesis wear period the neighbouring epochs were evaluated to determine whether the prosthesis had been put on/removed. Due to the design of the algorithm, it is unlikely that prosthesis removal or wear periods shorter than 20 minutes would be detected. The output from the function is a 10-column matrix containing the timestamp (minutes since recording began), the data from the two 4 column matrices passed into the function initially, and the categorisation (wear = 1, non-wear = 0). The non-wear algorithm was developed for use with the 60 s epoch vector magnitude data exported from Actilife, however the algorithm has the compatibility for use with other epoch sizes (please note the limitations detailed above). Subsequently to the collection of this dataset, we have also used the non-wear algorithm with data collected using Axivity sensors, where the raw data has been converted into ‘Actigraph activity counts’ using the codes developed by Brond et al . 14 . The Matlab code used to produce the spiral plots is not publicly available at this stage, however, by contacting the corresponding author, we would be happy to collaborate to enable you to reproduce your own data. The methods for producing these plots are published in Chadwell et al . 8 . Codes have also been developed to allow data from other sensors to be displayed in this format, for example the data from Axivity sensors has been visualised over a 14-day period following similar methods combined with the code developed by Brond et al . 14 . These spiral plots also have potential for use with other types of data besides upper limb activity data."
10.1038/s41597-019-0196-1,"The BIOMASS R-package is an open source library available from the CRAN R repository. The development version is publicly available and can be found on the GitHub platform at: https://github.com/AMAP-dev/BIOMASS . Furthermore, the BIOMASS R-package is accompanied by an open access paper describing the functionality in more detail 5 ."
10.1038/s41597-019-0221-4,MISSING
10.1038/s41597-019-0215-2,MISSING
10.1038/s41597-019-0224-1,"The scripts utilized to classify paragraphs and extract recipes as well as to perform the data analysis are home-written codes which are publicly available at the github repository https://github.com/CederGroupHub/text-mined-synthesis_public with acknowledgement of the current paper. The underlying machine-learning libraries used in this project are all open-source: Tensorflow ( www.tensorflow.org ), Keras ( keras.io ), SpaCy ( spacy.io ) 42 , gensim ( radimrehurek.com ) 41 and scikit-learn ( scikit-learn.org ) 48 ChemDataExtractor ( chemdataextractor.org ) 22 ."
10.1038/s41597-019-0226-z,Codes that were used for the RNA-seq data processing are available at figshare 7 . Software and their versions were described in Methods.
10.1038/s41597-019-0220-5,"The following software and source codes were used for the presented analysis and described in the main text: 1. MySQL v14.12 was used for analysis of HCUP SIDCA data. https://www.mysql.com/downloads/ 2. An in-house developed Java script was used to trace the diagnosis trajectories of over 10.2 million patients. The source codes are available via GitHub repository for academic researches. https://github.com/hypaik/HCUPSIDCA_trajectory_tracking 3. R package version 3.2, Java v1.8.0 and Python v2.7 were used for the presented statistical analysis. https://www.r-project.org/ https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html https://www.python.org/download/releases/2.7/ 4. Cytoscape, an open source platform, was used for the network analysis of disease trajectories and figure preparations. https://cytoscape.org/ 5. WebGL was used for the dynamic visualization of the presented disease trajectories. https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API 6. All the traced disease trajectories for over 10.4 million patients in California, US was presented via our web resource. http://52.89.56.137:3000/#/intro (Optimized only for Chrome) 7. We also have a website and video explaining these disease trajectories. (Appendix Movie S1 . Dynamic presentation of the traced diagnosis trajectories at scale) https://www.youtube.com/watch?v=jJMds31-e2g 8. Appendix Data S1 . All of traced diagnosis trajectories. ( https://github.com/hypaik/HCUPSIDCA_trajectory_tracking )."
10.1038/s41597-019-0219-y,The dataset was compiled within a spreadsheet and exported in the Comma Separated Value (CSV) format. No additional codes were used.
10.1038/s41597-019-0218-z,MISSING
10.1038/s41597-019-0222-3,MISSING
10.1038/s41597-019-0227-y,"The core LDMAP software is written in C, and made available at www.soton.ac.uk/genomicinformatics/research/ld.page ."
10.1038/s41597-019-0181-8,"We are highly committed to sharing all resources used to produce this data and analysis. Primary distribution of the scripts used in analysis is available through Sage Bionetworks’ Synapse platform through the AMP-AD Knowledge Portal ( https://ampadportal.org ), with links to the data from both ADNI 1 and ADNI GO/2 for Bile Acids in various stages, as well as the R scripts used to process the data, available at the links shown in Online-only Table 1 ."
10.1038/s41597-019-0223-2,"The code we developed to record EEG/EOG and goniometer data, run the paradigm and adapt cue rate online to the participants’ cadence in each trial are based on the TOBI SignalServer freely available online ( tools4bci.github.io/SignalServer) a custom SimulinkModel running in MATLAB (The MathWorks) and a Ruby application for playing the auditory cues which is available upon request from the authors."
10.1038/s41597-019-0217-0,MISSING
10.1038/s41597-019-0201-8,"The following bioinformatic tools and versions were used for generating all results as described in the main text: 1. Bowtie2, version 2.3.4.3, was used for aligning sequencing reads to long reference sequences with default parameters: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml 2. CLC Genomics Workbench, version 3.6.1, was used for genome assembly with default parameters: https://www.qiagenbioinformatics.com/products/clc-genomics-workbench/ 3. Velvet, version 1.2.08, was used for genome de novo assembly, which was previously described: https://www.ebi.ac.uk/~zerbino/velvet/ 4. SSPACE, version 3.0, was used for genome scaffolds assembly with default parameters: https://www.baseclear.com/services/bioinformatics/basetools/sspace-standard/ 5. GapCloser, version 1.12, was used to fill the gaps between scaffolds with default parameters: https://sourceforge.net/projects/soapdenovo2/files/GapCloser/ 6. DOGMA (an online tool), accessed at 12/2018, was used for annotating cp genomes with default parameters: http://dogma.ccbb.utexas.edu/ 7. Mitofy (an online tool), accessed at 12/2018, was used for annotating plant mt genomes with default parameters: http://dogma.ccbb.utexas.edu/mitofy/ 8. tRNAscanSE, VERSION 1.3.1, was used to search tRNA with default parameters: http://lowelab.ucsc.edu/tRNAscan-SE/ 9. Organellar Genome DRAW (an online tool), accessed at 12/2018, was used for creating high quality visual representation of cp gemome with default parameters: https://chlorobox.mpimp-golm.mpg.de/OGDraw.html 10. MISA,version 1.0, was used for annotating SSR with monomer (one nucleotide, n ≥ 8), dimer (two nucleotides, n ≥ 4), trimer (three nucleotides, n ≥ 4), tetramer (four nucleotides, n ≥ 3), pentamer (five nucleotides, n ≥ 3), hexamer (six nucleotides, n ≥ 3): http://pgrc.ipk-gatersleben.de/misa/misa.html 11. REPuter (an online tool), accessed at 1/2019, was used for annotating long repeated sequences with the following parameters: minimal length 50 nt; mis match 3 nt: https://bibiserv.cebitec.uni-bielefeld.de/reputer/ 12. PREP-cp (an online tool), accessed at 1/2019, was used for predicting RNA editor for plant cp genes with the cutoff score (C-value) setting to 0.8: http://prep.unl.edu/ 13. PREP-mt (an online tool), accessed at 1/2019, was used for predicting RNA editor for plant mt genes with the cutoff score (C-value) setting to 0.6: http://prep.unl.edu/ 14. MEGA, version 7.0.26, was used for phylogenomics and phylomedicine at 1000 bootstrap: https://www.megasoftware.net/ 15. ClustalW, version 2, was used for multiple sequence alignment with default parameters: https://www.ebi.ac.uk/Tools/msa/clustalw2/"
10.1038/s41597-019-0237-9,The newest version of database is available on Figshare and GitHub. Figshare https://doi.org/10.6084/m9.figshare.9638093 . GitHub https://github.com/XI-Lab/QM-sym-database .
10.1038/s41597-019-0213-4,"The source code for the PHP script and database schema are available from a public github repository ( https://github.com/LogIN-/fluprint ). Raw data files used to generate dataset are provided as single compressed file on Zenodo 24 . Full study population with demographic characteristics is provided as single CSV file 25 . Additionally, entire FluPRINT database export is available as CSV table and SQL file 35 . Database is also accessible at the project website https://fluprint.com ."
10.1038/s41597-019-0230-3,The code for extracting the three features from a shell can be found here: https://github.com/zqplus/shell-recognition/tree/master/ReadMe_how%20to%20generate%20shell%20features%20%26%20load%20data%20for%20classification .
10.1038/s41597-019-0231-2,The softwares used for data processing are included in the methods and available in the following list: 1. FastQC v0.11.6 was used for quality assessment of FASTQ data: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . 2. MultiQC was used for combining fastqc results into one: https://pypi.python.org/pypi/multiqc . 3. RNA-QC-Chain was used for data preprocessing of raw data: http://bioinfo.single-cell.cn/rna-qc-chain.html . 4. TopHat v2.0.12 was used for clean reads aligned to the reference genome: http://ccb.jhu.edu/software/tophat/downloads/ . 5. Cufflinks v2.1.1 was used for transcript assembly of samples: http://cole-trapnell-lab.github.io/cufflinks/ . 6. HTSeq v0.6.1 was used for counting the reads numbers mapped to each gene: https://htseq.readthedocs.io/en/release_0.11.1/history.html#version-0-6-1 . 7. DESeq package v1.18.0 was used for differential expression analysis of two groups with biological replicates: https://bioconductor.riken.jp/packages/3.0/bioc/html/DESeq.html . 8. Ggplot2 package was used for visualization of a correlation matrix between samples: http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2 .
10.1038/s41597-019-0232-1,MISSING
10.1038/s41597-019-0244-x,"The sequence data were generated using software provided by the sequencing platform manufacturer, and were processed with publicly available software and recommended settings, as cited in this report. No custom computer codes were generated in this work."
10.1038/s41597-019-0235-y,"Python and MATLAB scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on github: https://github.com/cicwi/WalnutReconstructionCodes . They make use of the ASTRA toolbox, which is openly available on www.astra-toolbox.com or accessible as a conda package (use conda install -c astra-toolbox/label/dev astra-toolbox to install the development version). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state very involved and version-dependent. For obtaining a comparable scaling of the image intensities between FDK and iterative reconstructions, it is required to use a development version of the ASTRA toolbox more recent than 1.9.0 dev. For each dataset, a text file containing information about motor positions (source 3D position, detector position and detector orientation) is provided and used by the aforementioned Python/MATLAB scripts to set up the reconstruction geometry. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow to sub-sample the projections and to choose a different image resolution, the reference reconstructions were computed with all projections and within a volume of 501 3 voxels of size 100 μ m 3 as mentioned above."
10.1038/s41597-019-0238-8,"Canu in the genome assembly and BLAT alignment in the gene prediction were utilized with specific parameters, described in Methods. The other bioinformatics tools were run with the default parameters. There were no any custom specific codes."
10.1038/s41597-019-0246-8,The code of the web application (Fig. 1 ) that 7 used to collect the neuroscientists’ inputs is not publicly available.
10.1038/s41597-019-0236-x,For the toolbox SNAP see http://step.esa.int/main/toolboxes/snap/ . Example code to read data products and generate Fig. 5 is available 62 .
10.1038/s41597-019-0243-y,"As mentioned above, some river and WRZ delineation steps can be programmed using serval ArcGIS standard process which was provided in our data set. These processes were packaged into an integrated model to improve the efficiency. This model, deposited in Figshare with our data named “Method-Toolbox.tbx”, contains four functions, “modify the original dem”, “generate the flow direction and accumulation”, “generate the river network” and “generate the WRZ”. Before running this, it needs to install ArcGIS 10.2 or above version in the Windows system. Then, the scripts can be opened by the ArcGIS and the corresponding dialog box will show to add the input and output files path. By finishing all the above steps, it will automatically generate all the output files, such as the “modified DEM”, “river network” and “the WRZ”."
10.1038/s41597-019-0241-0,"The code used in this study is made publicly available on the Maastricht University Clinical Data Science (UM-CDS) GitLab repository ( https://gitlab.com/UM-CDS/distributedradiomics ). The code repository has the following organization: a. D2RQ folder: contains the raw feature value to RDF mapping (D2RQ) script and the SPARQL query used to retrieve the local data into the local VLP connector application. b. VLP folder: contains the MATLAB codes submitted by the user into VLP, which then transmits it to the participating site for model validation and analysis. c. Analysis Centralized Learning folder: contains the Jupyter notebook from Radboudumc for model development and evaluation on the aggregated datasets. The open-access Radiomics Ontology (RO) is published via the National Center for Biomedical Ontology (NCBO) ontology registry. It is available to download in a range of formats from the following URL: https://bioportal.bioontology.org/ontologies/RO . As a domain ontology, the RO defines histogram-based, morphology-based and texture-based radiomic features, including (since v.1.6, 08 November 2018) all feature entities presented in the International Biomarker Standardization Initiative. The ontology also defines software properties, digital imaging filter operations and feature extraction settings, together with relational predicates to link these to each feature entity."
10.1038/s41597-019-0240-1,CD-HIT: http://www.bioinformatics.org/cd-hit/ (version 4.6.6). BUSCO: https://gvolante.riken.jp/index.html (version 2.3).
10.1038/s41597-019-0229-9,MISSING
10.1038/s41597-019-0206-3,"The extracted data from the investigated articles were stored in an SQL database. See the database structure section for details of database tables. A web application was developed as an interface to interact with this dataset to extract the statistics of the saved data. The server-side application was developed using Microsoft SQL Server Express 2016 and ASP.NET MVC 5. The user interface uses the Bootstrap framework in addition to customized JavaScript libraries for plots, tables, and menus. SQL database and web application source code are available at www.cadataset.com and within figshare 51 . While the administrator of the website can modify previous records and add new data to the system, its reports are available to all users. It is possible for users to request new reports to be added. The reports can be exported to CSV format. In addition, the results can be plotted and summarized in a simple figure as well. Our website is compatible with all popular modern web browsers (tested on Mozilla Firefox ver. 63, Microsoft Internet Explorer ver. 11, and Google Chrome ver. 70)."
10.1038/s41597-019-0239-7,The MatLab codes used to generate the LDCmgm90 are available upon request to W. Chen (wchen@sgg.whu.edu.cn).
10.1038/s41597-019-0251-y,"No computer program was used to generate this dataset 42 . Data was collected through in-person paper-based survey and online Qualtrics-based experience management service, both of which translated into consistent CSV format dataset."
10.1038/s41597-019-0257-5,No code was used in this study.
10.1038/s41597-019-0252-x,"Parameters to involved softwares tools are described in the following paragraph. DRAP ( De novo RNA-seq Assembly Pipeline): version 1.7, code available online at http://www.sigenae.org/drap/quick_start.html . BLAST : version 2.2.26, e-value under 1e-5 parameter. RNAmmer : version 1.2, standard parameters. RepeatMasker : version open-4-0-3, -engine crossmatch -gccalc -species Crassostrea gigas parameters. InterProScan : version 4.8, -goterms -pathways parameters. BWA : version 0.7.12, standard parameters, mem algorithm. SAMtools view , sort and index programs : version 1.1, standard parameters. SAMtools IdxStats program : version 1.1, standard parameters. SAMtools rmdup program : version 1.1, standard parameters. GATK : version v3.0-0-g6bad1c6, -glm BOTH parameter. BLAT : version 35 × 1, standard parameters. BUSCO : version 3.0.2 using the metazoa OrthoDB (v. 9), standard parameters."
10.1038/s41597-019-0242-z,The toolbox can be acquired as part of Brainstorm’s GitHub repository: https://github.com/brainstorm-tools/brainstorm3 .
10.1038/s41597-019-0214-3,The SCI and FSII were generated in GEE. The Hansen Global Forest Change v1.5 (2000–2017) data set used to obtain loss year is served by GEE. Tree cover for 2010 was uploaded as a GEE asset from GLAD at the University of Maryland. This was done for each continental portion of the study area using the Python API Earth Engine shell on a parallel processor computer at Northern Arizona University. The same procedure was used for the canopy height data set. Human footprint data were uploaded using the GEE Asset Upload procedure. The SCI weights were generated in GEE by assigning a mask of 0 where conditions are not met and 1 when they are met. An example conditional statement is: var sciImage1 = lossYear.gte(13).and(lossYear.lte(17)).or(forest.lt(25)).or(height.lte(5)). This formulation was then used to multiply the cells with mask = 1 times the appropriate SCI weight. This was done for each combination of input data values for each SCI weight. The resulting maps for each category of SCI weight were combined into a single map using the GEE add command. Similar coding was used for the FSII classification. The resulting SCI and FSII maps for each continent were exported from GEE to Google Drive. GEE code for the classifications are available at Africa: https://code.earthengine.google.com/f46e40ba74164a595f9be5067a7b26cf SE Asia: https://code.earthengine.google.com/e6595f8040af90a7b30a9b89e42c12f6 South America: https://code.earthengine.google.com/285ecd4784d7d4e0bc9a6e6aa966b54c
10.1038/s41597-019-0247-7,The code to reproduce the plots in the paper is available upon request by writing at luca.pappalardo@isti.cnr.it or info@sobigdata.eu.
10.1038/s41597-019-0250-z,"The resulting centroid datasets were created using R version 3.0.2 (R: A Language and Environment for Statistical Computing, https://www.r-project.org/ ) and the rgdal and maptools packages ( https://CRAN.R-project.org/package=rgdal , https://CRAN.R-project.org/package=maptools ). The scripts to generate the data and the data can be found on."
10.1038/s41597-019-0255-7,"The data was processed using R 3.5.2 and RStudio Version 1.1.463. The following packages were used for data processing “tidyr” (0.8.3), “dplyr” (0.8.1), “ggrepel” (0.8.1) and “stats” (3.5.2); and data visualization “RColorBrewer” (1.1-2), “gplots” (3.0.1.1), “ggplot2” (3.2.0), “ComplexHeatmap” 32 (1.20.0), “ggpubr” (0.2) and “gridExtra” (2.3). All code applied for processing and visualising the data is provided at the figshare deposit 25 ."
10.1038/s41597-019-0234-z,"The input data in this work can be accessed at the following website pages: Global CMT catalog, available at http://www.globalcmt.org/ (last accessed on April 2019); ISC bulletin, available at http://www.isc.ac.uk/iscbulletin/ (last accessed on April 2019); ISC-GEM catalog, available at http://www.isc.ac.uk/iscgem/ (last accessed on April 2019); and the USGS catalog, available at http://earthquake.usgs.gov/data/centennial/ (last accessed on April 2019). The SSN data was provided by the Mexican SSN authorities by direct request. The FORTRAN CODES 88 used for the declustering process as well as the final obtained earthquake 88 and focal mechanism 88 catalogs published in this study are available through the Supplementary Data Files on figshare ."
10.1038/s41597-019-0260-x,Simulated datasets and mentioned above scripts and algorithms are available at the Open Science Framework 9 . MATLAB Software version R2017b was used for the creation of scripts with additional toolboxes: Statistics and Machine Learning and Image Processing. CystoSpec Software version 2.00.01 was used for PCA denoising and FCM clustering. Voigt profile computation algorithm implemented by Sanjar M. Abrarov and Brendan M. Quine is available at: https://ww2.mathworks.cn/matlabcentral/fileexchange/47801-the-voigt-complex-error-function-second-version .
10.1038/s41597-019-0259-3,The R code used to prepare Figs 3 and 4 can be found in the electronic supplement.
10.1038/s41597-019-0256-6,MISSING
10.1038/s41597-019-0253-9,All tools used in this study were properly cited in the sections above. The settings and parameters were clearly described as well.
10.1038/s41597-019-0254-8,"Neuroimaging data were processed using standard processing pipelines in FreeSurfer v6.0 23 ( http://surfer.nmr.mgh.harvard.edu , https://github.com/freesurfer/freesurfer ). All code used for registration of volumes into standard stereotactic space is available within the open-source Lead-DBS v2.0 software 31 ( https://github.com/leaddbs/leaddbs ). Because registration involved multiple manual user interface steps, no ready-made code is provided, but the process can be readily reproduced with the provided data and software."
10.1038/s41597-019-0233-0,Microsoft Excel version 2010 and ArcGIS version 10.1 were used to determine the technical PV and wind potentials. QGIS version 2.14.10 was used for the transfer of wind and hydrothermal potential to the municipality level. MATLAB version 2017b and Microsoft Excel version 2016 were used to compile the census and mobility data. Microsoft Excel or another program for processing CSV or XLS files is required to process the data set provided with this data descriptor. The codes for generating the data can be made available on request.
10.1038/s41597-019-0228-x,Custom code for handling the dataset and all input data are available at Data Citation 15 . This Matlab code enables users to easily reconstructed the soil moisture product. Core of the code is the application of GRNN model. The code requires Matlab version 2016b or higher.
10.1038/s41597-019-0205-4,"All the PGP-UK data pre-processing, QC and analyses were performed with publicly available software packages, using versions and parameters described in the paper."
10.1038/s41597-019-0225-0,All code used to process data and generate figures in this paper are available from the Altschuler-Wu Lab public github page: https://github.com/AltschulerWu-Lab/MultimodalPDXHeterogeneity .
10.1038/s41597-019-0193-4,The full SPP source code is available in the SPP GitHub account under a Creative Commons CC 3.0 BY license at https://github.com/signaling-pathways-project/ominer/ .
10.1038/s41597-019-0198-z,"These are the scripts 58 used in the different analyses: • mRNA expression script. This script reads preprocessed and normalized expression data and makes a differential expression (DE) analysis between the samples defined in the parameter \(\#\#\#URL\#\#\#\) . As a result, a table of DE genes is generated. A second table with the normalized gene expression values per sample is also generated. • miRNA expression script. This script reads preprocessed and normalized miRNA expression data and makes a differential expression (DE) analysis between the samples defined in parameter \(\#\#\#URL\#\#\#\) . As a result, a table of DE miRNA is generated. Likewise, a second table with the normalized gene expression values per sample is generated. • Microbiome correlation script. This script generates a correlation plot between microbial abundance and methylation data from selected DM genes obtained from samples previously filtered. \(\#\#\#microbioma\,Genes-Methylated\#\#\#\) is a table whose variables are the microbiome data and the methylation levels of the genes for the selected samples. An extra variable called “class” contains the labels of the classes. It is also necessary to know the number of microbiome variables (### numgenus ###) and the number of genes (### numGenes ###). • Microbiome PCA script. This script generates a plot resulting from a principal components analysis (PCA) using the methylation values of the selected genes in the filtered samples, and the selected genus(s) (or families, or species). The color distinguishes the different groups of samples. • Methylation script. This script performs a differential methylation (DM) analysis using raw data from Illumina microarrays. The results are a DM gene table and the methylation values (beta values) of each sample. • MCA plot script. This performs a multiple correspondence analysis with the selected variables. The result is a graph with the relations between categories. All scripts are developed in R 59 and they are used as templates, which are instantiated using the values defined by the user. The following R packages have been used: limma 24 , tidyverse 25 , FactoMineR 22 , factoextra 23 , corrplot 29 , minfi 26 , IlluminaHumanMethylation450kanno.ilmn12.hg19 27 , IlluminaHumanMethylation450kmanifest 28 , cluster 30 and ape 31 ."
10.1038/s41597-019-0216-1,The code used for analysis of microarray data and differential expression for microarray sRNAomics and hormonomics data is available at FAIRDOMHub 13 for project MOA on the corresponding Experimental Assay subsections.
10.1038/s41597-019-0171-x,"The scripts used for analytics were made available on gitlab ( https://gitlab.unil.ch/mjan/Systems_Genetics_of_Sleep_Regulation ). The master branch contains the scripts used for our publication and mm9 analysis. A second branch was created for analysis performed on a mm10 mouse references (see Technical Validation). The intermediate files required to run these scripts were made available at Figshare 27 . Finally, a documentation file was generated documenting the hierarchical relationship between the scripts and datasets in a form of a dynamic html document (see Workflow documentation)."
10.1038/s41597-019-0262-8,No custom code was used to produce the dataset.
10.1038/s41597-019-0202-7,"Preprocessing scripts for each of the omics datasets, together with relevant intermediate files, are available at the STATegraData GitHub repository 72 . Preprocessing scripts collect in one.txt file all code and parameters required to transform raw data files into one consolidated data matrix with ready to use quantitative data, where samples are arranged in columns and features are arranged in rows. Script files may contain code for multiple programming languages or simply list parameters used in commercial software when applicable."
10.1038/s41597-019-0245-9,All code for the data presentation and processing has been done in program R 3.6.0 and can be obtained upon request.
10.1038/s41597-019-0266-4,The Python 3 code related to this project is available for download at https://github.com/sebbarb/times_to_hospitals_AU .
10.1038/s41597-019-0263-7,"Empusa is written in Java with Gradle as build system. Empusa code base is available at http://www.gitlab.com/Empusa under the MIT license. Documentation and tutorials can be found at associated website http://empusa.org . Regarding GBOL, the source file of the ontology encoded in the Empusa and associated generated OWL definition (Passes OWL validation according to the HermiT Reasoner), ShEx schema and visualization for Cytoscape are available at http://www.gitlab.com/GBOL under the MIT license. The generated Java and R API are available at https://gitlab.com/gbol/GBOLapi and https://gitlab.com/gbol/RGBOLApi under the MIT license. Tutorials on using the Java and R API are available at http://gbol.life/APIUsage/ . Instructions on how to perform a ShEx validation, with passing and failing examples are to be found at http://gbol.life/APIUsage/ . All terms in GBOL are resolvable and can be browsed at the associated website http://gbol.life ."
10.1038/s41597-019-0269-1,"Code used for taxa specific input data standardisation is not presented. Species name checks and changes were taxa specific and required a lot of manual processing after consultation with scheme organisers. Information on species aggregations, removals and name changes are, however, detailed in the “Species_Names.csv” spreadsheet. Functions for organising data into detection histories and for fitting the specified occupancy model are available in the R package sparta 29 . The function formatOccData was used to arrange the data into detection histories and to calculate the list length of visits. The function occDetFunc was used to run the models. Note that in order to run these models using sparta , JAGS must be downloaded separately in order to carry out the MCMC sampling 31 . An example workflow detailing function and model specifications has been supplied within Supplementary File 1. This PDF document runs through each subsection of the methods, except the raw data processing, providing the code used and examples of the outputs produced as a result. Raw data processing was not included since processes were group specific and raw data could not be supplied alongside the outputs due to data provider restrictions."
10.1038/s41597-019-0264-6,"The data extraction and parsing were done in Python 3.4 and 3.6. The construction of address fields, the cleaning, data processing, matching with Geonames, and imputation of missing information was implemented in PostgreSQL 9.6.6. The assignment of regions and cities was done in PostGIS 2.4. All Python and PostgreSQL code produced for this project can be accessed upon request. The PostgreSQL codes for the imputation of missing country codes and location information is available on Github ( https://github.com/seligerf/Imputation-of-missing-location-information-for-worldwide-patent-data )."
10.1038/s41597-019-0282-4,MISSING
10.1038/s41597-019-0281-5,"Codes that were used for data processing are included in the Methods and available as supplementary material (Supplementary File 1 includes the sequences Asc1-Pac1 of the 4WBS ORFs; Supplementary File 2 the Primers used for RT-PCR_WBS). The whole set of results is available in the GEO database 25 , 26 as “A transcriptomic study of Williams-Beuren syndrome associated genes in mouse embryonic stem”, SuperSerie code GSE96701 24 (Supplementary File 4 )."
10.1038/s41597-019-0265-5,The R scripts for generating the suite of accessibility layers and for performing the validation are available at the figshare repository 22 .
10.1038/s41597-019-0277-1,"Raw model output is available for further analysis from https://www.paleo.bristol.ac.uk/ummodel/scripts/papers/Armstrong_et_al_2019.html . A list of the snapshot simulation names used in this experiment is given in Table 1 . All scripts used to construct the climate timeseries have been written using the NCAR command language (NCL, Version 6.4.0) and are available within the NERC digital repository 51 ."
10.1038/s41597-019-0249-5,"Data acquisition interface: We use S-Interface, a modular software system available on figshare 24 that supports real-time data acquisition from sensors and computer peripherals. S-Interface’s imaging subsystem reads radiometric data from the thermal facial camera and applies certain operations on them to extract perinasal perspiration (PP) signals; it also acquires the facial and operational theater recordings from the visual cameras. S-Interface’s participant input subsystem captures the participants’ computer interactions. Specifically: S-Interface’s imaging configuration runs on the system computer and includes: 1. The tracker plug-in (an implementation in C# of ref. 17 ), which follows the participants’ perinasal area in the thermal imagery, nullifying the effect of head motion. 2. The perspiratory morphological signal extractor (an implementation in C# of ref. 15 ), which operates upon the orthorectified perinasal MROI, yielding a signal commensurate to the extent of activated perspiration pores. This perspiration signal serves as a proxy of arousal. 3. The visual facial and operational theater plug-ins that capture the corresponding video streams. S-Interface’s participant input configuration runs on the participant computer and includes: 1. The screen capture plug-in, which calls upon the VisioForge SDK to record the participants’ computer display. 2. The keyboard plug-in, which captures the participants’ interactions with the computer’s input device. Protocol interface : To enforce consistency in the execution of the experiment and the delivery of the stressors, we developed a custom interface (p-Interface). The p-Interface implements the experimental protocol, guiding participants step by step through the designed treatments. Specifically, in the ST and DT treatments, the p-Interface presents to the participants a basic editor to write their reports. It also features a basic email client to deliver the email interruptions and allow participants to send back their responses. The p-Interface is written in Javascript and is available under the Tools folder in 18 . Survey interface : We developed an interface to deliver questionnaires to participants and collect their responses. This survey interface is implemented via surveygizmo and is available under the Tools folder in 18 . Stroop application : We wrote an application to deliver the Stroop color word test for the BH and CH groups. This application is presented to the participants from within the p-Interface. The Stroop application is written in Javascript and is available under the Tools folder in 18 . Data curation, quality control, and validation scripts : We developed four sets of R scripts to curate, ensure quality control, and validate the raw data collected via the S-Interface and other tools in this project. Both the scripts and the raw data reside in GitHub (Zaman, S. & Pavlidis, I. Office-Tasks-2019-Methods. GitHub https://github.com/UH-CPL/Office-Tasks-2019-Methods ). The first set of scripts curates the raw data. The second set of scripts operates upon the curated data, performing the first level of quality control (QC1). The third set of scripts operates upon the QC1 data, performing a second level of quality control (QC2). The fourth set of scripts carries out validity tests on the QC2 data. The final outcomes of these script sets are the quality controlled data residing in 18 , as well as the statistical plots featured in the present manuscript and its Supplementary Information file. Some additional information about relevant practical issues can be found in 33 ."
10.1038/s41597-019-0279-z,No specific code or script was used in this work. All commands used in the processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-019-0285-1,The Matlab code used to calculate hand joint angles from CyberGlove instrumented gloves raw data can be accessed as open access 28 .
10.1038/s41597-019-0280-6,"No custom code was used in the generation or processing of datasets. All data is provided as ASCII text in CSV format and can be processed without custom code. An optional interactive Excel worksheet for convenient browsing, importing and resampling of the data is available upon request from the corresponding author."
10.1038/s41597-019-0261-9,"The data acquisition and the hyperspectral data processing was performed using software developed inhouse. The software developed is available on GitHub: https://github.com/unine-chyn/hics . As the software setup for acquisition depends on the configuration and the hardware used, only an enumeration of the modules used is given. Three different classes of modules where used. Hardware related modules are specific to the hardware setup, and are likely to be modified with a different setup. Preview and control modules are used by the operator to control the camera and check that the process is performing satisfactorily. Finally, the plugins used for capturing data contain the algorithmic logic of the capture. Hardware related: hics.hardware.specimswir.camera (camera control), hics.hardware.specimswir.scanner (mirror scanner control), hics.hardware.specimswir.framegrabber (frame grabber), hics.hardware.ev3.ev3focus (LEGO focus control), hics.hardware.ev3.ev3rotater (LEGO rotating device), hics.hardware.webcam (webcam capture) Preview and control: hics.daemon.frameconverter (basic correction for preview), hics.gui (graphical user interface) Plugins for capturing data: hics.plugin.photogrammetry (capture data from the webcam), hics.plugin.record (record hyperspectral data), hics.plugin.autofocus (find the focus maximizing contrast), hics.plugin.labelprint (print the label for the sample), hics.plugin.autoscan (automate all steps required for the scan, using the various plugins) Once data is captured, the following commands were used (@ replaces the scan name): python3 –m hics.datafile.calibrate–input @.scan —output @.calibrated —max 14000 python3 –m hics.datafile.merge —input @.calibrated —output @.hdr —clean–required_images 2 # A mask should be generated by the user in @.mask.png python3 –m hics.datafile.maskdata —input @.hdr —mask @.mask.png —output @.mhdr. For the 3D reconstruction, after multiple experiments with various tools, it was found that the best output quality was obtained using COLMAP 8 for the keypoints detection and OpenMVS 12 for the construction of the pointcloud, the mesh, and the texturing. A small change was applied to COLMAP in order to orient the sample correctly. This change however doesn’t affect the quality of the result. OpenMVS was used without any modification. The code to automate the build of the tools and the processing of the images is available at https://github.com/unine-chyn/sfm-bundle ."
10.1038/s41597-019-0267-3,Code is available on figshare 15 ( https://doi.org/10.6084/m9.figshare.8796944 ) and on the Global Ecology Flinders GitHub repository ( https://github.com/GlobalEcologyFlinders/FosSahul ).
10.1038/s41597-019-0293-1,"Most of the data analysis was completed by software running on the Linux system, and the version and parameters of main software tools are described below. (1) SMRTlink: version 5.1, parameters: no_polish TRUE, max_drop_fraction 0.8, min_zscore −9999.0, min_length 50, min_predicted_accuracy 0.8, max_length 15000, min_passes 2. (2) Arrow: parameters: bin_size_kb 1, hq_quiver_min_accuracy 0.99, qv_trim_5p 100, qv_trim_3p 30, bin_by_primer false. (3) LoRDEC: version V0.7, parameters: -k 23, -s 3. (4) CD-Hit-Est: version 4.6, parameters: -c 0.95 -T 6 -G 0 - aL 0.00 -aS 0.99. (5) BUSCO: version 3.0.2, default parameters. (6) Blastx: version 2.2.31, parameters: -outfmt 6, e value:1e-5, -num_descriptions 10, -line_length = 60. (7) CNCI: version 2, default parameters. (8) CPC: version 0.9, parameters: 1e-10. (9) Pfam-scan:31.0, parameters: -E 0.001 –domE 0.001. (10) PLEK: version 1.2, parameters: -minlength 200. (11) GMAP: version gmap.sse42, parameters: -f samse -n 0 -z sense_force -t 8. (11) SUPPA2: version 2.2.1, default parameters."
10.1038/s41597-019-0295-z,MISSING
10.1038/s41597-019-0290-4,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository ( https://github.com/maubreville/MITOS_WSI_CCMCT/ ). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai and OpenSlide 28 and provides some custom data loaders for use of the dataset.
10.1038/s41597-019-0294-0,MISSING
10.1038/s41597-019-0292-2,"Raw SeaFlow data are analyzed using our custom R package available on Github at https://github.com/armbrustlab/popcycle . The repository also includes a tutorial on the use of the software. Additional Github repositories are available for the virtual-core calibration, conversion of light scattering to cell size and conversion of light scattering to carbon quotas."
10.1038/s41597-019-0301-5,MISSING
10.1038/s41597-019-0302-4,There is no custom code used in the generation or processing of these datasets. The spreadsheets used for the calcite precipitation model and the carbon flux calculations are included in the original research paper 1 .
10.1038/s41597-019-0291-3,"The public softwares used in this work, were cited in the Methods section. If no detail parameters were mentioned for a software, default parameters were applied with the guidance."
10.1038/s41597-019-0296-y,"Sequencing data were generated using the software provided by sequencer manufacturers, and processed following the instruction manual of the software cited above. No custom codes were generated for this work."
10.1038/s41597-019-0289-x,MISSING
10.1038/s41597-019-0275-3,An example of the Python code used to analyze the raw energy monitor data is publicly available on the Open Science Framework data repository 10 . The data can also be analyzed using software which handles tabular timeseries data such as R or MATLAB. The code used to aggregate and plot the data for this paper are publicly available on the data repository on OSF.
10.1038/s41597-019-0283-3,"Custom code was used to validate the incoming data from the BMS for completeness and validity. The code had been very specific according to the system configuration and is not available anymore. Its value for future applications or future data usage would be very low because 90% of the code was written to check the syntactically correctness of the data. While the authors expected such syntactical correctness being granted for data exported from a BMS, the first month of monitoring (not included in the database) showed several problems with the structure of the data, which required many lines of custom code, very specific to the BMS in place and therefore not generalizable to any other application."
10.1038/s41597-019-0271-7,MISSING
10.1038/s41597-019-0287-z,MISSING
10.1038/s41597-019-0273-5,"All MATLAB scripts used to implement the post-processing steps described in the Data Records section are available upon request, as are the raw data files described in steps 1 and 2 (e.g., pre- and post- cleaning). Interested readers should contact the corresponding author for access to the scripts so that further instructions on their use may be provided. Additionally, links to the background survey instrument 27 , daily survey instrument 29 , and retrospective survey instrument conducted after each two week daily surveying period 30 are available for testing."
10.1038/s41597-019-0288-y,"The Python code used to prepare the archived data, and to enable incorporation of any subsequent observational data files, has also been made permanently available ( https://doi.org/10.5281/zenodo.3271678 ) 25 ."
10.1038/s41597-019-0272-6,"R software 26 was used for all steps requiring data manipulation described above in section Data preparation and below in section Technical validation. Custom code was developed for these steps and is made available (see Code availability below). The custom code consists of three main scripts with additional functions loaded when running the scripts. The first script (ScalesSurv_00_DataPreparationAndChecks.r) contains the steps for initial data screening and calculating additional variables for individual datasets. The second script (ScalesSurv_01_LoadAllDatasets_compl_final.R) combines individual datasets and harmonizes some factor levels, e.g. changing all season descriptors “fall” to “autumn” for consistency with other descriptors. The third script (ScalesSurv_02_Prepare_Data_final.R) is used for additional data preparations, such as additional harmonisations and adjustments to individual data points based on the results from technical validation. All R scripts used for validating incoming data, acquiring climate classifications, and calculating additional variables are available on OSF 23 . The R version used for processing the data was 3.5.2 (2019-04-23). The packages used for data preparation and validation were: knitr, gridExtra, grid, ggplot2, pander, xtable, png, kableExtra, reshape2, SparseM, plyr, data.table."
10.1038/s41597-019-0274-4,The pre-processing code for suppression and data processing is available online ( https://github.com/sdu-cfei/Building-Data-Occupant-Modeling ).
10.1038/s41597-019-0305-1,"Data in .mzXML format was filtered and merged with an R script available at https://github.com/dolivierj/MSDB_Maker , using the R 3.6.0 language 63 with the MSnbase 64 package."
10.1038/s41597-019-0312-2,"The code is available on demand from the EFFIS or GWIS team of the Joint Research Centre of the European Commission. Two parameters have been used in the execution of the application to the MCD64A1. First, the fire is considered as ended when it has been 16 days without activity. The second parameter is the time between two burnt areas that touch each other: if one of them has been active in the last 5 days it will become a single fire event. The code developed in GWIS uses the previously mentioned libraries and processes a single delivery of a burnt area product. The spatio-temporal clustering is defined during the creation process of the sparse matrix."
10.1038/s41597-019-0303-3,Preprocessing was performed using FEAT ( www.fmrib.ox.ac.uk/fsl ). ICA was performed with MELODIC v3.15 ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MELODIC ) and IC classifications were manually performed using melview ( https://git.fmrib.ox.ac.uk/fsl/melview ). All code for data denoising and technical validation is available on github.com/xingyu-liu/studyforrest_denoise.
10.1038/s41597-019-0299-8,"IdTracker 22 is a videotracking software that keeps the correct identity of each individual during video behavioral analysis and is publicly available at http://www.idtracker.es/ . ANY-MAZE@ is a licensed video tracking program, that can be downloaded from http://www.anymaze.co.uk/ . The customized Matlab code customized code Locomotion.m in publicly available on Figshare 72 ."
10.1038/s41597-019-0311-3,"The software packages used for the analysis with version numbers, are given in Table 2 . The parameters of scripts used for the analysis are available at figshare 26 ."
10.1038/s41597-019-0304-2,"Data were imported into the Paleo Core data repository using Python (version 3.6) standard libraries (re, datetime, pytz), the xlrd library (version 1.20) for reading Excel files, and the database API included with the Django web framework (version 1.11.20). All of the original source code used to process the data are freely and publicly available through the Paleo Core github repository at: https://github.com/paleocore/paleocore110/blob/master/eppe/import_1998_2005.py ."
10.1038/s41597-019-0313-1,"HISAT2: http://ccb.jhu.edu/software/hisat2/index.shtml . Version: 2.0.5. Parameters: –rna-strandness RF. Bowtie: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml . Version: 0.12.9. Parameters: -v 0 –k 1. miRDeep2: https://github.com/rajewsky-lab/mirdeep2 . Version: 2.0.0.8. Parameters: quantifier.pl -p -m -r -y -g 0 -T 10. miREvo: http://evolution.sysu.edu.cn/software/mirevo.htm . Version: 1.1. Parameters: -i -r -M -m -k -p 10 -g 50000. miRanda: http://miranda.org.uk/ . Version: 2.042. Parameters: -sc 140 -en -10 –scale 4 -strict -out. StringTie: http://ccb.jhu.edu/software/stringtie/ . Version: 1.3.1. Parameters: default. BWA: http://bio-bwa.sourceforge.net/ . Version: 0.7.12. Parameters: -T 25 -k 18. DeepTools Version: 3.0.2. Parameters: –cor Method Pearson. MACS2: http://liulab.dfci.harvard.edu/MACS/ . Version: 2.1.2. Parameters: -q 0.05–call-summits –nomodel –shift -100 –extsize 200 –keep-dup all. Homer: http://homer.ucsd.edu/homer/ . Version: 4.9.1. Parameters: -gc –len 8, 10, 12, 14."
10.1038/s41597-019-0308-y,Figshare File 2 17 : Parameters file used for Comet searches of mass spectrometry data. File is in .docx format. Figshare File 3 17 : Parameters file used for Abacus analysis of Comet search results. File is in .docx format.
10.1038/s41597-019-0307-z,MISSING
10.1038/s41597-019-0314-0,MISSING
10.1038/s41597-019-0306-0,"The compound data were extracted from the scientific literature using a UV/vis absorption spectroscopy tailored version of ChemDataExtractor, which is available at https://github.com/edbeard/chemdataextractor-uvvis2018 . A clean build of the current release of ChemDataExtractor version 1.3 can be found at http://chemdataextractor.org/download . The scripts used to filter the data to leave chemically valid compounds are available alongside the database 6 at https://doi.org/10.6084/m9.figshare.7619672.v2 in the ‘scripts.zip’ directory. Scripts used for the QMWF pipeline in Stage II can be found at https://github.com/alvarovm/qmwf ."
10.1038/s41597-019-0323-z,"In addition to the methods here presented, the instructions for reproducing the LAMB protocol are fully described in Rabuffetti et al . 25 . Moreover, the code used for computing the angular variables is provided together with data files in FigShare 34 . In details, the MATLAB function “CalcAngle.m” computes the angles in the three anatomical planes, starting from the reference frames of two adjacent body segments."
10.1038/s41597-019-0320-2,"The Barcode of Life Data System (BOLD; www.boldsystems.org ) 8 was used as the primary workbench for creating, storing, analyzing, and validating the specimen and sequence records and the associated data resources 48 . The BOLD platform has a private, password-protected workbench for the steps from specimen data entry to data validation (see details in Data Records), and a public data portal for the release of data in various formats. The latter is accessible through an API ( http://www.boldsystems.org/index.php/resources/api?type=webservices ) that can also be controlled through R 75 with the package ‘bold’ 76 ."
10.1038/s41597-019-0326-9,"The datasets generated have been created using code for Python 3.6, Elasticsearch 6.6 and PostgreSQL 10.6. The code is available through https://github.com/jensdebruijn/Global-Flood-Monitor ."
10.1038/s41597-019-0324-y,Readat (version 1.4.0) script is available on Bioconductor ( www.bioconductor.org ). SciData_R_script.R – this supplemental script is given as an example of a downstream statistical analysis of the HMS-16-007.HybNorm.MedNorm.Cal.20160218.adat dataset 27 .
10.1038/s41597-019-0300-6,"All data processing steps were performed using native tools and/or customized batch processing within ESRI’s ArcGIS 10.4 software package 31 in a dedicated computing setup (64-bit processing). The two core tools applied were ‘Zonal Statistics’ and ‘Flow Accumulation’. To support repetitive tasks of this work, a multitude of adjusted batch routines were developed as needed, mostly defining input and output path names for the standard tools and to handle internal object IDs. No stand-alone programming code was created that allows automatic processing of new data into the format of HydroATLAS. This is in alignment with the premise of our work, i.e. to produce standardized data by applying tedious, individual, and customized GIS steps specific to every input dataset so that other user do not have to repeat these time-consuming manual iterations."
10.1038/s41597-019-0319-8,"This implementation, built in the R statistical programming environment, was visualized by Shiny web application available at http://medi.dev.openlab.tw/data_analysis/ . The scripts used for all data analyses, implemented in R-3.5.2, are available in the supplementary code information."
10.1038/s41597-019-0327-8,"Aside from the image and 3D model processing steps performed with free or open source software described in this section, we used a custom script written in Python 3.6 with NumPy 1.15.4 ( https://numpy.org/ ) for cropping CT and PET scans in NRRD format, to cover the head and neck region only. This script furthermore outputs information about the volumetric medical data, such as the size and resolution, as seen in Table 1 . This python script utilizes the pynrrd 0.4.0 library ( https://pypi.org/project/pynrrd/ ) to parse information about slice resolution and volume size from the header of NRRD files. With this information, we calculate the ideal number of slices to be included in the final NRRD files for upload, as described by formula. These slices are then again automatically converted to NRRD format and saved to disk. These are the NRRD files available in our figshare repository 16 . The Python script is available on GitHub ( https://github.com/cgsaxner/PET-CT_preprocessing )."
10.1038/s41597-019-0316-y,The code used for this study is publically available at a Federated Research Data Repository (FRDR) 14 . No restrictions apply in accessing the code.
10.1038/s41597-019-0325-x,"Alongside the data, we provide an iPython notebook showing basic data loading and use (see Copenhagen_Networks_Study_Notebook.ipynb in the Figshare data repository 59 ). The notebook is intended to showcase the basic approaches to working with the released data."
10.1038/s41597-019-0322-0,"Due to the use of real patient information during the de-identification process, the code used to prepare the dataset cannot be made publicly available. Example usage code, including loading a DICOM study and linking it with its associated free-text radiology report, has been made available publicly 25 ."
10.1038/s41597-019-0318-9,All code used to generate the website and Odonate Super Tree are available on Github ( https://github.com/jhnwllr/shiny-server/tree/master/odonates ).
10.1038/s41597-019-0330-0,MISSING
10.1038/s41597-019-0328-7,"Stable release versions of TSMP are provided through a git development repository available at the model’s website ( https://www.terrsysmp.org ). The release version includes extensive instructions for installing the system, including sample reference test cases for typical application examples, as well as a suite of pre-processing and post-processing tools. TSMP is essentially released without its component models, i.e., the release contains the built system, all configuration files, such as namelists, for the sample cases, the component model code patches and all coupler related modifications. The user must download the component models from their respective separate repositories: All ParFlow releases are available via GitHub ( https://github.com/parflow/parflow ). The official CLM website ( http://www.cgd.ucar.edu/tss/clm/distribution/clm3.5/index.html ) offers all links to documentation, source code, and input data for the stand-alone version release of CLM as used in this study. The COSMO model is available only after registration (cosmo-licence@cosmo-model.org) and is also free of charge for research applications. More information on the procedure and licensing terms are available at the COSMO model website ( http://www.cosmo-model.org/content/support/software/default.htm ). It must be noted that the TSMP model system supports various combinations of different component model versions; e.g. ParFlow only, ParFlow-CLM, CLM-COSMO, ParFlow-CLM-COSMO."
10.1038/s41597-019-0329-6,The Code subdirectory at the above data repository contains the python scripts used to produce the urban class data layers and the zonal statistics tools upon which our analysis is drawn.
10.1038/s41597-019-0332-y,All code used to process and generate the data in this study can be found alongside the dataset 29 . A description of each script is provided in Online-only Table 5 . Source code for ASHLAR is available on GitHub ( https://github.com/jmuhlich/ashlar ). The newest histoCAT version can also be found GitHub ( https://github.com/BodenmillerGroup/histoCAT ).
10.1038/s41597-019-0331-z,The complete RNAseq pipeline used in analysis of RNAseq data is available on Figshare 22 . The script to remove batch effects from RNAseq data is also available of Figshare 26 .
10.1038/s41597-019-0336-7,"Raw data was processed using all, or a combination of Sequest HT, Mascot (v2.6.2), MSPepSearch, MSAmanda 2.0, and Byonic (ProteinMetrics; v3.2.0) algorithms via PD (v2.2.0.388)."
10.1038/s41597-019-0334-9,"The data cleaning, summarizing, merging and supplementing with additional columns were done in Python and R and the custom code used for this project can be provided upon request."
10.1038/s41597-019-0333-x,Codes/scripts used in this study for data processing and analysis were developed in Matlab 2017b. Codes/scripts are stored in the Figshare data repository and can be downloaded from the link 36 .
10.1038/s41597-019-0337-6,"The scripts used for all data analyses, implemented in R are available in the github repository github/InFlamUOB/NSDTraumaMODS 46 and accessible through figshare too 48 ."
10.1038/s41597-019-0338-5,Code used to create event data files from compiled E-prime data and to deface T1-weighted images are located in the code directory at the root level of the dataset. reading-events-to-tsv.py uses .csv containing merged data from all subjects per task and outputs events.tsv files into each subject folder as described in data records. reading_deface.bash and multiply_by_mask.py remove facial features from all T1-weighted images. stims_checking.py confirms that all stimuli referenced in participant events.tsv files exist in the stimuli directory at the root level of the dataset.
10.1038/s41597-019-0342-9,"A Git repository of the project is accessible at https://github.com/practikpharma/PGxCorpus/ . It includes the annotation guidelines, the corpus itself and the programmatic code of the baseline experiments presented in Technical Validation."
10.1038/s41597-019-0351-8,The R code used in the analysis of the scRNA-seq data is available on GitHub ( https://github.com/lessonskit/Single-cell-RNA-sequencing-of-human-kidney ). This R code is also available at figshare 31 .
10.1038/s41597-019-0340-y,MISSING
10.1038/s41597-019-0321-1,The code for the heatmap and PCA analysis is available in Supplementary file 1.
10.1038/s41597-019-0345-6,"All code used to generate the predictions is available from the Github ( https://github.com/yanfp/DTB100China ). The map products were calculated using R version 3.4.1 and packages “randomForest (4.6–14)”, “xgboost (0.71.2)”, “quantregForest (1.3–7)”, “sp (1.3–1)”, “rgdal (1.2–16)”, “raster (2.7–15)”, “pkgmaker (0.27)”, “lattice (0.20–35)”, “plotKML (0.5–8)”, “hexbin (1.27.2)” and “ggplot (3.1.0)”."
10.1038/s41597-019-0335-8,"The signal recording was performed using two programs in parallel: OT BioLab version 2.0.6254 available at www.otbioelecttronica.com for recording iEMG and synchronization signals, and the custom recording software developed in LabVIEW 2016 for force signals recording, generating synchronization pulses, visualizing forces and generating commands and cues. Data post-processing was done in Matlab. All custom codes are available by contacting the corresponding author."
10.1038/s41597-019-0344-7,"In addition to the Excel storage of the datasets, the CESTES database has also been stored as an.RData object to facilitate its further use for analyses in R 109 . It comes with R code scripts that allow further checking, processing, transforming and exploring the database content (for more details, see the Usage Notes section). We provide all this information in a folder called “rCESTES.zip” within the “CESTES” folder at the following links: Figshare repository (fixed version): https://doi.org/10.6084/m9.figshare.c.4459637 . iDiv Biodiversity Portal (evolutive version): https://doi.org/10.25829/idiv.286-21-2695 ."
10.1038/s41597-019-0339-4,"Swift Primerclip installation instructions, scripts, and examples can be found at https://github.com/swiftbiosciences/primerclip . Current available methods for downloading the Swift Primerclip tool are a pre-compiled binary for linux on x86_64 and building from source using Haskell-stack build tool. Additional requirements include SAMTools (1.6-2-gf068ac2), Picard Tools (2.1.0), BWA (0.7.17-r1188), GATK (3.5-0-g36282e4), and Java (1.8). Codes and parameters are described as below."
10.1038/s41597-019-0350-9,"Trimmomatic v. 0.35 parameters: trimmomatic-0.35.jar PE -phred33 in_forward.fq.gz in_reverse.fq.gz out_forward_paired.fq.gz out_forward_unpaired.fq.gz out_reverse_paired.fq.gz out_reverse_unpaired.fq.gz ILLUMINACLIP: TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 HISAT2 v 2.1.0 parameters: hisat2-build reference_index_name genome.fa hisat2 –x reference_index -1 reads_1a.fq,reads_1b.fq, reads_1c.fq,reads_1d.fq -2 reads_2a.fq,reads_2b.fq,reads_2c.fq,reads_2d.fq -S output.sam SamTools v. 1.9.0 parameters: samtools view -b -o output.bam samfile_from_hisat2.sam samtools sort -o sorted.bam output.bam samtools depth sorted.bam | awk ‘{sum +=$ 3} END { print “Average = ”,sum / NR }’ Trinity v. 2.7.0 parameters: Trinity -- seqType fq -- left reads_1a.fq,reads_1b.fq,reads_1c.fq,reads_1d.fq -- right reads_2a.fq,reads_2b.fq,reads_2c.fq,reads_2d.fq -- CPU 6 -- max_memory 20G CD-HIT-EST v. 4.8.1 parameters: cd-hit-est –i trinity_transcripts.fasta –o output file –c 0.9 . TransDecoder v.5.5.0 parameters: TransDecoder.LongOrfs -t cd-hit-est__0.95_transcripts.fasta BUSCO v. 3 parameters: python BUSCO.py -i unigenes -l OrthoDB v9 -o output_name BLAST v. 2.7.1 parameters: makeblastdb -in reference_trancriptome assembly.fasta -dbtype “nucl” blastn -query unigenes.fasta -db reference_trancriptome assembly.fasta -out outputfile.txt -evalue 1e-5 -max_target_seqs. 20 -outfmt 6 makeblastdb -in -in uniprot_sprot.fasta -dbtype “prot” blastx -query unigenes.fasta -db uniprot_sprot.fasta -out outputfile.txt -evalue 1e-3 -max_target_seqs. 20 -outfmt 6 ."
10.1038/s41597-019-0349-2,The Matlab code used to calculate hand joint angles from CyberGlove instrumented gloves raw data can be accessed as open access 29 .
10.1038/s41597-020-0355-4,"The code that was used to create the PIC database, calculate statistics of this paper, demonstrate a machine learning task and source code which underpins the PIC website and documentation is openly available, and contributions from the research community are encouraged: https://github.com/Healthink/PIC ."
10.1038/s41597-020-0352-7,Codes that were used for the RNA-seq data processing are available at figshare 27 . Software and their versions were described in Methods.
10.1038/s41597-020-0356-3,"All the data processing and data visualization were conducted using R (version 3.5.1) 25 . The source code is available on figshare 22 . The code is detailed with instructions for users. Generally, the function.R file (under RScript folder) defined several functions to obtain background information from external datasets, as well as the function to plot the samples spatial distribution (Fig. 1 ). The SoilHealthDB_quality_check.R file (under RScript folder) intends to check the data quality, and to explain how some soil health indicators are grouped based on the basic information. We also created a markdown file ( SHDB.Rmd ), which described the analysis and generated figures (Figs. 1 , 4 and 5 ) for this study. All the code and data used are available in figshare 22 and GitHub ( https://github.com/jinshijian/SoilHealthDB )."
10.1038/s41597-020-0353-6,"Several custom scripts were developed to process, manage, and clean the data using Stata 15.0. Two scripts are used to merge the source datasets using full outer join and geospatial methods. Two of the source datasets 4 , 5 are not publicly available, but were accessed via the CRADA described above. A third script updates the dataset and removes any duplicates based on the manual entries from visual verification. A fourth script formats and condenses the “master” dataset (which has separate attribute fields from each source dataset and also stores dismantled WTs, non-WT records, and old versions of retrofitted WTs) to compile the cleaned “public” dataset described in this paper. Visual verification of WT locations was conducted manually using ESRI ArcMap. Users may contact the corresponding author for details about these scripts and source data."
10.1038/s41597-019-0341-x,"The workflow by which CLDF datasets are analyzed and published within the CLICS framework, is available as a testable virtual container 36 that can be freely used on-line in the form of a Code Ocean capsule 37 ."
10.1038/s41597-020-0359-0,"A source code repository is available at ( https://simtk.org/projects/multis ), which provides scripts to parse the data files, create data overview plots, and register surgical tools data to the image (MRI or CT) coordinate systems. Install Python 2.7 with Anaconda ( https://docs.anaconda.com/anaconda/ ) and the npTDMS library (reading LabVIEW data files). Additional libraries can be installed using Anaconda (“conda install” command). A static download package is also available containing all raw and derivative data pertinent to the current study 23 ."
10.1038/s41597-020-0358-1,"The source code repository is available at ( https://simtk.org/projects/multis ). Additionally, two static packages containing thickness analysis software and all data/source code used in the technical validation section are available ( https://simtk.org/frs/?group_id=1032 ). In order to use the provided scripts, one needs to install Python 2.7 with Anaconda ( https://docs.anaconda.com/anaconda/ ). The two additional libraries required to read ultrasound and data files respectively are DICOM and npTDMS. Additional libraries used in the provided scripts can be installed using Anaconda (“conda install” command)."
10.1038/s41597-019-0347-4,"All scripts used are available at https://github.com/carderne/predictive-mapping-global-power (scripts and routines for preparing and finalizing data), https://github.com/carderne/gridfinder (primary code for preparing night-time lights and creating medium-voltage predictions), and https://github.com/carderne/access-estimator (access estimations and low-voltage predictions). The output data are available at the following, https://doi.org/10.5281/zenodo.3538890 . See also the original Facebook algorithm here: https://github.com/facebookresearch/many-to-many-dijkstra ."
10.1038/s41597-020-0354-5,"Our custom R package, which provides access to the data described here, is publicly available at https://github.com/ecohealthalliance/lemis . Installation of the package and subsequent download of the data enables efficient, on-disk manipulation of the entire cleaned dataset 32 , 33 . Basic package usage is outlined in the main package README file on the GitHub site. The code implementation of the data cleaning process is also available in the package codebase (via the ‘data-raw’ directory) and is outlined in the associated developer README file. These scripts span the entirety of our data processing and cleaning workflow, from importation and collation of the raw USFWS LEMIS data files through to generation of the single, cleaned data file as discussed in this manuscript. Thus, the scripts serve as transparent, reproducible documentation of our data processing in full."
10.1038/s41597-020-0360-7,MISSING
10.1038/s41597-019-0343-8,"The DC code used to produce the global database of future climates is publicly available under a Creative Commons Attribution 4.0 International license (CC BY 4.0). We carried out the procedure mainly in ArcInfo Workstation 10, and the R language for statistical computing 88 . The source code consists of two Arc Macro Language (AML) 89 (version 10.0) and two R (version 3.2.4) programming language scripts 90 ."
10.1038/s41597-020-0363-4,"All custom code used to generate and analyze the data presented here is available from https://doi.org/10.5281/zenodo.355113321 and from http://github.com/thackl/cr-genomes . Software versions and relevant parameters: Trimmomatic v0.32 (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:10:20 MINLEN:75 LEADING:3 TRAILING:3); proovread v2.12 (config settings: ‘seq-filter’ => {‘–trim-win’ => ‘10,1’, ‘–min-length’ => 500}, ‘sr-sampling’ => {DEF => 0}); Canu v1.8; Flye v2.3.7; WTDBG v2.1; SPAdes v3.6.1 (–diploid); minimap2 v2.13-r858-dirty (PacBio reads: -x map-pb; MiSeq readsL -x sr); bam-junctions SHA: 28dc943 (-a200 -b200 -c5 -d2 -f30 -e30); Redundans v0.14a (–noscaffolding –norearrangements –nogapclosing); Kaiju v1.6.3 (-t kaijudb/nodes.dmp -f kaijudb/kaiju_db_nr_euk.fmi); Prokka v1.13 (–kingdom Mitochondria –gcode 4); DEXTRACTOR rev-844cc20; jellyfish v2.2.4; samtools v1.7; Racon v1.3.1; BUSCO v3.1.0; WindowMasker 1.0.0; tRNAscan v2.0; BRAKER v.2.1.1; BLAST v.2.6.0+; Augustus v3.3.2; GeneMark-ES v.4.38; HISAT v.2.1.0; MAFFT v7.310; trimAl v1.4.rev22 (-strictplus); RAxML v8.2.9 (-p 13178 -f a -x 13178 -N 100 -m PROTGAMMAWAG -q part.txt); phytools v0.6-60; ggtree v1.14.4; barrnap (–kingdom euk); bcftools v1.9 (mpileup; call -mv -Ob);"
10.1038/s41597-020-0362-5,Arcmap 9.3 GIS software was used to mosaic the DEMs into continental tiles. PCRaster version 3 was used to derive the flow networks using standard operators. All spatial analyses were carried out using Arcmap 9.3.
10.1038/s41597-020-0368-z,MISSING
10.1038/s41597-020-0366-1,MISSING
10.1038/s41597-020-0370-5,"This work relied on many software tools. The versions, settings and parameters of these tools are given below. (1) FastQC : version 0.11.5, default parameters; (2) Scythe : version 0.994 BETA, parameters: -q sanger --quiet; (3) Sickle : version 1.33, parameters: pe -t sanger -q 20 -l 50 -n --quiet; (4) Lighter : version 1.0.7, parameters: -K 21 360000000; (5) FastUniq : version 1.1, default parameters (6) Trinity : trinityrnaseq-2.6.4, parameters: --seqType fq --JM 260G; (6) CD-Hit : version 4.6, default parameters; (7) TransDecoder : version 5.2.0, default parameters; (8) Jellyfish : version 1.1.10, parameters: count command: -m 17 -s 4G -c 7, dump command: -c -t, histo command: default parameters; (9) GenomeScope : version 2.0, parameters: 17 ( k -mer length) 150 (read length); (10) Platanus : version 1.2.1, default parameters for the all three steps, (11) GapCloser : version 1.12, parameter: -l 150; (12) HaploMerger2 : version HaploMerger2_20151124, default parameters for the followed running processes: carrying out batchA to batchE with the recommended pipeline, among which batchA was repeated 3 times and batchD was repeated 2 times, respectively; (13) HiC-Pro : version 2.10.0, default parameters; (14) LACHESIS : released in 2017, parameters: CLUSTER_MIN_RE_SITES=36 CLUSTER_MAX_LINK_DENSITY=1 CLUSTER_NONINFORMATIVE_RATIO=8 ORDER_MIN_N_RES_IN_TRUN=22 ORDER_MIN_N_RES_IN_SHREDS 22; (15) BWA : version 0.7.12-r1039, default parameters; (16) SAMtools : version 1.5, parameters: view command: -bS, sort command: -O BAM, depth command: -Q 40, mpileup command: -DSug -C 50, default parameters for the rmdup, index and flagstat commands; (17) Picard : version 1.80, parameters: SORT_ORDER =coordinate RGPL =illumina RGPU =illumina; (18) GATK : version 3.3-0-g37228af, default parameters for the two programs RealignerTargetCreator and IndelRealigner; (19) bcftools : version 0.1.19-44428 cd, parameters: view –Ncg; (20) TRF : version 4.07b, parameters: Match=2 Mismatch=7 Delta=7 PM=80 PI=10 Minscore=50 MaxPeriod=500 -d –h; (21) RepeatModeler : version 1.0.4, parameters: -pa 30 -database Fan; (22) RepeatMasker : version open-4.0.5, parameters: -pa 30 -species all -nolow -norna -no_is -gff; (23) RepeatProteinMasker : version 2.1, parameters: -engine abblast -noLowSimple -pvalue 1e-04; (24) LTR-FINDER : version 1.05, default parameters; (25) Augustus : version 2.5.5, parameters: --species=arabidopsis; (26) Geneid : version 1.4, parameters: -3 -P; (27) GeneMark : version 3.47, parameters: -f gff3; (28) GlimmerHMM : version 3.0.4, default parameters; (29) SNAP : version 2006-07-28, default parameters; (29) GeneWise : version 2.4.1, parameters: -tfor/-trev -gff; (30) EVM : version 1.1.1, default parameters; (31) PASA : version 2.0.2, parameters: for Launch_PASA_pipeline.pl step: -C -R -r–ALIGNERS blat, gmap, default parameters for the below two steps: asa_asmbls_to_training_set.extract_reference_orfs.pl and pasa_asmbls_to_training_set.dbi; (32) BLASTP : version 2.2.30+, parameters: -evalue 1e-5 -outfmt 7; (33) Interproscan : version 5.25-64.0, parameters: -dp -f tsv; (34) tRNAscan-SE : tRNAscan-SE-2.0, default parameters; (35) BUSCO : version 2.0, parameters: -m genome -c 20."
10.1038/s41597-020-0364-3,"An example script is provided with the dataset. It contains code snippets for reading and plotting the data and information related to the subjects and the task. All code is implemented in MATLAB (Mathworks Inc., version R2016B on Windows)."
10.1038/s41597-020-0365-2,All of the R code used for the computational analysis described in this report is included within this report as Supplemental Files 1 and 2 .
10.1038/s41597-020-0361-6,No specific code or script was used in this work. All commands used in the data processing were executed as the manual and usage instrument of the corresponding bioinformatics software.
10.1038/s41597-020-0369-y,"The code used in this work is not published along with the dataset because a non-free software named ANUSPLIN is invoked by this code, and cannot be redistributed without permission."
10.1038/s41597-020-0367-0,UWWTD database changes were developed through several R scripts whereas checks and corrections of spatial coordinates were conducted using ArcGIS interface. Final assembly of GIS dataset was conducted using SQL scripts. We regret it is not possible to package all the codes and procedure into sharable documentation.
10.1038/s41597-020-0371-4,"The code used in this study was provided under the GNU General Public Licence v3.0 and is available in 29 . The images were processed and classified on Amazon Web Services (AWS). The R package sits (Satellite Image Time Series) provides a set of visualization methods for an image time series, clustering methods for the time series samples, machine learning methods for the time series classification, including SVM, LDA, QDA, GLM, and other tools that support analysis of long-term satellite image time series. The development version of sits is available on GitHub at https://github.com/e-sensing/sits ."
10.1038/s41597-020-0372-3,The code can be accessed at https://github.com/StevenWingett/CloseCall without restrictions. Specific variables or parameters used are listed in Usage Notes.
10.1038/s41597-020-0373-2,MISSING
10.1038/s41597-020-0375-0,MISSING
10.1038/s41597-020-0377-y,"Custom scripts are shared at GitHub ( https://github.com/feifei/scripts_to_share ), including the script to scaffold the contigs with optical maps, and the script to update reference genome and annotation based on changes from BWA mapping pileup results. Software including their version information were already listed in the method section."
10.1038/s41597-020-0382-1,"The following R and Python codes used to process the cleaning and datasets can be accessed along with their data without any restrictions. The 1-erasmus_edgelist_merge_clean.Rmd shows how datasets were cleaned and standardized. The file can be read using the freely available computer program RStudio and contains R codes of data wrangling in the form of chunks and comments about the processing steps all the way from the open datasets to the merged and cleaned data. The program 2-findplacefromtext.py helped to harvest the geocodes of the HEIs from the POI database. HEIs were also recorded as POIs in the database, so their names, addresses and coordinates could be collected. The power of the database was proven by the historical institutional data. The geographically nearest POIs, e.g. pubs and museums, to the HEIs were identified by the programme 3-POI_Here_neighbour.py . Based on the geocoordinates of the HEIs, the nearest ETER and GRID institutions that included university type institutes were matched using files 4-merge_eter_DB.Rmd and 5-merge_grid_DB.Rmd , respectively. The aim was to identify matching Erasmus and ETER as well as GRID institutions. The validation determined using R are shown in the file 6-data_validation.Rmd . All the calculations made according to the published database can be seen in the Rmd file. The Gephi software, which is popular among network scientists, can also read the data. The Network on map subsection in the file 6-data_validation.Rmd includes the data preparation steps to read three subject layers, moreover, maps in Fig. 3 are provided with the help of the file 7-network_on_map.gephi ."
10.1038/s41597-020-0381-2,MISSING
10.1038/s41597-020-0383-0,MISSING
10.1038/s41597-020-0380-3,"The post-processing procedure described above was implemented in Matlab and executed in compiled form with Matlab 2016b. The relabeling procedure was implemented in Python and interpreted with Python 3.6. Censoring of the videos was done using a custom Bash script that interfaced with the FFmpeg 4.1.3 video manipulation tool. The GNU Parallel tool was used to run jobs in parallel 60 . A copy of the code that was used to create the data records from the raw recordings is publicly available in the megane_postprocess . tgz archive within the MDSScript 61 . This archive also contains a README file that lists and describes the main processing steps. Although the original data files cannot be released for privacy reasons, the code is made available to allow the user to thoroughly investigate and comprehend the procedure we used to process the data as well as to provide the community with a tool that can be re-used and re-adapted for similar tasks."
10.1038/s41597-020-0374-1,MISSING
10.1038/s41597-020-0376-z,Not applicable.
10.1038/s41597-020-0387-9,MISSING
10.1038/s41597-020-0388-8,MISSING
10.1038/s41597-020-0384-z,MISSING
10.1038/s41597-020-0389-7,"The complete PLAID dataset and all mentioned scripts are available in 5 . In the same repository, code written to capture the data can be found. The files are two scripts, namely ‘collecting_data.vi’ (written with LabVIEW) and ‘collecting_data.m’ (written in MATLAB)."
10.1038/s41597-020-0386-x,"The source code of the converter tool that transfers ECG data files from XML format to CSV format can be found at https://github.com/zheng120/ECGConverter , which contains binary executable files, source code, and a user manual. Both the MATLAB ( https://www.mathworks.com/ ) and Python version programs for ECG noise reduction are available at https://github.com/zheng120/ECGDenoisingTool ."
10.1038/s41597-020-0390-1,"All custom user code used for the technical validation is available from https://gitlab.com/kilinshi/scidata_vsmdb . Furthermore, a dataset viewer is also included in the repository which can be used to easily view any dataset that is loaded into the MATLAB workspace. The code was written and tested using MATLAB R2018b for Microsoft Windows ."
10.1038/s41597-020-0395-9,The version and parameter of all bioinformatics tools used in this work are described in the Methods section.
10.1038/s41597-020-0393-y,"The MATLAB Code used to generate the emission inventories with energy inventories is published below for transparency and verifiability. We take Anhui 2017 as an example. % Copyright 2019 Yuli Shan. All rights reserved. % Read emission factors and energy data from the excel files NCV = xlsread(‘Emission factors’, ‘NCV’, ‘A2:Q2’); %NCV i refers to Table 1 in Shan, et al . 17 NCV = repmat(NCV,68,1); CC = xlsread(‘Emission factors’, ‘CC’, ‘A2:Q2’); %CC i refers to Table 1 in Shan, et al . 17 CC = repmat(CC,47,1); O = xlsread(‘Emission factors’, ‘Oxygenation Efficiency’, ‘B2:R48’); %O ij refers to Table 3 in Shan, et al . 17 Energy = xlsread(‘Province Energy inventory 2017’, ‘Anhui’, ‘B3:R70’); % Convert physical energy consumption to calories E_PJ1 = Energy.* NCV; E_PJ1(1,:) = sum(E_PJ1(2:48,:)); E_PJ2 = E_PJ1; % Remove non-energy use from the total consumption E_NE = zeros (68,17); E_NE (23:27, 1:17) = repmat (E_PJ1(65,:),5,1) .* E_PJ2(23:27,:) ./ repmat (sum(E_PJ1(23:27,:)),5,1); E_NE (:,16) = E_PJ2 (:,16); E_NE(isnan(E_NE)) = 0; % Include energy combustion consumption during transformation process E_Trans = zeros (68,17); E_Trans (40,:) = E_PJ1(52,:) + E_PJ1(53,:); E_Trans (44,10:13) = E_PJ1(67,10:13) + E_PJ1(68,10:13); E = E_PJ2(1:48,:)-E_NE(1:48,:) + E_Trans(1:48,:); E(1,:) = sum(E(2:48,:)); % Calculate the energy-related emissions CO2_E = E (2:48,:) .* CC .* O/100; CO2_E(CO2_E < 0) = 0; % Calculate the process-related emissions EF_cement = 0.2906; Prod_cement = 133.94; %Prod_cement here refers to Anhui’s cement production in 2 017 CO2_cement = Prod_cement .* EF_cement; % Construct the emission inventory CO2 = zeros (48,19); CO2 (2:48,1:17) = CO2_E; CO2 (28,18) = CO2_cement; CO2 (:, 19) = sum(CO2,2); CO2 (1,:) = sum(CO2(2:48,:))"
10.1038/s41597-020-0391-0,"No custom code was used to generate this dataset. Simulations were run using G romacs 4.6.6, and all input files used to generate the trajectories are contained in the present dataset. The output data contains a number of binary files, all generated from G romacs . G romacs is available across many platforms and architectures (see Gromacs manual) and its output files can be read either from a compiled version of the freely-available code, or from other analysis modules, such as mdanalysis 41 or mdtraj 42 . The WHAM-reconstructed PMFs were obtained by running a command that can be found in each DIM_bead1-bead2/pmf/bsResult.xvg file."
10.1038/s41597-020-0396-8,"Custom code for loading in the training and testing datasets is available in the data repository for Python, R, and MATLAB. The Python code was tested in Python 3.6 and requires the following libraries: “os”, “h5py”, and “numpy”. The R code was tested in R 3.5.2 and requires the following libraries: “rhdf5”. Lastly, the MATLAB code was tested in MATLAB 2016. The codes are intended to load in the data from the hdf5 files in a user-friendly manner."
10.1038/s41597-020-0379-9,"The main template creation script used in this work is the antsMultivariateTemplateConstruction.sh script from ANTs. The auxiliary scripts for preprocessing and post processing are mostly implemented in ITK. The entire code-base used to build the FLAIR and NCCT atlases are provided in the GitHub repository ( https://github.com/deepthirajashekar/FLAIR-and-NCCT-atlas-for-elderly ). The pre-processing module contains the C++ code to perform the CT brain extraction and the registration-based interpolation. The atlas generation module is a bash script with the calls to the ANTs package and the specific parameters for FLAIR and NCCT. The post-processing module is a bash script that invokes various ANTs and FSL packages to generate the symmetric counterparts of the atlases and to obtain the deformation fields for the MNI atlases. The repository also includes the final templates (asymmetric FLAIR, symmetric FLAIR, asymmetric CT, symmetric CT), masks, and deformation fields to the MNI space. Additionally, the same information is made available on the official MIPLAB website ( https://www.ucalgary.ca/miplab/downloads )."
10.1038/s41597-020-0385-y,"All electronic structure data contained in this work was generated with the FHI-aims code 43 , 44 , 45 . The code is available for a license fee from https://aimsclub.fhi-berlin.mpg.de/aims_obtaining_simple.php . Parsing of outputs and data collection were performed with custom-made Python scripts, which will be available upon request. Finally, the published archive contains a tutorial detailing how to access the dataset."
10.1038/s41597-020-0397-7,We provide the code we used to validate our data in the form of a python script.
10.1038/s41597-019-0346-5,"The R code to download datasets directly from the hydrological databases and to combine them with the spring discharge time series obtained from the other sources (see above) is available at https://github.com/KarstHub/WoKaS . The code is provided in R programming language version 3.5.0, and commented following a recommended programming comment guidelines 24 . Comprehensive instructions on how to run the code and system requirements are provided by a “README” file included in the GitHub repository."
10.1038/s41597-020-0402-1,"The repository MeganePro Script Dataset 44 also hosts the MATLAB (version 2016b, MathWorks, Natick, MA, USA) code used for the post-processing procedure and the validation scripts used to obtain the results reported in this manuscript. The README file contained in the megane_postprocess.tgz archive describes and reports the scripts used for the data processing. The code used for the technical validation included in this paper and to produce the corresponding figures are within the meganepro_validation_ex245.tgz archive. These scripts are commented in a step-by-step manner, and they can be inspected and run using R studio (Version 1.1.442). The file ex2_validation.R contains the code used for the technical validation of exercise 2, while the file ex4_validation.R was used for technical validation of exercises 4–5. The original data cannot be released to ensure the privacy of the participants. However, the provided code contains all the steps taken during each stage of the data processing and technical validation. Furthermore, they can be adapted and applied to similar tasks and similar validation/statistical questions by interested researchers."
10.1038/s41597-020-0403-0,MISSING
10.1038/s41597-020-0392-z,"The Clinical Trial Processor (CTP) that was created by the Radiological Society of North America (RSNA) is available at: https://www.rsna.org/ctp.aspx . The RSNA CTP aligns meticulously to regulations for image anonymization, per the HIPAA Privacy Rule and the DICOM Working Group 18 Supplement 142. Being compatible with all commercially available picture archiving and communication systems (PACS), the RSNA CTP is designed to transport images to online data repositories where the anonymization process takes place. This program replaces patient tags in the DICOM files in a folder (and sub-folders) with anonymized tags that were assigned. ImageJ , a free software offered by the National Institutes of Health, USA, as a public domain Java processing program. The code for the software is accessible at: https://imagej.nih.gov/ij/ ."
10.1038/s41597-020-0401-2,"Fox Insight is built by several technology partners, each with its own policies on code availability. Routine longitudinal assessments are developed through a web-based application built on Ruby on Rails® software by Mondo Robot and the code base is proprietary 18 . One-time questionnaires are deployed through Qualtrics®; while the survey platform code is proprietary, Qualtrics® provides an open source application programming interface (API) for data processing. SQL code, developed at the Laboratory of Neuro Imaging, used to collate and process data is proprietary."
10.1038/s41597-020-0400-3,"The software versions, settings and parameters used are described below. (1) GCE, version 1.0.0, parameters used: kmer_freq_hash -k 21 -l reads.list -t 24 -i 5000000 -o 0 -p wild_yak & > kmer_freq.log; gce -f wild_yak.freq.stat -c 46 -g 155866493014 -m 1 -D 8 -b 1 > wild_yak.Table 2 > wild_yak.log. (2) Lighter, version 1.1.1, parameters used: -k 17 3000000000 -trim -t 20. (3) Platanus, version 1.2.4, parameters used: platanus assemble -o wild_yak -f <insert size 280 bp pair-end reads> <insert size 500 bp pair-end reads> <insert size 800 bp pair-end reads> -t 30 -m 500 -tmp temp; platanus scaffold -o wild_yak -c wild_yak_contig.fa -b wild_yak_contigBubble.fa -IP1 < insert size 280 bp pair-end reads> -a1 280 -IP2 <insert size 500 bp pair-end reads> -a2 500 -IP3 < insert size 800 bp pair-end reads > -a3 800 -OP4 <insert size 2 k pair-end reads> -a4 2000 -OP5 <insert size 5 k pair-end reads> -a5 5000 -OP6 <insert size 10 k pair-end reads> -a6 10000; platanus gap_close -o wild_yak -c wild_yak_scaffold.fa -IP1 <insert size 280 bp pair-end reads> -IP2 <insert size 500 bp pair-end reads> -IP3 <insert size 800 bp pair-end reads> -OP4 < insert size 2 k pair-end reads > -OP5 <insert size 5 k pair-end reads> -OP6 <insert size 10 k pair-end reads>. (4) Gap Closer, version 1.12, parameters used: -l 150 -t 30, in configFile: max_rd_len = 100; Paired-end libs: reverse_seq = 0, asm_flags = 3; Mate-pair libs: reverse_seq = 11, asm_flags = 2. (5) BUSCO, version 3: mammal default parameters, mammalia_odb9. (6) BWA, version 0.7.15-r1140: default parameters. (7) SAMtools, version 1.3; default parameters. (8) RepeatMasker, version 4.0.7 (with RepBase library release-20170127). (9) RepeatModeler, RepeatModeler-open-1.0.11. (10) TRF, version 4.09, parameters used: trf wild_yak.gapclose.fa 2 7 7 80 10 50 500 -d -h. (11) TBLASTN, version 2.5.0, parameter used: –e 1E-5. (12) GeneWise, version 2.4.1, parameters used: -tfor/-trev (-rfor for genes on forward strand and -trev for reverse strand) -gff. (13) Augustus, version 3.2.3, parameter used: -species = human. (14) Geneid, version 1.0, parameters used: -3 -P. (15) Genemark, version 3.9, parameter used: -f gff3. (16) Snap, version 2006-07-28, parameter used: –gff. (17) GlimmerHMM, version 3.0.4, default parameters. (18) EVM, version 1.1.1, default parameters. (19) InterProScan, version 5.25-64.0, parameters: -f tsv -iprlookup -goterms -pa -t p."
10.1038/s41597-020-0398-6,"The Modelica Buildings Library and EnergyPlus are freely available for download 30 , 31 . EnergyPlus runs on Windows, Mac OSX, and Linux operating systems. A Windows or Linux-based computer and Dymola solver are required to run Modelica, and Dymola can be licensed from Modelica Buildlings Library. HVACSim+ is also freely available, upon request from NIST, and has no operating system requirements. The HVACSim+ AHU model that was used in this work is available within the ASHRAE Research Project 1312 Results 29 . The data acquisition system that is implemented in FLEXLAB comprises a custom-built National Instruments platform utilizing distributed Compact RIOs (cRIOs), PC-based servers and workstations, and a Unix-based database running sMAP2.0 32 . Data are typically collected and recorded at a one-second (1 Hz) rate with averaging to one minute for database storage – suitable for most research purposes. 1 Hz data are stored in CSV files for use as needed. Control and data acquisition are implemented over the same architecture, with most control sequences running in the NI TestStand environment. A simple API also allows remote data acquisition and control using text-based messages, which allows use of other programming or scripting environments such as python or java. Experimental data are accessed from sMAP using a browser-based GUI or via text-based query. At the Flexible Research Platform, the data acquisition system utilizes Campbell Scientific data loggers. It includes measurements of the zone set point temperature and humidity, supply and return air temperature and flow rates, and energy consumption of individual components including compressor, condenser, supply fan, VAV reheating. Data are typically collected and recorded at a one-second rate with averaging thirty seconds for database storage. The data file format is CSV, with automated transfer from the data loggers to storage on an ORNL internal server, at time resolutions of 30 seconds, 1 min, 15 min, and 60 min intervals."
10.1038/s41597-020-0399-5,"DDA data were analyzed with Proteome Discoverer 2.2 and DIA data were analyzed with Spectronaut (Version: 13.2.19). Data comparison was performed with Microsoft Excel 2016, GraphPad Prism 6.07, and Venny (https://bioinfogp.cnb.csic.es › tools › venny)."
10.1038/s41597-020-0409-7,The code that supports the application of the results of this study is available in the GitHub repository https://github.com/hcwi/SemanticLMM and Zenodo 33 .
10.1038/s41597-020-0412-z,Preprocessing scripts for each of the omics datasets are available at the Github repository ( https://github.com/ConesaLab/MultiMip6 ).
10.1038/s41597-020-0414-x,The data were exported from SurveyCTO in Stata format. The data summarizing and files’ renaming were performed with Stata 16. The code used can be provided upon request.
10.1038/s41597-020-0408-8,"1. FASTQC v0.11.5 17 , options: default 2. Trimmomatic v0.36 18 , options: PE ILLUMINACLIP:Adapters.fa:2:30:10 HEADCROP:10 SLIDINGWINDOW:4:15 LEADING:20 TRAILING:20 MINLEN:25 3. Pseudo-it v1 20 , options: –PE1 R1.fastq.gz –PE2 R2.fastq.gz –iupac 5 REF NAME 4. Trinity v2.2.0 24 , default 5. Transrate v1.0.3 25 , options: –left –right –reference 6. CD-HIT-EST v4.6.4 26 , options: -c 0.95 -M 0 7. TransDecoder.LongOrfs v3.0.0 27 , default 8. hmmscan v v3.1b2 47 , options: default 9. TransDecoder.Predict v3.0.0 27 , options: –retain_pfam_hits–retain_blastp_hits –single_best_only 10. crb-blast v0.6.9 29 , options: -e 1.0e-05 11. BWA v0.7.15 21 , options: mem -M -R ‘@RG\tID:\tLB:\tPL:\tSM:’ 12. Samtools v1.3.1 23 , options: sort; fixmate, index 13. Picard v1.140 (“Picard Toolkit.” 2019. Broad Institute, GitHub Repository: http://broadinstitute.github.io/picard/ ), options: MarkDuplicates REMOVE_DUPLICATES = true ASSUME_SORTED = true 14. GATK v3.7 22 , options: RealignerTargetCreator; IndelRealigner 15. BCFtools v1.6 31 , options: mpileup -f REF.fa -Ou -a DP -b list|bcftools call -m -f GQ,GP 16. Plink v1.90b3.45 32 , options: –bp-space 10000 –pca 17. VCFtools v. 0.1.14 34 , vcftools –gzvcf target.vcf.gz–max-missing 1.0 –min-alleles 2 –max-alleles 2 –mac 2 –remove-indels–recode–stdout|bgzip -c > Filtered.biallelic.vcf.gz 18. fastStructure v1.0 35 , options: –full –cv = 10 19. Simon Martin’s scripts (Simon Martin, GitHub Repository: https://github.com/simonhmartin/genomics_general accessed on July 2018), options: parseVCF.py –skipIndels –minQual 30 –gtf flag = DP min = 20 (or 5); filterGenotypes.py –fixedDiffs–minCalls 8 20. Genepop v4.2 40 , 41 , options: 1. Hardy Weinberg Exact Tests and 2. Linkage disequilibrium, default 21. STRUCTURE v2.3.4 42 , 43 , 44 , options: burnin = 100,000, numreps = 1,000,000, usepopinfo = 0; inferalpha = 1; maxpops = 2."
10.1038/s41597-020-0417-7,"No code per se was developed for this article, as available tools were applied. The quality control pipeline used is available at bitbucket: https://bitbucket.org/RolfKaas/foodqcpipeline/ The CSI Phylogeny pipeline is available as a webtool through Center for Genomic Epidemiology ( www.genomicepidemiology.org ). Following options were used: Select min. depth at SNP positions: 10x. Select min. relative depth at SNP positions: 10%. Select minimum distance between SNPs (prune): 10. Select min. SNP quality: 30. Select min. read mapping quality: 25. Select min. Z-score: 1.96."
10.1038/s41597-020-0411-0,"R programming language and Matlab scripts 29 used to produce (Eq. 1 ) and validate (Eq. 2 ) this data as well as the Monte Carlo coefficient analysis are publicly available with a public access license through GitHub: https://github.com/JVFayne/HRAC-Precip_v1 . Due to the simplicity of the correction formula, the scripts can be easily translated to other programming languages; the free to use open source packages ‘raster’, ‘rgdal’, and ‘rgeos’ are required to use the R scripts, although the code functions of these packages that are used in the scripts (such as reading and writing geospatial files) do not change over the course of version updates, and many other programming languages such as Matlab and Python use similar packages to read and write raster files. Additional software packages are not required to produce these data."
10.1038/s41597-020-0407-9,"VASP version 5.3.5 used to perform DFT calculations is a proprietary code. The Bilbao Crystallographic Server (BCS) is freely available on-line at http://www.cryst.ehu.es . Fireworks, atomate, and pymatgen are python packages accessible on GitHub. Fireworks and atomate are released under a modified Berkeley Software Distribution (BSD) License. pymatgen is released under a Massachusetts Institute of Technology (MIT) License. Both MIT and BSD licenses are open-source and permit both commercial and non-commercial use. Our workflow code is included since atomate version 0.6.7 and our analysis code is available in pymatgen since v2019.2.4. We also use the following python packages in our analysis and Graphical Representation of Results: numpy, scipy, matplotlib, ipython, and jupyter 80 , 82 , 83 , 84 , 85 , 86 . These packages are freely available through the Python Package Index ( https://pypi.org/ ). Our code for recovering the same branch polarization from polarization calculations has been contributed to pymatgen under the pymatgen.analysis.ferroelectricity module. Our code for the DFT and polarization analysis workflows for performing polarization calculations has been contributed to atomate under the atomate.vasp.workflows.base.ferroelectric module. We also provide code for the interface that we used to view our candidates in aggregate. The web interface for the current work is hosted at http://blondegeek.github.io/ferroelectric_search_site/ . The code for the interface can be found at http://github.com/blondegeek/ferroelectric_search_site ."
10.1038/s41597-020-0413-y,"All the tools and source codes for the accessibility tools used for producing these datasets are openly available from the following links: MetropAccess-Reititin: helsinkimatrix.github.io/reititin DORA: helsinkimatrix.github.io/dora In addition, the full analytical workflow that was used to produce these matrices are published openly that makes it possible for anyone to assess the process pipeline: helsinkimatrix.github.io"
10.1038/s41597-020-0415-9,"All code associated with this project is available as open source. The code is available on GitHub under the BSD license ( https://github.com/rutishauserlab/recogmem-release-NWB ). Both Python and MATLAB scripts are included in this repository along with the matNWB API. We also provide a streamlined workflow as a Jupyter Notebook. Note, we tested our code with the following versions of the Python Packages: numpy (1.17.2), pandas (0.23.0), scipy (1.1.0), matplotlib (2.2.2), pynwb (1.1.0), hdmf (1.2.0), and seaborn (0.9.0). Detailed instructions on installing and running the code in this repository are found in our online documentation on GitHub."
10.1038/s41597-020-0418-6,"All reported crystal structure prediction calculations were performed using the USPEX code, which is based on evolutionary algorithms to predict structures with only elemental information 21 , 42 . Relaxation of structures and DFPT method were carried out by the VASP code 22 ."
10.1038/s41597-020-0410-1,We created the individual output datasets using geoprocessing scripts in the Python programming language and the arcpy package. All geoprocessing scripts are available on GitHub and are accessible through the Open Science Framework (OSF) repository 28 .
10.1038/s41597-020-0421-y,"All R codes (R3.5.3, https://www.r-project.org ) for creating provincial and gridded population datasets for China are stored in public repository Figshare 41 . Explanations are internalized in the script to help users with implementation."
10.1038/s41597-020-0424-8,"The software versions, settings, and parameters are described in Table 7 . If not mentioned otherwise, the command line at each step was executed using default settings."
10.1038/s41597-020-0422-x,MISSING
10.1038/s41597-020-0425-7,MISSING
10.1038/s41597-020-0419-5,No custom code was used in the study.
10.1038/s41597-020-0423-9,"All data and code necessary to replicate each stage of the mapping procedure, including training the CNN, mapping polygons, measuring microtopography, and validating results, is included in the same repository as the final data sets 28 . All code was written in MATLAB (version R2017b), and uses the Image Processing, Mapping, Neural Network, and Parallel Computing toolboxes."
10.1038/s41597-020-0429-3,Applications are available on the IS-EPOS Platform of Thematic Core Service Anthropogenic Hazards: tcs.ah-epos.eu. The source code of the platform itself is available at bit.ly/ISEPOSsourcecode.
10.1038/s41597-020-0428-4,MISSING
10.1038/s41597-020-0426-6,"The data analysis methods, software and associated parameters used in this study are described in the Methods section. Default parameters were applied if no parameter was described. No custom scripts were generated in this work."
10.1038/s41597-020-0427-5,"Software versions used are listed in Table 8 . Code for sequence quality control and trimming, shotgun and 16S metagenomics profiling and generation of figures in this paper is freely available and thoroughly documented at https://gitlab.com/JoanML/colonbiome-pilot . This repository includes instructions for the analysis and reproduction of the figures on this paper from the publicly available samples, as well as pipelines used for the analysis. This repository is arranged in folders, each containing a README: • qc : Scripts for quality control and preprocessing of samples • analysis_shotgun : Scripts to run softwares for metagenomics analysis • regions_16s : In-house scripts for splitting IonTorrent reads into new FASTQ files • analysis_16s : DADA2 pipeline adapted to this dataset • assembly : Scripts to run the assembly, binning and quality control software • figures : Scripts used to generate the figures in this manuscript • shannon_index_subsamples : Scripts used to compute alpha diversity in subsampled FASTQs"
10.1038/s41597-020-0431-9,MISSING
10.1038/s41597-020-0406-x,The code for (i) trimming the raw tracks and (ii) the state space filtering have been made available on the SCAR github page ( https://github.com/SCAR/RAATD ). Additional information is provided in the Technical Validation section below.
10.1038/s41597-020-0436-4,MISSING
10.1038/s41597-020-0435-5,MISSING
10.1038/s41597-020-0433-7,"The GDHY aligned version v1.2 + v1.3 dataset is produced by combining versions 1.2 and 1.3 using a purpose-build program written in Fortran90 with the standard mathematical library. The program code was compiled on the MacOS platform but is potentially applicable to other platforms (e.g., Windows and UNIX). The code is available from the corresponding author upon request."
10.1038/s41597-020-0441-7,"In the study, we did not use any custom specific code. The command line for each step is executed as indicated for each step of all bioinformatics procedures."
10.1038/s41597-020-0440-8,The MATALB ( https://www.mathworks.com/ ) program for ECG denoising is put under https://github.com/zheng120/PVCVTECGDenoising .
10.1038/s41597-020-0448-0,All code used to clean data has been uploaded to the repository and is also on our Github page: https://github.com/beoutbreakprepared/nCoV2019/tree/master/covid19/src .
10.1038/s41597-020-0439-1,The example Jupyter notebook for calculation of dataset statistics and getting specific images (e.g. images of the specific sample) are available with the dataset.
10.1038/s41597-020-0443-5,The codes for SAR processing using the ISCE algorithms and for generating proxy maps are available via the following GitHub repositories: https://github.com/isce-framework/isce2 and https://github.com/earthobservatory/slcp2pm .
10.1038/s41597-020-0449-z,"The software in this study has been described 34 , 35 , 44 . The workflows to analyze SWATH-MS data have been published 45 and are described on http://www.openswath.org ."
10.1038/s41597-020-0437-3,Code is available via https://github.com/hooge104/2020_global_nematode_dataset .
10.1038/s41597-020-0442-6,"Code used to produce files within Dataset 1 12 : • Raw Penguin Watch time-lapse photographs are renamed and resized using an R (currently v3.6.0) script. The code is publically available via GitHub at https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Penguin_Watch_ImageProcessingScript.R . A static version (written using v3.4.1) is archived on Figshare 26 . • Raw Penguin Watch volunteer classifications (xy coordinate clicks) were clustered into ‘consensus clicks’ using agglomerative hierarchical clustering (Fig. 1 , left, ‘Step 2’) 10 , 14 . The aggregation algorithm is written in Python (v 2.7) and can be found on GitHub at https://github.com/zooniverse/aggregation/blob/master/penguins/aggregate.py . A static version of this script is also archived on Figshare 14 . Code used to produce files within Dataset 2 15 : • The Kraken Script (Fig. 1 , left, ‘Step 3’; output = Kraken Files ) is written in R (v3.6.0); it can be accessed through GitHub at https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Kraken_Script.R , and a static version is archived on Zenodo 27 . • The Narwhal Script (Fig. 1 , left, ‘Step 4’; output = Narwhal Files ) is written in R (v3.6.0); it can be accessed through GitHub at https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Script.R and a static version is archived on Zenodo 28 . • The Narwhal Plotting Script (output = Narwhal Plots – graphs displaying Narwhal summary statistics) is written in R (v3.6.0); it can be accessed through GitHub at https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Plotting_Script.R and a static version is archived on Zenodo 29 . • The Pengbot model 11 , associated code and instructions, and the dataset used to train the neural network can be found at the following address: https://www.robots.ox.ac.uk/~vgg/data/penguins/ . • The Pengbot Counting Script (Fig. 1 , right, ‘Step 2’ and ‘Step 3’) is written in R (v3.6.0); it can be accessed through GitHub at https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Pengbot_Counting_Script.R and a static version is archived on Zenodo 30 ."
10.1038/s41597-020-0446-2,"Fortran code: getStat.f , getStatDir.f , getHsEx.f The Fortran code developed to derive the COWCLIP statistics can be requested via the COWCLIP website ( https://cowclip.org/data-access ). The code - as described in the Data Generation Method section, consists of a set of commands ( getStat.f, getStatDir.f and getHsEx.f ) which can be compiled with a Fortran compiler, linked against netCDF4 and HDF5 libraries. The documentation for setup, usage and requirements for the code is described within the technical reports 30 , 31 , 32 that complement this manuscript. These commands can be executed by COWCLIP contributors to generate the set of wave statistics from their raw simulations. With the specific purpose of sharing in an open data format, and adhering to relevant data standards, the processed data is given in netCDF format, the global metadata attributes from the submitted netCDF data recorded, and additional information added where possible to ensure both CF Conventions and Attribute Convention for Dataset Discovery (‘ACDD’) standards compliance. Python code: COWCLIP_stats_mat2nc.py and COWCLIP_extremes_mat2nc.py The Python commands developed to produce the final standardised netCDF files (which comprise this data publication) are available in the COWCLIP website ( https://cowclip.org/data-access ). The code is written in Python 3 as well as standard python modules, depends on numpy, pandas, scipy, tables and netCDF4 python modules. Both python scripts require setting of the descriptive metadata location (path to file COWCLIP-GlobalProj-Metadata-merged.xlsx, structured to be readily usable with python’s pandas library), and the location of the Matlab matrix (.mat) and script (.m) files for the standardised data. The python scripts take as command line arguments the climate modelling group (e.g., ‘LBNL’) and time-slice simulation period (i.e., ‘Historical’ or ‘Future’). They produce, where possible, CF and ACDD standards-compliant output files in a Data Reference Syntax (DRS) structure akin to that used in CMIP and CORDEX modelling projects."
10.1038/s41597-020-0452-4,"ToposPro (version 5.0) is available at http://topospro.com ; Materials Studio 2018 (version 18.1.0.2017) is a commercial program package, which is available at BIOVIA."
10.1038/s41597-020-0434-6,"We selected Python 3 as main programming language and identify the following dependencies of SynD: Pandas 0.22, Numpy 1.15, and NILMTK 0.3. We aimed at providing compatibility to the latest versions of these software packages and released code examples, an extensive user guide, and supplemental material under the licence Attribution 4.0 International ( https://creativecommons.org/licenses/by/4.0/ ) on our GitHub repository ( https://github.com/klemenjak/synd/ ). Along with the dataset SynD, we release the first public version of our dataset generator tool via figshare 22 . This tool was used to create SynD and can also serve to generate new datasets on-demand. We release this early version of our tool under the licence CC0 ( https://creativecommons.org/publicdomain/zero/1.0/ )."
10.1038/s41597-020-0451-5,A GitHub record of this project is accessible at https://github.com/gimur/leishmaniadb . It includes the Shiny app code used to verify the coordinates of each report.
10.1038/s41597-020-0453-3,"Code archives for CRU TS releases are available on the CRU website, accompanying each release. The automatic archiving of code for each release was introduced recently, so archives are not available for releases prior to v4.03."
10.1038/s41597-020-0450-6,"The CLM-PDAF setup is available through the Terrestrial System Modelling Platform (TSMP). TSMP is provided through a git repository available at the model’s website ( https://www.terrsysmp.org/ ). The users are required to register to the git repository to get access to the code, pre- and post-processing tools and documentations for installing the code with examples setups. TSMP is released without the component models. For the coupled CLM-PDAF configuration, the code for PDAF library is available through website (pdaf.awi.de) which also provide links to the documentation and the source code. The CLM (version 3.5), as used in this study, is available as an open source model through the official CLM website ( http://www.cgd.ucar.edu/tss/clm/distribution/clm3.5/index.html ) which offers all links to documentation, source code, and input data for the stand-alone version release of CLM."
10.1038/s41597-020-0444-4,"Cropland biomass maps were created in the R statistical computing environment 80 . All other coding was done in Google Earth Engine 81 (GEE), wherein our workflow consisted of 18 interconnected scripts. All code can be found on GitHub ( https://github.com/sethspawn/globalBiomassC ) and permanently archived by Zenodo 82 ."
10.1038/s41597-020-0438-2,"The population re-analysis of 3K-RG dataset and 12 genome assemblies were obtained using several publicly available software packages. To allow researchers to precisely repeat any steps, the settings and the parameters used are provided below: Population structure: ADMIXTURE was run with default options. The R scripts for further population structure analysis, including setting up CLUMPP files, can be found in Github repository https://github.com/dchebotarov/Q-aggr . Genome size estimation: The K-mer and GCE program were employed for genome size estimation. Command line: kmer_freq_hash -k (13-17) -l genome.list -a 10 -d 10 -t 8 -i 400000000 -o 0 -p genom_kmer(13-17) & > genome_kmer(13-17)_freq.log , and gce -f genom _kmer(13-17).freq.stat -c $peak -g #amount -m 1 -D 8 -b 1 -H 1 > genome.Table 2 > genom_kmer(13-17).log Genome assembly: (1) MECAT2 : all parameters were set to the defaults. Command line: mecat.pl config_file.txt , mecat.pl correct config_file.txt and mecat.pl assemble config_file.txt (2) Canu1.5 : all parameters were set to the defaults. Command line: canu -d canu -p canu genomeSize = 400 m -pacbio-raw rawreads.fasta (3) FALCON : all parameters were set to the defaults. Command line: fc_run.py fc_run.cfg & > fc_run.out (4) GPM : manual edit with merging de novo assemblies from MECAT2 , Canu1 . 5 , and FALCON Polishing: (1) arrow : all parameters were set to the defaults except alignment length = 500 bp . The arrow polish was carried out by the SMRT Link v6.0 webpage ( https://www.pacb.com/support/software-downloads/ ). (2) pilon1.18 : all parameters were set to the defaults. BUSCO: The BUSCO3.0 version was employed in this study. Command line: run_BUSCO.py -i genome.fasta -o genome -l embryophyta_odb9 -m genome -c 16 RepeatMasker: The repeat sequences were employed with the library rice7.0.0_liban in-house. Command line: RepeatMasker -pa 24 -x -no_is -nolow -cutoff 250 -lib rice7.0.0.liban.txt genome.fasta"
10.1038/s41597-020-0458-y,"Code for IDR analysis, SE calling, filtering, heatmap generation is available on Github 50 ."
10.1038/s41597-020-0445-3,"Code for working with the LiPD data files, including basic functionality in three programming languages, is available on GitHub ( https://github.com/nickmckay/LiPD-utilities ). MATLAB code used to map site locations (Fig. 2 ) and to compute composites (Figs. 4 – 8 ) is available at https://github.com/nickmckay/Temperature12k under an MIT license 581 ."
10.1038/s41597-020-0461-3,"All validation files, procedures and scripts are provided in Padovani and Borges 52 . All files are available in Unicode (UTF-8) format."
10.1038/s41597-020-0459-x,Data management was performed using R computing language 61 . The functions developed to manage and flag the dataset are permanently available in a Github repository ( https://github.com/jorgeassis/marineforestsDB ).
10.1038/s41597-020-0462-2,"Most of the temporal profiles data processing has been done using the software R version 3.5 and Python version 3.6. The computation of heating degree days maps was based on the 2 m air temperature of ECMWF ERA5 re-analysis 62 and produced by using IDL8.6 programming software. Further computations, such as mapping sectors and countries have been performed using Microsoft Access 2010. The implementation of the EDGAR_temporal_profiles_r1 library into the Emissions Database for Global Atmospheric Research has been developed using a dedicated EDGAR development tool of the Joint Research Centre named EOLO based on Php and Microsoft SQL Server. This system cannot be accessed outside the institution but further details can be provided upon request. All scripts related with this work are available at figshare 20 ."
10.1038/s41597-020-0455-1,The Python code to identify possible CvT data sources in literature is available at our GitHub repository. Download of the database is required to train the model.
10.1038/s41597-020-0465-z,Code is available for download at https://doi.org/10.5066/P9VKJ23R . There are no restrictions to the access or use of this code. Code was implemented in R (version 3.6.1; https://r-project.org ) using the package shiny (version 1.3.2).
10.1038/s41597-020-0467-x,"Usage demonstration scripts and the code used for the preparation, pre-processing and technical validation of the Localize-MI dataset are publicly available at https://github.com/iTCf/mikulan_et_al_2020 ."
10.1038/s41597-020-0468-9,"The bioinformatic tools used in this work, including versions, settings and parameters, have been described in the Methods section. Default parameters were applied if no parameters were mentioned for a tool. The scripts used in generating the orthoMCL results and preparing input sequences for PhyML were deployed on the Github repository ( https://github.com/comery/For_soptted_hyena_genome )."
10.1038/s41597-020-0464-0,"The code used to calculate the indices is available via: https://cran.r-project.org/web/packages/ClimInd/index.html . The R scripts necessary to calculate the different indices, from the ECA&D and ERA5 gridded datasets in a 3-D array format, can be accessed via: https://gitlab.com/fergusrg/indecis_example ."
10.1038/s41597-020-0466-y,MISSING
10.1038/s41597-020-0463-1,MISSING
10.1038/s41597-020-0469-8,"The code used to extract and process the OSM data is publicly available through the Figshare repository 35 . The code consists of four R programming language scripts (R version 3.6.2) numbered 1-4: the first extracts the latest OSM data; the second processes the data into wind and solar farms; the third contains the power models, and the fourth conducts the technical validation. Each script includes text that guides the user through the process and details the functions being performed. The README file, included with the scripts, provides more detail on rerunning the analyses. We regret that we cannot provide the full, geospatial Wiki Solar dataset as it was provided on the condition of confidentiality. We have provided a copy of the spatial join between the Wiki Solar dataset and the processed OSM data, with all geospatial data stripped out. This can be used as an input to the power estimations on its own. However, the power estimation can be rerun omitting these data if users require models trained on truly open-access data. Alternatively, users can contact the Wiki Solar data provider. When we ran this analysis, the accuracy of the solar model to predict unseen data in the two remaining independent datasets increased (RMSE = 3.153742, R 2 = 0.7442277, MAE = 1.386226, n = 1889). However, the input data for this model only managed to capture 96.8% of the variation in panel area in the wider OSM dataset and subsequently predicted 253 occurrences of power capacities outside of the model range. For this reason, we elected to keep the Wiki Solar data in the final model."
10.1038/s41597-020-0470-2,"The sequence data were generated using the software provided by the sequencing platform manufacturer and processed with commands provided for the public software cited in the manuscript. No custom computer code was generated in this work. The following bioinformatic tools and versions were used to generate all results as described in the main text. Default parameters were used if not stated. 1. CellQuest version 5.1. 2. GCE (Genome Characteristics Estimation) version 1.0.0 was used to estimate genome size, ftp://ftp.genomics.org.cn/pub/gce/. 3. ALLPATHS-LG version 48894 was used for genome assembly, http://software.broadinstitute.org/allpaths-lg/blog/ . 4. SSPACE version 3.0 was used for genome assembly scaffolding, https://www.baseclear.com/services/bioinformatics/basetools/sspace-standard/ . 5. GapCloser version 1.12 was used to fill the gaps between scaffolds, http://soap.genomics.org.cn/about.html . 6. BWA (Burrows-Wheeler Aligner) version 0.7.15 was used for short read mapping, https://github.com/lh3/bwa/ . 7. BUSCO (Benchmarking Universal Single-Copy Orthologs) was used to check the completeness of the genome assembly, with coverage ≥ 90% and identity ≥ 90% parameters, https://gitlab.com/ezlab/busco/ . 8. Trinity version v2.0.6 was used to assemble the RNA sequencing reads, https://github.com/trinityrnaseq/trinityrnaseq . 9. GMAP version 2014-10-2 was used to map the assembled transcripts to the genome sequence with coverage ≥ 90% and identity ≥ 90% parameters, http://research-pub.gene.com/gmap . 10 LACHESIS was used for ultra-long-range scaffolding with Hi-C data with CLUSTER_N = 12, CLUSTER_MIN_RE_SITES = 300, CLUSTER_MAX_LINK_DENSITY = 8, ORDER_MIN_N_RES_IN_TRUNK = 100, and ORDER_MIN_N_RES_IN_SHREDS = 10 parameters, http://shendurelab.github.io/LACHESIS/ . 11. RepeatMasker version 4.0.3 was used to mask the repeat sequences in the genome, http://repeatmasker.org/ . 12. Augustus version 2.7 was used for de novo gene prediction, http://augustus.gobics.de/ . 13. SNAP version 2006-07-28 was used for de novo gene prediction, https://github.com/KorfLab/SNAP . 14. Exonerate version 2.2.0 was used to align proteins to the genome sequence, https://www.ebi.ac.uk/~guy/exonerate/ . 15. GeneWise version 2-2-0 was used to predict gene structure using similar protein sequences, http://www.ebi.ac.uk/~birney/wise2 . 16. GenBlastA version 1.0.1 was used to link the high-scoring pairs (HSPs), http://genome.sfu.ca/genblast/download.html . 17. PASA (Program to Assemble Spliced Alignments) was used to exploit gene structure using transcripts, http://pasapipeline.github.io/ . 18. EVidenceModeler (EVM) version 1.1.1 was used to combine gene predictions generated from different methods into consensus gene structures, http://evidencemodeler.github.io/ . 19. BLAST version 2.2.28 was used to find regions of local similarity between sequences, https://blast.ncbi.nlm.nih.gov/Blast.cgi/ . 20. KEGG (Kyoto Encyclopedia of Genes and Genomes), https://www.kegg.jp/ . 21. Pfam database: http://pfam.xfam.org/ . 22. Blast2GO: https://www.blast2go.com/ . 23. The tRNAscan-SE algorithm (version 1.23) was used for the identification of tRNA genes, http://lowelab.ucsc.edu/tRNAscan-SE . 24. The RNAmmer algorithm was used for the identification of rRNA genes, http://www.cbs.dtu.dk/services/RNAmmer/ . 25. Snoscan version 1.0 was used for the identification of snoRNA genes, http://lowelab.ucsc.edu/snoscan/ . 26. INFERNAL version 1.1.2 was used for the identification of snRNA genes, http://eddylab.org/infernal/ . 27. Rfam database release 9.1, rfam.xfam.org/. 28. miRBase release 21, www.mirbase.org/ . 29. psRNATarget server; parameters: maximum expectation = 3.0, length for complementary scoring = 20 bp, target accessibility – allowed maximum energy to unpair the target site = 25.0, flanking length around the target site for target accessibility analysis: 17 bp upstream and 13 bp downstream, and range of central mismatch leading to translational inhibition = 9~11 bp, http://plantgrn.noble.org/psRNATarget/ . 30 Rice Genome Annotation Project Database, http://rice.plantbiology.msu.edu/ . 31. agriGO server, http://bioinfor.cau.edu.cn/agiGO/ . 32. BlastKOALA, http://kegg.jp/blastkoala/ . 33. Repbase TE library (version released on January 31, 2014). 34. RepeatProteinMask, http://www.repeatmasker.org/RepeatProteinMask.html . 35. RepeatModeler version 1.0.10 was used for de novo repeat family identification and modelling, http://www.repeatmasker.org/RepeatModeler/ . 36. RECON version 1.08. 37. RepeatScout version 1.0.5. 38. LTR_STRUC was used for the identification of LTR retrotransposons, http://www.mcdonaldlab.biology.gatech.edu/ltr_struc.htm . 39. ClustalW was used to perform multiple sequence alignment, https://www.genome.jp/tools-bin/clustalw . 40. Gypsy Database (GyDB), http://gydb.org/ . 41. Tandem Repeat Finder (TRF) version 4.07b was used to find the tandem repeats in the genome with the parameters Match = 2, Mismatch = 7, Delta = 7, PM = 80, PI = 10, Minscore = 50, and MaxPeriod = 12, https://tandem.bu.edu/trf/trf.html . 42. RepeatMasker version 4.0.3 was used to mask the repeat sequences in the genome with the parameter -noint, http://www.repeatmasker.org . 43. The MISA (MIcroSAtellite) identification tool was used for the identification and localization of microsatellites, http://pgrc.ipk-gatersleben.de/misa/ . 44. Trimmomatic version 0.33 was used for the quality filtering of sequencing reads, http://www.usadellab.org/cms/index.php?page=trimmomatic ."
10.1038/s41597-020-0472-0,MISSING
10.1038/s41597-020-0456-0,MISSING
10.1038/s41597-020-0473-z,"All electronic structure calculations were computed with the Gaussian 09 [cite] or ORCA electronic structure packages [cite]. All molecular dynamics simulations for sampling were carried out with the atomic simulation environment (ASE). The analysis and active learning scripts are available upon request. The C++/CUDA implementation of our ANI code is available online in binary format [ref], but source code is not publicly released. Alternatively a PyTorch version ANI is available as open source. [ https://github.com/aiqm/torchani ]."
10.1038/s41597-020-0475-x,MISSING
10.1038/s41597-020-0480-0,"The software mentioned in methods section are described blow. 1) falcon : version 1.2.4, parameters: (length_cutoff=5000, length_cutoff_pr=10000); 2) pbalign : contained in SMRT Link 4 toolkit, parameters:(--algorithm=blasr); 3) variantCaller : contained in SMRT Link 4 toolkit, parameters:(--algorithm=arrow); 4) bowtie : verion 1.1.2, default parameters; 5) SSPACE-STANDARD : version 3.0, parameters:(-p 1 -g 2); 6) SSPACE-LongRead : version 1-1, default parameters; 7) blasr : version 1.3.1.121193, default parameters; 8) GATK : version 4.0.0.0, default parameters; 9) nucmer : contained in mummer version 4.0.0beta2, default parameters; 10) BUSCO : version 3, parameters:(-l embryophyta_odb9); 11) Tandem repeats finder : version 409, default parameters; 12) RepeatMasker : version open-4.0.7, parameters: (-nolow -no_is -norna -engine ncbi -parallel 1); 13) RepeatProteinMask : version open-4.0.7, parameters:(-engine ncbi -noLowSimple -pvalue 0.0001); 14) RepeatModeler : version open-1.0.11, parameters:(-engine ncbi -pa 9); 15) ltr_finder : version 1.06, parameters:(-w 2); 16) GMAP : version 2017-05-08, parameters:(-z sense_force -f samse -n 0); 17) TransDecoder : version 4.0.1, default parameters; 18) augustus : version 3.2.3, default parameters; 19) tblastn : version 2.6.0+, parameters:(-evalue 1e-5 -seg no); 20) GeneWise : version 2.4.1, default parameters; 21) hisat2 : version 2.1.0, parameters:(--dta --no-discordant --no-mixed); 22) StringTie : version 1.2.4, default parameters; 23) EVidenceModeler : version 1.1.1, parameters:(--weights:PROTEIN GeneWise:5, TRANSCRIPT StringTie:8, ABINITIO_PREDICTION AUGUSTUS:2, OTHER_PREDICTION transdecoder:20); 24) Trinity : version 2.4.0, parameters: (--group_pairs_distance 500 --path_reinforcement_distance 80 --min_glue 3 --min_kmer_cov 3 --min_contig_length 100 --KMER_SIZE 25 --bflyHeapSpaceInit 1G --bflyHeapSpaceMax 4G --bfly_opts “-V 5 --edge-thr=0.1 --stderr”); 25) PASA : version 2.1.0, default parameters; 26) MCScanX : latest version, default parameters; 27) tRNAscan-SE : version 1.3.1, default parameters; 28) infernal: version 1.1.1, default parameters. 28) tRNAscan-SE : version 1.3.1, default parameters; 28) infernal: version 1.1.1, default parameters."
10.1038/s41597-020-0476-9,Versions and parameters of all the bioinformatic tools that were used in this work are described in the “Methods” section.
10.1038/s41597-020-0460-4,The code used to generate the data is freely available on GitHub under the MIT license 43 . Further details on how to use it to generate the data are given in the Usage Notes.
10.1038/s41597-020-0482-y,An R script and wrapper for the data analyses presented here are available as.rmd and.rproj files on the Open Science Framework (OSF) platform together with the questionnaires and data files 13 . These can be used to generate exploratory factor analyses and estimates of split-half reliabilities for the focal scales (SBS and the EDAS) independently in each country. All data presented in tables and figures in the manuscript can be easily reproduced using the provided code. Those interested in following up on our analyses can use our base-script as a starting point.
10.1038/s41597-020-0485-8,"The pypofatu Python package is open-source software, maintained on GitHub and distributed via the Python Package Index ( https://pypi.org/project/pypofatu ), with released versions archived with Zenodo 182 . The two output formats listed above are created and stored as part of the GitHub repository where the dataset is curated ( https://github.com/pofatu/pofatu-data/releases/tag/v1.0.0 ), and each release of the dataset is also archived on Zenodo 176 . Additionally, the dataset is loaded into a clld 183 web application, providing an online, browsable user interface for “window-shopping”, before downloading and using the dataset locally. Released versions of the Pofatu dataset meet the requirements on FAIR data as laid out by Wilkinson and colleagues 177 . The data is findable thanks to Zenodo’s integration in the research data landscape on the web, and the metadata we provide. It is accessible via the DOI doled out by Zenodo. “It is interoperable due to the open standards” used to encode the data and reusable because it is provided under an open CC-BY license."
10.1038/s41597-020-0484-9,CoryneRegNet7 code is available on GitHub: https://github.com/baumbachlab/CoryneRegNet7 .
10.1038/s41597-020-0481-z,MISSING
10.1038/s41597-020-0492-9,"All the data processed in this dataset used the WASS (“ An open-source pipeline for 3-D stereo reconstruction of ocean waves ”) code version 1.4 provided by 9 and available at http://www.dais.unive.it/wass (the WASS is a full-frame and dense-matching evolution of the previous stereo code developed by 8 , 26 and further refined by 10 ). This tool is able to (i) reconstruct the sea surface by means of a dense cloud of 3-D points from stereo images, (ii) compute the extrinsic parameters of the stereo rig without the need of tedious calibration on-the field, and (iii) provide the parameters necessary to transform the wave data to a local geographical reference system. The fast 3-D dense stereo reconstruction is based on the OpenCV library 29 and analyzes the photometrically distinctive features of the water surface to eventually provide the 3-D surface elevation map. When the stereo images are processed by WASS, the generated 3-D point cloud is re-sampled in a uniform grid and the final space-time surface elevation is stored in a standard NetCDF4 file. We developed an additional tool ( https://github.com/fbergama/wassncplot ) that provides the mapping between each image pixel (either from left or right camera) to the 3-D geographical reference frame of the grid stored in the NetCDF4 file. This way, any further processing can take into account both the luminance of each pixel and its 3-D location onto the sea-surface. Possible applications include the analysis of white-capped areas, the 3-D tracking of floating objects and measuring the shape and extent of individual waves. Note that this mapping is in general one-way (ie. from image to 3-D space) since cameras are angled with respect to the sea-surface and some 3-D points in the back-side of the waves are occluded by the crests. In this context, the wassncplot tool computes the correct intersection between the 3-D ray in space passing through the camera center and a pixel \(p\) with the gridded surface. Since multiple intersections may occur, only the one nearest to the camera is considered for the mapping. Operatively, the operation is performed by projecting each triangle of the grid to the image plane corresponding to one of the two stereo cameras. The coordinate of each grid vertex is interpolated considering the barycentric coordinates within each triangle corrected by the depth of the point. A Z-buffer is used to discard occluded points and produce the final mapping as a 3-dimensional data cube \(D\) of size ( \(W\times H\times 3\) ) in which the first two dimensions spans the image space and the third the 3-D coordinates of a point (ie. \({D}_{i,j,0},{D}_{i,j,1},{D}_{i,j,2}\) are the \(x,y,z\) coordinates of the 3-D point corresponding into the pixel i , j in the image). Optionally, the tool can produce a rendering of the 3-D surface grid on top of the original images for a qualitative evaluation of the reconstructed data (See Fig. 6 )."
10.1038/s41597-020-0487-6,The algorithm generating all possible cis and trans spliced peptides was originally described by Liepe et al . 56 . Scripts for MySQL database setup have been deposited in the MySQL database dump to the Mendeley repository with the dataset access doi:10.17632/nr7cs764rc.1 58 .
10.1038/s41597-020-0489-4,"All codes associated with the data portal are open source; they are available on GitHub under the MIT license. We created the codes in two popular languages, JavaScript for the formal OCDP website and Python to facilitate data availability and interpretation. The GitHub repository hosts these codes. The 12 Python programs are designed for data reading, plotting, mapping and exporting on local server ( https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/pythonCode ). All of these Python files have also been uploaded to the Figshare data repository 50 . Users can directly clone and download the repository to their local machine and run the Python programs in the Jupyter notebook environment to manipulate the data. HTML5 and JavaScript files for the formal data portal can be found in Figshare 50 and GitHub ( https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/JavaScripts ). Users, who are interested in web application development, can download and use them for their own projects."
10.1038/s41597-020-0490-y,"To assist users with accessing and querying our database, we have written an accompanying RMarkdown document (Supplementary File 1 ) that provides a commented workflow for utilizing CetuaOPEN with the RSQLite 31 and dplyr 32 packages in R."
10.1038/s41597-020-0488-5,The official HEMCO code is a collection of FORTRAN-90 routines that are freely available at http://wiki.geos-chem.org/HEMCO . The official HEMCO code with a few necessary changes (based on HEMCO v2.1) to run at the standalone mode are available at Peking University Atmospheric Chemistry & Modeling Group ( http://www.phy.pku.edu.cn/~acm/acmProduct.php#NATURAL-EMISSION ) and Figshare 29 .
10.1038/s41597-020-0494-7,MISSING
10.1038/s41597-020-0474-y,"The CAVD, BVSE and hierarchical computational codes have been integrated in SPSE, and they can only be run if the user has access to a SPSE account. The SPSE manuals are available in the website: https://www.bmaterials.cn/static/help/SPSE-UserManuals.pdf . Source codes of SPSE are freely available for download at figshare 73 ."
10.1038/s41597-020-0491-x,Source codes of CAVD are freely accessed in the repository: https://gitee.com/shuhebing/cavd .
10.1038/s41597-020-0495-6,The code for dataset preparation is not intended to be released as it does not entail any potential for reusability. We provide the stratified sampling routine in Supplementary File 1 to allow users to create stratification folds based on user-defined preferences.
10.1038/s41597-020-0483-x,"The code used to clean and analyze the data is published in Zenodo (ref. 12 ) with the corresponding DOIs: • v1.1 (specific version): https://doi.org/10.5281/zenodo.3737085 • Concept (all versions): https://doi.org/10.5281/zenodo.3678854 The code is available under the MIT License. The repository contains four notebooks that reproduce the following workflow: Step 1: Query the EIA database for raw demand data Step 2: Screen the data for anomalous values Step 3: Impute missing and anomalous values with the Multiple Imputation by Chained Equations (MICE) procedure Step 4: Distribute the imputation results to publication-ready files Step 1 , Step 2 , and Step 4 are based on python code, were written in python 13 , and use the pandas 14 and numpy 15 , 16 packages. Step 3 is written in the R programming language 11 and relies on the mice package 10 . The versions of python, high-level python packages, R, and high-level R packages used in the analysis are: • python = 3.7.3 numpy = 1.16.2 pandas = 0.24.2 urllib3 = 1.24.1 • r-base = 3.5.1 r-mice = 3.6.0 r-dplyr = 0.7.6 r-data.table = 1.11.4 r-zoo = 1.8_3 parallel = 20200122 r-reshape2 = 1.4.3 r-markdown = 0.8 r-rmarkdown = 1.10 r-lubridate = 1.7.4 multiprocess = 0.70.9 The archived code used to clean and analyze the data is supplemented by an archived version of the Conda computing environment used for the analysis (ref. 17 ) with the corresponding DOIs: • v1.0 (specific version): https://doi.org/10.5281/zenodo.3736784 • Concept (all versions): https://doi.org/10.5281/zenodo.3736783 The data cleaning was run on Mac OSX 10.14.6. A complete list of every package present in the Conda computing environment can be found in the package-list.txt file in the code archive. The archived Conda environment can be used on Mac OSX systems circa 2020. Instructions for setting up the environment are included in both the code and computing environment archives. For other operating systems, we include an environment.yml file containing the high-level packages mentioned above. Instructions for setting up a Conda computing environment using the environment.yml file are also included in the code archive."
10.1038/s41597-020-0504-9,No computer codes were used to generate the dataset. A commercially available spreadsheet program was used to collect the data and do quality checks.
10.1038/s41597-020-0501-z,The data analysis was performed in R version 3.5.1 24 . The example of the custom code  used to disaggregate cassava production and harvested area data is publically available via GitHub at https://github.com/aniaszy/CassavaMap .
10.1038/s41597-020-0499-2,The scripts used in data processing are available from Figshare 35 (see “Code used in data processing”).
10.1038/s41597-020-0496-5,MISSING
10.1038/s41597-020-0493-8,"The codes for the pre-processing steps above were assembled in the released Dataset. We also shared a package of code for running the pre-processing pipeline and the technical validation, and can be accessed in figshare 39 ."
10.1038/s41597-020-0478-7,"We used the following open source software packages to compute the full processing chain: ● Geospatial Data Abstraction Library (GDAL, version number 2.1.2) 51 , 52 . ● Geographic Resources Analysis Support System software (GRASS, version number 7.4.0) 33 , 53 , 54 . ● Processing Kernel for geospatial data (PKTOOLS, version number 2.6.3) 55 , 56 . ● R: a language and environment for statistical computing 57 , with the following libraries: randomForestSRC 34 , 35 , geoR 58 , plyr 59 , 60 , moments 61 , data.table 62 , reshape 63 , 64 , dplyr 65 , ggplot2 66 , 67 All of these tools provide fast and scalable functions for raster-based workflows that are easily automated using a scripting language, such as Bash or Python 68 . They also allow for the processing of very large geo-datasets owing to efficient algorithms and optimised memory management. In the spirit of reproducible research we provide the scripting procedure at the GitLab repository ( https://gitlab.com/Ferdinand18/np_us_streams ). The full procedure, starting from the N and P observations treatment to the 30-arc-second raster predictions, is provided below. ● 01_Cleaning.sh: cleaning the raw observation data. ● 02_Snapping.sh: snapping the observation data points onto the gridded stream network. ● 03_Extraction.sh: extracting descriptors corresponding to the snapped points. ● 04_Modelling.sh: building predictive models based on the observation data. ● 05_Prediction.sh: making predictions for all the US streams and building gridded GeoTiff maps as the final output."
10.1038/s41597-020-0479-6,"Geomorphometric layer computation Prior to computing the geomorphometric layers, we reprojected the DEMs (MERIT, 3DEP-1, LiDAR DMS and DTM) to the Equi7 projection with a cell size 100 m (the projection parameters are available from 38 ). To harmonise the different spatial grains, we used a bilinear algorithm implemented in gdalwarp within the open-source Geospatial Data Abstraction Library (GDAL). We kept the seven projection zones as defined in 36 , and employed the T6 tiling method to allow parallel and distributed processing of our work-flow. Tile size was 600 × 600 km where we buffered the borders by 401 km to avoid border artefacts between tiles. These overlapping and duplicate grid cells were removed when merging all tiles to seamless, continental maps. These large tile size increments were needed to avoid border effects, especially for multiscale deviation and multiscale roughness. We used the following open source software packages to compute the geomorphometric layers (Table 1 reports the software and the specific commands used for each derived variable calculation): Geospatial Data Abstraction Library (GDAL),version number 2.1.2 66 . Geographic Resources Analysis Support System software (GRASS), version number 7.3.0 67 . Whitebox Geospatial Analysis Tools (Whitebox GAT), version number 3.3.0 68 . Processing Kernel for geospatial data (Pktools), version number 2.6.3 69 , 70 . All of these tools provide fast and scalable computation features and functions for raster-based workflows that are easily automated using a scripting language, such as Bash or Python 71 . They also allow for the processing of very large datasets owing to efficient algorithms and optimised memory management. After computing all geomorphometric layers within the Equi7 projection, the layers were reprojected back to the WGS84 coordinate reference system (EPSG:4326 code 35 ) with a bilinear algorithm (or near for categorical variables) implemented in GDAL. This reversion of Geomorpho90m to WGS84 allows for the data to be seamlessly integrated with a broad set of global datasets. We used a tiling system identical to MERIT-DEM, in terms of dimension and nomenclature, i.e. 5 × 5 degree tiles with 6000 × 6000 cells each, to ensure data integration and comparisons with the original MERIT-DEM. All calculations were processed in parallel using open-source software at the Center for Research Computing, Yale University. LiDAR data processing Two common products that can be extracted from LiDAR data are: DTM and DSM. The DTM is generated using ground echoes from the LiDAR point cloud, in conjunction with an interpolation technique 72 . The approach employed in this paper for calculating the DTMs used the LiDAR processing tools found in the pktools software 70 . This is a two-stage approach, where the first stage uses a minimum composite rule, which retains the LiDAR pulse for the minimum height of each cell. pklas2img -a_srs EPSG:26911 -dx 100 -dy 100 -comp min -i input.las -o dtm_min.tif For the second stage, the output is then filtered using a progressive morphological filter 73 , which uses an iterative filter based on increasing kernel sizes to remove non-ground points from the final DTM. pkfilterdem -f promorph -dim 3 -dim 11 -i dtm_min.tif -o dtm_min_promorph.tif The calculation of the DSM is more straightforward and uses either a maximum composite rule (pulse retention with the maximum height of the cell) or a defined percentile composite rule. In our case, we use the rule to retain the pulse with the value corresponding to the 95th percentile of all pulses within the cell. pklas2img -a_srs EPSG:26911 -dx 100 -dy 100 -comp percentile -percentile 95 -i input.las -o dsm2.tif The LiDAR projection parameters (EPSG code) were set to EPSG:26911 in accordance with the associated metadata. The DSM and DTM cell size was set to 100 m to allow a simple reprojection to Equi7, which enables a comparison with the other discussed layers."
10.1038/s41597-020-0506-7,The source code of dbPSP 2.0 database has been uploaded to GitHub: https://github.com/BioCUCKOO/dbPSP2.0 .
10.1038/s41597-020-0505-8,"For the inter-agent calibration results, as well as the cellular homogeneity verification, a code was set up using RStudio (version 1.2.5001). This code can be accessed by contacting either Carine Sauger ( carine.sauger@gmail.com ) or Laurent Dubroca ( laurent.dubroca@gmail.com )."
10.1038/s41597-020-0502-y,"The ESRF High Speed Tomography in Python (PyHST2) software which was used to reconstruct the phase images is open source and can be found at: https://software.pan-data.eu/software/74/pyhst2 . The current pipeline for processing the raw data prior to its use in the PyHST2 algorithm is a large collection of scripts in MATLAB, Python and GNU Octave which makes it difficult to bundle into a single tomography pipeline. However, the ESRF is currently working to convert all of the scripts to Python to create a completely open source pipeline, though additional computing power, such a high performance computing cluster will likely be necessary. All additional image processing was done using open source Python libraries; these have been noted at the appropriate stages in the text."
10.1038/s41597-020-0503-x,MISSING
10.1038/s41597-020-0511-x,"The analyses presented in this work were done with the freely available software R 25 and the transformation surfaces developed are distributed as GeoTIFF files, which can be read in any Geographic Information System software."
10.1038/s41597-020-0510-y,The programs used to generate all the results were MATLAB (R2018b) and ArcGIS (10.4). Analysis scripts are available on request from Y.Z. (yuyuzhou@iastate.edu).
10.1038/s41597-020-0500-0,The Grid/Collection stitching plug-in 59 of Fiji used for generating the HSIMs collections is publicly available as a Java Script.
10.1038/s41597-020-0497-4,The complete data workflow was scripted in the programming language R ( https://www.R-project.org ) and instructions for generating the merged data sets accompanying this data descriptor can be found at GitHub ( https://github.com/bacteria-archaea-traits/bacteria-archaea-traits/releases/tag/v1.0.0 ).
10.1038/s41597-020-0513-8,No code was used in this study.
10.1038/s41597-020-0498-3,"A detailed description of the bimodal EEG-fMRI NF platform is given in 20 : the platform software package for real-time analysis and visualization is well documented but not publicly available. Python pipelines for the analysis of structural and functional MRI are available on github ( https://github.com/glioi/BIDS_fMRI_analysis_nipype ), in form of commented jupyter notebooks. Other scripts used for the technical validation in this paper can be provided by the authors upon request."
10.1038/s41597-020-0512-9,MISSING
10.1038/s41597-020-0517-4,"All Google Earth Engine and R scripts are available from the Environmental Data Initiative GLCP repository 27 within the entity “glcp.tar.gz”. The entity “glcp.tar.gz” also includes a standardized directory architecture that can be downloaded and run locally if users wish to reproduce the GLCP. If future users would like to access only scripts used to create the GLCP, the EDI GLCP repository also contains the entity “glcp_scripts.tar.gz”, which contains Google Earth Engine and R scripts."
10.1038/s41597-020-0518-3,MISSING
10.1038/s41597-020-0507-6,"We have prepared scripts to demonstrate how to load the datasets into Matlab ready to be pre-processed. These scripts are listed and introduced in Table 3 . All scripts are placed within the sub-folder ‘code’ in the respective BIDS formatted datasets. Interested readers are also directed to the BIDS Matlab repository, which provides tools for working with BIDS-structured data in Matlab ( https://github.com/bids-standard/bids-matlab ). We have also provided the code used by the music generator in the experiments that generated the ‘BCMI tempo’ and ‘BCMI testing’ data-sets. This is located in the ‘code’ sub-folder in each dataset along with a document describing the music generator. Please note, the music generator is not a single piece of code, but rather a collection of scripts and functions that work together with some proprietary software to generate the music. The code was written in Max/MSP version 6.1 and this software will be needed to get the music generator to work. Further details are also provided in 20 ."
10.1038/s41597-020-0514-7,The scripts and source codes of raw DAS and DEG analyses are deployed in Docker image and deposited at Docker Hub with the public tag sfrs/dasdegdocker:latest 156 . The docker image for signature comparison analysis workflow also was deposited at Docker Hub with the public tag sfrs/sfsigdb:latest 156 .
10.1038/s41597-020-0532-5,MISSING
10.1038/s41597-020-0521-8,The characterization techniques were developed on Matlab 2017b. Futher details on these methods are given by Lodomez 10 .
10.1038/s41597-020-0509-4,"This study was carried out using the following software packages: Dragonfly 2017 by Object Research Systems, free for non-commercial users, and available for download at https://www.theobjects.com/dragonfly/index.html . Amira 6.2.0 by Thermo Fisher Scientific. MATLAB 2017a by MathWorks. The MATLAB script is deposited with The Cell Image Library ( https://doi.org/10.7295/w9cil50728 ), available for download as Dataset 8 75 of this submission."
10.1038/s41597-020-0537-0,No specific code was developed in this work. The data analyses were performed according to the manuals and protocols provided by the developers of the corresponding bioinformatics tools that are described in the Methods section together with the versions used.
10.1038/s41597-020-0528-1,"Source code is available at on github. It contains the following repositories: training-data-synthesis Code for generating synthetic training data for nucleus segmentation model training. training-data-real-patch-extraction Code for converting the format of real training data. segmentation-of-nuclei Code for training a nucleus segmentation model on patches generated by the above-mentioned repositories, and applying a trained model on WSIs. Detailed descriptions are in the README files in the Github repository. We also provide a Dockerfile in Github, containing a trained model for easy deployment."
10.1038/s41597-020-0535-2,The custom MATLAB script MI2_preprocess.m was used to pre-process the raw recorded data. The MI2_saveMat.m file was used to extracted the 4s-trial data from continuous data and save them to the.mat files. The channel_dict.ced file contains the location information of channels for our EEG record data and can be easily imported into EEGLAB. These scripts and files are stored in the code folder and shared with the dataset.
10.1038/s41597-020-0533-4,The JavaScript code of the MODA interface developed to collect the annotations is open source 48 . The Matlab code to manage the PSG files and generate the GS from the spindle scoring files is also open source 38 .
10.1038/s41597-020-0526-3,"We provide the Glottis Analysis Tools software on request ( http://www.hno-klinik.uk-erlangen.de/phoniatrie/forschung/computational-medicine/gat-software/ ). The Pixel-Precise Annotator tool (PiPrA) is available open source online ( https://github.com/anki-xyz/pipra ). We provide a Jupyter notebook for training, evaluating and using the deep neural network as used in the Usage Notes section online under an open source license ( https://github.com/anki-xyz/bagls )."
10.1038/s41597-020-0508-5,"A Github repository ( http://github.com/sage-bionetworks/JHU-biobank ) con tains the codes required to generate the figures with a versioned repository available at Zenodo 18 . The tutorials are provided in R and Python languages, contained in the r_demos and py_demos directories respectively. All of the analytical code is provided in the directory marked “analysis”. Additionally, we have provided Docker containers and R scripts to facilitate reproducibility of the figures in the paper."
10.1038/s41597-020-0527-2,We used the following software and versions to process our dataset as described in the text: 1. STAR v2.5.3a was used for mapping reads to the Human reference genome NCBI build 37/hg19: https://github.com/alexdobin/STAR 2. featureCounts v1.6 was used to summarize gene counts: http://bioinf.wehi.edu.au/featureCounts/ 3. Picard v2.15.0 was used to measure 5′ to 3′ bias: http://broadinstitute.github.io/picard 4. DESeq2 v1.20.0 was used for differential expression analysis: https://bioconductor.org/packages/release/bioc/html/DESeq2.html . 5. IGV v2.8.2 was used to visualize MECP2 coding regions for sequence variation: http://software.broadinstitute.org/software/igv/ 6. Salmon was used to align reads to the Human GRCh38 reference transcriptome and estimate counts for each transcript: https://combine-lab.github.io/salmon/ 7. tximport v1.12.1 was used to summarize gene counts: https://bioconductor.org/packages/release/bioc/html/tximport.html 8. limma v3.40.2 was used to convert count data to log counts per million (logCPM) and to estimate weights: https://bioconductor.org/packages/release/bioc/html/limma.html 9. GeneMeta v1.56.0 was used to perform a random effects meta-analysis: https://www.bioconductor.org/packages/release/bioc/html/GeneMeta.html
10.1038/s41597-020-0529-0,"The code used for checking, cleaning and analysing the data is available in the open GitHuB repository “ https://github.com/Plant-Functional-Trait-Course/PFTC_1_2_China ”, of which a versioned copy is available at Zenodo 44 . There is also a link to the code from the published dataset 43 ."
10.1038/s41597-020-0522-7,Fire and Tree Mortality Database (FTM) is available from Forest Service Research Data Archive https://doi.org/10.2737/RDS-2020-0001 . All reformatting of contributed data was completed in R version 3.6.1 99 . Original contributed data are only available by contacting data contributors. The code used to reformat the data may be obtained contacting C. A. Cansler.
10.1038/s41597-020-0539-y,The data as reported were generated from experiments and are not relevant to any computer codes.
10.1038/s41597-020-0541-4,"The following open access software and versions were used for quality control and data analysis as described in the main text: Trimmomatic, version 0.36 was used to filter and trim low quality reads and bases from FASTQ sequencing data files: http://www.usadellab.org/cms/?page=trimmomatic FastQC, version 0.11.5 was used for quality analysis of raw FASTQ sequencing data: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Kallisto, version 0.42.3 was used to index and psedudoalign sequencing reads to the human hg38 transcriptome as well as to quantify transcripts in each samples: https://pachterlab.github.io/kallisto/ MultiQC, was used to aggregate and visualize FastQC, Trimmomatic, and Kallisto data outputs: https://multiqc.info/ Sleuth, was used to analyze transcript abundances quantified by Kallisto as well as to calculate and summarize differential transcript expression: https://pachterlab.github.io/sleuth/about All code and walkthroughs used for quality assessment and data analysis in this study is available at: https://github.com/enkera/Schumacker2019_Sci_Data"
10.1038/s41597-020-0540-5,"Digital image correlation is performed with Fast Iterative Digital Image Correlation (FIDIC) software 35 written in Matlab and available from https://github.com/FranckLab/ or https://github.com/jknotbohm/FIDIC . Code to compute cell boundaries, cell-substrate tractions, and monolayer stresses is available at https://github.com/jknotbohm/Cell-Traction-Stress . One script in the software package to compute tractions is from https://github.com/CellMicroMechanics/Easy-to-use_TFM_package (ref. 36 ). Code to compute cell trajectories and representative code to plot results are available from https://github.com/jknotbohm/Cell-Traction-Stress-Velocity-Plots ."
10.1038/s41597-020-0545-0,"All the pre-processing steps were performed using the available software TopSpin 3.1 (Bruker BioSpin, Germany), AMIX v.3.9.10 (Bruker BioSpin, Germany), and PLS-Toolbox 6.5 (Eigenvector Research, Wenatchee, WA, USA)."
10.1038/s41597-020-0536-1,"All source code is provided at ( https://gitlab.com/buchserlab/viraldomains ). The software is designed to be compiled and run with the publicly available DotNetCore, which can be downloaded free with VS Code, or Visual Studio Community Edition."
10.1038/s41597-020-0538-z,MISSING
10.1038/s41597-020-0544-1,"The code used in the Usage Notes section is publicly available through the Zenodo repository 73 . The Usage Notes section was performed using R computing language 64 and the packages: finch 71 , tidyverse 74 , knitr 68 and here 75 ."
10.1038/s41597-020-0543-2,"We have made the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained , and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert ."
10.1038/s41597-020-0516-5,"Our Python code that converts the JSON format of NJC19 network data (NJC19_network.json 35 ) to the format of Supplementary Table 1 can be downloaded from the Dryad Digital Repository 35 . For the taxonomic profiling of microbiome samples for the NJC19 construction, we used MetaPhlAn v2.0 with the “sensitive-local” mapping option and QIIME v1.8.0 with Greengenes v13_8_pp reference files 30 , 31 , as described above. The aforementioned cys file of NJC19 for network visualization was produced by Cytoscape v3.7.2 36 ."
10.1038/s41597-020-0542-3,Our software is available through U.S. Geological Survey code repository (https://doi.org/10.5066/P9XZCPMT) 10 . Our serial code is also available in our Github page: https://github.com/mehdiheris/RasterizingBuildingFootprints .
10.1038/s41597-020-0551-2,"The R code used to generate Figs. 3 , 4 , 5 , 7 , and 8 can be found in Zenodo Digital Repository 37 ."
10.1038/s41597-020-0546-z,MISSING
10.1038/s41597-020-0530-7,"The code for working with the LiPD data files, including basic functions in three programming languages, is available on GitHub ( https://github.com/nickmckay/LiPD-utilities ). The code used to compute the temperature reconstructions, including the ensembles and the composites, and to the code for generating Figs. 2–6, is available at: https://github.com/nickmckay/Temperature12k under a MIT license 37 ."
10.1038/s41597-020-0549-9,Scripts used to generate the MARES reference databases and technical validation are freely available from https://github.com/wpearman1996/MARES_database_pipeline
10.1038/s41597-020-0547-y,"The cheminformatic tool for probabilistic identification of carbohydrate (CTPIC) program was developed using Python and is available on our website ( http://ctpic.nmrfam.wisc.edu ) as a web server. In addition, the source codes are available through GitHub ( https://github.com/htdashti/ctpic )."
10.1038/s41597-020-0550-3,The QuickNII (RRID:SCR_016854) tool was used for spatial co-registration of atlases. Microscoft Access 2016 was used to create the database.
10.1038/s41597-020-0555-y,"Data pre-processing tools for (1) pre-determining epitope and paratope residues, (2) re-numbering antibody residues with numeric identifiers, and (3) re-labelling multiple chains have been uploaded to GitHub at https://github.com/baddtongji/CE_BLAST . The methods involved in the technical validation are integrated into the CE-BLAST web server and can be accessed at http://bidd2.nus.edu.sg/czw/ce_blast/ ."
10.1038/s41597-020-0565-9,"All the software programs used in the bioinformatics workflow (transcriptome assembly, pre and post-assembly processing stages and transcriptome annotation) are presented in the Methods section. All programs and databases have the versions, download dates, and parameters described. Software programs with no parameters associated were used with the default settings."
10.1038/s41597-020-0554-z,The fire indices have been generated using the open source model GEFF v3.1 ( https://git.ecmwf.int/projects/CEMSF/repos/geff ). The code to reproduce the results of this manuscript is openly available on a public repository on GitHub ( https://github.com/cvitolo/paper_geff_era5 ).
10.1038/s41597-020-0519-2,"The workflow utilized for quality filtering the RNAseq reads, alignment to references genomes, and counting of mapped reads has previously been described 11 , and can be found in the “SysMetEx – Data analysis” repository 57 . Reference genomes of the three strains used in the project and auxiliary files can be accessed at the “SysMetEx – Reference genomes” repository 58 . Proteomics data processing was carried out according to the MaxQuant parameter (FAIRDOMHub repository “SysMetEx – Dataset collection” 55 , file “maxquant parameters”)."
10.1038/s41597-020-0553-0,Matlab code for reproducing the case-study analysis of diverse empirical time-series data is available at https://github.com/benfulcher/Empirical1000_LowDimProj . Source code for the web implementation of the CompEngine interface is available at https://github.com/compenginecode/compengine-frontend . Source code for the API for querying the CompEngine database is available at https://github.com/compenginecode/compengine-api .
10.1038/s41597-020-0561-0,All analyses were performed in R (v.3.6.2). The BIOMASS R-package is an open source library available from the CRAN R repository. Codes associated to specific functions are available upon request.
10.1038/s41597-020-0556-x,Data processing was performed using open source software. The approach of tools and parameters used were as below. SOAPnuke: https://github.com/BGI-flexlab/SOAPnuke . Version: 1.5.2. Parameters: filter -A 0.5 -M 2 -l 10 -q 0.3 -n 0.05 -Q 2 -d. Cutadapt: https://cutadapt.readthedocs.io/en/stable/ . Version: 1.16. Parameters: -m 5 -e 0.10. HISAT2: http://www.ccb.jhu.edu/software/hisat . Version 2.0.1-beta. Parameters: -p 4 –phred33 –sensitive –no-discordant –no-mixed -I 1 -X 1000. featureCounts: http://subread.sourceforge.net/ . Version 1.5.3. Parameters: -T 5 -p -t exon -g gene_id. MACS2: https://github.com/taoliu/MACS . Version 2.1.2. Parameters: macs2 callpeak -t input.bam -f BAM -g 259040147 -n name.output -B -q 0.01 --nomodel. Bedtools: https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html . Version: 2.26.0. Parameters: bedtools intersect -a standardpeak.bed -b input.bam -c > output.count. The R code used for calculating the correlation and comparative analysis are available in the supplementary materials.
10.1038/s41597-020-0560-1,The code used for the processing of the FIDUCEO Microwave UTH CDR is available on GitHub ( https://github.com/FIDUCEO/CDR_UTH ).
10.1038/s41597-020-00575-2,All data records were generated using code developed in Python 3 16 . The code is available upon request from the corresponding author.
10.1038/s41597-020-0564-x,"Python scripts for basic processing of seismic data and aggregate visualization of both acoustic events and pressure profiles are published in the data repository. Requirements for scripts execution are Python 2.7 and the additional modules numpy and matplotlib . The code in the repository relies on methods widely available in literature, and is therefore provided “as-is”, with no other warranties, expressed or implied, of correctness and completeness. It is not to be considered as a software for professional data elaboration but rather as an example in support of the research community to develop specific codes by providing a starting template. Users are encouraged to view the code and make necessary changes as required."
10.1038/s41597-020-0563-y,The custom MATLAB script to process data is provided on the following Github repository: https://github.com/UF-ISE-HSE/UnevenWalkingSurface . A Python script (python_version.py) was also provided for converting the processed data into Python compatible format. The .h5py file can be directly use as a standard file object in Python to process.
10.1038/s41597-020-0534-3,"The ONEFlux collection of codes used to create data intercomparable with FLUXNET2015 has been packaged to be executed as a complete pipeline and is available in both source-code and executable forms under a 3-clause BSD license on GitHub: https://github.com/AmeriFlux/ONEFlux . The complete environment to run this pipeline requires a GCC compatible C compiler (or capability to run pre-compiled Windows, Linux, and/or Mac executables), a MATLAB Runtime Environment, and a Python interpreter with a few numeric and scientific packages installed. All of these can be obtained at no cost."
10.1038/s41597-020-0559-7,MISSING
10.1038/s41597-020-0558-8,"The codes for the LoRTIA and SeqTools (the toolkits developed by our laboratory) analysis are available at: https://github.com/zsolt-balazs/LoRTIA and https://github.com/moldovannorbert/seqtools , respectively."
10.1038/s41597-020-0520-9,"The gridding and statistical calculation procedures described in the Methods section are based on open source routines, provided within GMT ( https://www.generic-mapping-tools.org/ ) and GDAL ( https://gdal.org/ ), embedded in Python scripts. Codes are available upon request."
10.1038/s41597-020-0568-6,The code for H2ThermoBank has been made available on the H2ThermoBank GitHub page ( https://github.com/aliakbarhssnpr/H2ThermoBank ).
10.1038/s41597-020-0548-x,"Our open-source repositories for MVTS generation, task-based sampling, and model validation is available on Bitbucket 55 . Interested parties are encouraged to get involved in the ongoing development for and extensions to the dataset."
10.1038/s41597-020-0566-8,The source code of FIO-ESM v2.0 will be provided upon request for the purpose of replicating the data described in this paper. The code may be requested from the corresponding author by email.
10.1038/s41597-020-0574-8,"All the software described in Technical Validation is available in a public GitHub repository ( https://github.com/MPBA/TAASRAD19 ), along with the Python scripts for sequence pre-processing, installation scripts for the MXNet 44 framework, pre-trained network model weights, and examples of radar prediction output sequences. All the code was written in Python 3.6 and tested on Ubuntu releases 16.04/18.04. Some pre-processing steps (e.g. sequence and outlier mask generation) require a non trivial amount of computing resources and memory. Training the deep learning model with the same parameters described in the paper requires either two GPUs with 8GB of RAM or one GPU with 16GB. Please refer to the README files in the code release for further instructions."
10.1038/s41597-020-0567-7,The workflow included several custom made python scripts (labelled by star in the Fig. 2 ) which are accessible here: https://github.com/VetrovskyTomas/GlobalFungi .
10.1038/s41597-020-0569-5,"All R scripts used in data preparation and technical validation, along with the un-prepared data, are available upon request from the corresponding author. Examples of how to load the data and how to change factor/category columns to character columns in R (Box 1 ) and Python (Box 2 ) are provided. Additionally, the code used to create Fig. 2a in R (Box 3 ) is listed as an example of how to combine data from both data files into a single plot."
10.1038/s41597-020-0562-z,"The data for this study was mainly processed in R (version 3.6.0), but with cross-checking and corrections of spatial coordinates conducted using ArcGIS. Sample R codes, including instructions for reading the database files and reproducing summary files and figures of this paper, is available as part of the data record 35 ."
10.1038/s41597-020-0571-y,MISSING
10.1038/s41597-020-0570-z,"All the code used for the pre-processing, brain reconstruction, and template creation can be found in the repository ( https://doi.org/10.12751/g-node.16wrxa ) along with the code for automatic slice segmentation."
10.1038/s41597-020-0572-x,All figures have been produced using R (3.5.1) and RStudio (version 1.1.463). This script can be accessed by contacting either Thibault Cariou (thibault.cariou@gmail.com) or Camille Vogel (camille.vogel@ifremer.fr).
10.1038/s41597-020-0552-1,Code used to generate our dataset is available on the Open Science Framework 35 .
10.1038/s41597-020-0557-9,"The scripts used to perform both the linear and nonlinear registrations (including the ANTs code with all the selected registration parameters), the obtained transformations that were used to register the DKT atlas to the MNI-ICBM2009c template, the code for resampling the labels based on these transformations, as well as the registered DKT atlas in the MNI space, after applying the transformations are available at https://gin.g-node.org/anamanera/CerebrA/src/master/ ."
10.1038/s41597-020-00579-y,MISSING
10.1038/s41597-020-00576-1,Blast2GO PRO: https://www.blast2go.com/blast2go-pro BUSCO v4.0.5: https://busco.ezlab.org FastQC v0.11.5: https://www.bioinformatics.babraham.ac.uk/projects/fastqc Magic-BLAST v1.5.0: https://ncbi.github.io/magicblast MISA: http://pgrc.ipk-gatersleben.de/misa/misa.html PRIMER3: https://github.com/primer3-org/primer3 Trimmomatic v.0.35: http://www.usadellab.org/cms/?page=trimmomatic Trinity v2.8.4: https://github.com/trinityrnaseq/trinityrnaseq/wiki
10.1038/s41597-020-00583-2,"The codes used to generate this dataset can be shared by contacting the authors. Three main types of codes were used: 1. Codes to download and extract the data. Most of the data were downloaded via requests to the public-facing servers using their API (WSC and USGS hydrometric data, for example) and through their data dissemination mechanisms (ECMWF ERA5 and ERA5-Land). Other data were downloaded from FTPs and thus did not require specific codes (SNODAS snow water equivalent data, GHCND station data). The “ghcnd_access” toolbox 42 was used to extract the GHCND station daily data from the.dly file format into a more accessible format. The “usgsrdbread.m” MATLAB script written by Kelly Kearney was used to extract the USGS streamflow data to the MATLAB format. The code is available here: https://github.com/kakearney/usgsrdbread-pkg . 2. Codes to process data at the catchment scale. These codes were mainly a) to perform Thiessen polygons to extract meteorological data from stations at the catchment scale, b) average netCDF file outputs at the catchment scale (or take the closest point), and c) extract the physiographic data from the land-cover, elevation and soil properties raster images. PAVICS was used in the creation of this database, however any GIS software could be used to extract the zonal statistics from the raster data for each catchment. 3. Codes to ensure data quality and producing the final output formats. These codes are a set of checks to ensure that missing data was handled properly (set to NaN rather than 0), that catchments outside of a particular dataset’s boundaries were excluded for that product, and that basins with smaller areas than 1 km 2 or were misrepresented by the official agencies were excluded. For example, the USGS NWIS contains data for the island of Guam, which was excluded using these validation tools. The data production tools are simple aggregation tools that convert the data from the MATLAB processing software to more convenient netCDF files."
10.1038/s41597-020-00582-3,"The code implementation was done in Python3 using Jupyter notebook. The scripts to perform data pre-processing, technical validation, visualization are available at SGRU github repository ( https://nbviewer.jupyter.org/github/mpipatta/mpipatta.github.io/blob/master/CHAM5.ipynb )."
10.1038/s41597-020-00577-0,All R code used to generate the archived data is available at Zenodo 26 .
10.1038/s41597-020-00580-5,"The WSF2015 is the result of several processing steps involving tens of sub-modules run on multiple architectures and using different software. While S1 pre-processing and feature extraction has been supported by Google through its Earth Engine platform, the computation of Landsat-8 temporal statistics, the training point extraction and classification tasks have been performed in the IT4Innovations Czech supercomputing center by means of DLR proprietary software, GDAL (Geospatial Data Abstraction Library v.2.4) and Pktools (Processing Kernels for geospatial data v2.6) scripts. Post-classification has been carried out in the Calvalus system available at DLR’s Earth Observation Center by means of proprietary software and dedicated Python (v3.5) scripts. Given the use of proprietary tools, the code cannot be openly released to the public."
10.1038/s41597-020-00588-x,"Code used to perform the high-throughput calculations are available at https://github.com/pstjohn/bde . The code relies on cclib and RDKit to process molecular information in Python, Gaussian to perform the DFT calculation, and pandas for data processing. Some of the code relating to the PostgreSQL database and NREL’s HPC infrastructure is site-specific and will likely need to altered to run these types of calculations on alternative HPC systems."
10.1038/s41597-020-00587-y,"The codes used are written in C++ and R and are included in SAGA GIS Version 6.1.0, freely available at www.saga-gis.org under the GNU public license including the necessary source codes. Calculations were done in SAGA Version 6.1.0."
10.1038/s41597-020-00581-4,"All used Bash, Perl, Python, and R/Markdown custom code and scripts complemented with intermediate and processed data (input and output files), and all other supporting information that enable reproduction and re-use are available at FAIRDOMHub under project name _p_stRT ( fairdomhub.org/projects/161 ) under CC BY 4.0 licenses. Data were locally stored in a ISA-tab compliant project directory tree generated using pISA-tree ( github.com/NIB-SI/pISA ) and uploaded to FAIRDOMHub repository using FAIRDOMHub API and R package pisar ( github.com/NIB-SI/pisar )."
10.1038/s41597-020-00591-2,The code used in the processing of our ground-truth data is open-source and was published in the Comprehensive R Archive Network (CRAN). The workflow for the processing of ground-truth data are available online in the form of an html vignette ( https://CRAN.R-project.org/package=CAWaR/vignettes/CAWaR.html ) and can be tested using example ground-truth data provided within the corresponding R package.
10.1038/s41597-020-0573-9,MISSING
10.1038/s41597-020-00593-0,"The following tools were used for the presented analysis and described in the main text: Busybee web, Maxbin2, MyCC, and DAS Tool with standard parameters were used for the reconstruction of metagenome-assembled genomes (MAGs). 1. Busybee web https://ccb-microbe.cs.uni-saarland.de/busybee 2. Maxbin2 https://sourceforge.net/projects/maxbin2/ 3. MyCC https://sourceforge.net/projects/sb2nhri/files/MyCC/ 4. DAS Tool https://github.com/cmks/DAS_Tool 5. CheckM was used to estimate obtained genomes completeness and contamination https://github.com/Ecogenomics/CheckM 6. RefineM was used to remove contamination https://github.com/dparks1134/RefineM 7. QUAST helped to access quality metrics http://cab.spbu.ru/software/quast/ 8. fastANI was used to calculate ANI https://github.com/ParBLiSS/FastANI 9. IDTAXA helped to obtain taxonomic assignments for the studied genomes 16S rRNA genes http://www2.decipher.codes/Classification.html 10. GTDB-Tk was used to find 120 single-copy bacterial marker protein sequences, to construct their multiple alignments and to get the taxonomic assignment using the GTDB r86 database https://github.com/Ecogenomics/GTDBTk 11. MAFFT was used for aligning amino acid sequence sets of the MamA, -B, -M, -K, -P, and -Q proteins https://mafft.cbrc.jp/alignment/server/ 12. Gblocks helped to curate sequences aligned in MAFFT http://molevol.cmima.csic.es/castresana/Gblocks_server.html 13. Phylogenetic trees were inferred with IQ-TREE http://www.iqtree.org/ 14. Obtained trees were visualized with iTOL https://itol.embl.de/"
10.1038/s41597-020-00592-1,"R scripts for raw data reading, normalisation, QC, and plotting were available at https://github.com/LuChenLab/Rscript_for_BeadChip.git ."
10.1038/s41597-020-00597-w,"All the software used in this study are open source (see the previous sections for references or URLs). In addition, the dataset contains several custom-made Matlab and Python scripts (see section “Data Records”)."
10.1038/s41597-020-00599-8,All programming code necessary to reproduce the map in Google Earth Engine is supplied together with the data (see Data records) and on https://github.com/Martin-Jung/Habitatmapping .
10.1038/s41597-020-00596-x,The AgTile-US model R code and example R code to read dataset are provided in GitHub ( https://github.com/NCAR/AgTile-US ) and Figshare 37 .
10.1038/s41597-020-00595-y,"See https://github.com/wiheto/esfmri_data_descriptor for code used for: fMRIPrep execution, MRIQC comparision, and confound differences between pre and postop."
10.1038/s41597-020-00602-2,The source code used to generate the database is available at https://github.com/ShuHuang/batterydatabase . The code of ChemDataExtractor 1.5 that has been modified for database auto-generation in the battery domain is available at https://github.com/ShuHuang/batterydatabase/tree/master/chemdataextractor_batteries . The GUI application source code can be found at https://github.com/ShuHuang/batterygui .
10.1038/s41597-020-00603-1,"We developed R scripts to compute the spatial and temporal interpolated ice cover values. Spatial interpolation for Grid-510 is processed by “Resampling_Raster.R”, and temporal interpolation for non-daily data is estimated by “Time_Interp.R”. Both scripts utilize RStudio version 1.1.463, and are available on the NOAA GLERL GitHub Repository at https://github.com/NOAA-GLERL/icegridresampling . This repository also contains sample scripts (Python, MATLAB and R) to demonstrate how to load the ASCII data into memory."
10.1038/s41597-020-00601-3,"Both the MATLAB code script named read_AGWD.m and the IDL code named read_AGWD.pro for reading the ocean wave parameter products are provided as supplementary material 1 and 2 , respectively."
10.1038/s41597-020-00605-z,All Google Earth Engine codes to pre-process the earth observation input features and perform the actual LCZ classification are available upon request. The pixel-based random-sampling assessment was done using the randomForest v4.16-14 package available in R-project 109 . The CONUS LCZ map figure is produced with QGIS v3.4. All other data processing and visualizations are done in Python v3.6.9.
10.1038/s41597-020-00594-z,MISSING
10.1038/s41597-020-00608-w,"We provide code to transform predicted annotation masks in TIFF-format to SVG-files for curation by experts as well as the transformation from SVG-files to TIFF-files. The contour sampling rate when transforming mask objects to SVG-descriptions can be set in accordance to the size of predicted nuclei. Therefore, new nuclei image annotation datasets can easily be created utilizing the proposed framework and a tool to modify SVG-objects, such as Adobe Illustrator. The code is written in python and is publicly available under https://github.com/perlfloccri/NuclearSegmentationPipeline ."
10.1038/s41597-020-00604-0,"Plaque2.0 batch image analysis for infection scoring . The MATLAB (version R2016b, The MathWorks, Natick, USA) script AntiVir_batchprocessing.m used by UZH for image analysis is provided for download at IDR, accession number idr0081, under idr0081/3-Screen/Analysis . It is based on the Plaque2.0 software available on GitHub under GPLv3 open source license: https://github.com/plaque2/matlab . To batch analyse the HAdV screening data by Plaque2.0, fork or download the Plaque2.0 AntiVir code from GitHub: https://github.com/plaque2/matlab/tree/antivir . Place the AntiVir_batchprocessing.m file from idr0081/3-Screen/Analysis into the Plaque2/matlab folder and follow the instructions in AntiVir_batchprocessing.m . A MATLAB license is required. Hit filtering using R . The R 39 (version 3.6.1 (2019-07-05)) script AntiVir_hitfiltering.R used by UZH for data processing and hit filtering is provided at IDR accession number idr0081 under idr0081/3-Screen/Analysis ."
10.1038/s41597-020-00598-9,The R script used to collate and curate the Amphibia Cytb database is available at figshare (ACDCv1.0.R 30 ).
10.1038/s41597-020-00600-4,All our codes are available from GitHub ( https://github.com/celsohlsj/gee_brazil_sv ) under the GNU General Public Licence v3.0 35 . In the GitHub repository users will find the freely available codes of our method and the Toolkit Download.
10.1038/s41597-020-00614-y,3D Slicer is available at https://www.slicer.org/ . All the content is available under Creative Commons Attribution-ShareAlike. The source code of the application itself is available at https://github.com/Slicer/Slicer .
10.1038/s41597-020-00612-0,Code for calculating yearly and monthly crop specific blue and green water requirements is available in the freely and publicly available repository in figshare ( https://doi.org/10.6084/m9.figshare.c.4893084 ) 25 .
10.1038/s41597-020-00578-z,MISSING
10.1038/s41597-020-00613-z,The statistical model (L2SWBM) used to produce the new estimate for Great Lakes water balance was programmed in R (version 3.6.1). The scripts are open source and available for download as part of the published dataset 66 .
10.1038/s41597-020-00619-7,The Python scripts for the generation of accessibility indicators are available upon request.
10.1038/s41597-020-00618-8,MISSING
10.1038/s41597-020-00617-9,Custom scripts were not used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-020-00610-2,Codes for pulling the standardized dataset from HIT-COVID database and reproducing the figures on HIT-COVID website ( https://akuko.io/post/covid-intervention-tracking ) are available on Github repository ( https://github.com/HopkinsIDD/hit-covid ).
10.1038/s41597-020-00609-9,A live version of this project is accessible on GitHub at: https://github.com/amel-github/covid19-interventionmeasures . The codes used to describe the CCCSL dataset and the codes used to explore the CCCSL dataset are written in R language 19 . They are available at: https://github.com/amel-github/CCCSL-Codes . Please refer to the README file in the code release for further instructions.
10.1038/s41597-020-00620-0,All code used to generate and process the datasets described above is freely available at https://github.com/WengLab-InformaticsResearch/CHIA in the file titled chia.R . It was written in R version 3.3.3.
10.1038/s41597-020-00622-y,"In addition to releasing the data, we also make available the code used in the experiments. All code and additional data required for the experiments are available on GitHub at http://www.github.com/simula/hyper-kvasir ."
10.1038/s41597-020-00626-8,"All data records were generated using code developed in MySQL, using the Laravel Framework. The database source code is available upon request from the corresponding author."
10.1038/s41597-020-00623-x,"EndometDB uses open source components listed in the Table 3 . Code for pre-processing of the data is available upon request. The Expression BeadChips were loaded using R function calls in the publicly available beadarray R package 27 . Log transformation and quantile normalization was performed using standard Bioconductor R packages 28 , 29 , 30 . The ComBat batch adjustment algorithm 35 within the SVA R-Package 36 was used to correct the variation in the different Expression BeadChips arrays. The EndometDB source code is available at our GitHub repository https://github.com/micawo/EndometDB ."
10.1038/s41597-020-00634-8,"The optical properties of the chromophores were extracted from the scientific literatures, which is available at https://doi.org/10.6084/m9.figshare.12045567.v2 12 . We have opened a user-friendly webpage ( http://Deep4Chem.korea.ac.kr/search ) where users can search for chromophores in the database. The database of this webpage will be updated regularly."
10.1038/s41597-020-00635-7,The Matlab code used to extract the frame number of BVH files to calculate durations can be found at https://physionet.org/content/kinematic-actors-emotions/2.1.0/ .
10.1038/s41597-020-00639-3,Detailed code generating the database can be accessed from the source code hosted via Gitlab ( https://gitlab.iscpif.fr/vverbavatz/mrt-access-project ).
10.1038/s41597-020-00637-5,"The source code of AiiDA, the AiiDAlab, Appmode ( github.com/oschuett/appmode ), and most AiiDAlab applications is released under the MIT open-source license, and made available under the aiidateam ( github.com/aiidateam ) and aiidalab ( github.com/aiidalab ) GitHub organisations. The Quantum Mobile virtual machine can be downloaded from materialscloud.org/quantum-mobile . Its source code (in the form of ansible roles and playbooks) is released under the MIT license and made available under the marvel-nccr GitHub organisation ( github.com/marvel-nccr )."
10.1038/s41597-020-00625-9,MISSING
10.1038/s41597-020-00638-4,The source code of AiiDA is released under the MIT open-source license and is made available on GitHub ( github.com/aiidateam/aiida-core ). It is also distributed as an installable package through the Python Package Index ( pypi.org/project/aiida-core ).
10.1038/s41597-020-00630-y,"Python codes implementing outlier detection using Chauvenet’s criterion, majority voting, mode-subtraction, and other utility functions, including the generation of heatmap plots, are available on https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes . The Krippendorff package ( https://github.com/pln-fing-udelar/fast-krippendorff ) was used for the computation of Krippendorff’s alpha. Python version 3.6.9 was used throughout. Codes for preprocessing the raw log-level data in SQL databases to CSV files were implemented in Python with the SQLAlchemy package. However, these codes and the raw log-level data are not made available as they include privacy-sensitive information outside the agreed boundary for public sharing of the dataset, which was collected only for logistic reasons. Nevertheless, we welcome the dataset users to contact the corresponding authors if they need any further assistance or information regarding the raw data, and it’s preprocessing."
10.1038/s41597-020-00621-z,All the Python scripts used in the Technical Validation section for analysis and figure generation are available online 31 . Python scripts for simulation are also available in G-Node repository ( simulation/run_simulation.ipynb ).
10.1038/s41597-020-00640-w,The code used in the IPSI 1.4 web interface is open and based on HTML language. The server procedures are developed in PHP open-source language and the client-side procedures are developed in JavaScript language with specific features of jQuery ( http://jquery.com/ ) and Google Maps API ( https://developers.google.com/maps/ ).
10.1038/s41597-020-00633-9,Source code can be found in the Github repository: ( https://github.com/ZuchnerLab/Fazal2020Scripts ).
10.1038/s41597-020-00624-w,"All the code and geoprocessing scripts used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) 61 from the repository www.github.com/fineprint-global/app-mining-area-polygonization 35 . The processing scripts were written in R 58 , Python 59 , and GDAL (Geospatial Data Abstraction Library 62 ). The web application to delineate the polygons was written in R Shiny 63 using a PostgreSQL 64 database with PostGIS 65 extension for storage. The full app setup uses Docker 65 containers to facilitate management, portability, and reproducibility. The web application supports the delineation of areas from the satellite images layers. It systematically displays the regions of interest (e.g., buffer around the mines) and several background options of satellite images, which the users can take into account to draw and edit polygons. Note that mining coordinates are not part of the web application and must be fed into the database by the user. To learn more about the application setup see www.github.com/fineprint-global/app-mining-area-polygonization . The current version of app provides image layers from Sentinel-2 Cloudless 33 , Google Satellite, and Microsoft Bing Imagery. Further sources of satellite images can be added to the application via WMS."
10.1038/s41597-020-00629-5,"The entire code used for the technical validation is available from https://gitlab.com/sven_schellenberger/scidata_phase1 . Furthermore, a script for viewing the data is also included in the repository which can easily be used by configuring the subject ID and scenarios which shall be viewed. The code was written and tested using MATLAB R2020a for Microsoft Windows ."
10.1038/s41597-020-00636-6,MISSING
10.1038/s41597-020-00627-7,MISSING
10.1038/s41597-020-00586-z,The R script used in this manuscript to deal with the references collected from the WoS is provided as additional information (Supplementary file 3 ).
10.1038/s41597-020-00631-x,"The neural networks used to produce the PPDIST dataset were implemented using the MLPRegressor class of the scikit-learn Python module 113 . The P occurrence for different thresholds was calculated using the percentileofscore function of the scipy Python module 70 , whereas the P magnitudes for different return periods were calculated using the percentile function of the numpy Python module 72 , 73 . The other codes are available upon request from the first author. The predictor, IMERG, and ERA5 data are available via the URLs listed in Table 1 . Most of the gauge observations are available via the URLs provided in the “Gauge observations and quality control” subsection. Part of the GSDR database and some of the national databases are only available upon request."
10.1038/s41597-020-00643-7,"The CHIRTS-daily data products are derived using approximately 500 lines of code written in the Interactive Data Language. While not written as a portable library or toolset, access to the code is not restricted, and it is available upon request."
10.1038/s41597-020-00644-6,MISSING
10.1038/s41597-020-00646-4,"Code used to generate the training samples, compute harmonics, train a random forest classifier, create the final maps, and produce the validation analyses are available publicly as Google Earth Engine scripts, R markdown files, or Jupyter notebooks. Links to scripts and data for analyses can be found in the GitHub repository at https://github.com/LobellLab/csdl . The software used in this work include: • R version 3.5.1, dplyr 0.8.0.1, sf 0.6–3, raster 2.6–7, rgdal 1.3–4, salustools 0.1.0, sp 1.3–1 • Python 3.7.3, numpy 1.16.4, pandas 0.24.2, matplotlib 3.1.0, sklearn 0.21.2, plotly 4.5.0"
10.1038/s41597-020-00645-5,"The processing of all the data was performed using R v3.3.2 within R Studio v1.1.447 for Mac. The library openair 34 v2.0.0 was used for time aggregation, and the library caTools v1.17.1.1 was used to compute rolling means and rolling standard deviations of the time series. Figure 3 was produced with MATLAB TM R2019b Update 4. The code is publicly available in figshare 35 . It contains all the functions to read, homogenize, and quality control the raw data files to produce the currently shared dataset. Also included are functions to read the data and QC flag codes on the final dataset in conjunction with the script used to produce the aggregated data for Fig. 3 that could be used as an example to process the data."
10.1038/s41597-020-00652-6,MISSING
10.1038/s41597-020-00648-2,"All code for analyses included within this manuscript as well as meta-data files (including unique identifiers, repository and manuscript data, lake characteristics, water chla and chemistry data, and water sample collection method) are provided in an open access repository 38 . Within the repository, we also provide code for unit conversion (e.g. µg L −1 to mg L −1 ), and extracting climate data from the Climatic Research Unit at the University of East Anglia ( http://www.cru.uea.ac.uk/ )."
10.1038/s41597-020-00628-6,SPP source code is available in the SPP GitHub account under a Creative Commons CC BY 4.0 license at https://github.com/signaling-pathways-project .
10.1038/s41597-020-00654-4,The MATLAB codes for spatial visualization of files in HDF format are published along with our datasets in PANGAEA. The codes and datasets for training and estimation process can be accessed at the figshare 37 ( https://doi.org/10.6084/m9.figshare.c.4891302 ).
10.1038/s41597-020-00656-2,MISSING
10.1038/s41597-020-00653-5,"We used the software by Wright and Ziegler 33 , available at https://github.com/imbs-hl/ranger . The code for data processing was written in ZeScript ( https://www.zegraph.com/z-script/ ) and is available upon request."
10.1038/s41597-020-00651-7,MISSING
10.1038/s41597-020-00650-8,The authors do not have code specific to this work to disclose.
10.1038/s41597-020-00657-1,"The codes for processing the collected data, for plotting the figures, and for downloading SMAP data are included in the dataset 28 . The codes for filtering the reflected solar signals and for plotting the results are included in the update 29 together with the updated brightness temperature that contain corrected local time stamps. Overviews of the codes, data and explanations can be found in Online-only Table 2 28 and Table 2 29 ."
10.1038/s41597-020-00660-6,All the codes for preparing input data and reproducing the edge list file in RICON 17 are publicly available in RICON-toolkit 31 .
10.1038/s41597-020-00662-4,"Matlab program is used to generate the hybrid method calculation and Mote Carlo simulation. The full custom MATLAB script is provided on the open-access online dataset figshare 48 and GitHub ( https://github.com/conanbean/A-2015-inventory-of-embodied-carbon-emissions-for-Chinese-power-transmission-infrastructure-projects.git ). The carbon emissions are calculated by multiplying the various inputs of the project by the corresponding embodied carbon intensity. The sectoral embodied carbon intensities in China are calculated by using EEIOA (see algorithm in Method), based on EXIOBASE database. And then, the stochastic modelling is adopted to carry out Monte Carlo simulation in terms of the standard deviation (SD). We define the order of magnitude of each source data x as lg x , then the perturbation of x (denoted as x Φ ) is \(lg{x}^{\Phi }\approx lg(x+dx)=lgx+lg((x+dx)/x)=lgx+lg(1+Rx)\) . The Rx represents the relative SD, which is also named as coefficient of variation (CV). Consequently, the perturbation be conducted 10000 times for every raw data, including input inventory, carbon emission inventory, and each item in MRIO table, to obtain the SD of the embodied carbon emissions for each project."
10.1038/s41597-020-00661-5,Code used in MATLAB 2016b to plot global climate space is available via https://doi.org/10.6084/m9.figshare.7442915 22 .
10.1038/s41597-020-00669-x,"The source code of GCAM and Demeter used in this paper s available at https://doi.org/10.5281/zenodo.3713432 43 and https://doi.org/10.5281/zenodo.3713378 44 , respectively."
10.1038/s41597-020-00667-z,The code to process MEDIQA-AnS and reproduce the results of the experimental benchmarks shown here can be found at https://www.github.com/saverymax/qdriven-chiqa-summarization .
10.1038/s41597-020-00671-3,The versions of any software used and any specific variables or parameters used to process the current dataset have been detailed in the Methods section. The custom code produced during the validation of this dataset and the source code that underlies the interactive web interface ( http://210.77.86.67/VP.html ) have been shared on GitHub ( https://github.com/pr839ok/FVPGD ).
10.1038/s41597-020-00659-z,All code used to perform the analysis for data displayed and deposited on lncRNAKB is available through https://github.com/seifudd/lncRNAKB
10.1038/s41597-020-00666-0,MISSING
10.1038/s41597-020-00665-1,"There is no custom code in the generation of the CEAP dataset. In this study, Microsoft Excel is employed to process all the data and Origin is used to draw the figures. Three model inputs have been used in the construction of this dataset, i.e., the measurements from China’s CEMS network, theoretical flue gas rates and activity data. First, the CEMS-monitored data are released by the Ministry of Ecology and Environment of China through online platforms for different provinces, and we have documented all the links to these platforms in Supplementary Table 1 . Second, theoretical flue gas rates are available in Table 5 . Third, activity data are exclusively offered by the Ministry of Ecology and Environment of China."
10.1038/s41597-020-00672-2,"The position- and energy-dependent flux data were generated with MAVRIC, a serial-only code in the SCALE 6.2 package 19 . Some custom codes were developed to make the mesh-based sources outside of MAVRIC, but these codes are not available. However, the methods used in these custom codes have been adopted by Shift, a new parallel Monte Carlo code, which will be released in the next version of SCALE. Shift will be able to use the same geometry and materials as MAVRIC and perform similar calculations."
10.1038/s41597-020-00658-0,The code of the MARAS database was designed in SQL and written in Postgre SQL. It is being periodically upgraded but the latest version (10 th February 2020) is freely available through the “maras-files.tgz” file in figshare 29 . Future versions will be uploaded to this dataset on an annual basis.
10.1038/s41597-020-00676-y,"The figshare repository contains R v3.6.1 code for downstream data processing (‘Preprocessing_data’), and code to create the figures that appear in this publication (‘Publication_figures’) 22 . The GitHub repository contains the most up-to-date code for downstream data processing 21 ."
10.1038/s41597-020-00673-1,No custom code was used in the development of VOLCORE.
10.1038/s41597-020-00649-1,The ProNEVA code is available at http://amir.eng.uci.edu/software.php .
10.1038/s41597-020-00688-8,"Code used for the creation of this database is not included in the files uploaded to figshare. Our scripts for data collection, processing, and transformation, are available for inspection in the public GitHub repository that hosts our data ( https://github.com/owid/covid-19-data/tree/master/scripts/scripts/testing )."
10.1038/s41597-020-00678-w,"Source code used for RNAseq data processing and pre-processing of transcript, protein and phosphorylation site data files is available in GitHub 59 ."
10.1038/s41597-020-00685-x,MISSING
10.1038/s41597-020-00679-9,The source code that implements https://lod.proconsortium.org/ website is available from GitHub at https://github.com/PROconsortium/PRoteinOntology/tree/master/pro_lod The source code that implements PRO RESTful APIs is available from GitHub at https://github.com/PROconsortium/PRoteinOntology/tree/master/proapi
10.1038/s41597-020-00663-3,"Code used to generate and validate StableClim is available at https://github.com/GlobalEcologyLab/StableClim , with bash scripts to download the CMIP5 data from ESGF available at https://github.com/GlobalEcologyLab/ESGF_ClimateDownloads ."
10.1038/s41597-020-00683-z,MISSING
10.1038/s41597-020-00681-1,Codes used for bias correction of CMIP6-GCMs are available through the Github link: https://github.com/udit1408/cmip6_downscaling
10.1038/s41597-020-00642-8,An R package with all code for the gene expression normalization is available at https://github.com/Sage-Bionetworks/ampad-diffexp . All other analyses were generated using packages publicly available from their respective authors.
10.1038/s41597-020-00690-0,MISSING
10.1038/s41597-020-00680-2,Scripts used in this manuscript are available at https://github.com/lab-lab/nndb (Online-only Table 1 ). Additional information can be found at http://naturalistic-neuroimaging-database.org/.
10.1038/s41597-020-00677-x,"All statistical analyses were conducted using STATA (SE 15.1) software. All visualizations were created using R Version 3.6.2 and Tableau Professional 2019.1 software. All software code is open access on our website ( https://sites.tufts.edu/naumovalabs/analecta/ ) and figshare, and is available for public reuse with proper citation of this manuscript 36 ."
10.1038/s41597-020-00686-w,"The software versions and parameters used in this study are described below.1.SMRTlink: version 6.0, parameters: pbccs.task_options.max_length = 20000 pbccs.task_options.min_length = 300.2.CD-Hit-Est: version 4.7, parameters: -c 0.99 -T 20 -G 0 -aL 0.90 -AL 100 -aS 0.98 -AS 30 -M 0 -d 0.3.BUSCO: version 3.0.2, default parameters. -m tran -e 1e-05.4.BLASTx: version 2.2.29+, parameters: -outfmt 6, -e value 1e-5 --max-target-seqs 1.5.DIAMOND: version 0.9.24.125."
10.1038/s41597-020-00695-9,"The options used for the generation and processing of the virome data are as follows: Trimmomatic (v. 0.33): ILLUMINACLIP: TruSeq. 3-PE-2.fa:2:30:10 LEADING:10 TRAILING:10 SLIDINGWINDOW:4:16 MINLEN:100 SPAdes (v. 3.5.0 and v. 3.8.2): -k 27, 47, 67, 87, 107, 127--careful"
10.1038/s41597-020-00664-2,MISSING
10.1038/s41597-020-00687-9,No custom code was made for the compilation and validation procedures in this dataset.
10.1038/s41597-020-00696-8,R scripts for the analysis of DNA microarray and RNA-Seq transcriptomics data are available for download at: https://github.com/Greco-Lab/psoriasis-dermatitis-analysis .
10.1038/s41597-020-00691-z,MISSING
10.1038/s41597-020-00668-y,There was no custom code for this analysis
10.1038/s41597-020-00697-7,MISSING
10.1038/s41597-020-00655-3,"All code for collecting, formatting, processing, and learning on the data is made freely available at https://github.com/usc-sail/tiles-dataset-release . Information about the code dependencies and package requirements are available in the same Github repository."
10.1038/s41597-020-00675-z,"To guarantee transparency and reproducibility, the harmonisation workflow was carried out with open-source tools, namely PostgreSQL (9.5.17)/PostGIS (2.1.8 r13775)) and R (3.4.3) 56 ). The code is provided as a R package containing 17 functions along with the documentation on 51 . The LUCAS package includes all the scripts and documentation (also provided in pdf). Additionally, along with the package, a script ( main.R ) builds the harmonised database step by step. The workflow is schematically shown in Fig. 2 . All the processing is done with SQL with only column reordering and consistency checks being done in R. The code is freely available under GPL (> = 3) license."
10.1038/s41597-020-00670-4,"Metadata concerning the stimuli presented during the BOLD fMRI runs are publicly available at https://github.com/hbp-brain-charting/public_protocols . They include: (1) the task-stimuli protocols; (2) demo presentations of the tasks as video annotations; (3) instructions to the participants; and (4) scripts to extract paradigm descriptors from log files for the GLM estimation. Task-stimuli protocols from Preference, TOM and VSTM + Enumeration batteries were adapted from the original studies in order to comply with the IBC experimental settings, without affecting the design of the original paradigms. MTT battery pertains to an original protocol developed in Python under the context of the IBC project. Protocols of Self and Bang tasks were re-written from scratch in Python with no change of the design referring to the original paradigms. The scripts used for data analysis are available on GitHub under the Simplified BSD license: https://github.com/hbp-brain-charting/public_analysis_code . Additionally, a full description of all contrasts featuring data derivatives (see Section “ Derived statistical maps ” for details) as well as a list of the main contrasts are also provided under the folder hbp-brain-charting/public_analysis_code/ibc_data."
10.1038/s41597-020-00694-w,The numerical code corresponding to the X-TRACK/ALES processing and post-processing system is not public. It is based on the merging of methodologies previously described in 7 and 16 . Further code evolutions and associated data sources are indicated in the present manuscript.
10.1038/s41597-020-00698-6,The documented code and examples to implement the automated image analyses method from this study can be found in the following repository: https://github.com/mgonzalezrivero/reef_learning .
10.1038/s41597-020-00692-y,"Code for downloading the data and annotations in bossDB can be found in the ‘data_access_notebooks’ folder here: https://github.com/nerdslab/xray-thc . A Jupyter notebook for generating the results in Figs. 2 , 3 can be found in the ‘analysis_notebooks’ folder in the same repo. Annotations, images, and analysis notebooks used for the inter-rater reliability study, are also provided through figshare to facilitate reproducibility 28 . All of these examples are written in Python 3 and executed using Jupyter notebooks, a cross platform Python solution."
10.1038/s41597-020-00684-y,The execution of this work was not involved using any custom code.
10.1038/s41597-020-00700-1,MISSING
10.1038/s41597-020-00707-8,Python-language based codes for carrying out calculations and analyzing the results are provided at the JARVIS-Tools GitHub page ( https://github.com/usnistgov/jarvis ).
10.1038/s41597-020-00699-5,Scripts used to insert required metadata into the published BIDS dataset are freely available at https://github.com/MonashBI/Monash_rsPET-MR_prep .
10.1038/s41597-020-00709-6,MISSING
10.1038/s41597-020-00701-0,The code for compiling and generating the dataset is available in the Github repository for the project 14 .
10.1038/s41597-020-00703-y,MISSING
10.1038/s41597-020-00704-x,"The R code to replicate the analyses of this article can be found in a series of R markdown files and at figshare 67 : Dutkiewicz, Russo, Lee, & Bentz. SignBase: collection and analysis of geometric signs on mobile objects in the Paleolithic. figshare https://doi.org/10.6084/m9.figshare.c.4898643 (2020). Figure 2: Supplementary File 1 and figshare File 1 Figure 5: Supplementary File 2 and figshare File 2 Figure 6: Supplementary File 3 and figshare File 3 Figure 7: Supplementary File 4 and figshare File 4 Figure 8: Supplementary File 5 and figshare File 5 Figure 9: Supplementary File 6 and figshare File 6"
10.1038/s41597-020-00713-w,"SOAPnuke: Version: 2.1.2. Parameters: filter -l 5 -q 0.5 -n 0.1 -Q 2 –5 1 -c 50. Bowtie2: Version: 2.2.5. Parameters: -q --phred64 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0, -0.1 -I 1 -X 1000 -p 16 -k 200.deepTools: Version: 3.4.3. Parameters: --corMethod pearson --whatToPlot scatterplot --skipZeros --removeOutliers.MACS2: Version:2.2.5. Parameters: --nomodel --extsize 200 --shift -100 -- format BAM --gsize 2.17e8 -- call-summits.IDR: Version: 2.0.3. Parameters: --input-file-type narrowPeak --rank p.value --plot --log-output-file."
10.1038/s41597-020-00710-z,The source for the website code and database schema are available from a public GitHub repository ( https://github.com/gerontomics/synergyage ).
10.1038/s41597-020-00706-9,MISSING
10.1038/s41597-020-00712-x,The Building Data Genome 2 data set and the custom code used for its creation and analysis is hosted in a public Github repository ( https://github.com/buds-lab/building-data-genome-project-2 ) and its v1.0 release has been deposited in Zenodo 18 . This codebase includes several Jupyter notebooks with Python and R data analysis workflows that can be easily reproduced.
10.1038/s41597-020-00693-x,"The codes for Sobol’ sensitivity analysis, DEMC parameter optimization, PT-JPL model and LAI preparation are available at https://doi.org/10.17605/OSF.IO/MERZN . The codes require MATLAB version 2014a or higher."
10.1038/s41597-020-00711-y,The code to generate the back-calculated size-at-age data is available at https://github.com/JWicquart/fish_growth .
10.1038/s41597-020-00714-9,MISSING
10.1038/s41597-020-00705-w,MISSING
10.1038/s41597-020-00682-0,Data and code for the ClimActor R package functions is available on GitHub: https://github.com/datadrivenenvirolab/ClimActor .
10.1038/s41597-020-00721-w,MISSING
10.1038/s41597-020-00708-7,"The generated datasets are available from https://doi.org/10.6084/m9.figshare.12685937.v4 and https://github.com/zhudeng94/dailyCO2 . Codes for industrial emission calculation and summary table generation are available on the GitHub repository presented as worksheets. Also the raw data of power generation in U.S., EU27 & UK, India, Russia, Japan and Brazil are available on the GitHub repository. Other raw data and codes for emission estimation in other sectors are only available upon reasonable requests."
10.1038/s41597-020-00719-4,The codes used in this article were deposited in https://github.com/LuChenLab/40Tcells .
10.1038/s41597-020-00720-x,All Python algorithms used to generate the datasets described here are publicly available via https://github.com/NBloemendaal/STORM-return-periods . The STORM Python script can be found on https://github.com/NBloemendaal/STORM and the data-preprocessing is available via https://github.com/NBloemendaal/STORM-preprocessing .
10.1038/s41597-020-00722-9,MISSING
10.1038/s41597-020-00718-5,No custom computer code was used to generate the data described in the manuscript. A CHANS Excel Calculator describes the data we used is available in figshare data record ( https://doi.org/10.6084/m9.figshare.12637391.v5 ).
10.1038/s41597-020-00725-6,"Box 1 describes the algorithm to segment the audio (WAV file) into snippets of 300 ms. Supervised segmentation is a critical process for most of the audio analysis applications, its purpose being to split an audio stream into homogeneous segments. We used the pyaudioanalysis library to generate and extract 34 features, represented in Box 2 . This is an open python library that provides audio related functionalities such as: audio features, visualization, classification and segmentation 22 . Box 2 shows the function to create, extract and manipulate the 34 features. Both algorithms can be found in the Github repository 24 . We used the library audio feature extraction from pyaudioanalysis using the method dirWavFeatureExtraction() to extract the short-term feature sequences of WAV files (one audio signal per specimen), using a sliding window with a time overlap of 50% (frame size of 300 ms and frame step of 150 ms). The resulting 34-element feature vector for each audio file is extracted by mid-term averaging the short-term features."
10.1038/s41597-020-00689-7,MISSING
10.1038/s41597-020-00715-8,"All code is available on Github for our morphological segmentation , GPU data augmentation , and pre-trained model . The pre-trained model is packaged in a Docker container for ease of use. (Please see https://github.com/bbrister/ctOrganSegmentation , https://github.com/bbrister/cudaImageWarp , https://github.com/bbrister/ctorgansegdocker and https://hub.docker.com/repository/docker/bbrister/organseg .)"
10.1038/s41597-020-00723-8,"The AMP 2 package used for constructing the present database is available at https://github.com/MDIL-SNU/AMP2 and was released under a GPLv3 (GNU General Public License). The package requires pre-installation of numpy, scipy, spglib, and PyYAML modules. Detailed guidelines and examples can be found in the manual ( https://amp2.readthedocs.io/en/latest/ )."
10.1038/s41597-020-00727-4,"The academic version of the chemoinformatics toolkit CACTVS is available for free download from https://www.xemistry.com/academic/ for evaluation and for use in research and education (a paid license is required for commercial use). The transforms used in the generation of the SAVI database are freely available from https://cactus.nci.nih.gov/download/savi_download/ . The source code of the “lhasa” command in CACTVS that was developed for the SAVI project can be obtained from W.-D. Ihlenfeldt (info@xemistry.com, +49 6174 201455) upon request. Development of a different, more public, way of using CHMTRN/PATRAN transforms for SAVI-type product generation based on open-source code has begun but is in its early stages 93 ."
10.1038/s41597-020-00732-7,Analyses were conducted and figures were produced using the R environment 56 including the package ade4 57 . Scripts are available at Figshare 30 .
10.1038/s41597-020-00735-4,"All code is available in the github repository 57 https://github.com/mvdoc/budapest-fmri-data . The code includes scripts to process the stimuli, presentation scripts, and scripts for the analyses presented in this paper. The scripts rely heavily on open source Python packages such as PyMVPA 58 , nilearn 44 , pycortex 59 , scipy 60 , and numpy 61 ."
10.1038/s41597-020-00724-7,MISSING
10.1038/s41597-020-00736-3,The programs used to generate all the results were MATLAB (R2017b) and ArcGIS (10.5). The PSO-BP codes for matching the scales of the nighttime light data and modelling the relationships among the provincial energy-related CO 2 emissions are presented in Suppl. File 1 .
10.1038/s41597-020-00729-2,"The code used for data reprocessing (see usage notes in Technical validation section) is available on github gist ( https://gist.github.com/marinegor/96102c9b7ce87509a0832649d11ba927 ). The utility xdscc12 is available through XDS-Wiki website ( https://strucbio.biologie.uni-konstanz.de/xdswiki/index.php/Xdscc12 ). In previous publications, for SFX data processing, CrystFEL version 0.6.3 + 23ea03c7 (available on https://stash.desy.de/projects/CRYS/repos/crystfel/commits ) was used. For SWSX data processing, XDS (version BUILT = 20161101 for CysLT1R_6RZ4 and BUILT = 20161205 for CysLT2R_6RZ5–9) and XSCALE (version BUILT = 20161101 for CysLT1R_6RZ4 and BUILT = 20180319 for CysLT2R_6RZ4–9) were used in the original publication, together with the “neggia” library for reading HDF5 images, as described ( https://strucbio.biologie.uni-konstanz.de/xdswiki/index.php/Eiger ). For data reprocessing, XDS and XSCALE version BUILT = 20190315, and CrystFEL 0.8.0, were used."
10.1038/s41597-020-00734-5,Data processing and data analysis were performed on a Linux server using the Python version 3.7. All codes used for analysis are available in the public GitHub repository that hosts the data: https://github.com/GeoDS/COVID19USFlows .
10.1038/s41597-020-00739-0,"Source code for producing our dataset is freely available online 26 . Requirements are Python 3.7 or later, as well as PostgreSQL with PostGIS extensions (we used PostgreSQL 10.12)."
10.1038/s41597-020-00716-7,"IMPUTE v2.3.2: https://mathgen.stats.ox.ac.uk/impute/impute_v2.html QUICKTEST Version 0.98: http://toby.freeshell.org/software/quicktest.shtml METAL: https://genome.sph.umich.edu/wiki/METAL All other analyses, including the Prentice-weighted Cox-regression analyses, were performed using R 3.4.2 17 ."
10.1038/s41597-020-00728-3,Not applicable because the reported data were generated from experiments.
10.1038/s41597-020-00726-5,"All the BioClimInd calculations were conducted exploiting NetCDF data manipulation and analysis tools, namely CDO (Climate Data Operators) and NCO (NetCDF Operators) combined through DOS Batch (.bat) commands and scripts available through Github at https://github.com/CMCC-Foundation/BioClimInd ."
10.1038/s41597-020-00731-8,The versions of software used have been detailed in the Methods section. The custom website source code is available on github ( https://github.com/vivical/ButteLabCOVID ). A version-controlled Docker container is also available on dockerhub ( https://hub.docker.com/r/pupster90/covid_tracker ).
10.1038/s41597-020-00717-6,"The custom code used for reading the signals of the database was created in MATLAB R2017b and is freely available at figshare 42 or at the GitHub repository https://github.com/lyanet-upc/hd-emg-app.git . We provide: A readme file ( readmeapp.txt ) with instructions about how to run the code in a 2017b or higher Matlab version. A zip file ( hd_emg_app.rar ) containing: –the code main function ( app_hd_emg.m ). This function deploys an interactive Matlab app from which users can load, and friendly visualize data of a specific subject. Here, parameters like the type of task, effort level and signal window size can be set easily. Plots of AM and sEMG of all or a specific channel are provided and can be modified by selecting different times. –a function folder with auxiliary functions ( read_hd_emg_signals.m, get_color_scale.m, plot_hd_emg_maps.m ) needed to run the main function. A Matlab script ( db_reader.m ) with a simple example about how to read and plot data of a specific subject, tasks, effort level and muscle using Matlab code."
10.1038/s41597-020-00733-6,MISSING
10.1038/s41597-020-00743-4,"Bioinformatic tools used for validation are all open source and feely available. We used jellyfish 33 version 2.2.10 to count k-mers, pbmm2 version 1.2.0 to map to a reference, and samtools 35 version 1.9 to summarize metrics. Sequencing accuracy breakdowns, error type determination and sequencing coverage measured across GC composition bins were determined as previously described 21 ."
10.1038/s41597-020-00746-1,The newest version of QM-symex is available on figshare ( https://doi.org/10.6084/m9.figshare.12815276 ).
10.1038/s41597-020-00742-5,The code used to produce the figures included in the manuscript as well as the full cleaned and raw datasets are available on Github at https://github.com/bansallab/exemptions-landscape . The code runs in Python 3.6.
10.1038/s41597-020-00738-1,MISSING
10.1038/s41597-020-00740-7,MISSING
10.1038/s41597-020-00745-2,Instructions for downloading and installing psana can be found: https://confluence.slac.stanford.edu/display/PSDM/Offsite+Installation .
10.1038/s41597-020-00749-y,The Leiden algorithm was used for clustering and is freely available at https://github.com/vtraag/leidenalg .
10.1038/s41597-020-00750-5,"All data files created and used in processing are formatted in the Network Common Data Form (NetCDF), providing detailed metadata for each variable within the file, and can be read using MATLAB, Python, Fortran, C, C++, Java, and other languages. A Code file used to interpolate the raw sand elevation data is included in the repository folder. Code is written in MATLAB (R2019a) and is fully commented. Although MATLAB is a proprietary language, the.m files can be read with a text viewer."
10.1038/s41597-020-00748-z,"All scripts are accessible here: https://github.com/RAbolafiaRosenzweig/ESMAP . R code was used for the calculations of each component in Eq. 1 and gridding outputs from individual pixels to the E-SMAP grid. MATLAB was used to produce the final data product and conduct the technical validation. Further, processing of the data in network Common Data Form (netCDF) format was done for remapping and aggregating using the open source Climate Data Operators (CDO) and netCDF Operator (NCO) utilities. Hydrus-1D simulations were performed with publicly available model code ( https://github.com/bilke/hydrus )."
10.1038/s41597-020-00755-0,"In the process of constructing the dataset provided in this work, we used several automatic algorithms developed in our previous works 38 , 39 . The source code for the bounding box refining network (BBR-net) can be accessed at https://github.com/YijinHuang/BBR-Net (or https://doi.org/10.5281/zenodo.4041331 ) 49 and code for OD detection and fovea localization are available upon request. Also, we have embedded all involved algorithms into a cloud platform that we developed. Users of this dataset can access the two automatic algorithms by visiting our website at https://www.aiforeye.cn/ and uploading fundus images for analysis. The functions provided by our platform include classification of left and right eyes, DR grading, lesion detection, identification of OD and fovea, as well as some additional functions such as retinal vessel segmentation and statistical analyses of vessel morphometrics and lesion abnormalities. Please note that our algorithms for segmenting and classifying corneal ulcers from ocular staining images (the dataset we published before) 21 can also be accessed on this platform."
10.1038/s41597-020-00754-1,"As described in the Methods section, all of the analyses in this study were performed with the following open-access programs: QC checking for RNA-Seq data was performed by FastQC version 0.11.9. ( https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ). QC results were summarized by multiQC version 1.9. ( https://multiqc.info/ ). QC filtering was performed by using fastp 0.20.1 program with default setting. ( https://github.com/OpenGene/fastp ). After QC, adaptor sequences were trimmed by Trimmomatic-0.39 program. ( http://www.usadellab.org/cms/?page = trimmomatic ). All reads were aligned to the human reference genome sequence by STAR version 2.7.3a program. ( https://github.com/alexdobin/STAR ). Gene expression analyses were performed by RSEM version 1.3.3. ( https://deweylab.github.io/RSEM/ )."
10.1038/s41597-020-00744-3,A code used to generate the primary human T-cell SWATH spectral library building using the TPP is described in a recent publication 14 .
10.1038/s41597-020-00737-2,SAS code used for data preparation and analysis has been published at Open Science Framework under the same name as the publication ( https://doi.org/10.17605/OSF.IO/2DAK5 ) 37 . SAS code and output for the residual analysis can be accessed from this DOI.
10.1038/s41597-020-00751-4,"Parameters to software tools involved are described in the following paragraph. FastQC: version 0,11,2, --nogroup --casava. DIAMOND: version v0.9.22, parameters: -f 6 qseqid qlen qcovhsp pident score evalue length sseqid slen stitle. InterProScan: version 5.29–68.0, --goterms -t n -dp -f TSV, gff3 parameters. BWA: version 0.7.17, standard parameters, mem algorithm. SAMtools programs (view, sort, index and idxStats, flagstat): version 1.8, standard parameters. EdgeR: version 3.26.5. StreamSampler.jar: version 1.0."
10.1038/s41597-020-00741-6,MISSING
10.1038/s41597-020-00757-y,The coding of the measures is available in a Stata do file format. The author used Stata 14 to produce the final spreadsheet.
10.1038/s41597-020-00702-z,"The LCVP generally consists of (1) the LCVP itself, available as R data package (version 1.0.3 as of July 2020) and as tab-delimited textfile file and (2) the R-package lcvplants. The LCVP version 1.0.3 is available in both Microsoft Excel and text formats in the iDiv data portal ( https://idata.idiv.de/ddm/Data/ShowData/1806 ; https://doi.org/10.25829/idiv.1806-40-3009 ). A developmental version of the LCVP and the lcvplants package are publicly available via GitHub ( https://github.com/idiv-biodiversity/lcvplants ). We will constantly update the LCVP and plan to release a new version once every second to third year. We plan to closely collaborate with plant synonymy services and tools like e.g. BIEN, GNR, R packages taxonstand and taxize, to include LCVP as reference option. Requests for integrating LCVP can be made via the projects GitHub ( https://github.com/idiv-biodiversity/LCVP/issues )."
10.1038/s41597-020-00753-2,"The SHMAS was built using Apple’s open source ResearchKit framework ( https://github.com/researchkit/researchkit ) and AppCore ( https://github.com/ResearchKit/AppCore ). The SHMAS source code is available upon request. Participant data hosting was handled by IBM Watson Health Cloud ( https://www.ibm.com/watson-health/ ), a data collection solution where participant data were stored before being imported into Sage Synapse. The code used to process and clean the raw study data can be found on GitHub ( https://github.com/apratap/SleepHealth_Data_Release ). Suggested thresholds for excluding outliers are included in the code but commented out."
10.1038/s41597-020-00756-z,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository ( https://github.com/DeepPathology/MITOS_WSI_CMC/ ). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai 23 and OpenSlide 29 and provides some custom data loaders for use of the dataset.
10.1038/s41597-020-00752-3,MISSING
10.1038/s41597-020-00761-2,The Python scripts used for KuNG Fu database curation are available at Zenodo ( https://doi.org/10.5281/zenodo.3996125 ) 52 . The source code is available at https://github.com/BadSeby/KuNG-FU-Database .
10.1038/s41597-020-00760-3,"All the code used in the generation of this dataset is in the public domain, and it has been linked in the corresponding sub-section in this manuscript. Here we provide a summary of the linked repositories: Main repository for this publication: https://github.com/garikoitz/paper-scidata . Useful examples on data handling: https://github.com/garikoitz/paper-reproducibility . Code to create the preprocessing container: https://github.com/vistalab/RTP-preproc . Code to create the tracking container: https://github.com/vistalab/RTP-pipeline . Code to systematically access data and create tables: https://github.com/vistalab/scitran/ The Docker containers with the algorithms and the configuration parameters can be run in a computer with Linux, macos (tested) or Windows (not tested). The Docker containers can be downloaded and run both in Docker and Singularity for free."
10.1038/s41597-020-00762-1,MISSING
10.1038/s41597-020-00764-z,The code which was used to create Figs. 2 – 4 and analyse the data records in huskinDB can be found under the following link: https://github.com/RhDm/huskinDB_publication This repository contains a detailed guide on how to install the requirements and run the code.
10.1038/s41597-020-00765-y,"The IFS forecast model and the Meteorological Archival and Retrieval System (MARS) software are not available for public use as the ECMWF Member States are the proprietary owners. However, the YOPP dataset and the MARS data extraction features are freely available through the YOPP dataset website’s API ( https://apps.ecmwf.int/datasets/data/yopp/levtype=sfc/type=cf/ ) following a registration step."
10.1038/s41597-020-00763-0,"The code used to carry out lapse-rate based elevation adjustments is publicly available under a Creative Commons Attribution 4.0 International license (CC BY 4.0). The ClimateEU software package can be obtained from the figshare repository 14 , with the latest version of this software and gidded database also available via anonymous download without registration requirements at http://tinyurl.com/ClimateEU ."
10.1038/s41597-020-00758-x,The custom written code (R scripts) to pre-process the speed-based data is included in the data release.
10.1038/s41597-020-00768-9,"Data processing, validation and statistical plotting were performed using visualization tools on Citrination and Jupyter notebooks 24 in a Python 3 25 environment. The code is available on GitHub ( https://github.com/CitrineInformatics/MPEA_dataset )."
10.1038/s41597-020-00773-y,The source code of the anonymization pipeline developed for the LEOSS PUF is publicly available as open source software 34 .
10.1038/s41597-020-00778-7,"All code for reading, processing and graphically representing the data is freely available at https://github.com/elenamondino/nationwide_survey . Information on the packages used are listed in the same repository."
10.1038/s41597-020-00775-w,"Read count enriched positions and the RNA-Seq read density across the positions were determined using two source codes in Python (version 3.5.2) programming language, which are publicly available in Figshare 30 ."
10.1038/s41597-020-00774-x,MISSING
10.1038/s41597-020-00776-9,"We made available the following functions that allow the user to load and process the data. Further information on how to use the code can be found in the correspondent MATLAB files. • loadDataset, loadAction : allow the user to load and save the MoCA dataset in an easy-to-use data structure. loadAction gives the user the possibility of loading only part of the data streams, specifying for example an action label, a marker or an instance; • segmentAction : segments the single instances of action from the full MoCap streams. It makes use of the i ndex array mentioned above. Three types of visualisation functionalities are available: • visualiseAction : it produces a 3D plot of each marker’s trajectory • visualiseSkeleton : shows the arm skeleton over time while performing a complete action instance, from MoCap data; • initSynch and synchronizedView: jointly shows the action using RGB and Kinematic data. By means of the csv file synch_index.csv, the function initSynch prepares the data structures used in synchronizedView for the actual visualisation. All the functions we provided can be used also on the test scenes."
10.1038/s41597-020-00772-z,"The bioinformatics analysis pipeline, with a detailed list of command lines, was deposited on the repository Figshare ( https://doi.org/10.6084/m9.figshare.c.4860843 ) 76 ."
10.1038/s41597-020-00783-w,"The data were generated using SurveyCTO Collect v2.70, and downloaded using Microsoft Excel 2013. The datasets were then saved in CSV format and imported to Stata software. All codes that were used to clean the data (drop multiple columns, check missing data and remove duplicates) have been uploaded to the repository 10 ."
10.1038/s41597-020-00771-0,"DKAN Open Data Platform ( https://getdkan.org ) is used as a basis the for implementation of the INPTDAT platform. The INPTDAT code base particularly includes the implementation of Plasma-MDS and is publicly available at GitHub ( https://github.com/markus-m-becker/INPTDAT ). INPTDAT is freely-available under the “GNU General Public License, version 2 or any later version” license in agreement with the DKAN licensing."
10.1038/s41597-020-00767-w,"The source files for the data collection using the MEDAL measurement units are available in the BLOND data repository 20 . For completeness sake, we have also added these files to the CREAM repository 19 in the data collection folder. This repository contains all the scripts used for the technical validation of the measurement hardware capabilities. The code to reproduce the extraction of the product and maintenance events through the serial maintenance ports of the coffeemakers is available in the coffeemaker project repository provided by Q42 18 . We implemented the data processing, labelling tools, and utility functions in Python 3. The labelling tools were implemented in three Jupyter Notebooks, one corresponding to each step of the labelling pipeline. The individual source files are available in the CREAM repository 19 . All labelling steps can be fully reproduced and extended if necessary, using the supplied tools. Furthermore, we provide the utility class containing all necessary functions for loading and pre-processing the signals."
10.1038/s41597-020-00769-8,"The code, including documentation and examples in Jupyter notebooks for implementing the data transformations in the workflow, is available as hextof-processor ( https://github.com/momentoscope/hextof-processor ) 30 and mpes ( https://github.com/mpes-kit/mpes ) 31 . The parser for integrating preprocessed experimental data into the NOMAD database is available as parser-mpes ( https://gitlab.mpcdf.mpg.de/rpx/parser-mpes ) 64 ."
10.1038/s41597-020-00777-8,The code used to create and process the presented data is provided in 55 or is part of open source repositories.
10.1038/s41597-020-00780-z,"Software used to query the BioPortal API extends an existing suite of metadata analysis tools maintained by the Center for Expanded Data Annotation and Retrieval (CEDAR) and is available at https://github.com/metadatacenter/metadata-analysis-tools/ . Python notebooks which reproduce all other analyses, tables, and figures are available at https://github.com/lauramiron/CTMetadataAnalysis ."
10.1038/s41597-020-00770-1,MISSING
10.1038/s41597-020-00784-9,Raw data and R-code for cleaning are available at https://doi.org/10.17605/OSF.IO/Z39US
10.1038/s41597-020-00786-7,All the code needed to reproduce this study is released under the GNU GPL v3 license. The code is available at https://github.com/pierre-prandi/rsl .
10.1038/s41597-020-00781-y,EEGLAB has been used which is available as open-source. CIMVA documentation is available online: https://ohridal.org/cimva/CIMVA-Core-Description.pdf . No proprietary code has been deployed in this study.
10.1038/s41597-020-00779-6,"The code used to perform all steps described here and shown in Fig. 1 can be accessed via the Zenodo dataset repository entry for GCP-GridFEDv2020.1 ( https://doi.org/10.5281/zenodo.4277267 ) 74 . GCP-GridFEDv2020.1 uses the same code and methodology as GCP-GridFEDv2019.1 but includes updated estimates of national annual emissions through to 2019 from the GCP, as discussed in the Usage Notes and also detailed at ref. 74 ."
10.1038/s41597-020-00795-6,"All code necessary for generating the overall virtual water trade network is published in Zenodo 22 , https://doi.org/10.5281/zenodo.3891722 . Most of the coding was completed using the R scripting language and open source packages, with one script written using Python to access the IEA API. See Methods for a description and breakdown of use for each script file. Versions and Packages: • R-base: 4.0.0 • comtradr (R): 0.2.2 • Python: 3.7.4 • pandas (Python): 0.25.3 • numpy (Python): 1.18.1 • requests (Python): 2.22.0 • json (Python): 2.0.9"
10.1038/s41597-020-00789-4,"The given Matlab data variables were obtained by exporting the raw files (i.e., *.vhdr, *.vmrk, and *.eeg) using the EEGLAB 23 toolbox (v2019.1.0) and exporting the data using the “bva-io” plugin (v1.5.13). We wrote a short Matlab script (ExportingCode_vhdr2mat.m) to export and save the desired features of the recordings, as thoroughly detailed in the section Data Records. The code is included in the same repository as the rest of the data, and it is accompanied by a brief document (ExportingCode_vhdr2mat.docx) explaining details of the code."
10.1038/s41597-020-00792-9,"The preprocess script, validation dataset and the R code that performs the statistical analysis are available through https://github.com/quanturban/firm ."
10.1038/s41597-020-00791-w,WinADV software was used to process the ADV velocity data. Data were filtered using the despiking technique of 31 . The data measured with the PIV system were post-processed with Davis software from LaVision company.
10.1038/s41597-021-00799-w,"As an OBO library ontology ( http://www.obofoundry.org/ontology/cido.html ), the code of CIDO is openly available: http://purl.obolibrary.org/obo/cido.owl . The license of the code is Creative Commons BY 4.0. The source code uses the license CC-BY. CIDO has been deposited to the Ontobee ontology repository ( http://www.ontobee.org/ontology/CIDO ), the BioPortal repository ( https://bioportal.bioontology.org/ontologies/CIDO ), and the OLS repository ( https://www.ebi.ac.uk/ols/ontologies/cido )."
10.1038/s41597-020-00790-x,"An example script is provided with the dataset 26 , 29 . It contains commented scripts for reading and plotting the data in NIX format 25 . We have also included scripts for the generation of Figs 2 and 3 . All code is implemented in MATLAB (Mathworks Inc., version R2019a)."
10.1038/s41597-020-00794-7,"Any custom code used to generate and analyse this dataset is openly available on Git-based repositories. For data FAIRification see https://github.com/stkenny/grefine-rdf-extension and https://github.com/LUMC-BioSemantics/rett-variant , for VEP data analysis see https://github.com/counsyl/hgvs , and https://gitlab.bsc.es/inb/fair-rett for _summary_plots and HGVS pipelines."
10.1038/s41597-020-00782-x,"Code was written in R and Python for the purposes of this project. The arcpy module was run in the Python version that accompanies ArcGIS 10.5.1, which is the Python IDLE 2.7.13. Code is available at https://github.com/USDAForestService/TreeMap2014_scripts . The code: 1)       prepared the target data rasters (script names: “reclass_Landfire_disturbance_rasters_for_tree_list.py” and “write_EVG_remap_files.r”) 2)      performed the random forests imputation, using the R package yaimpute (script name: “yai-parallel_v02202019-final-Yes-disturbance_z1.r”) 3)     validated the output grid of imputed plot IDs by comparing it to FIA plots measured in 2014 and to the target rasters (script names: “national_validation_plots_Landfire.py” and “national_validation_plots_Landfire_step2.r”) 4)         compared the imputed raster to the target rasters as a measure of imputation accuracy (script name: “analyze_national_tree_list_output.r”)"
10.1038/s41597-021-00798-x,"The code supporting the finding of this study have been deposited at figshare 31 . All code needed for the user interface of PFAS, as well as the repeating of data pre-processing are included in “PFAS-Map” folder. The folder also contains a detailed PDF instruction and a demonstration video for the installation and use of the PFAS-Map. The code is also available on the Materials Data Engineering Laboratory - MaDE@UB portal ( http://madeatub.buffalo.edu )."
10.1038/s41597-021-00801-5,Matlab version 2020a was used for all analyses. An external MoCap Toolbox (version 1.5) is required to open C3D files (the Toolbox can be downloaded from https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/materials/mocaptoolbox ). The code used is available upon request.
10.1038/s41597-021-00796-z,"All relevant data (D1-D12) and figure (F1) presented in this article are available online at figshare 13 . They were developed using the collecting data and Eqs. ( 1 – 3 ) in detail described in the methods. Equation ( 1 ) was employed for EEE and wiring & cable to measure the waste outflows, and Eq. ( 2 ) was adopted for vehicle to measure the waste outflows. It should be highlighted here that some obtained data covers the year until the year of 2018 or 2019 (D1-D8), which has updated the given data until the year of 2015 or 2016 at the published article in Nature Communication 34 ."
10.1038/s41597-021-00800-6,No code was used in this study.
10.1038/s41597-021-00803-3,MISSING
10.1038/s41597-020-00788-5,The authors declare no custom code necessary for the interpretation or use of dataset.
10.1038/s41597-020-00785-8,"Echosounder raw data files are recorded in proprietary formats that typically require dedicated commercial or open-source acoustic processing software for visualization and processing. The custom Java software tool ‘ basoop.jar ’ used for incoming data registration and management, along with the MATLAB® GUI used for controlling data processing steps in Echoview® and NetCDF file creation can be obtained at: https://github.com/CSIRO-Acoustics/IMOS-Bioacoustics . The MATLAB® codes used for generating relevant figures are available at: https://github.com/CSIRO-Acoustics/Publications/tree/main/Scientific_Data/Data_Descriptor ."
10.1038/s41597-021-00805-1,"All code provided in DynaRev_Lib is written in MATLAB (R2019b). This folder contains the scripts used to process the raw data in order to obtain the post-processed data provided within the repository. The 3D Lidar point clouds described in Table 3 are provided in “.xyz” format which can be opened using the open source CloudCompare software package. The filename for each scan includes the date collected and the run after which the scan was completed, e.g. 20170918_DR2_7.xyz was completed after Run DR2–7 on 18 th September, 2017. A table providing the timings and notes about each scan is included within the DynaRev_3Dscans data record. Table 3 Data files associated with the DynaRev experiment available from https://doi.org/10.5281/zenodo.3889796 23 . Full size table"
10.1038/s41597-021-00802-4,MISSING
10.1038/s41597-021-00797-y,The different visualizations and observations presented in this research are also made available at the project website ( http://onto-apps.stanford.edu/lslodminer ). The scripts for the extraction algorithm and the community detection algorithm used in this research are made available on GitHub ( https://github.com/maulikkamdar/LSLODQuery ).
10.1038/s41597-021-00810-4,Source code for the GlyGen QC and integration can be found in the Github repository: https://github.com/glygener/glygen-backend-integration/blob/master/pipeline/integrator/make-proteoform-dataset.py
10.1038/s41597-021-00807-z,"The IDL code named MODIS_FUI.pro for calculating FUI from MOD09A1 data is also available via Figshare 25 . We note that the code contains a few steps that need ENVI software, so that it needs to be run under the ENVI + IDL environment. The ENVI version 5.3 and the IDL version 8.5 were used in the code development."
10.1038/s41597-021-00816-y,An R notebook used for generating images with package versions can be found in the AncientMetagenomeDir repository at https://github.com/SPAAM-community/AncientMetagenomeDir/tree/master/assets/analysis (commit 4308bb7). Code for validation of the dataset (with version 1 used for the first release of AncientMetagenomeDir) can be found at https://github.com/SPAAM-community/AncientMetagenomeDirCheck and https://doi.org/10.5281/zenodo.4003826 .
10.1038/s41597-021-00811-3,"Beyond the data, a small library of MATLAB (The Mathworks, MA, USA) custom functions accompanies the dataset. In particular, a binary file reader for MATLAB is provided, enabling loading the signals acquired with the Porti7 electrophysiological recording system in a MATLAB variable. Using the source code, analogous readers in different programming languages can be also developed. We also provided a graphical user interface enabling simultaneous scrolling of the long PWD image (first loaded as.bmp file) and all the related Porti7 channels and internal signal (in.bin format). This interface can be used to browse the raw data easily. A scientist can modify its source code to show the fECG signals extracted by the preferred method without any limitation. This feature was not added in this version to avoid any bias in the data evaluation. Additionally, the envelope extraction function described in this paper is available for PWD processing. In this work, we presented the dataset and discussed its potentialities for scientific and technological advancements in the field. As proof of concept, we provided figures of merit enabling researchers to quantitatively evaluate the dataset. The code for fECG extraction and processing is available in the open-source electrophysiological toolbox (OSET) 34 ."
10.1038/s41597-021-00819-9,"The new global vector-based hydrography dataset, consisting of basins, watersheds, and river networks of variable and constant D d , is produced using Python v3.7.3 and the TauDEM software v5.3.8. All computations are completed using the Della high-performance computing clusters at Princeton University. For geospatial analysis, we use the freely available GeoPandas library in Python; for some figure displaying purposes, we use the ArcPro version 2.4.1. Key Python scripts developed for this work are openly shared with the scientific community at Github: https://github.com/peironglinlin/Variable_drainage_density ."
10.1038/s41597-021-00820-2,We used various Python scripts to process the data for input into the databases. These scripts were used to calculate climate value summaries and convert phenology observations to a uniform reference date (January 1). The scripts are available at https://github.com/clara-risk/tree-provenance-trials .
10.1038/s41597-021-00813-1,MISSING
10.1038/s41597-021-00821-1,MISSING
10.1038/s41597-021-00817-x,Scripts using the R and MATLAB programming language are provided to produce figures and extract data from external databases. Additional code and related files are available from the corresponding author upon request.
10.1038/s41597-021-00815-z,Custom generated code for the described methods is available at https://github.com/ISIC-Research/2020-Challenge-Curation .
10.1038/s41597-021-00806-0,"We provide the python scripts to inject artificial defects to the healthy skulls on GitHub ( https://github.com/Jianningli/SciData ), which can serve as a starting point for future development based on our skull dataset for other researchers. We also provide additional python scripts for the extraction of point clouds from 3D image volumes and Matlab scripts to convert the triangular, surface meshes of the skulls back to voxel grids (voxelization). The dependencies and usage of the scripts are described in our GitHub repository."
10.1038/s41597-020-00793-8,MISSING
10.1038/s41597-021-00828-8,"The genomic and transcriptomic sequence data were produced by corresponding software provided by the sequencing platform manufacturer, and the software (including versions, parameters and settings) used for genome assembly was cited in the Methods section, with default parameters used when no detailed parameters were mentioned. The code for the NGD construction in Java is available in the Figshare database 49 ."
10.1038/s41597-021-00825-x,"In order to get the internal parameters of the camera we have used the Camera Calibration Toolbox for Matlab, available at the following link: http://www.vision.caltech.edu/bouguetj/calib_doc/ . The undistort freak wave image has been obtained by means of the TriDmetrix 2014 software, which is available at the following link: http://www.tridmetrix.com/ ."
10.1038/s41597-021-00809-x,All code used in processing the scRNA-seq data and in drawing the figures are available on Github at the following link: https://github.com/oxwang/SciData_scRNAseq .
10.1038/s41597-021-00827-9,JavaScript code used to generate the cropland layer and crop type maps are available from the figshare repository 46 .
10.1038/s41597-021-00812-2,The initial structure generation was carried out by using Open Babel 36 . Further structure optimization and the creation of non-equilibrium structures was performed by utilizing an in-house version of DFTB + 47 together with ASE 48 . Note that all necessary features regarding the utilized DFTB3+MBD approach are available in the current DFTB + version 62 . All DFT calculations were carried out using FHI-aims 53 (version 180218).
10.1038/s41597-021-00826-w,The DALEC model and the model-data fusion code used to generate the assimilated data products can be obtained through the GitHub repository at ( https://github.com/ultradove/model-data-fusion ). Further questions can be directed towards: Rong Ge (ge7218@163.com).
10.1038/s41597-021-00814-0,"The software Phytobs, used for some trait compilation or calculation (biovolumes), was created by our team in a first version in 2009 by Hadoux and Laplace-Treyture 51 for phytoplankton counting purposes. It was upgraded over the years to allow biovolume calculations and to integrate taxonomy and synonymy 31 . This free software is now in version 3.2 (French, English and Spanish languages) and publicly available on https://hydrobio-dce.inrae.fr/phytobs-software/ ."
10.1038/s41597-021-00822-0,"With respect to the technical validation section, we provide the code to reproduce the Figs. 6 – 8 . The code was created with R Studio ( https://rstudio.com/ ) based on R Project for Statistical Computing ( https://www.r-project.org/ ). The code file “Technical_validation.Rmd” as well as the corresponding input data are available on Zenodo 20 ."
10.1038/s41597-021-00844-8,No computer code was used in the data curation process.
10.1038/s41597-021-00835-9,MISSING
10.1038/s41597-021-00829-7,"The software developed by EAV is at https://github.com/eduardo-aubert/CHBMP-Code and https://doi.org/10.7303/syn22324937 . The codes are: 1) Anomplg.exe: to anonimize the EEG records, deleting all the personal information stored in the EEG recordings, which could facilitate the identification of the participants. 2) Plg2bids.exe: to read the original EEG recordings in NEURONIC format and convert them to BIDS structure. 3) Joinbids.exe: to combine BIDS-EEG and MRI-BIDS into only one structure EEG-MRI-BIDS."
10.1038/s41597-021-00830-0,The only data processing procedures that we performed on the dataset were the ones described above. The first procedure was carried out to temporally align the data collected using different sensors. The second procedure was carried out to obtain an evenly-sampled timeseries.
10.1038/s41597-021-00831-z,The only data processing procedures that we performed on the dataset were the ones described above. The first procedure was carried out to temporally align the data collected using different sensors. The second procedure was carried out to obtain an evenly-sampled timeseries.
10.1038/s41597-021-00808-y,"Preprocessing of the data was performed on R version 3.5.2. The preprocessing of Agilent and Affymetrix expression data was performed using eUTOPIA 9 , an R shiny software freely available on https://github.com/Greco-Lab/eUTOPIA . Custom scripts used for preprocessing of Illumina BeadChip and RNA sequencing data are available on GitHub on https://github.com/grecolab/Public_Nano ."
10.1038/s41597-021-00834-w,Scripts used to extract information from similarity matrix can be found on Github 42 .
10.1038/s41597-021-00837-7,The version and parameter of all bioinformatics tools used in this work are described in the Methods section.
10.1038/s41597-021-00836-8,The raw data files and processed data matrices are available in the online repository MTBLS679 33 . The complete history of the used workflow for the raw LC-MS data pre-processing is available in Galaxy-W4M 31 from https://doi.workflow4metabolomics.org/W4M00008 . We provide the complete R 32 script used to process the data along with a detailed tutorial in the supplemental material (Supplementary File 1 ).
10.1038/s41597-021-00841-x,No code was used to generate the data records.
10.1038/s41597-021-00833-x,MISSING
10.1038/s41597-021-00839-5,"All code for the Zoltar system is available under a GPL-3.0 license from https://github.com/reichlab/forecast-repository/ . Zoltr and zoltpy packages are available under GPL-3.0 licenses from http://reichlab.io/zoltr/ and https://github.com/reichlab/zoltpy , respectively."
10.1038/s41597-021-00824-y,Python-language-based codes with examples are given at the JARVIS-Tools page https://github.com/usnistgov/jarvis .
10.1038/s41597-021-00840-y,No custom code was developed for the generation or processing of this dataset.
10.1038/s41597-021-00842-w,The sequence data were generates using the Genome Sequence Annotation Server “GenSAS” ( https://www.gensas.org ) 44 . No custom computer codes were generated in this work (Table 11 ).
10.1038/s41597-021-00823-z,"Table 1 below reports the links to each web service and github.com URL implementing the processing pipeline. All code not found on brainlife.io, including visualization code, can be found at https://github.com/bacaron/athlete-brain-study ."
10.1038/s41597-021-00847-5,"The Met Office Unified Model is available for use under licence. Several research organisations and national meteorological services use the UM in collaboration with the Met Office to undertake basic atmospheric process research, produce forecasts, develop the UM code, and build and evaluate Earth system models. For further information on how to apply for a licence, see https://www.metoffice.gov.uk/research/approach/collaboration/unified-model/partnership Python and R code used to process the RAL2 data is available from Zenodo 46 ."
10.1038/s41597-021-00846-6,An R script used for the descriptive analyses presented can be found in the SEDAC repository together with the data files. All data presented in tables and figures in the manuscript can be reproduced using the provided code. Note that for Fig. 3 this also requires downloading and joining data from EM-DAT.
10.1038/s41597-021-00843-9,"The signal recording was performed using two programs in parallel: OT BioLab version 2.0.6254 available at www.otbioelecttronica.com for recording HD-sEMG and synchronization signals, and the custom recording software developed in LabVIEW 2016 for force signals recording, generating synchronization pulses, visualizing forces, and generating commands and cues. Data post-processing was done in Matlab and Python. The custom codes for temporal re-labeling and outlier scores are available at the GitHub repository: https://github.com/Neuroengineering-LTH/HDsEMG-database-Associated-codes ."
10.1038/s41597-021-00850-w,The codes for constructing the dataset were written in Python language and can be downloaded from https://urbanclimate.tse.ens.titech.ac.jp/ or may be requested directly from the corresponding author. The inputs used to construct the present and future AHE datasets are all publicly available online with sources cited within this manuscript. Specific pre-processed inputs may be requested from the corresponding author upon request.
10.1038/s41597-021-00845-7,"To convert DICOM images to Nifti files, the open sources software dcm2niix is freely available at https://github.com/rordenlab/dcm2niix . To construct machine learning decoders, all studies employed the freely available Sparse Logistic Regression Toolbox, implemented in Matlab. The toolbox implements classification with sparsity prior and is available for both binary and multiclass problems. The toolbox can be found at https://bicr.atr.jp/~oyamashi/SLR_WEB.html . Software to run DecNef experiments can be obtained freely upon reception of a signed agreement form, and participating the resulting data towards the DecNef data collection. The usage of the software is intended for academic purposes. The software is implemented in Matlab and can be found at https://bicr.atr.jp/decnefpro/software/ ."
10.1038/s41597-021-00832-y,"Well-documented scripts, written in Python 3.6.4 40 , are present alongside the dataset (also available on GitHub 41 ). These include the scripts used to generate the results described in the Technical Validation section as well as a script to calibrate magnetic field measurements against hard/soft-iron distortions. The data required to replicate the experiments reside in OutFin/Code/temporal_data . Depending on the script, some of the following libraries may be required: os, pandas, scipy, random, sklearn, matplotlib, numpy, statistics, keras, math . Additionally, a thorough description of the collection environment in the form of an interactive map (developed using QGIS 3.10 27 ) is provided. The map is composed of several layers that display information such as RP coordinates (both ground truth and smartphone estimated), pictures of the collection sites, and building height and ground elevation (as provided by the City and County of Denver 42 ). High-resolution aerial imagery (3-inch), provided by the Denver Regional Council of Governments 43 , are used as the basemap."
10.1038/s41597-021-00848-4,Images were acquired using an Operetta CLS system combined with Harmony v4.9 followed by image analysis with Columbus v2.9.0 (all from PerkinElmer). Cytotoxicity data based on the high-content images were analysed using ActivityBase v8.0.5.4 (IDBS). Data were visualised in Prism8 for Windows 64-bit version 8.0.0 (GraphPad Software) and TIBCO Spotfire Analyst v7.11.2 (PerkinElmer). A collection of R scripts and KNIME workflows for the analysis of our datasets have been made available ( https://github.com/agiani99/KNIME_Screen ) with no usage restrictions.
10.1038/s41597-021-00838-6,"The MATLAB code developed for this work is available from Zenodo 33 . In addition, we created a small sample dataset that can be with the code as well as a complete guide that illustrates fundamental steps from the preparation of the input data to making the VS measurements. The guide and the example dataset are also hosted in Zenodo 33 ."
10.1038/s41597-021-00860-8,"The customized Matlab code (Matlab 2018a) used for the examples provided below is available in the GitHub repository ( https://github.com/liaochen1988/MSKCC_Microbiome_SD2021_Scripts ) with each part in a separate directory: • Figure 1 : example 1_display_patient_timeline/main.m • Figure 2 : example 2_visualize_compositional_states/main.m • Figure 3 : example 3_drug_administration_stats/main.m • Figure 4 : example 4_impacts_of_antibiotics/main.m • Figures 5 , 6 : example 5_survival_analysis/main.m"
10.1038/s41597-021-00859-1,MISSING
10.1038/s41597-021-00861-7,GEE codes that calculates global LST cont ( t ) and the validation of the dataset along with explanations on the usage of the code are publicly available through Github ( https://github.com/shilosh/ContinuousLST.git ) and Zenodo 43 ( https://doi.org/10.5281/zenodo.3952603 ).
10.1038/s41597-021-00856-4,MISSING
10.1038/s41597-021-00849-3,"Code for processing is publicly available and can be found on GitHub under the scripts folder ( https://github.com/neurodata/neuroparc ). Examples of useful functions include resampling parcellations to a desired voxel size, the ability to register parcellations to any given reference image, and center calculation for regions of interest for 3D parcellations. Jupyter notebook tutorials are also available for learning how to prepare atlases for being added to Neuroparc. All code is provided under the Apache 2.0 License. Visualizations are generated using both MIPAV 8.0.2 and FSLeyes 5.0.10 to view the brain volumes in 2D and 3D spaces 50 , 51 . Figure 1 can be created using MIPAV triplanar views of each atlases with a striped LUT."
10.1038/s41597-021-00857-3,The different available software and the versions used to acquire and process data presented in the datasets are summarized in Table 6 .
10.1038/s41597-021-00864-4,"All code used for data extraction, processing, and visualization is available online (two JUPYTER notebooks in Python 3.7) and two MATLAB’s Live Script files) in the PhysionNet project associated with the present paper 15 . These scripts are publicly available to allow for reproducibility and code reuse. The associated PhysioNet project contains 3 folders. The Data folder contains the data subsets described in this paper. The content of the other folders is explained below. Queries folder Contains the queries to obtain the raw insulin entries and blood glucose readings. The queries can be run on Google’s BigQuery. The file glucose_readings.sql contains the code to extract glucose readings, and the file insulin.sql the codes for insulin entries. Notebooks Contains the following files: JUPYTER notebooks 1.0-ara-data-curation-I.ipynb: This notebook contains the processing stages to obtain the curated entries of glucose readings and insulin inputs. 2.0-ara-pairing-II.ipynb: This notebook contains the pairing rules to link a preceding glucose reading with a regular insulin input. MATLAB Live Scripts Glucose_Analysis.mlx: This contains a deeper statistics analysis on the glucose readings. It is a complementary analysis for 1.0-ara-data-curation-I.ipynb notebook. Pairing.mlx: Contains the results related to the pairing of a preceding glucose reading and an insulin event. It is a complementary analysis for the 2.0-ara-pairing-II.ipynb notebook. Glucose_Analysis.html & Pairing.html: Contain the same information as the scripts mentioned above, but readable in a web browser. Functions subfolder: Contains MATLAB functions that are called in the Live Scripts described above."
10.1038/s41597-021-00866-2,The code used to generate the combined AOD product (sgpqcaod.c1) is available on Github ( https://github.com/ARM-Development/qc_aod ). All computations have been performed using the Python environment (v3.6.8) and the ARM Data Integration (ADI) library ( https://github.com/ARM-DOE/ADI ). More information about ADI is available at the ADI Documentation website ( https://engineering.arm.gov/ADI_doc/index.html ).
10.1038/s41597-021-00871-5,MISSING
10.1038/s41597-021-00869-z,MISSING
10.1038/s41597-021-00870-6,"All code used for curating, annotating, and (pre)processing AOMIC are version-controlled using git and can be found in project-specific Github repositories within the NILAB-UvA Github organization: https://github.com/orgs/NILAB-UvA . Many pre and postprocessing steps were identical across datasets, so the code for these procedures is stored in a single repository: https://github.com/NILAB-UvA/AOMIC-common-scripts . Possible parameters are all hard-coded within the scripts, except for a single positional parameter pointing to the directory to be processed. For custom Python-based scripts, we used Python version 3.7. All code was developed on a Linux system with 56 CPUs (Intel Xeon E5-2680 v4, 2.40 GHz) and 126GB RAM running Ubuntu 16.04. All curation, preprocessing, and analyses were run on said Linux system, apart from the Fmriprep, MRIQC, and Freesurfer analyses, which were run in a Docker container provided by those software packages. Custom code was parallelized to run on multiple CPUs concurrently using the Python package joblib ( https://joblib.readthedocs.io ). For curation, preprocessing, and analysis of the datasets, we used a combination of existing packages and custom scripts (written in Python or bash). To convert the data to the Brain Imaging Data Structure (BIDS), we used the in-house developed, publicly available software package bidsify (v0.3; https://github.com/NILAB-UvA/bidsify ), which in turn uses the dcm2niix (v1.0.20181125) 94 to convert the Philips PAR/REC files to compressed nifti files. In contrast to the data from PIOP1 and PIOP2 (which were converted to nifti using dcm2niix ), r2aGUI (v2.7.0; http://r2agui.sourceforge.net/ ) was used to convert the data from ID1000. Because r2aGUI does not correct the gradient table of DWI scans for slice angulation, we used the angulation_correction_Achieva Matlab script (version December 29, 2007) from Jonathan Farrell to do so (available for posterity at https://github.com/NILAB-UvA/ID1000/blob/master/code/bidsify/DTI_gradient_table_ID1000.m ). To remove facial characteristics from anatomical scans, we used the pydeface package (v.1.1.0) 86 . Finally, to convert the raw physiology files (i.e., Philips “SCANPHYSLOG” files) to BIDS, we used the in-house developed, publicly available Python package scanphyslog2bids (v0.1; https://github.com/lukassnoek/scanphyslog2bids ). The outputs from the BIDS-conversion pipeline were checked using the bids-validator software package (v1.4.3). Anatomical and functional MRI preprocessing were done using Fmriprep (v1.4.1; see the Derivatives section for extensive information about Fmriprep’s preprocessing pipeline) 42 . For our DWI preprocessing pipeline, we used tools from the MRtrix3 package ( www.mrtrix.org ; v3.0_RC3) 60 and FSL (v6.0.1) 95 . For the VBM and dual regression pipelines, we used FSL (v6.0.1) 71 , 72 , 73 . To create the files with Freesurfer-based metrics across all participants, we used Freesurfer version 6.0.0 96 . Physiological nuisance regressors (RETROICOR and HRV/RVT regressors) were estimated using the TAPAS PhysIO Matlab package (v3.2.0) 75 . First-level functional MRI analyses for technical validation were implemented using the Python package nistats (v0.0.1b2) 59 and nilearn (v0.6.2) 59 , 97 . For the inter-subject correlation analysis the Brain Imaging Analysis Kit was used (BrainIAK, http://brainiak.org , v0.10; RRID:SCR_014824) 92 . Plotting brain images was done using FSLeyes (v0.32) 98 and plotting statistical plots was done using the Python packages seaborn 99 and Matplotlib 100 ."
10.1038/s41597-021-00868-0,A MATLAB script (data2FiSH.m) for extraction of the data from the database in a ready-to-use format for the FiSH code and an accompanying readme text file (README_script_data2FISH.txt) with instructions for its use. These are contained within the “Data2FiSH” folder along with 2 supporting MATLAB scripts: “utm2deg.m” and “deg2utm.m”.
10.1038/s41597-021-00873-3,Two R scripts are available with the data files in the database. The “ WOODIV_working_file_generation.R ” script in the “SPECIES” folder combines all the information about species occurrences and nomenclature into one table to run the analyses. The “ WOODIV_trait_table_generation.R ” in the “TRAITS” folder uses the species nomenclature to compute species mean traits and impute values when no data is available using nomenclature. They run under R software version 3.6 (last tests under version 3.6.2 34 ).
10.1038/s41597-021-00875-1,MISSING
10.1038/s41597-021-00872-4,"Four scripts for FASTQC, Trimmomatic, Salmon index creation and Salmon quantification have been created and are available from the authors on request. There is not any custom code involved with these bioinformatic tools and they can be freely downloaded from: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ; http://www.usadellab.org/cms/?page=trimmomatic ; https://combine-lab.github.io/salmon/ . Scripts used to generate EORNA portal pages, database components including plotly visualisations can be found in the github repository https://github.com/cropgeeks/eorna 49 . Essential source code components of the web page such as the JavaScript code for the plotly visualisations can also be viewed via the page source code."
10.1038/s41597-021-00878-y,"The source code for the WNTRAC automated NPI curation system, including the data processing pipeline, WNTRAC Curator tool and NPI data browser is available in a public GitHub repository at https://github.com/IBM/wntrac/tree/master/code alongside the up-to-date version of the dataset https://github.com/IBM/wntrac/tree/master/data . Please refer to the README file in repository for further instructions on using the code."
10.1038/s41597-021-00818-w,MISSING
10.1038/s41597-021-00863-5,"Our Github repository ( https://github.com/cxr-eye-gaze/eye-gaze-dataset ) contains code (Python 3) for: 1.Data Preparation Inclusion and exclusion criteria on MIMIC dataset (see details in Inclusion and exclusion criteria section). Case sampling and image preparation for eye gaze experiment (see details in Preparation of images section). 2.Data Post -Processing Speech-to-text on dictation audio (see details in Audio extraction and transcript generation section). Mapping of eye gaze coordinates to original image coordinates (see details in Fixations and eye gaze spreadsheets section). Generate heatmap images (i.e temporal or static) and videos given eye gaze coordinates. The temporal and static heatmap images were used in our demonstrations of machine learning methods in Use of the Dataset in Machine Learning section. 3.Technical Validation Validation of eye gaze fixation quality using calibration images (see details in Validation of eye gaze data). Validation of quality in transcribed dictations (see details in Validation of transcripts section). The t-test for eye gaze fixations for each anatomical structure and condition pairs (see details in Statistical analysis on fixations section) 4.Machine Learning Experiments, as described in Use of the Dataset in machine learning section. Software requirements are listed in https://github.com/cxr-eye-gaze/eye-gaze-dataset ."
10.1038/s41597-021-00804-2,MISSING
10.1038/s41597-021-00862-6,"We have used Octave/MATLAB for processing the data. Along with the downscaled GRACE data, we also provide the script freely available for download from figshare https://doi.org/10.6084/m9.figshare.c.5054564 62 ."
10.1038/s41597-021-00867-1,"The remotely sensed oil palm map is available to view in GEE at https://olhadanylo.users.earthengine.app/view/oilpalmseasia . The GEE code, documentation and validation data are available in Github at: https://github.com/odanylo/oilpalmseasia ."
10.1038/s41597-021-00877-z,No custom code was used in this analysis. Examples of how to combine this photosynthetic pathway dataset with other TERN data infrastructure in the R statistical environment has been provided in the supplementary material (Supplementary File 1 ).
10.1038/s41597-021-00883-1,The code used to produce the figures in this manuscript is available at https://github.com/bfinl/BCI_Data_Paper .
10.1038/s41597-021-00880-4,Generalized linear models used in false negative estimation were conducted in JMP Pro V14 40 . Network figures were created with the igraph package in R Version 3.6.2 51 .
10.1038/s41597-021-00876-0,"The files uploaded to Figshare 31 also contain two independent programs for data visualization and for the example demonstration of the DCNN network presented in the above section. The first program can be used to generate the distance vs. time graph for user 1, as shown in Fig. 6 . The dataset, its subfolders and the code (having the ‘m’ file extension) are extracted to the same directory where the code can be executed. After running the code, the user-interface instructions and comments in the code can be used to plot the distance-time (fast time vs. slow time) samples of the hand gestures. The same program can be used to plot the graphs for the other human volunteers as well. The second program is uploaded to a separate directory called “Exemplary CNN demonstration”, which can generate results similar to those shown in Table 3 . Note that the exact accuracy may vary across trials."
10.1038/s41597-021-00881-3,"A Matlab® script ( CodeAvailability.m available in figshare 32 ) is provided to demonstrate how the dataset can be accessed and how to visualize the processed knee joint angles from a single participant (in this case, Participant 1) in the sagittal plane, for the walking speed of 4 km/h. Moreover, the script also presents an example of creating the graph of the vertical component of the raw GRF for a walking speed of 3 km/h. Additionally, it is provided a simple segment of code to access the stride time of all trials for a speed of 2.5 km/h. To run the provided code, the script must be placed on a folder that contains the MAT files folder."
10.1038/s41597-021-00853-7,Data were processed in R version 3.5. and code was written in RStudio 1.1.463. The sample code to calculate residential electricity and LPG use using the National Sample Survey can be downloaded from 52 .
10.1038/s41597-021-00882-2,"The data set can be used without any further code. As stated in the usage notes, we recommend using the scripts provided for the ROBUST-MIS and surgical workflow challenges ( https://phabricator.mitk.org/source/rmis2019/ and 19 ) as well as the challengeR package 20 ( https://github.com/wiesenfa/challengeR ) for comparative benchmarking of algorithms."
10.1038/s41597-021-00884-0,"The presented data were analyzed with the following software and applications: 1.  All files were analyzed using the MaxQuant software suite 1.6.0.16 ( www.maxquant.org ). 2.  Quantitative data analysis and Pearson correlation were performed with Perseus ( www.perseus-framework.org ). 3.  Functional analysis was performed with the IPA software (Qiagen Inc.,USA, https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis )."
10.1038/s41597-021-00885-z,Python-language based scripts for obtaining and analyzing the dataset are available at https://github.com/usnistgov/jarvis.
10.1038/s41597-021-00887-x,No custom code has been used during the generation and processing of this dataset.
10.1038/s41597-021-00893-z,"Python scripts 26 for the simulation setup and the creation of the dataset are publicly available on Github ( https://github.com/jleuschn/lodopab_tech_ref ). They make use of the ASTRA Toolbox 36 (version 1.8.3) and the Operator Discretization Library 35 (ODL, version ≥0.7.0). In addition, the ground truth reconstructions from the LIDC/IDRI database 21 are needed for the simulation process. A sample data split into training, validation, test and challenge part is also provided. It differs from the one used for the creation of this dataset in order to keep the ground truth data of the challenge set undisclosed. The random seeds used in the scripts are modified for the same reason. The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI database used in this study."
10.1038/s41597-021-00886-y,"All code for formatting, cleaning, and quality assurance was written in Python (python.org) with use of the NumPy (numpy.org) and Pandas (pandas.pydata.org) libraries. This code is available on the studies OSF page, along with the code used to produce Online-only Tables 1 and 2 All code is released under a free and open source license (BSD three-clause): https://doi.org/10.17605/OSF.IO/GPXWA ."
10.1038/s41597-021-00888-w,There is no custom R code produced during the collation and validation of this dataset.
10.1038/s41597-021-00889-9,Custom scripts were not used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-021-00891-1,"All code used for downloading and processing the data used in this project, including the machine learning and technical validation code, may be accessed at https://doi.org/10.5281/zenodo.4499264 55 . To ensure that our work is reproducible, all code is written in open-source languages. Some scripts are in R and others are in Python. Python scripts need Python 3; R versions beyond 3.5.1 should suffice. The scripts on Zenodo are a copy of our GitHub repository of scripts and contains the following files and directories: • General_Project_Functions : Scripts to obtain the prediction set locations and tools that are generally useful during the data processing, such as making buffers around points and reprojecting point coordinates. • Get_PM25_Observations : Scripts to process PM 2.5 observations from across the western U.S. These observations are used to train our machine learning models. • Get_Earth_Observations : Scripts to download and process observations from data sets that are used both as inputs for our machine learning models during training and as inputs for our models in the prediction stage. The file Overall_steps provides all necessary directions. Individual README files (in each folder) provide more details if there are any. • Merge_Data : Scripts to merge all the data together and derive some spatio-temporal variables. • Machine_Learning : Scripts to run and evaluate our machine learning models. The folder Final_scripts contains all code used for our final analysis. The code in the Exploring_models folder was all preliminary testing. • Estimate_PM25 : Scripts to use our machine learning models to make final predictions and to explore the prediction data sets over time and space."
10.1038/s41597-021-00895-x,The code used to build the systemic architecture of the GDKM is available on GitHub: https://github.com/yang4851/gdkm .
10.1038/s41597-021-00894-y,"Statistical processing and analyses were performed in R version 3.4.0. Quality control and median normalization were performed using the R package pmartR version 0.9.0, freely available on GitHub ( https://github.com/pmartR/pmartR ) 30 . Default parameter values for pmartR function calls were used. QC-RLSC and the calculation of log2 fold changes and z-scores were carried out using in-house R functions and are available on Github ( https://github.com/pmartR/qcrlsc )."
10.1038/s41597-021-00898-8,All data and code is available without restrictions from figshare 19 and from the corresponding author on request.
10.1038/s41597-021-00890-2,The scripts used to generate the final quantification values (and selected intermediate files) are available at: https://github.com/J-Andy/Protein-expression-in-human-cancer .
10.1038/s41597-021-00897-9,The latest version of the code is available at https://github.com/babakkhavari/Clustering (GNU General Public License v3.0). The code is Python-based and runs in Jupyter Notebook. The code repository includes instructions for how to install and run the algorithm as well as a country example displaying the necessary inputs and expected outputs. The datasets published with this paper were ran using Python 3.6 and the packages listed in the full_project.yml file uploaded to the repository.
10.1038/s41597-021-00896-w,"The workflow for spectral library generation was scripted using a gnu-make. The make file and a companion document are included with the data in the pride repository 17 , the make file companion document is also included with this article as supplementary file 1. The make file and the companion document are available on github ( https://github.com/M-Russell/Mouse_iPSC_Spectral_Library ). These files should enable precise replication of the library from raw data as presented here and re-use of the raw data through varied processing. The library was created with a series of open source software packages, the precise versions and sources of these programs are given in the documentation. Python scripts are required for the pipeline and instructions are given on how to install the versions used."
10.1038/s41597-021-00900-3,"The Python code used to generate the statistical analysis and plots is shared within the same Figshare link 12 , with the name “Statistical_Analysis.py”."
10.1038/s41597-021-00904-z,"The MATLAB code for the preprocessing steps described above are included in the released dataset 83 . The code make use of the functions in the FSL 62 , 63 , 64 toolbox and Freesurfer 76 that are freely available for download at https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation and https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall ."
10.1038/s41597-021-00899-7,"The pre-processing and feature extraction of the MEG data, as well as the single-trial classification were performed using custom M atlab codes based on Fieldtrip toolbox 12 functions. All codes are available at our GitHub repository https://github.com/sagihaider/MEGBCI2020.git ."
10.1038/s41597-021-00901-2,"The R programming language 36 , version 3.6.1, was used for the automation of the entire process for making the MeteoSerbia1km dataset, using the following packages: climate 37 , meteo 30 , nabor 38 , CAST 39 , caret 40 , sp 41 , 42 , spacetime 42 , 43 , gstat 44 , 45 , raster 46 , rgdal 47 , doParallel 48 , ranger 49 , plyr 50 , ggplot2 51 . To automate the development, tuning, cross-validation and prediction processes for the RFSI method, five additional R functions were created and added to the R meteo package 30 ( https://github.com/AleksandarSekulic/Rmeteo , http://r-forge.r-project.org/projects/meteo ): • near.obs - for finding n nearest observations and distances to them from desired locations, • rfsi - for RFSI model fitting, • tune.rfsi - for RFSI model tuning, • cv.rfsi - for RFSI model cross-validation, • pred.rfsi - for RFSI model prediction. In order to make this work reproducible, a complete script in R and datasets used for the modelling, tuning, validation, and prediction of daily meteorological variables is available via the GitHub repository at https://github.com/AleksandarSekulic/MeteoSerbia1km ."
10.1038/s41597-021-00905-y,The code to create the RDF from a Google spreadsheet that can be created from the TSV file is available at: https://github.com/BiGCAT-UM/raredisease-omim/ . The code to create the Linkset for CyTargetLinker is available at: https://github.com/CyTargetLinker/linksetCreator . The queries to retrieve information about the publications using the PMIDs in Wikidata can be found here: https://github.com/BiGCAT-UM/pubmedWikidata .
10.1038/s41597-021-00906-x,"All scripts used for predictor extraction, predictor pre-processing, model training/validation, and surge reconstruction are available for download under the DOI listed in Table 2 ."
10.1038/s41597-021-00909-8,No custom code has been used for generation or processing of the data or generation of figures.
10.1038/s41597-021-00908-9,"We listed the names and versions of the softwares used for data analysis. FastQC, version 0.11.3, was used for quality check of the raw FASTQ sequencing file. https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . PRINSEQ, version 0.20.4, was used to remove low-quality reads. http://prinseq.sourceforge.net/ . PEAT, version 1.2, was used to remove the adaptor sequence. https://github.com/jhhung/PEAT . STAR, version 2.6.1 was used for mapping. https://github.com/alexdobin/STAR . featureCount, SUBREAD, release 1.6.5 was used for the expression counting. http://subread.sourceforge.net . R package, version 4.0.3, was used for the downstream analysis. https://www.r-project.org . TCC-GUI tool was used for the downstream analysis. https://github.com/swsoyee/TCC-GUI."
10.1038/s41597-021-00910-1,"Code used to process the data is available at https://github.com/moritzbuck/metasssnake , code for the computing of mOTUs is available at https://github.com/moritzbuck/mOTUlizer and some additional scripts, particularly the script used for the submission of the data and summary of the quality reports is available at https://github.com/moritzbuck/0023_anoxicencyclo ."
10.1038/s41597-021-00913-y,No custom code was used in the data acquisition of these datasets.
10.1038/s41597-021-00915-w,"The code to compute all geometric features from all tumor nuclei in our dataset, along with notebooks to illustrate usage of our data and reproduce all survival regression results, is publicly available at https://github.com/stanfordmlgroup/DLBCL-Morph ."
10.1038/s41597-021-00914-x,MISSING
10.1038/s41597-021-00912-z,All code used to format and clean the dataset for publication is available on GitHub ( www.github.com/helenphillips ).
10.1038/s41597-021-00919-6,"All of FinBIF’s code is open source and available primarily in GitHub ( https://github.com/luomus ), though partly in Bitbucket ( https://bitbucket.org/luomus/ ). The project is covered by the MIT license ( https://opensource.org/licenses/MIT )."
10.1038/s41597-021-00923-w,"The in-house reconstruction pipeline used to reconstruct the 250 µm data is publicly available on Github ( https://github.com/fluese/reconstructionPipeline ). For the QSM reconstructions an in-house pipeline was used and its successor is publically available ( https://gitlab.com/acostaj/QSMbox ). The ANTs scripts to create an unbiased interparticipant average volume used for the 250 µm and 700 µm datasets, the scripts to conduct the DTI pre-processing as well as the script used to register the data to the 250 µm dataset are included in the repository. Furthermore, the biasfield correction script of SPM12 is included in the repository as well."
10.1038/s41597-021-00922-x,MISSING
10.1038/s41597-021-00916-9,"The Python code used for the transfer, organising of the data, estimation of the precision and accuracy metrics as well as the plots included in this manuscript can be found in https://github.com/vgkinis/neem_isotope_data_descriptor_code . In the repository, we also provide auxiliary code with basic routines for post-processing of the PANGAEA data file."
10.1038/s41597-021-00926-7,"All codes that were used in the preparation and the analysis of this dataset are made available along with the dataset and in our code repository 38 . Codes are written in Python language. Specifically, two Jupyter Notebooks were provided; one that contains the code that describes the data tables and calculates the morphological feature, and another for contour detection and evaluation of the segmentation model. The code includes all custom methods, references to common libraries, and a full set of stepwise instructions to replicate the calculations in this study."
10.1038/s41597-021-00925-8,MISSING
10.1038/s41597-021-00920-z,"In addition to releasing the data, we also publish code used for the baseline experiments. All code and additional data required for the experiments, including our splits into training and test datasets, are available on GitHub via http://www.github.com/simula/kvasir-capsule ."
10.1038/s41597-021-00927-6,A Python code was developed to process the data set to compare the baseline test results with the faulted test results. The code was stored on figshare 17 and on a shared platform that can be accessed publicly. The data acquisition system used LabVIEW. The data file format was automatically transferred from data loggers to storage on an Oak Ridge National Laboratory local PC with sample time of 1 or 3 s.
10.1038/s41597-021-00921-y,The lightweight processing that happened on the incoming sensor data is described above. Processing code will be available on request.
10.1038/s41597-021-00924-9,For access to the code used to calculate the perimeter to convex hull ratio: https://doi.org/10.26180/5efec4822e873 15 .
10.1038/s41597-021-00907-w,"The code for compiling the time series of useful energy consumption and energy service profiles is published at https://github.com/FCN-ESE/JERICHO-E-usage 34 under the open MIT license. Detailed instructions for using the code are included in the repository. All code is implemented in Python. For easy use of the scripts, we have added a Jupyter Notebook with further instructions on the workflow. The required input data, comprising pre-calculated data and data from official reports, are included with references."
10.1038/s41597-021-00929-4,We used the Python programming language for all activities. The entire code is permanently available in Zenodo ( https://zenodo.org/record/4266962 ) 54 or GitHub ( https://bit.ly/github-clinical-abbreviations ).
10.1038/s41597-021-00931-w,No code was used in this study.
10.1038/s41597-021-00933-8,"The classifications where made using https://database.cric.com.br . The source code of the website is available on GitHub, https://github.com/CRICDatabase/searchable-image-database , under GNU Affero General Public License v3.0."
10.1038/s41597-021-00936-5,"The workflow for FEFF9 calculation including input generation, output parsing and workflow management is available in open-source materials science packages pymatgen 25 , FireWorks 26 and atomate 27 . The error handler for automatic error detection and recovery can be found in custodian 28 ."
10.1038/s41597-021-00932-9,The tool can be installed from the Python Package Index (PyPI) at https://pypi.org/project/emobpy/ . The code is provided under a permissive license in Zenodo 35 . We also provide the script created to generate the 200 BEV profiles for the current case study at https://gitlab.com/diw-evu/emobpy/emobpy_examples .
10.1038/s41597-021-00938-3,"All Python-code corresponding to the evaluations described in this work is distributed on GitHub under the MIT license https://github.com/haugjo/TueEyeQ . To run the evaluation, the following packages are required (note that older or more recent versions might also work): • python (v3.7.3) • numpy (v1.18.1) • pandas (v0.25.1) • scikit-learn (v0.21.3) • matplotlib (v3.1.3) • shap (v0.34.0) • lime (v0.2.0.1) • dcor (v0.5.2) • pickleshare (v0.7.5, only required to load the precomputed Distance Correlation scores)."
10.1038/s41597-021-00930-x,"The codebase used to fly the UAV autonomously, record and synchronize data, and interface between different sensors is available for public use under BSD license. The codebase can be accessed at https://bitbucket.org/castacks/workspace/projects/DOE ."
10.1038/s41597-021-00937-4,"The aforementioned XGB classifier used to remove non-cough recordings, feature extraction source code, cough preprocessing methods, cough segmentation function, and SNR estimation algorithm are available on our public repository ."
10.1038/s41597-021-00940-9,"MATLAB codes to produce figures and extract data from external databases, as well as the PERSIANN-CCS-CDR algorithm are available in a public GitHub repository ( https://github.com/mojtabasadeghi77/PERSIANN-CCS-CDR.git ). With any technical difficulties with data downloading and codes, please contact the corresponding author."
10.1038/s41597-021-00943-6,"The IBM® SPSS® software v26, licensed to the National and Kapodistrian University of Athens ( http://www.cc.uoa.gr/texniki-ypostiri3h/egkatastash-paketwn-logismikoy/spss.html ), was used for the dataset analysis 31 with no specific variables or parameters used to process the current dataset. The Microsoft® Excel for Mac Version 16.16.14 (190909) was used to tabulate the responses to the inventory and generate the dataset 22 before analysis with the IBM® SPSS® software v26. No custom code was used in the generation or processing of the dataset. The codebook that accompanies the second Data Record was generated by the IBM® SPSS® v26 software (SPSS > Analyse > Reports > Codebook)."
10.1038/s41597-021-00944-5,"All image processing and extraction of quantitative measurements was done using the TissueMAPS framework, an open-source software project developed in our lab ( https://github.com/pelkmanslab/TissueMAPS ). In addition to the detailed description of the image processing workflow in Table 1 , parameter settings files are provided in the data record (IDR, idr0093) 11 . Using these parameter settings combined with the open-source code of TissueMaps allows for reproduction of the exact image analysis pipeline. Additionally, the Ilastik project used for the nucleolar pixel classification is also provided in the data record (IDR, idr0093). Custom code used to computationally scale our analysis to the required data volume is specialised for our computing architecture and therefore not provided here."
10.1038/s41597-021-00942-7,"All the code used to generate this database is available in the TritonDB repository on github ( https://github.com/IFenton/TritonDB ), as well as in “Triton code” in the figshare data repository 18 ."
10.1038/s41597-021-00945-4,"The data preprocessing and validation procedures presented in the validation section were conducted in MATLAB 2019b. Feature extraction of physiological signals was performed using NoiseTools toolbox, which need be previously installed in MATLAB. The README files and the data analysis code are included in the dataset 39 . Therefore, readers can easily reproduce the experimental results with the demo code and toolbox."
10.1038/s41597-021-00935-6,"The versions of the tools used and their parameters are described as follows. Flye v2.3.6-release: flye --pacbio-raw -g 2.9 g Redbean (Wtdbg2) v2.3: wtdbg2 -x rsII -g 2.7 g -L 5000 wtpoa-cns -i ctg.lay.gz For cynomolgus macaque, rsII was replaced by sq. SMARTdenovo (git commit 3d9c22e25bdf4caf6c08ea1acb41ee58e52f61a8): Default parameters with consensus generation. Minimap2 v2.10-r761 and miniasm (git commit 17d5bd12290e0e8a48a5df5afaeaef4d171aa133): minimap2 -x ava-pb | gzip -1 > m.paf.gz miniasm -f reads.fastq m.paf.gz > tigs.gfa For cynomolgus macaque, only reads longer than 10 kb were considered for the assembly. Hi-C scaffolding: Trim Galore v0.4.5: --paired --phred33 -e 0.1 -q 30 HiC-Pro v2.11.1: default parameters Juicer v20180805: default parameters 3d-dna v20180929: -m haploid -i 5000 -r 2 Juicebox v1.3.6 Polishing: Pbmm2 v0.12.0: pbmm2 align assembly.referenceset.xml reads.subreadset.xml aln.alignmentset.xml --sort -j 18 -J 18 -m 5000 M Variant Caller v2.3.2: The consensus sequence was split into 50 parts and the arrow algorithm was executed using default parameters. Arrow polishing was iteratively executed twice for flye and SMARTdenovo assemblies, and thrice for miniasm and redbean assemblies. Validation: gVolante v1.2.1: (CEGMA v2.5, CVG v10042017, NCBI BLAST v2.2.31, GeneWise v2.2.3-rc7, GeneID v1.4, HMMER v3.1b2) cegma.mod.pl --interlen 100000 --boundaries 10000 --ext --protein CVG/CVGs.fa --hmm_profiles CVG/hmm_profiles --cutoff_file CVG/profiles_CVG_cutoff.tbl --complete_file CVG/completeness_CVG_cutoff.tbl"
10.1038/s41597-021-00947-2,The versions and parameters of published software used in this study were described in the Methods. MumSV is a set of custom scripts to call large SVs based on MUMer alignments and it can be accessed at https://github.com/jeff-sc-chu/MumSV .
10.1038/s41597-021-00939-2,The codes are available from: https://github.com/fmidev/GlobSnow3.0 . and also from: http://www.globsnow.info/swe/archive_v3.0/source_codes/ .
10.1038/s41597-021-00954-3,The datasets as reported are generated from experiments and are not relevant to any computer codes.
10.1038/s41597-021-00948-1,"All data for this project were processed in MATLAB R2017b. The code used to process these data has been deposited in the Figshare repository, and is publicly available 28 ."
10.1038/s41597-021-00946-3,The code used during the development of the dataset can be found on Synapse under Fetal Tissue Annotation Challenge FeTA Dataset 37 (website: https://doi.org/10.7303/syn23747212 ).
10.1038/s41597-021-00956-1,Replication code to support the findings of this study has been deposited to the following GitHub repository 36 : https://doi.org/10.5281/zenodo.4408528 .
10.1038/s41597-021-00962-3,"All custom code created to process of manipulate external datasets in the construction or subsequent update of the AOP-DB v.2 relational database tables are made publicly available by the U.S. Environmental Protection Agency, Office of Research and Development (ORD) 36 ."
10.1038/s41597-021-00964-1,The LSTM model implemented in this study and figure scripts are available from https://github.com/osungmin/SciData2021_SoMo_v1 . Note that the LSTM model is built by adopting python modules obtained from https://github.com/kratzert/ealstm_regional_modeling .
10.1038/s41597-021-00966-z,The Python Code used to draw Fig. 3 and Fig. 4 is published at Supplementary File 1 to show how the data can be loaded and visualized.
10.1038/s41597-021-00960-5,Matlab program is used to conduct emissions calculation and Monte-Carlo simulation. Sample codes for calculating GHG emissions from rice cultivation are available on the open-access online platform figshare 46 .
10.1038/s41597-021-00972-1,Auxiliary File 1 (milk cells) and Auxiliary File 2 (pbMECs) contain the R code (R version 3.6.3) for scRNA-seq data processing and subsequent visualization of the output. They are publicly accessible via Figshare 50 .
10.1038/s41597-021-00963-2,"The full data collection system is published under MIT license and is available under https://DEDDIAG.github.io . The dataset itself is published as tab-separated text files together with code to import all data into a PostgreSQL instance. An SQL function called get_measurements() is provided to get seconds-based measurements, where readings are converted from value-changes to seconds-based readings using interpolation; timestamps are rounded to nearest seconds. There is also a python package available https://github.com/DEDDIAG/DEDDIAG-loader.git that assists in retrieving data into a pandas-DataFrame/numpy-array."
10.1038/s41597-021-00968-x,"To prepare this dataset, we used LTRharvest and LTRdigest from genometools version 1.5.10 software 55 and REXdb database ( http://repeatexplorer.org/ ) 27 . The sources for the 301 plant genomes can be downloaded through the link provided in Supplementary Table 1 and scripts for intact LTR-RTs annotation are available at GitHub link ( https://github.com/sszhou9/intact-LTR-RTs )."
10.1038/s41597-021-00953-4,All analyses were performed using open sources software tools with default parameters (please see Methods).
10.1038/s41597-021-00970-3,"All software used for the visualization, processing and analysis of this dataset are open access or custom written, Python- and MATLAB-based programs. The Kilosort2 MATLAB package was used for spike sorting ( https://github.com/MouseLand/Kilosort ; version 2.0; commit date, 8 April 2019) 37 , and the Phy Python module for subsequent manual curation of the data ( https://github.com/cortex-lab/phy ; version 2.0b1; release date, 7 February 2020). Spatial and temporal features of the single unit spike waveforms were calculated using custom MATLAB scripts or the CellExplorer MATLAB module ( https://cellexplorer.org/ ; https://github.com/petersenpeter/CellExplorer ; version 1.2; commit date, 25 September 2020) 40 . Single unit quality metrics were computed using the SpikeMetrics module ( https://github.com/SpikeInterface/spikemetrics ; version 0.2.2; commit date, 30 December 2020) of the SpikeInterface Python-based framework ( https://github.com/SpikeInterface/spikeinterface ; version 0.11.0; commit date, 10 December 2020) 36 . SpikeMetrics relies on the code of the quality metrics module developed at the Allen Institute for Brain Science ( https://github.com/AllenInstitute/ecephys_spike_sorting/tree/master/ecephys_spike_sorting/modules/quality_metrics ) 3 . NWB files were created using the MatNWB ( https://github.com/NeurodataWithoutBorders/matnwb ; version 2.2.4.0; commit data, 9 February 2021) application programming interface."
10.1038/s41597-021-00951-6,The code used in the generation of the data sets is open-sourced on Github repository 19 .
10.1038/s41597-021-00955-2,Data are available in CSV format from our GitHub repository: https://github.com/ccodwg/Covid19Canada . Data are also available in JSON format from our API: https://opencovid.ca/api/ . All code used to generate and verify the data are also available from our GitHub repository: https://github.com/ccodwg/Covid19Canada/tree/master/scripts . All data and code required to reproduce the figures and tables in this manuscript are available in the following GitHub Repository: https://github.com/ccodwg/ccodwg-scientific-data . All code can be run using any recent version of R.
10.1038/s41597-021-00950-7,"The latest code to process the files can be downloaded from the Zenodo release 1 or accessed from the GitHub repository ( https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker ). The code is compatible with Stata versions 16 or higher  which are also recommended due to improved functionality with maps and graphs. General data processing can be done in any version. As specified in the Data Records section, the dofiles are in the /02 dofiles/ folder. Within this folder dofiles exist for each country plus a set of five dofiles that setup, merge, map, and validate the final data file. These files are explained as follows: COUNTRY_SETUP.do initializes the code for running the country files. One can run each country file independently as well, but they need the directory structure and packages to be loaded in order to function correctly. Directory and packages can be initialized using the first few lines marked in the beginning of the COUNTRY_SETUP.do file. This syntax is as follows: clear global coviddir “<your directory path>/<your directory name>” * package for maps and correcting map projections ssc install spmap, replace ssc install geo2xy, replace * packages for color schemes ssc install palettes, replace ssc install colrspace, replace * package for replicating plots net install tsg_schemes, /// from(“ https://raw.githubusercontent.com/asjadnaqvi/Stata-schemes/main/schemes/ ” ) set scheme white_w3d, perm * set the detault graph font graph set window fontface “Arial Narrow” Each country .do file is annotated with notes where necessary. COUNTRY_MERGE.do combines all the country datasets saved in 04_master in one file EUROPE_COVID19 master.dta. The master file is also saved in the 04_master folder. COUNTRY_GIS_setup do sets up the GIS layers in Stata format for the combined NUTS regions and for individual countries. A mixed NUTS3 and NUTS2 shapefile is also created accommodate the data from Poland and Greece. The logic can also be applied to add data at the provincial (NUTS 1) or country (NUTS 0) level if one needs to add other countries not in the dataset. This file also extracts shapefiles for individual countries and generates a file used for labeling the individual country maps. COUNTRY_GIS_map do create the maps that are saved in the 05_figures folder. See Fig. 5 for the overall map. Individual country COVID-19 maps can be viewed on GitHub ( https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker ). COUNTRY_validation do collapses the Tracker to a country-date level and merges it with OWID COVID-19 dataset, for validation. This file also produces Fig. 7 . GitHub and Zenodo files are updated every four weeks. Each Zenodo release is assigned a unique DOI, but the generic https://doi.org/10.5281/zenodo.4244878 , will always link to the latest version 1 ."
10.1038/s41597-021-00971-2,All code related to this data set can be found in the Smart Distance Lab OSF project 12 .
10.1038/s41597-021-00949-0,"Code used to format the dataset for reuse and generate manuscript figures (Fig. 3a,b ) is available for download in the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare data repository at https://doi.org/10.4211/hs.de4190f0eff74b09a5e0844a0de482a5 25 . Note that, for Fig. 2 , this also requires downloading hydrological basin from HydroBASINS 33 and catch data from FAO 32 . Upon free registration in HydroShare, there are no restrictions to the access or use of this code. Code was implemented in R (version 4.0.2; https://r-project.org ) 24 ."
10.1038/s41597-021-00967-y,"Synthetic Protected Health Information (PHI) was generated using the Faker software package ( https://pypi.org/project/Faker ) and inserted into selected DICOM Attributes using an extended version of the Posda 7 tool suite ( https://code.imphub.org/projects/PT/repos/oneposda ), the open source package used for curation and de-identification by TCIA. Posda incorporated the open source software package ImageMagick ( https://imagemagick.org/index.php ) to insert multiple lines of text into Pixel Data."
10.1038/s41597-021-00969-w,No customized code was produced to prepare or analyse the dataset.
10.1038/s41597-021-00959-y,The distributed csv files were generated by first converting the edf output files produced by the Eyelink 1000 to a text-based asc file format using a vendor-provided utility. These files were subsequently converted to csv files of the specified format using a customized MATLAB script. All relevant MATLAB scripts for preparation of the repository and figure generation may be found at the following link - https://github.com/griffihk/GazeBaseCodeRepo . Data may be extracted from the repository into the target computing environment using traditional csv import functions.
10.1038/s41597-021-00952-5,The code used to generate the INPRO viewer is available via: https://github.com/lcsc/inpro .
10.1038/s41597-021-00976-x,"This dataset is accompanied by a code repository ( https://github.com/usc-mrel/usc_speech_mri.git ) that contains examples of software and parameter configurations necessary to load and reconstruct the raw RT-MRI in MRD format. Specifically, the repository contains demonstrations to illustrate and replicate results of Figs. 2 – 6 . Code samples are available in MATLAB and Python programming languages. All software is provided free to use and modify under the MIT license agreement."
10.1038/s41597-021-00978-9,"The scripts and the files of resistancebank.org are available in its GitHub repository ( https://github.com/hegep-eth/resistancebank.org ). We developed the platform through the R software (version 4.0.2) and the RStudio Integrated Development Environment (IDE; version 1.3.1073). In its current version resistancebank.org relies on the following dependencies: aws.s3 (0.3.21), dplyr (1.0.2), europepmc (0.4), ggimage (0.2.8), ggmap (3.0.0), ggplot2 (3.3.2), gmailr (1.0.0), gsubfn (0.7), leaflet (2.0.4.1), lemon (0.4.5), raster (3.3–13), rcrossref (1.0.0), rdrop2 (0.8.2.1), rgdal (1.5–16), rmarkdown (1.15), scales (1.1.1), shiny (1.5.0), shinyBS (0.61), shinyjs (1.1), shinyWidgets (0.5.3), stringr (1.4.0), svglite (1.2.3.2), sp (1.4–5)."
10.1038/s41597-021-00965-0,"The scripts for shear spectrum integration and maximum likelihood estimation (MLE) fitting are available together with the dataset (except for some functions of the ODAS libraries released by Rockland Scientific International Inc., to which we refer the interested reader)."
10.1038/s41597-021-00973-0,The source code implementing the core Daymet algorithms (Eq. 2 through 17 and associated text) is available at https://doi.org/10.5281/zenodo.4737573 29 .
10.1038/s41597-021-00981-0,No specific code was generated for analysis of these data.
10.1038/s41597-021-00982-z,"All the code and processing scripts used to produce the results of this paper were written in GEE, MATLAB. Links to scripts and data for analyses can be found in the GitHub repository at https://github.com/tzhang-edu/GOWT ."
10.1038/s41597-021-00957-0,MISSING
10.1038/s41597-021-00975-y,"Processing of raw data has been performed using ad-hoc scripts in the statistical software R, version 4.0.2 (R Core Team, 2020). Code is available on http://gipeyop.uv.es/gipeyop/base_datos/21_otros/CODIGO_SEA.zip ."
10.1038/s41597-021-00934-7,The code for creating computationally-reproducible research samples is available as part of the archive.
10.1038/s41597-021-00958-z,The Google Earth Engine source code used to produce the Micronesia maps presented in this dataset (and all regions) is fully open access ( https://github.com/CoralMapping/gee-mapping-source ) and in full detail at Zenodo 87 [ https://doi.org/10.5281/zenodo.3833246 ].
10.1038/s41597-021-00987-8,The above methods indicate the programs used for analysis within the relevant sections. The code used to analyse individual data packages is deposited at https://github.com/TlaskalV/Deadwood-microbiome .
10.1038/s41597-021-00977-w,"In this study we used Sleepware G3 software for all PSG data acquisition and events annotation. The software is provided by Philips Inc. The custom code used in this work refers to: (a) the algorithm for the synchronization of audio recordings with the PSG signals, so that the information of all annotated episodes and sleep stages can be accurately transferred to the audio signals as well and (b) the detection of false positive apnea/hypopnea events due to the inability of the doctors to precisely quantify the reduction in the airflow amplitude measured through the flow rate channels of the PSG (thermistor and pressure cannula sensors). All custom code developed for this study is available online (“code.rar” file) along with the files of the dataset 79 ."
10.1038/s41597-021-00983-y,All data compilation and creation of figures were conducted in R version 4.0.2 45 . R code can be found at https://www.github.com/rmpilla/GlobalTempProfileData .
10.1038/s41597-021-00979-8,The whole platform is openly available at https://github.com/ncRNAome-OSU/miredibase and at Zenodo ( https://doi.org/10.5281/zenodo.4421966 ) 57 .
10.1038/s41597-021-00993-w,The program to calculate and determine VC affiliation is Stata 15.0. Box 1 demonstrates a pseudocode of Stata when coding the VC type in a non-exclusive way.
10.1038/s41597-021-00980-1,"All newly generated sequencing raw reads (see Table 3 ) have been deposited in the NCBI Sequence Read Archive (SRR12854172, SRR12854173, SRR12854174, SRR12854175). Six compressed fastq files per sample were uploaded. The fastq files have the same names as the libraries described in Table 3 . The genetic pipeline used to process the data is available at https://github.com/EvolEcolGroup/data_paper_genetic_pipeline . The filtered compressed vcf file used for the analyses has been uploaded to figshare 30 with the title “A curated dataset of modern and ancient high-coverage shotgun human genomes”."
10.1038/s41597-021-00995-8,All Matlab code used for visualization and spatial alignment is available in a public GitHub repository ( https://github.com/IveW/IS2B ) accompanied with detailed usage notes and commentary.
10.1038/s41597-021-00985-w,Details of software packages and specific versions used were indicated in the Usage Note section. Suggestion of their usage and specification for each command was also incorporated in the Usage Note.
10.1038/s41597-021-00984-x,"The source codes written in R programming language are available on the Code Ocean cloud-based computational reproducibility platform as a Code Ocean “compute capsule,” together with the dataset analysed in this article 31 : snapshot of the ACO2 dataset as of June 8, 2020, /data/LOVD_full_download_ACO2_2020-06-21_18.03.52.txt (LOVD flat file format); the Human Phenotype Ontology data-version hp/releases/2020-06-08, /data/hp.obo.2020-06-08.txt (OBO flat file format). Thus, readers can reproduce and verify the results of this article without having to download or install anything. All the content is available under MIT License."
10.1038/s41597-021-00986-9,"Our computational infrastructure for high-throughput and automated DFT calculations using the Q-Chem electronic structure code is implemented in existing open-source Python packages developed by the Materials Project, namely pymatgen 63 , custodian , and atomate 76 . The modules in these codes used specifically for Q-Chem, along with their purposes, are described in Fig. 8a . The basic functionality to generate, process, analyze, and manipulate molecules is included in pymatgen . We have added functionality to read and write Q-Chem input files and to parse Q-Chem output files. In addition, we have developed a number of “Sets”, pre-defined collections of input parameters appropriate for common types of calculations. While these sets can be used with any level of theory available in Q-Chem, it is especially facile to use the advanced level of theory used for the LIBE dataset ( ω B97X-V/def2-TZVPPD/SMD). The custodian Q-Chem module defines the interface between Q-Chem and our automation framework in atomate . It can execute arbitrary Q-Chem jobs and can automatically check for, detect, and correct errors in Q-Chem calculations. custodian also handles the logic for FFOpt calculations. The Q-Chem module in atomate combines the Q-Chem input and output modules in pymatgen and the Q-Chem interface and error handlers in custodian to perform Q-Chem jobs and analyze their data in a high-throughput fashion. An example calculation, or Firework, for a single-point optimization is shown schematically in Fig. 8b . First, based on some input parameters, a Q-Chem input file for a geometry optimization calculation is written. Then, the optimization job is run, with custodian waiting for completion and, upon completion, checking for errors. If the job completes without errors, then the output is parsed and stored in a database. Individual Q-Chem calculations, represented in atomate by Fireworks like SinglePointFW , can be combined to form more complex workflows. Other than Q-Chem itself, all the necessary code used to generate and analyze the LIBE dataset ( pymatgen : http://github.com/materialsproject/pymatgen ; custodian : http://github.com/materialsproject/custodian ; atomate : http://github.com/hackingmaterials/atomate ; and deliberate : http://github.com/espottesmith/deliberate ) can be found on Github."
10.1038/s41597-021-00990-z,MISSING
10.1038/s41597-021-00994-9,No custom code was made during the collection and validation of this dataset.
10.1038/s41597-021-00992-x,"MATLAB scripts . To help facilitate the use of the dataset provided in this manuscript, custom MATLAB scripts are provided (MATLAB_scripts.zip) that read the data files, produces a set of files that can be manually reviewed, and serves as a starting point for interested parties to modify the scripts to further investigate the data set. These scripts are to be used by running them from within the same working directory as the data files. The provided scripts perform the following actions: • create_3D_track_plots.m ○ Reads data from the following files: ▪ Tagged_Fish_List.csv ▪ LGS_Hydrophone_Configuration.csv ▪ LMN_Hydrophone_Configuration.csv ▪ LGS_3D_Tracks.csv ▪ LMN_3D_Tracks.csv ○ If the MATLAB installation includes the Mapping Toolbox KML files will be generated that can be opened in Google Earth: ▪ Files to display the location of the hydrophones at LGS and LMN. ▪ A file for each tag detected at each dam that displays up to the last 1000 3D tracked locations. ○ For each tag detected at each dam a multi-panel plot will be generated that displays: ▪ Upper-left: XY coordinates of the cabled hydrophones and the 3D track for the tagged fish specified ▪ Upper-right: Time vs. X-location ▪ Lower-left: Time vs. Y-location ▪ Lower-right: Time vs. Z-location • create_event_history_plots.m ○ Reads data from the following files: ▪ Tagged_Fish_List.csv ▪ Autonomous_Receiver_Deployment_Data.csv ▪ Autonomous_Receiver_Event_Data.csv ▪ LGS_Event_Data.csv ▪ LMN_Event_Data.csv ○ For each tag that had an acoustic detection event, a plot was generated that shows the number of river kilometers from the Pacific Ocean against the days since the tagged fish was released. ▪ Since autonomous receivers are processed on an individual basis there are several overlapping events for each autonomous receiver array. HBET JSATS C# Source code . In addition to the simple, readily adaptable, MATLAB scripts provided for visualizing the data set and serving as a starting point for additional analysis, the C# source code for the JSATS features integrated into the HBET software are also provided. The source code was developed using C# via Microsoft Visual Studio 2019. To install and operate the C# code, the users must install Microsoft SQL Server express 2012 and attach the database file that contains the information contained within the data set (HBET_Database_Files.zip). The data handling codes are contained within a compressed folder included in the data repository (HBET_Source_Code.zip). The provided source code files perform the following functionality: • JSATSFishDectionForm.* ○ Used to display fish detection information • JSATSQueryForm.* ○ Used to retrieve data from the database • JSATSStartUpForm.* ○ Main form of the application • JSATSStudyManagement.* ○ Used to manage studies • JSATSUploadFileForm.* ○ Used to upload data files • TrackForm.* ○ Used to display fish migration information"
10.1038/s41597-021-00961-4,MISSING
10.1038/s41597-021-01000-y,Code used in processing the data are available on Github at the following link: https://github.com/skatejones/SustainableFoods .
10.1038/s41597-021-00989-6,"A step-by-step guidance and the source-code to generate this dataset, and a notebook to explore and visualize the data can be found at the dataset’s GitHub page ( https://lbnl-eta.github.io/AlphaBuilding-SyntheticDataset )."
10.1038/s41597-021-00998-5,The R code used to construct and validate the Meta-dataset is available at Data Citation 2. Analyses were executed in R within the R Studio desktop (version 1.1.1103) suite. Microsoft’s open R version 4.0.2 ( https://mran.microsoft.com/open ) was used to take advantage of a multicore system to improve multithreaded processes and reduce computation time.
10.1038/s41597-021-00999-4,"The scripts used to create the VICGlobal data set can be found on the corresponding author’s Github page ( https://github.com/jschap1/vicglobal-prep and https://github.com/jschap1/vegpar ). The VICGlobal parameters were subset to the Upper Colorado River Basin using the subsetting codes included with the VICGlobal dataset, archived on Zenodo 48 ."
10.1038/s41597-021-00991-y,"The code to make the display of a neutrino event is provided as Auxiliary files 76 . In the example shown, the event 9190097972 is used, but the code can be adapted to draw your own display of any neutrino candidate downloaded from the Open Data Repository. The code (Visualization.ipynb) is written as a Jupyter Notebook. The installation of Python and Jupyter using the Anaconda Distribution is recommended. Anaconda Distribution includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science. More details can be found at: https://jupyter.org/install.html . Among the auxiliary files, the one called visualization archive (Visualization.zip) has all the necessary files to run the display. Data folder contains input files, which have been downloaded from the Open Data Repository. Python script (opera_tools.py) provides auxiliary functions that were used in the Notebook. Running Visualization.ipynb requires dedicated libraries to be installed, as reported in the file requirements.txt. There is also a possibility to access the code via binder interactive environment ( https://tinyurl.com/binder-OPERA )."
10.1038/s41597-021-00974-z,All associated code is hosted under the Materials-Consortia organisation on GitHub ( https://github.com/Materials-Consortia ).
10.1038/s41597-021-00997-6,"The software was developed and deployed using licensed third-party products (Microsoft Access and Sybase), so the code cannot be shared or be considered Open Source. Readers wishing to request access to or discuss the software currently in place, should use the contact address on the ‘Feedback’ page on the WCVP website https://wcvp.science.kew.org/feedback."
10.1038/s41597-021-00941-8,"Data were processed using Python and shell scripts contained in the spine-generic package ( https://github.com/spine-generic/spine-generic/releases/tag/v2.6 ), which is distributed under the MIT license. A comprehensive procedure is described in the “Analysis pipeline” section of the spine generic website ( https://spine-generic.rtfd.io/ ). This procedure includes the list of dependent software packages to install, a step-by-step analysis procedure with a list of commands to run, a procedure for quality control and for manual correction of intermediate outputs (e.g. cord segmentation and vertebral labeling). The procedure includes embedded video tutorials and has been tested by external users. The analysis documentation also includes a section on how to generate the static figures that are shown in this article (in PNG format) as well as the interactive figures embedded in the spine-generic website. Notable software used in this study include: the Spinal Cord Toolbox v5.0.1 ( https://spinalcordtoolbox.com ) to analyse the MRI data, pandas 36 to perform statistics, plotly v4.12.0 ( https://plotly.com ) to display the interactive plots, brainsprite v0.13.3 ( https://brainsprite.github.io/ ) for embedding in the online documentation an interactive visualization of example datasets, pybids 37 for checking the acquisition parameters on the BIDS datasets, FSLeyes v0.34.0 ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes ) for manually-correcting the segmentations."
10.1038/s41597-021-00988-7,"The code to reproduce the composite layers, for post-processing and for assessing recent human disturbance with remote sensing is available together with the database in Figshare ( https://doi.org/10.6084/m9.figshare.13194095.v1 ) 33 . We included seven scripts: • 00_ComposeMap.R – Identifies overlapping polygons across individual datasets. • 01_CreateComposite_Points.py – Creates the composite point feature class. • 02_CreateComposite_Polygons.py – Creates the composite polygon feature class. • 03_PostProcessing.R – Extracts additional information on each primary forest. • 04_Add_Postprocessing.py – Imports post-processing output into the geodatabase. • 05_Summary_stats.R – Calculates summary statistics of primary forests • 06_DisturbanceAssessment_Step1_exportIntermediateChangeImg.txt – Runs LandTrendr in Google Earth Engine, tiles the area of interest, creates Change-Images for each tile, and exports these as intermediate .tif files containing the LandTrendr metrics. • 07_DisturbanceAssessment_Step2_extractPolygonValuesFromChangeImg.txt – Extracts LandTrendr metrics for each forest polygon from Change-Images and exports as .csv. Python (.py) scripts were run in ESRI ArcGIS (v10.5) and are available also as ArcGIS Models inside the Geodatabase. R (.R) scripts were run using R (v 3.6.1) 46 . The remaining .txt scripts were run in Google Earth Engine."
10.1038/s41597-021-01005-7,The code used for the extraction of ITS2 regions using ITSx and for the Blastn searches are provided in Supplementary Note 1 .
10.1038/s41597-021-01002-w,The script for the extraction of the IS signal and the plots in this data descriptor is available in this repository: https://bitbucket.org/Denice_van_Herwerden/ils-validation/src/main/ .
10.1038/s41597-021-01003-9,"The codes we developed for computing hPET and dPET are available at https://github.com/Dagmawi-TA/hPET . In addition, we are providing users with a simple Python script to enable easy access to specific parts of the complete hPET and dPET datasets, based on user needs. This latter script allows the user to specify a geographical box and a selection of years for which data are required. The code then accesses the raw data files and downloads the data for relevant temporal and spatial domains from an open-access data server ( https://doi.org/10.5523/bris.qb8ujazzda0s2aykkv0oq0ctp ). Since the data are freely available, one can download the script and run it on a local machine to download all or a portion of the data. The scripts are all documented, and readme files are provided from the relevant repository."
10.1038/s41597-021-01007-5,"The scripts for generating unbiased average templates are publicly available at https://github.com/vfonov/build_average_model , also re-implemented in Python and publicly available at (see https://github.com/vfonov/nist_mni_pipelines : iplScoopGenerateModel.py as well as examples/synthetic_tests/test_model_creation/scoop_test_nl_sym.py). The scripts for tissue classification tools and FALCON are publicly available at http://nist.mni.mcgill.ca/?p=2148 , https://github.com/philnovv/CNN_NeuroSeg/ , and https://github.com/NIST-MNI/falcon , respectively. To replicate our results or generate a new average template, the user needs to provide preprocessed T1w images to the pipeline. Raw T1w images can be preprocessed using our standard pipeline available at https://github.com/vfonov/bic-pipelines . Afterwards, either build_average_model.rb or scoop_test_nl_sym.py can be used to generate the average template."
10.1038/s41597-021-01008-4,The Proteomics Signature Pipeline (PSP) is available online at https://github.com/cmap/psp . Avant-garde is available at https://github.com/SebVaca/Avant_garde and can be downloaded from the Skyline Tool Store directly in the Skyline interface or at https://skyline.ms/skyts/home/software/Skyline/tools/details.view?name=AvantGardeDIA .
10.1038/s41597-021-01009-3,"Model code for the linear regression as well as the code for the analysis and visualisation of figures is publicly available in the project repository 28 . NetCDF files have been processed using cdo 30 . We used the Python language for most of our scripts with a few bash scripts as wrappers. The workflow for the data generation process is managed by Snakemake 31 . The linear regression is based on the statsmodels package 32 . All visualisations are made with matplotlib 33 using cartopy 34 for maps. Other Python packages used are (in alphabetical order): adjustText 35 , BeautifulSoup4 ( https://www.crummy.com/software/BeautifulSoup/ ), netCDF4 36 , numpy 37 , pandas 38 , 39 , scipy 40 , scikit-image 41 , and tqdm 42 ."
10.1038/s41597-021-01004-8,The face-masking code is available on our GitHub project. ( https://github.com/bicr-resource/deface ).
10.1038/s41597-021-01010-w,"All codes for calculating the indoor and outdoor UTCI, MRT, and other empirical thermal indices, written in Python (3.8) using cdsapi (0.3.1), numpy (1.19.2), pandas (1.1.3), netCDF4 (1.5.4), and scipy (1.5.3) libraries, were developed on Linux (CentOS 6.10) and can be easily adapted to Windows and other platforms. The codes are freely available at the abovementioned repository 52 ."
10.1038/s41597-021-01013-7,MISSING
10.1038/s41597-021-01014-6,"A custom script for tracing and replicating the used processing of the force plate data in Matlab (The MathWorks, Inc., Natick, Massachusetts, United States, 2019a) and custom scripts for importing and merging (with the GaitRec dataset) the data in Matlab (The MathWorks, Inc., Natick, Massachusetts, United States, 2019a) and Python (Python Software Foundation, 3.7) are publicly available at figshare 50 ."
10.1038/s41597-021-01015-5,The source code for the web app is available at https://github.com/droush/cydrasil-web-app . Current and in development versions of the reference package are available at https://github.com/FGPLab/cydrasil . Scripts used in the generation of the reference package and a Markdown file describing the phylogenetic pipeline used are available at https://github.com/FGPLab/cydrasil-helper .
10.1038/s41597-021-01017-3,"The Snakemake analyses pipeline was deposited at GitHub and Zenodo repositories 29 , 30 . Links to software used as well as relevant conda and docker containers are given in Table 2 ."
10.1038/s41597-021-01019-1,All R scripts used to process the STMF data are available at Zenodo 23 .
10.1038/s41597-021-01022-6,"The calculations of the electrical transport properties in this work rely heavily on TransOpt 21 . The code of TransOpt is available at https://github.com/yangjio4849/TransOpt . In the initial structure checking part, we used the phonopy ( http://phonopy.github.io/phonopy/ ). All the home-made codes used to generate the data is available at https://github.com/yangjio4849/MIP ."
10.1038/s41597-021-01018-2,"All code necessary to calculate the CFs is freely available from a Zenodo repository 57 . The ASOC and SOC data used to calculate CFs for crop LU classes were obtained from Morais et al . 34 (Zenodo repository 34 ). We used MATLAB release R2018a to calculate the CFs, including the new RothC runs for transitions to forests, which are available in Morais et al . 57 (Zenodo repository). The script to run the MATLAB version of the RothC model is the script “RothC_TMorais.m”, and the script to calculate foreground and background CFs is “CFs_calculation.m”. The code is not commented, but detailed instructions for how to use the MATLAB scripts is in the Zenodo repository can be provided by the authors after e-mail contact. The Zenodo repository also indicates the list of data needed to run the model (references for collecting the data can be found throughout the paper). Morais et al . 53 (Zenodo repository) includes all the SOC dynamics and obtained CFs."
10.1038/s41597-021-01016-4,The scripts used to generate the BaRAD dataset are available in this GitHub repository: https://github.com/TC25/BaRAD/tree/main/Scripts .
10.1038/s41597-021-01025-3,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-021-01012-8,MISSING
10.1038/s41597-021-01024-4,"In the data repository 65 , the readme files explain the location of the files and folders. All raw measurements records can be found in one Excel sheet. All the field data and satellite images were processed and analysed in IDL and Python. The source codes are available at the Github. https://github.com/BowenSong123/Code ."
10.1038/s41597-021-01021-7,"Two sets of code are available in conjunction with the resulting data, at https://doi.org/10.17605/OSF.IO/GFWHJ . News aggregation code is available in Python. Geolocation code (querying Google Maps and Open Street Maps APIs) is freely available, in R, upon completion of the data use agreement, which is also available in the same repository 48 ."
10.1038/s41597-021-01023-5,The programs used in the data generation is based on MATLAB and GAMS. The associated code can be found in Zenodo repository 38 .
10.1038/s41597-021-01030-6,"A Python notebook is provided in the supplementary material section of this manuscript, which demonstrates data retrieval from IRIS DMC. The notebook also demonstrates how to process data to obtain the results shown in Fig. 4 . The user will require a working installation of the ObsPy package 26 to execute the notebook 27 ."
10.1038/s41597-021-01020-8,No codes were developed for this research.
10.1038/s41597-021-01029-z,"EEGLAB has been used which is available as open-source 21 . ERPlab plug-in for EEGLAB can be used as a freely available solution for further EP analysis 22 . No proprietary code has been deployed in this study except for WATISA (GJB Datentechnik GmbH, Ilmenau, Germany)."
10.1038/s41597-021-01033-3,All code used for aggregating and analyzing the data is version-controlled and publicly available via the associated GitHub repository ( https://github.com/snastase/narratives ) and the code/ directory in the top-level BIDS directory distributed via DataLad ( https://datasets.datalad.org/?dir=/labs/hasson/narratives ). The GitHub repository contains both scripts used to prepare the data for sharing ( staging/ ) and scripts used to analyze the BIDS-formatted data ( code/ ). See Table S1 for a brief description of the scripts used to process the “Narratives” data.
10.1038/s41597-021-01031-5,"Codes for the database, raw data outputs and data visualizations appearing on our website are available on our project GitHub ( https://github.com/COBAPteam/COBAP ). The full text of the survey used to code each policy is available in the Supplementary Materials ."
10.1038/s41597-021-01028-0,"All code is available in the figshare repository 32 . The code includes scripts for the analyses presented in this paper. The scripts rely on open source Python packages such as numpy 35 , pandas 36 , matplotlib 37 , scipy 38 , sklearn 39 and pickle 40 ."
10.1038/s41597-021-01035-1,"The MATLAB Code used to merge the 149 sectors into 45 sectors, 49 sectors, 91 sectors, and 96 sectors is shown below for transparency and verifiability. We take merging 149 sectors into 96 sectors as an example. % Copyright 2020 Nanchang University. All rights reserved. % Read the binary matrix from the excel files MFA = xlsread(‘2017CEEIO.xlsx’,‘149S-96SMM’,‘B2:ET97’); % MFA refers to the binary matrix used to merge 149 sectors into 96 sectors %Read the data in the 2017 Chinese benchmark IO table and environmental database IIA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘D7:EV155’); % IIA refers to the intermediate input matrix in the 2017 Chinese benchmark IO table TIUA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘EW7:EW155’); % TIUA refers to the total intermediate uses vector in the 2017 Chinese benchmark IO table FUA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘EX7:FG155’); % FUA refers to the final uses matrix in the 2017 Chinese benchmark IO table IMA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘FH7:FH155’); % IMA refers to the import vector in the 2017 Chinese benchmark IO table GOA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘FI7:FI155’); % GOA refers to the gross output vector in the 2017 Chinese benchmark IO table TIIA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘D156:EV156’); % TIIA refers to the total intermediate input vector in the 2017 Chinese benchmark IO table VAA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘D157:EV161’); % VAA refers to the value added matrix in the 2017 Chinese benchmark IO table TIA = xlsread(‘2017CEEIO.xlsx’,‘149DIOT’,‘D162:EV162’); % TIA refers to the total input vector in the 2017 Chinese benchmark IO table EPA = xlsread(‘2017CEEIO.xlsx’,‘149-S 2017CEEIOT(RMB)’,‘D163:EV206’); % EPA refers to the environmental pressure matrix in the 2017 Chinese EEIO table of 149 sectors %Merge the 149 sectors into 96 sectors IIFA = MFA*IIA*(MFA)’; % IIFA refers to the intermediate input matrix in the 2017 Chinese EEIO table of 96 sectors TIUFA = MFA*TIUA; % TIUFA refers to the total intermediate uses vector in the 2017 Chinese EEIO table of 96 sectors FUFA = MFA*FUA; % FUFA refers to the final uses matrix in the 2017 Chinese EEIO table of 96 sectors IMFA = MFA*IMA; % IMFA refers to the import vector in the 2017 Chinese EEIO table of 96 sectors GOFA = MFA*GOA; % GOFA refers to the gross output vector in the 2017 Chinese EEIO table of 96 sectors TIIFA = TIIA*(MFA)’; % TIIFA refers to the total intermediate input vector in the 2017 Chinese EEIO table of 96 sectors VAFA = VAA*(MFA)’; % VAFA refers to the value added matrix in the 2017 Chinese EEIO table of 96 sectors TIFA = TIA*(MFA)’; % TIFA refers to the total input vector in the 2017 Chinese EEIO table of 96 sectors EPFA = EPA*(MFA)’; % EPFA refers to the environmental pressure matrix in the 2017 Chinese EEIO table of 96 sectors"
10.1038/s41597-021-01037-z,"ImageJ2 was used for image crop, subtraction, and average. Hessian-based Frangi Vesselness filter 43 was used for vessel enhancement. VMTK ( http://www.vmtk.org ) was used for vessel segmentation. ImageJ2 was used for breast and vessel 3D surface generation. ABAQUS/Standard 2018 was used for breast compression. Custom code (Matlab R2016b) ‘vesselDeform’ publicly available on figshare 27 and stlTools 44 were used for vessel deformation. SOLIDWORKS 2020 was used for breast and vessel surface combination. Autodesk Netfabb premium 2020 was used for deformed vessel surface mesh repair. COMSOL Multiphysics 5.5 was used for optical simulation. Custom code (Matlab R2016b) ‘imageReconstruction’ publicly available on figshare 27 was used for image reconstruction."
10.1038/s41597-021-01006-6,"All code, raw and compiled data are hosted within GitHub repositories under the Trait Ecology and Evolution organisation ( http://traitecoevo.github.io/austraits.build/ ). The archived material includes all data sources and code for rebuilding the compiled dataset. The code used to produce this paper is available at http://github.com/traitecoevo/austraits_ms ."
10.1038/s41597-021-01034-2,"Both the code used for generating this dataset and input files for the OpenFOAM simulations are available on the Kaggle page for this dataset 19 . The software used was OpenFOAM v2006, with all scripts written in Python 3."
10.1038/s41597-021-01041-3,"A MATLAB code which generates a video from the dynamic data is available on GitHub https://github.com/IADI-Nancy/ArtSpeech . This code also allows reading alignment annotation .trs and .textgrid files, and reading audio and dicom files. It uses mPraat third party toolbox which is available on bbTomas GitHub https://github.com/bbTomas/mPraat . The code was tested on Linux distribution of MATLAB2018b and MATLAB2020a. A Python toolbox for .textgrid files parsing also exists 42 ."
10.1038/s41597-021-01038-y,MISSING
10.1038/s41597-021-01043-1,"Three main types of codes were used in generating the datasets. Another one is also developed to interpolate between mid-months, while still conserving the monthly mean values. For spatial regression coefficients, we utilize a shell script calculates spatial regression for different calendar months, implementing various Climate Data Operators commands, and another shell script is used to select best sea ice analogs based on correlation coefficients between reconstructed subpolar SSTs and its instrumental target are available on GitHub ( https://github.com/shamakson/PaleoSST_SIC_ARM.git ). The basic form of the data assimilation code written in R, is available on GitHub ( https://github.com/jf256/reuse.git ) and a python script that implements the AMIP II interpolation scheme is also available via GitHub ( https://github.com/shamakson/AMIP_bc_interpolation.git )."
10.1038/s41597-021-01049-9,R code for quality assurance/quality control and prey taxonomic name cleaning can be found in the “database_error_checking.R” script in the database building project’s Github repository ( https://github.com/hurlbertlab/dietdatabase ). An R package with functions for loading and querying the database is called ‘aviandietdb’ and is available at https://github.com/ahhurlbert/aviandietdb .
10.1038/s41597-021-01032-4,"SOIL-WATERGRIDS has been generated using the BRTSim (BioReactive Transport Simulator) computational solver version v4.1a (2020). The BRTSim software is multiplatform and can be deployed on Microsoft, Unix/Linux/Ubuntu, and Mac operating systems. The full BRTSim package, inclusive of executables, examples, basic post-processing scripts, and User Manual and Technical Guide are available for download at the BRTSim home page https://sites.google.com/site/thebrtsimproject/home under the CC BY4.0 license. Seeding datasets used in our modelling are not distributed here because they are publicly accessible from the links reported in Online-only Table 1 . BRTSim input files required to produce the full-size global-scale simulation of SOIL-WATERGRIDS are distributed in this data release and are available at the Zenodo repository 49 (about 50 GB). The BRTSim modelling distribution package for SOIL-WATERGRIDS includes 7 continental regions (Africa, Asia insular, Asia, Europe, North America, South America, Oceania) organized in compressed folders that include the BRTSim executable and license file, bash files (BRTSim_v41a_GLNXA64_R2019b, run_BRTSim_v41a_GLNXA64_R2019b.sh, license.txt), the Param_*.inp files to instruct BRTSim to run each grid cells of the computational domain, and the tables of boundary conditions (Table_*.txt, WB_*.txt, and RNF_*.txt) organized by continental regions. The license file is valid until the end of 2021 and a new license can be obtained with no charge from the BRTSim home page after expiry. The distributed package allows to model about 168,000 grid cells globally. Raw outputs of the full-scale model run occupy about 10TB (uncompressed) and is not distributed. Full details on the BRTSim modelling including data structure, file naming, supported operating system, modelling launching and workflow, and accompanying scripts are available in the SOIL-WATERGRIDS Technical Documentation 49 ."
10.1038/s41597-021-01050-2,"Source code for the HydroBlocks land surface model is available at https://github.com/chaneyn/HydroBlocks . The Random Forest model used to parameterize the merging scheme was implemented using the RandomForestRegressor class of the scikit-learn Python module. While not written as a portable library or toolset, code is available upon request."
10.1038/s41597-021-01040-4,"To finish the RNA-seq data preprocessing, quality validation, and mapping to the human reference genome, only public-domain software but no other custom code was used. These software tools and their versions are listed as follows: 1. FastQC v0.11.8 was used for quality assessment of the raw reads and the trimmed reads of RNA-sequencing data: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/. 2. Trimmomatic 0.38 was used to remove adaptors and do quality trimming: http://www.usadellab.org/cms/?page=trimmomatic. 3. MultiQC 1.0 was used to perform cross-sample quality assessment of the RNA-sequencing reads: https://multiqc.info/ . 4. STAR 2.7 was used to map the cleaned RNA-seq reads to the human reference genome assembly, GRCh38: https://github.com/alexdobin/STAR . 5. edgeR 3.30.3 was used to carry out trimmed mean of M values (TMM) normalization for gene expression quantification, and to find the differentially expressed genes (DEGs) between sample groups: https://bioconductor.org/packages/release/bioc/html/edgeR.html ."
10.1038/s41597-021-01048-w,"The MRB floodplain land use change dataset is derived entirely through ArcGIS 10.5 and ENVI 5.1 geospatial analysis platforms (see Methods section for details). We developed additional open-access codes and visualization interfaces, however, to promote reproducibility and widespread application of the dataset. The python code is accessible at: https://colab.research.google.com/drive/1vmIaUCkL66CoTv4rNRIWpJXYXp4TlAKd?usp = sharing . The visualization interface is available online at: https://gishub.org/mrb-floodplain . See Usage Notes section for details."
10.1038/s41597-021-01039-x,The bespoke R script can be accessed at SAVSNET TUMOR REGISTRY DOCUMENTS figshare collection 19 with no restriction to access.
10.1038/s41597-021-01042-2,Scripts used to insert required metadata into the published BIDS dataset are freely available at https://github.com/BioMedAnalysis/petmr-bids under Apache License 2.0.
10.1038/s41597-021-01054-y,WGCNA package is available at https://github.com/mortazavilab/5xFAD_WGCNA .
10.1038/s41597-021-01045-z,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-021-01047-x,"The creation of the described datasets requires several steps, each step is performed by a different software. All the software is freely available, in particular: • the code for the processing of the JISC UK Web Domain Dataset for producing both the WET and tokenized files: https://github.com/alan-turing-institute/UKWebArchive_semantic_change ; • the software for building both co-occurrences matrices and TRI: https://github.com/alan-turing-institute/temporal-random-indexing ; • the code for the experiments can be found at https://github.com/alan-turing-institute/DUKweb ; For information on our input data, refer to: https://data.webarchive.org.uk/opendata/ukwa.ds.2/ ."
10.1038/s41597-021-01053-z,MISSING
10.1038/s41597-021-01046-y,"The latest version of all accompanying code for this dataset can be acquired within this repository: https://github.com/ngebodh/GX_tES_EEG_Physio_Behavior . MATLAB, version 2018b and 2019b were utilized with functions from EEGlab 96 , Raincloud plots toolbox 97 , and ANT neuro’s import functions 95 ."
10.1038/s41597-021-01062-y,The code for the segmentation is a shell script which is provided in the Zenodo repository 32 . It performs an automatic cropping of the input dataset prior to registering to a cropped custom template. This is done to speed up registration and for accuracy by focusing on the thalami as the crop region encompasses both thalami. A mask for automatic cropping and the cropped custom template are also provided.
10.1038/s41597-021-01065-9,All code used in this study is available at GitHub: https://github.com/rs-dl/CropIntensity .
10.1038/s41597-021-01068-6,The code used to generate the dataset can be obtained under the same doi 22 .
10.1038/s41597-021-01052-0,The dataset was created using Python scripts with ArcGIS 10.6.1. Codes are publicly available at GitHub through Zenodo 15 . All input data are publicly available as described in “Data Records”.
10.1038/s41597-021-01066-8,"1. Data preparation DenseNet model applied to the MIMIC-CXR dataset which was subsequently used to extract chest radiographs with catheters and lines from the NIH Chest X-ray dataset. The model been made available https://github.com/jarrelscy/cxr-lines-tube-model , which interfaces with the MD.ai interface to perform inference and identify chest radiographs with catheters and lines in the NIH Chest X-ray dataset. 2. Technical validation Preprocessing script describing used in determining final labels for the chest radiographs. This code has been made available at https://github.com/jsntang/ranzcr_kaggle_preprocessing however the raw data export is not released as this contains identifying personal information of the labellers."
10.1038/s41597-021-01060-0,"A compiled, executable version of the in-house developed segmentation algorithm which was used to annotate the VerSe dataset is publicly accessible through a website ( https://anduin.bonescreen.de ). This final version has been trained with the entire VerSe dataset. The entire dataset and its helper code (including the data reading, writing, and evaluation scripts) can be accessed here: https://github.com/anjany/verse . For an overview of the various algorithms submitted as part of the segmentation challenges held at the MICCAI conferences in 2019 and 2020 based on the public VerSe datasets, kindly refer to Sekuboyina et al . 19 . These nicely demonstrate the feasibility and potential of automated segmentation tasks and processing of spine CT imaging."
10.1038/s41597-021-01057-9,The dataset and the code to post-process the data and control the Bertec treadmill can be found on figshare 35 . The data can be accessed though MATLAB and is documented with a README file describing the data hierarchy. The MATLAB code that remotely controls the Bertec treadmill running a designed protocol is available with the dataset. This code is documented with its own README file.
10.1038/s41597-021-01055-x,"All code used to collect, process, and validate the data was written in Python and is available for download 29 ( https://github.com/mhsjacoby/HPDmobile ). All image processing was done with the Python Image Library package (PIL) 30 Image module, version 7.2.0. Audio processing was done with SciPy 31 io module, version 1.5.0. Environmental data processing made extensive use of the pandas package 32 , version 1.0.5. The code base that was developed for data collection with the HPDmobile system utilizes a standard client-server model, whereby the sensor hub is the server and the VM is the client. Note that the term “server” in this context refers to the SBC (sensor hub), and not the the on-site server mentioned above, which runs the VMs. All collection code on both the client- and server-side were written in Python to run on Linux systems. Technical validation of the audio and images were done in Python with scikit-learn 33 version 0.24.1, and YOLOv5 26 version 3.0."
10.1038/s41597-021-01058-8,The R code used in the analysis of the scRNA-seq data is available on GitHub ( https://github.com/sikh09/Medaka-pituitary-scRNA-seq ).
10.1038/s41597-021-01056-w,Code used for circRNA identification is available in the Supplemental data. Software versions used for analysis are as follows: STAR v2.4.0j for DCC and CIRCexplorer bowtie2 v2.2.1 for find_circ and KNIFE bowtie v0.12.9 for MapSplice2 bwa v0.7.13 for CIRI2 bedtools v2.26.0 Data were analyzed using the R v3.3.2 statistical package ( https://cran.r-project.org ). UpSet plots were generated using the UpSetR v1.3.3 package 54 . The ReadqPCR v1.20.0 and NormqPCR v1.20.0 Bioconductor v3.4 packages were used for qRT-PCR data analysis 59 .
10.1038/s41597-021-01063-x,"The codes used for aligning the RNAseq reads to the mm10 mouse reference genome with TopHat v2.1.1, quantifying the read counts using HTSeqCount, comparison of the gene expression using DESeq 2, and PCA analysis using Plotly 3-D PCA, are available in Figshare 46 ."
10.1038/s41597-021-01059-7,"The S1GBM mosaics were produced with geodata management software and scientific algorithms contained in the SAR Geophysical Retrieval Toolbox ( SGRT v2.4) software suite, which embeds also open-access python libraries ( GDAL 36 , NumPy 45 ) and Sentinel-1 preprocessing functions of the SNAP v6.0 toolbox 30 . The SGRT suite has been developed by TU Wien and is not openly accessible, and is only available under conditions to project- and research-partners of TU Wien. For the usage of the Equi7Grid we provide data and tools via the openly accessible python package on GitHub 33 . Furthermore, we encourage users to use TU Wien’s open-source Python package yeoda , a datacube storage access layer that offers functions to read, write, search, filter, split and load data from the S1GBM datacube. The yeoda package is openly accessible on GitHub 55 ."
10.1038/s41597-021-01064-w,"We have released a code repository for automated segmentation of VS with deep learning ( https://github.com/KCL-BMEIS/VS_Seg ). The applied neural network is based on the 2.5D UNet described in 9 and 10 and has been adapted to yield improved segmentation results. Our implementation uses MONAI, a freely available, PyTorch-based framework for deep learning in healthcare imaging ( https://monai.io ). This new implementation was devised to provide a starting point for researchers interested in automatic segmentation using state-of-the art deep learning frameworks for medical image processing. To apply the algorithm to the VS dataset, we randomly split the final 242 patients into 3 non-overlapping groups: 176 for training, 20 for hyperparameter tuning, and 46 for testing, with median tumour volumes of 1.37 cm 3 (range 0.04–9.59 cm 3 , IQR 0.64–3.14 cm 3 ), 0.92 cm 3 (range 0.12–5.50, IQR 0.51–2.40 cm 3 ), and 1.89 cm 3 (range 0.22–10.78, IQR 0.74–4.05 cm 3 ), respectively. Thirty-four patients (19%) in the training dataset had undergone previous surgery compared with 2 patients (2%) in the hyperparameter tuning set and 14 patients (30%) in the testing dataset. The algorithm leads to excellent average Dice scores between predicted segmentations and ground truth (see Technical Validation). To apply the released code to the VS data set, the DICOM images and RT Structures have to be converted into NIfTI files. The repository includes a script and instructions for this purpose."
10.1038/s41597-021-01069-5,The codes used in this study have been deposited to https://github.com/theochem/B3DB and https://doi.org/10.6084/m9.figshare.15634230.v3 (version 3) 33 . All the calculation were done with Python 3.7.9 under a virtual environment created with Anaconda on Linux.
10.1038/s41597-021-01061-z,Source code in Python for the data processing is available in the data record 12 .
10.1038/s41597-021-01067-7,No custom code was used to generate or process the data.
10.1038/s41597-021-01073-9,No custom code was used in this study.
10.1038/s41597-021-01074-8,Code to reformat the relational database to the LiPD and Rdata formats was adapted from this example ( https://github.com/nickmckay/sisal2lipd ) and is available in PalaeoWISE 26 . Code to produce the figures are available in PalaeoWISE 26 . Correlations were all produced using code published within the original publications cited within.
10.1038/s41597-021-01072-w,"Computer codes used to generate and clean the Carabidae – Fungi database, as well as those to produce plots in the article and the Shiny app are freely available at https://github.com/pozsgaig/CaraFun ."
10.1038/s41597-021-01079-3,The code used to produce the bias-corrected global CMIP6 data is publicly available 45 . The code consists of an NCL (version 6.6.2) script to compute non-linear trends and a few CDO (version 1.7.0) scripts to regrid data and correct CMIP6 data biases.
10.1038/s41597-021-01076-6,"The SDC is based on the Open Data Cube platform that can be obtained at: https://www.opendatacube.org and freely available under an Apache 2.0 license ( https://opensource.org/licenses/Apache-2.0 ). The Analysis Ready Data workflows are developed as a suite of Python (3.6) and R (2.7) scripts. Both programming languages are freely available at: https://www.python.org and https://cran.r-project.org respectively under Python Software Foundation License (PSFL) and GNU General Public License. Landsat, Sentinel-1, Sentinel-2 workflows are freely available on the Swiss Data Cube GitHub repository https://github.com/GRIDgva/SwissDataCube/tree/master/ingestors under GNU GPL 3.0 license ( https://www.gnu.org/licenses/gpl-3.0.en.html ). The Landsat Ecosystem Disturbance Adaptive Processing System (LEDAPS) (version 3.4.0) is an atmospheric correction model developed by USGS. The repository was recently removed from Github and is being re-established on the USGS Official Source Code Archive https://code.usgs.gov/espa and will be soon freely available under Unlicense conditions ( https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes ). Land Surface Reflectance Code (LaSRC) (version 1.4.1) is an atmospheric correction model developed by USGS. The repository was recently removed from Github and is being re-established on the USGS Official Source Code Archive https://code.usgs.gov/espa and will be soon freely available under Unlicense conditions ( https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes ). Sen2cor (version2.5) is a processor for Sentinel-2 Level 2 A product generation and formatting. It is freely available at: https://step.esa.int/main/third-party-plugins-2/sen2cor/ under an Apache 2.0 license. The Sentinel-1 processing is performed using UZH’s in-house developed radiometric image simulation and geocoding software radsim v2.0, currently undergoing packaging for wider distribution. More information can be obtained by contacting Dr. David Small (University of Zurich): https://www.geo.uzh.ch/geolean/en/department/Staff/?content=davidsmall . However, the algorithm used to generate Sentinel-1 analysis ready radiometrically terrain-corrected (RTC) Synthetic Aperture Radar (SAR) gamma nought backscatter data described in 46 is also available in the European Space Agency (ESA) Sentinel Application Platform (SNAP) that is freely available at: http://step.esa.int/main/download/snap-download/ under a GNU GPL v3 license. GeoNetwork is a web-based application (version 3.10.5) used for cataloguing, publishing and managing metadata description of spatially referenced resources using OGC and ISO standards. It is freely available at: https://geonetwork-opensource.org under a GNU General Public License v2.0. Datacube-ows has been used to expose data collections according to OGC standards for visualization (Web Map Service – WMS) and download (Web Coverage Service – WCS). It is freely available at: https://github.com/opendatacube/datacube-ows under an Apache 2.0 license."
10.1038/s41597-021-01077-5,"All codes used in processing the whole genome, exome-seq and RNA-seq data are available to download at NCBI’s ftp site ( https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/tools/NGS_Preprocessing_Pipeline ) 21 ."
10.1038/s41597-021-01071-x,"All source code for ExeTera is made available through github under the Apache 2.0 license, at the time of writing. The code is split up into two separate projects. ExeTera ExeTera is hosted at https://github.com/KCL-BMEIS/ExeTera and is available through pypi via pip install exetera . ExeTera has a wiki that can be found at https://github.com/KCL-BMEIS/ExeTera/wiki . ExeTeraCovid ExeTeraCovid is hosted at https://github.com/KCL-BMEIS/ExeTeraCovid and is available through pypi via pip install exeteracovid . Installing exeteracovid installs exetera . ExeTeraCovid has a wiki that can be found at https://github.com/KCL-BMEIS/ExeTera/wiki ."
10.1038/s41597-021-01086-4,No code was used in the generation of the data.
10.1038/s41597-021-01083-7,"Primary data (crash and disengagement data) extraction does not utilise any automated pipelines or scripts due to implicit and subjective data in the original reports. Experts manually analysed each report to register the data into processable formats. GeoPandas 19 , OSMnx 20 , and Pandas 21 Python custom modules are used for the extraction of secondary data. Code documentation for GeoPandas ( https://github.com/geopandas/geopandas ) and OSMnx ( https://github.com/gboeing/osmnx ) can be found in the corresponding GitHub repositories. A more elaborative procedure is explained in the Methods section."
10.1038/s41597-021-01088-2,"Throughout this work, the Gaussian software package was used for geometry optimizations, frequency calculations, and composite (RO)CBS-QB3 calculations. The Gaussian software package can be purchased from Gaussian Inc. ( http://gaussian.com/ ) under a commercial license. CSD conformer generator was used for conformer generation. The CSD conformer generator can be purchased under a commercial license from https://www.ccdc.cam.ac.uk/solutions/csd-enterprise/applications/conformer-generator/ . Fullmonte software package was also used along with MOPAC16 (PM6-DH2 method). Fullmonte software package can be downloaded free-of-cost from https://github.com/bobbypaton/FullMonte . Whereas MOPAC16 software package can be installed after acquiring a free license from http://openmopac.net/ . The Avogadro molecular editor and visualizer is an open-source program available at https://avogadro.cc/ ."
10.1038/s41597-021-01082-8,No code was used in the generation of this data. No code is required to access or analyze this dataset.
10.1038/s41597-021-01091-7,"All software used in this work is in the public domain, with parameters being clearly described in Methods. If no detail parameters were mentioned for a software, default parameters were used as suggested by developer."
10.1038/s41597-021-01075-7,"The original input data, the amount of products quantified by per-household, per-capita, and absolute in each province, is stored as xlsx files and shared in Figshare (Input data.xlsx) 36 . Data processing is performed using MATLAB software (MatlabR2019), and the codes for creating provincial and national stocks and flows datasets are also stored in Figshare (code_calculation.m) 36 . We share these datasets and scripts for data transparency and computational reproducibility, and to assist users for further exploration and development."
10.1038/s41597-021-01081-9,Code in R language to recreate the database and the figures in the Usage Notes is available on Figshare 41 .
10.1038/s41597-021-01089-1,"The code of patient image SPM analysis using our template data has been uploaded to both the NITRC repository 23 ( https://www.nitrc.org/projects/cnpet/ ) and our group’s GitHub website ( https://github.com/DlutMedimgGroup/Chinese-Brain-PET-Template ) along with the template data. In detail, the code includes the steps of patient image spatial normalization, intensity normalization and the two-sample t-test."
10.1038/s41597-021-01080-w,"The processing was performed mainly using brainlife.io services ( https://brainlife.io/apps ), which together with the code are available online. The offline preprocessing was performed with freely accessible neuroimaging tools. The preprocessing steps, together with the references to the Software/Apps are provided separately for the T1w, DW, fMRI data, and hand-curated ROIs and mask (Tables 1 – 4 , respectively). The web links to source code are provided separately in the Table 5 ."
10.1038/s41597-021-01087-3,"Computational codes used to, firstly, perform the depth-corrected mosaics synthesis used in the study and, secondly, for automated nerve tracing were developed by the academic institutions of Karlsruhe Institute of Technology and University of Padua, respectively, and are exclusively intended for scientific research use 30 , 31 . The developers of the respective algorithms are willing to apply the code to user-supplied raw IVCM data in the form of academic collaborations. Interested parties are requested to contact the respective researchers for mosaic creation (Allgeier) and automated nerve analyses (Scarpa)."
10.1038/s41597-021-01084-6,The code calculating the bias correction on the CHELSA V2.0 precipitation data is written in Python 2.7 and C++ (via the SAGA-GIS api). The code for the cloud cover refinement is available here: https://gitlabext.wsl.ch/karger/chelsa_earthenv . The code for the validation is available here: https://gitlabext.wsl.ch/karger/chelsa_earthenv_validation .
10.1038/s41597-021-01078-4,The codes used in this article were deposited in https://github.com/LuChenLab/hemato .
10.1038/s41597-021-01093-5,"All the code used in the process of data acquisition, processing and analysis have been written in Python Language (version 3.7) and executed in a Linux environment. For data handling, analysis and visualisation we have used the following python libraries: Pandas (0.11.0) SciPy (1.5.2) and Seaborn (0.11.0). All spatial operations have been conducted using GeoPandas (version 0.8.1). The database used to host COVID-19 Flow-Maps is MongoDB (version 4.2.8). The REST-API used to expose the data is implemented using python eve (version 1.1.2). The source code used in this work is available in the public GitHub repository that also hosts the data-set. COVID-19 Flow-Board interactive dashboards were implemented in plotly (version 4.11.0)."
10.1038/s41597-021-01090-8,The R script used in this research is publicly available and can be found in github ( https://github.com/hazhay/Meta-analysis_AD ).
10.1038/s41597-021-01095-3,The dataset and the source code for the data analysis are publicly available as the ProMetIS R package 27 on GitHub ( https://github.com/IFB-ElixirFr/ProMetIS ). The package includes several vignettes describing the pre-processing of the data as well as the tables and figures from this article ( 6_article_data ).
10.1038/s41597-021-01085-5,All data analysis was conducted using ElectroMap software with processing and analysis settings as described above. ElectroMap MATLAB code is available at https://github.com/CXO531/ElectroMap .
10.1038/s41597-021-01094-4,"The MATLAB scripts are available for loading data, for evaluating classification performance or signal quality, and for plotting figures at https://github.com/youngeun1209/MobileBCI_Data ."
10.1038/s41597-021-01104-5,"Feedback, community engagement and updates The R data package presented on GitHub ( https://github.com/RBGKew/BIFloraExplorer ) 49 is intended to be a dynamic representation of the data. As changes in the flora arise or new associated information becomes available, these will be incorporated into future releases of the R package, allowing for a dynamic representation of the changing flora as well as version control reflecting database development. While there are gaps in our current knowledge, especially regarding non-native species in Britain and Ireland, we aim to update the dataset as new information becomes available. The data stored in the EIDC repository will remain static and reflect the dataset as of publication date of this data descriptor."
10.1038/s41597-021-01106-3,"Code used to create BIDS data is located in the code directory at the root level of the dataset. The ELP_convert_to_bids.py is the main code for BIDS organization from the pre-existing file storage. It calls two sub-functions: beh_process.py, which creates events files from compiled E-prime data and dicom_proc.py, which utilizes dcm2niix to convert DICOM to NIFTI files with its corresponding json file. The ELP_acc_rt.ipynb and four excel files (i.e., Gram.xlsx, Phon.xlsx, Plaus.xlsx, and Sem.xlsx), which are located in the code/func_qa/acc_rt folder, are the code and files used to produce the calculated_response, calculated_accuracy, and calculated_RT for each trial in the events files. The code used to evaluate the movement of the T2-weighted images is located in the code/func_qa/mv folder. The main_just_for_movement.m is the main code used for movement evaluation that calls other sub-functions: realignment_byrun.m and art_global_bdl.m. SPM12 and ArtRepair need to be added before running this code. The count_repaired_rt_acc.m is the code used to count the outlier volumes as marked by ArtRepair and calculates the accuracy and RTs for each condition of each run for each task. Code used to examine the quality of diffusion weighted images is located in the code/dwi_qa folder. The dwi_eddy.sh is the main code that runs FSL eddy which calls other parameters stored in the acqparam.txt, index1.txt, index2.txt, or index3.txt files. The eddy_qa_output.sh is the code that calls the FSL eddy outputs and calculate the quality metrics stored in the derivatives/dwi_mv_snr folder. The DTI_qa_composite.m is an additional code to calculate a composite quality score for the dwi. Code used to calculate the composite score for the quality of the T1-weighted images is located in the code directory and named T1w_qa_composite.m."
10.1038/s41597-021-01103-6,No new code was used or developed for the study.
10.1038/s41597-021-01102-7,"Code and detailed instructions to reproduce the technical validation analyses and figures presented in this manuscript are available from the Open Science Framework ( https://doi.org/10.17605/OSF.IO/HD6ZK ) 29 , which also contains links to the data repositories."
10.1038/s41597-021-01101-8,"The authors make available the custom code: • written in MATLAB, to obtain the trajectories of points of interest in the utensils, from the motion capture data; • written in MATLAB, to display the movements in a 3D plot. • written in MATLAB, to interact with Nexus and automatically split C3D and ASCII files, because this may be useful for any study that requires processing a huge amount of MoCap data; • written in Python, to track food in video records; • written in Python, to calculate the metrics to assess the performance of the tracking algorithm. • written in C++, to synchronize the clocks of the two computers, that was run before every experimental session. All the code is freely available on the following repository: https://github.com/deborapereira/foodFlippingDataset . More detailed information on how to use it is embedded in the code files. The code to process the videos uses Python 3.6.4, OpenCV 4.1.2, Keras 2.2.4 and Tensorflow 1.15.0. The code for the mask R-CNN can be found at https://github.com/matterport/Mask_RCNN/releases/tag/v2.1 . The version 2.1 was used with this dataset. The code to calculate the performance metrics was adapted from https://github.com/rafaelpadilla/Object-Detection-Metrics , a tool developed by the authors of 46 ."
10.1038/s41597-021-01099-z,"To provide an easy way to detect and analyze electrophysiological activity in iEEG signals we make our codes available at GitLab ( https://gitlab.com/brainandmindlab/memory_encoding ). Currently, the repository contains a python script to automatically process the individual BIDS layers of the dataset (subject, task, run, channel) using the EPYCOM library 47 . The library is focused on iEEG processing and contains a set of algorithms for automated detection of high frequency oscillations 24 , 48 , 49 , interictal epileptiform discharges 50 and for computation of univariate and bivariate feature calculation 51 , 52 . The repository will be gradually updated with scripts for statistical analyses and result visualizations."
10.1038/s41597-021-01100-9,No custom code was used to generate the data.
10.1038/s41597-021-01092-6,"The Matlab codes for gradient nonlinearity correction have been modified into Python codes, which are publicly available ( https://github.com/ksubramz/gradunwarp ). The FMRIB Software Library used in the diffusion data pre-processing and DTI and BEDPOSTX model fitting is publicly available ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation ). The FreeSurfer software is publicly available ( https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall ). The software used for face masking is publicly available ( https://nrg.wustl.edu/software/face-masking ). The SPM software for image intensity bias correction is publicly available ( https://www.fil.ion.ucl.ac.uk/spm ). The MRtrix3 software for the constrained spherical deconvolution is publicly available ( https://www.mrtrix.org ). The DIPY software used for the CSA-QBI and GQI reconstruction is publicly available ( https://www.dipy.org ). The codes for the GDSI reconstruction are publicly available ( https://github.com/qiyuantian/GDSI ). The DESIGNER software for the DKI and WMTI model fitting is publicly available ( https://github.com/NYU-DiffusionMRI/DESIGNER ). The NODDI Matlab toolbox is publicly available ( http://mig.cs.ucl.ac.uk/index.php?n=Tutorial.NODDImatlab ). The SMT software for the MicroDTI and MCMicro model fitting are publicly available ( https://github.com/ekaden/smt )."
10.1038/s41597-021-01116-1,"The dataset contains a code/folder with scripts, and .ini files for generating each derivative. CMP3 is freely available at https://connectome-mapper-3.readthedocs.io , the Cartool software via https://sites.google.com/site/cartoolcommunity ."
10.1038/s41597-022-01123-w,"These simulations were conducted using ParFlow version 3.6.0 ( https://github.com/parflow/parflow/tree/v3.6.0/ ). The data processing step was done using Python3.5 programming language with necessary toolboxes including NumPy ( https://numpy.org/ ), the Geospatial Data Abstraction Library (GDAL; https://gdal.org/ ) and the Python Data Analysis Library (PANDAS; https://pandas.pydata.org/ )."
10.1038/s41597-022-01120-z,"R version 3.6.1 (R Foundation for Statistical Computing, Vienna, Austria) was used to manage data and perform analyses in this study. The code used to generate datasets and plots is available for download from the repository on GitHub at https://github.com/LaiShengjie/Holiday ."
10.1038/s41597-021-01115-2,Code used in this paper is available here: https://github.com/wsag/GAEZ-_2015_code . This repository includes the scripts that: a) Convert annual harvested area to monthly crop physical area b) Aggregate the Hydrosheds 5-minute river network into 20–200 km 2 watersheds c) Compares GAEZ+ 2015 harvested area with FAOSTAT harvested area d) Compares GAEZ+ 2015 yield data to GDHY yield data e) Compares GAEZ+ 2015 cropland physical area data to HYDE 2.3 cropland physical area data
10.1038/s41597-021-01112-5,The source code of ICEO can be downloaded from the ICEO GitHub website: https://github.com/ontoice/ICEO . Term requests and updating can also be reached on the ICEO GitHub issues tracker ( https://github.com/ontoice/ICEO/issues ). And wide community participation in ICEO development and applications is highly welcome.
10.1038/s41597-022-01122-x,"Three Jupyter notebooks were implemented to facilitate the handling of the dataset (one notebook for each subset). The notebooks are publicly available to download from https://github.com/jpchavat/ecd-uy . For a correct execution of the notebooks, Python version 3 and the Pandas and Numpy libraries are required."
10.1038/s41597-021-01105-4,"No code has been provided because the data were analysed using ESRI’s ArcGIS software package and through queries to a postGIS database where the data were collected and stored during the Geo-Wiki campaign. The data sets are provided in comma-separated files, which can be imported into any type of analysis or GIS package for further processing."
10.1038/s41597-022-01124-9,"The Java based FMF software used to create the synthetic microdata is made available for free under a GNU General Public Licence and can be downloaded from: https://github.com/MassAtLeeds/FMF/releases . The R (version 4.0.3; https://www.r-project.org ) code developed for generation, aggregation, and validation of the synthetic microdata are publicly and freely accessible through Figshare 28 . The script is documented to both explain its purpose and guide the user through its customisation."
10.1038/s41597-021-01117-0,"The code can be accessed on the public GitHub repository: https://github.com/psychosensing/popane-2021 . It is licensed under MIT OpenSource license, i.e., the permission is granted, free of charge, to obtaining a copy of this software and associated files (e.g., the Jupyter IPython Notebooks), subject to the following conditions: the copyright notice and the MIT license permission notice shall be included in all copies or substantial portions of the software based on the scripts we published. Scripts that we used to transform the data from proprietary acquisition formats into coherent CSV files utilized Python 3.6 83 . The list of the specific modules and their versions is available in the “requirements.txt” file in the GitHub repository. Jupyter Notebooks use Python version: 3.5.3, as well as the following Python modules: packages related to Jupyter Notebook: notebook module v. 6.1.4; jupyter-core module v. 4.6.3, jupyter-client v. 6.1.7; ipython v. 7.9.0; ipykernel v. 5.3.4 84 ; and a data organization and manipulation module – pandas v. 0.25.3 73 ."
10.1038/s41597-021-01098-0,"All custom code created to process of manipulate external datasets in the construction or subsequent update of the NaKnowBase relational database tables are made publicly available by the U.S. Environmental Protection Agency, Office of Research and Development (ORD) 9 , 10 ."
10.1038/s41597-021-01114-3,All of the annotated custom code and scripts used for the generation of this dataset is publicly available at the DToxS GitHub repository at https://github.com/DToxS .
10.1038/s41597-021-01111-6,The code used for data imputation and dataset formatting is available in Figshare 65 .
10.1038/s41597-022-01121-y,All R code that was used in the GCBD are embedded in the database.
10.1038/s41597-021-01107-2,The code used for post-processing of the kinematic data is available at https://github.com/tuniklab/scientific-data .
10.1038/s41597-021-01108-1,No custom code was used in the creation of this database.
10.1038/s41597-021-01110-7,Harutyunyan’s code for its benchmark suite is available on a Zenodo repository 54 . Our code to reproduce the results and apply the analytical fairness and generalizability assessment framework also to other contexts is available on GitHub ( https://doi.org/10.25740/tb877wd0973 ) 55 .
10.1038/s41597-022-01118-7,Code used to prepare the datasets and for the data quality analyses reported above were developed using both the R and Python computing languages.
10.1038/s41597-022-01130-x,No custom code was used to generate or process the data described in this data descriptor.
10.1038/s41597-022-01127-6,No code was used in the generation of this data. No code is required to access or analyze this dataset.
10.1038/s41597-022-01125-8,Example scripts are provided with the dataset 25 . They contain code for reading and plotting the neural and behavioural data. Code examples are provided in Matlab and Python Jupyter Notebooks.
10.1038/s41597-022-01131-w,The R code used to visually summarize the data in the current study is available at OSF 15 .
10.1038/s41597-022-01126-7,"Technical validation and data visualization was performed in RStudio (Version 1.1.463) 35 , using R (version 3.6) 23 , and Bioconductor (version 3.9) 36 , with packages tidyverse (version 1.3.1) 24 , UpSetR (version 1.4.0) 25 , patchwork (version 1.1.1) 26 , plot3D (1.3) 27 . The R code used for these analyses, in the form of an annotated R notebook, is freely available in Figshare 19 . Additional software tools used to analyse this miRNA-seq dataset were the following: FastQC (version 0.11.5) 16 , Chimira (version 1.5) 17 , and Mirnovo (version 1.0) 21 as described in the Methods section."
10.1038/s41597-022-01119-6,The software produced throughout this study is available at https://github.com/alexdesiqueira/fcn_microct/ under the BSD-3 license.
10.1038/s41597-022-01136-5,No post hoc manipulation of the data has occurred. The ARHCD files accessible via the ADA are available in .sav; .sas; .dta; and .sas7bdat formats 8 .
10.1038/s41597-022-01128-5,"Three scripts are available in figshare 87 . “1_EU-Trees4F_sdm_present_future.R” runs BIOMOD2 to model tree species distributions until the end of the century. It uses tree species occurrences and current/future environmental parameters that are stored in the “datasets” directory. This script does the following tasks: a) projects BIOMOD2 model output into the current and future environmental conditions. The projection into the future is made using two approaches: 1) A climatic ensemble mean of the 11 RCMs, or 2) an ‘SDM’ ensemble mean that projects the consensus model for every single RCM, and a posteriori averages the outputs of the 11 SDMs; b) calculates multivariate similarity between current and future projections, to avoid extrapolation to non-analog climates, c) derives the realized niche for each tree species by trimming the potential distribution maps with their native ranges distributions. The other two scripts calculate dispersal into the future using output from the first script. “2_EU-Trees4F_dispersal_migclim_ens_clim.R” using a climatic ensemble mean, whereas “3_EU-Trees4F_dispersal_migclim_ens_sdms.R” uses the ‘SDM’ ensemble mean. The directory “datasets” contains various files that are needed for the code to run, such as the environmental parameters and the species occurrences. All scripts were written and run in R software version 3.6.3 93 (2020–02–29)."
10.1038/s41597-022-01132-9,"We provide two Python scripts to accompany the GLOBathy dataset: “Generate_Bathymetry_Rasters.py” prepares bathymetric maps of the GLOBathy dataset. It requires two inputs: 1) a csv file containing maximum depth of the waterbodies (e.g., “ GLOBathy_basic_parameters(ALL_LAKES).csv ” can be used as a template), and 2) polygon shapefiles of the corresponding waterbodies (e.g., “HydroLAKES_polys_v10.shp” obtained from the HydroLAKES dataset can be used as a template). This script can be used to re-generate GLOBathy data with new Dmax estimations/observations or for any other case study, as long as the waterbody Dmax value and shapefile are available. In addition, we provide the “WGS_84_cell_dimesion_calculator.py” script which can be used to calculate cell dimensions of the GLOBathy raster files in South-North and East-West directions. It will provide the cell dimensions for any given location, so that accurate distances and volumes may be calculated. This is necessary because the geocentric coordinate system of the input raster data (WGS84) does not preserve distances. This script requires either the average latitude of the domain to be updated in the script header manually or a path to at least one bathymetry raster file to obtain the dimensions. In the first case the outputs are average cell dimensions of the study area. In the latter case for each bathymetry raster input, a csv file is generated that includes cell dimensions for every cell in the raster file. This script can also be used for other cases with a similar geocentric coordinate system. Script options (at the beginning of the script) need to be updated based on the input raster file."
10.1038/s41597-022-01133-8,The scripts used for constructing reference standards are available in a public repository GitHub 46 ( https://github.com/Yonsei-TGIL/Mosaic-Reference-Standards.git ) and are accompanied by markdowns for a step-by-step description.
10.1038/s41597-022-01134-7,Data processing was done in R using the tidyverse packages. Data came from a variety of sources in many different formats most processing had to be done on a reservoir by reservoir basis. The dataset is currently hosted with a static DOI (see below) on Zenodo and is 299.3 MB. DOI link for dataset: https://doi.org/10.5281/zenodo.5893641 23 .
10.1038/s41597-022-01145-4,All R (version 3.3.2) codes necessary for analysing the data and producing this data descriptor are publicly available at https://github.com/b-montevideo/French_yields_code . Any requests should be directed to Bernhard Schauberger.
10.1038/s41597-021-01113-4,The developed codes and tools for generating and validating the population datasets are written in Python and openly available on GitHub: https://github.com/DigitalGeographyLab/mfd-helsinki .
10.1038/s41597-022-01138-3,"To decode the JHPCN-DF compression, no special attention was required. (The easy-to-use sample code for generating the LAMMPS input data file is attached in the Supporting Information ) To encode/segment the data into two parts with JHPCN-DF compression, as shown for the above data, the main part of the reference code 41 is as follows: union fi64{ double f; uint64_t i64; }; double fval0,fval1,allowerr,logallo; int i,ntotal,ival,ival2,sval; union fi64 fival,fival1; double *posi_before_compress, *posi_after_compress; uint64_t *tailing_fraction_bits_posi; allowerr = 0.00001 logallo = log(allowerr)/log(2.0); for(i = 0;i < 3*ntotal;i++){ fval0 = posi_before_compress[i]; frexp(fval0,&ival); ival2 = (int)(-logallo + ival); sval = (int)(53-ival2); if(sval > 52) sval = 53; do { sval–; fival.f = fval0; fival.i64 = (fival.i64 ≫ sval); fival.i64 = (fival.i64 ≪ sval); fval1 = fival.f; } while ((fval1-fval0)*(fval1-fval0) >allowerr*allowerr); posi_after_compress[i] = fval1; fival1.f = fval1; fival.f = fval0; tailing_fraction_bits_posi[i] = (fival1.i64 ^ fival.i64); } For software developers, RIKEN has released the open library “JHPCN-DF” at the following GitHub repository: https://github.com/avr-aics-riken/JHPCN-DF ."
10.1038/s41597-022-01149-0,"In the current study, the following open access software was used as described in the Methods section. For all the software, we used default parameters, and no custom code was used beyond the tools listed. 1. FastQC (version 0.11.9) was used to check the quality of raw FASTQ sequencing data: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . 2. Fastp (version 0.20.1) was used to trim adapters and filter quality reads: https://github.com/OpenGene/fastp . 3. HISAT2 (version 2.2.0) was used to map sequence reads to the mouse mm9 genome: http://daehwankimlab.github.io/hisat2/ . 4. DESeq2 (version 1.10.1) was used to identify differentially expressed genes: https://bioconductor.org/packages/release/bioc/html/DESeq2.html . 5. FactoMineR (version 2.4) was used to perform PCA: https://cran.r-project.org/web/packages/FactoMineR/index.html . 6. Pheatmap (Version 1.0.12) was used to plot the heatmap: https://cran.r-project.org/web/packages/pheatmap/ . 7. Ggplot2 (version 3.3.4) was used to generate the volcano plot: https://cran.r-project.org/web/packages/ggplot2/index.html . 8. The Combat_Seq function from R package SVA was used to correct the batch effect of samples in each batch: https://bioconductor.org/packages/release/bioc/html/sva.html . 9. The GeneBodyCoverage.py script from RseQC package (version 4.0.0) was used to evaluate the quality of the reads: https://sourceforge.net/projects/rseqc/files/ ."
10.1038/s41597-022-01135-6,"Data download and processing scripts. The scripts used to download, extract, and process the MEDMI and AURN datasets are implemented in Python and are available under the GPL-3.0 license 18 . Instructions on how to use them are included in the associated README.md files. Regional estimations package. The concentric regions algorithm described above is implemented in Python, using the publicly available GeoPandas library ( http://geopandas.org/ ). We have made this regional estimations code available as a Python library, accessible under the MIT licence 19 . Instructions on how to use the package are included in the associated README.md file. This package receives inputs from users as three CSV files (site metadata, site measurement data and region metadata), and the algorithm iterates through each requested region and timestamp combination, outputting an estimation for each. As we present the concentric regions algorithm as a baseline method, this package is built in a modular fashion to facilitate users adding further regional estimation algorithms as new components, further to the two currently implemented (concentric regions and a simple shortest distance based algorithm). EMEP modelling system. The EMEP model source code is available to download from Zenodo 20 . The input files, and NAEI emissions used for this project are also available for download from Zenodo 21 , 22 ."
10.1038/s41597-022-01137-4,The code used for RNA sequencing data analysis can be found on GitHub ( https://github.com/csoneson/WagnerEMT2020 ) and can be accessed without restrictions. Please refer to the Methods section above for more details on software versions.
10.1038/s41597-022-01141-8,"No custom code was used in the production of this dataset. Use of existing software (Microsoft Excel) is described in the Methods and Data Records sections. The translation code that converts the dataset into the format developed by Heeren and Fishman (2019) 6 and the sample code for querying the dataset are coded in Python. The dataset, translation script and sample code are made available in an open source repository 15 and the Python packages used in the sample code are listed in the “requirements.txt” file ( https://doi.org/10.5281/zenodo.5576147 )."
10.1038/s41597-022-01146-3,All R code are available at https://github.com/open-AIMS/tropical_ucrit_data .
10.1038/s41597-022-01139-2,All the custom code written in MATLAB used for image processing and for all the analyses presented is available at https://github.com/anacmota/miFISH . The DOTTER suite written in MATLAB is available at github.com/elgw/dotter . The MATLAB code for chromatic aberration correction is available at https://github.com/elgw/df_cc . The Python documentation for 3D segmentation and lamina distance calculation is available here 37 .
10.1038/s41597-022-01148-1,No custom code was used to process the data described in this paper.
10.1038/s41597-022-01144-5,Commented R codes used to generate CSI are available at Figshare 21 .
10.1038/s41597-022-01142-7,Scripts to obtain plots starting from the database are available at the University of Liverpool repository 46 .
10.1038/s41597-021-01109-0,"To make the CMS \({\rm{H}}({\rm{b}}\bar{{\rm{b}}})\) Open Dataset more accessible, we provide notebooks 55 from the course “Particle Physics and Machine Learning” at University of California San Diego. The course notebooks provide a guide for the use of the ROOT format dataset. We also have released a second set of interactive Jupyter Notebooks on GitHub 56 , where we visualize feature distributions and feature correlations, and provide machine learning examples on low-level features in this dataset. The Jupyter notebooks that we released show how the HDF5 -formatted data can be accessed."
10.1038/s41597-022-01147-2,"In line with reproducible research philosophy, all codes used in this paper are publicly available and can be accessed at https://github.com/N-Nieto/Inner_Speech_Dataset . The stimulation protocol and the auxiliary MatLab functions are also available. The code was run in PC1, and shows the stimulation protocol to the participants while sending the event information to PC2, via parallel port. The processing Python scripts are also available. The repository contains all the auxiliary functions to facilitate the load, use and processing of the data, as described above. By changing a few parameters in the main processing script, a completely different process can be obtained, allowing any interested user to easily build his/her own processing code. Additionally, all scripts for generating the Time-Frequency Representations and the plots here presented, are also available."
10.1038/s41597-022-01153-4,All versions of third-party software and scripts used in this study are described and referenced accordingly in the Methods sub-sections for ease of access and reproducibility.
10.1038/s41597-022-01129-4,"Most of the data analyses were completed by standard bioinformatic tools running on the Linux system. The version and code/parameters of the main software tools are described below. (1) BWA-mem (v0.7.15); code for mapping reads : bwa mem -t 1 -M -R “@RG\tID:${SAMPLE}\tSM:${SAMPLE}\tPL:Illumina\tLB:${SAMPLE}\tPU:unkn-0.0” ${REF} ${READS_1} ${READS_2} > ${SAMPLE}.sam (2) Picard (2.9.2): code for sorting sam file and converting to bam : java -jar picard.jar SortSam I = ${SAMPLE}.sam O = ${SAMPLE}_sorted.bam SORT_ORDER = coordinate TMP_DIR = tmp_${SAMPLE} (3) Picard (2.9.2): code for marking duplicate reads: java -jar picard.jar MarkDuplicates I = ${SAMPLE}_sorted.bam O = ${SAMPLE}_mdup.bam CREATE_INDEX = true M = ${SAMPLE}_mdup_metrics.txt TMP_DIR = tmp_${SAMPLE} MAX_FILE_HANDLES_FOR_READ_ENDS_MAP = 4000 OPTICAL_DUPLICATE_PIXEL_DISTANCE = 2500 (4) GATK (3.8-0): codes for BQSR steps # Analyse patterns of covariation in the sequence dataset java -jar $gatk -T BaseRecalibrator -R ${REF} -I ${SAMPLE}_mdup.bam -knownSites ${KNOWNVAR} -o ${SAMPLE}_recal_data.table # Analyse covariation post-recalibration java -jar $gatk -T BaseRecalibrator -R ${REF} -I ${SAMPLE}_mdup.bam -knownSites ${KNOWNVAR} -BQSR ${SAMPLE}_recal_data.table -o ${SAMPLE}_post_recal_data.table # Generate before/after plots # Requires R packages gsalib, reshape and ggplot2 installed java -jar $gatk -T AnalyzeCovariates -R ${REF} -before ${SAMPLE}_recal_data.table -after ${SAMPLE}_post_recal_data.table -plots ${SAMPLE}_recalibration_plots.pdf # Apply the recalibration to your sequence data java -jar $gatk -T PrintReads -R ${REF} -I ${SAMPLE}_mdup.bam - BQSR ${SAMPLE}_recal_data.table -o ${SAMPLE}_recal.bam (5) GATK (3.8-0) Variant calling in GVCF mode by HaplotypeCaller java -jar $gatk -T HaplotypeCaller -R ${REF} -I ${SAMPLE}_recal.bam -o ${SAMPLE}.g.vcf.gz -ERC GVCF (6) GATK (3.8-0) Joint genotyping of a cohort of samples # used the --variant option as many times as needed to specify the gvcf files to be used for joint genotyping (the code below shows three samples only as example). java -Xmx4g -jar $gatk -T GenotypeGVCFs -R ${REF} --variant SAMPLE1.g.vcf.gz --variant SAMPLE2.g.vcf.gz --variant SAMPLE3.g.vcf.gz -o ${COHORT}.vcf.gz -D ${KNOWNVAR} (7) GATK (3.8-0) VQSR steps # Variant recalibration step java -Xmx4g -jar $gatk -T VariantRecalibrator -R ${REF} -input ${COHORT}.vcf.gz -resource:GRCg6a_dbsnp,known = true,training = false,truth = false,prior = 2.0 ${KNOWNVAR} -resource:GRCg6a_validated_snp,known = false,training = true,truth = true,prior = 12 ${TRUEVAR} -an DP -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -mode SNP -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 -recalFile ${COHORT}.SNPs.recal.gz -tranchesFile ${COHORT}.SNPs.tranches -rscriptFile ${COHORT}_recalSNPS.plots.R # Apply Recalibration java -Xmx4g -jar $gatk -T ApplyRecalibration -R ${REF} -input ${COHORT}.vcf.gz -mode SNP --ts_filter_level 99.0 -recalFile ${COHORT}.SNPs.recal.gz -tranchesFile ${COHORT}.SNPs.tranches -o ${COHORT}_recalSNPs_rawIndel.vcf.gz"
10.1038/s41597-022-01151-6,No custom code was used to generate or process the data.
10.1038/s41597-022-01155-2,"The datasets were extracted via queries directly in MySQL Workbench and through the free software environment R 31 , using: the dbConnect function from the RMySQL package, a Database Interface and ‘MySQL’ Driver for R 32 . The maps in Fig. 2 were created using several R packages like tidyverse 39 for data handling, rgdal 40 a package providing bindings to the “Geospatial Data Abstraction Library”, sf package, a standardized way of encoding spatial vector data in R 41 and the package ggplot 42 for graphical visualization. The package geosphere 43 was also used for the calculation of haul midpoints, starting from the coordinates of gear deploying and retrieving of each fishing operation. A few examples of these operations (MySQL and R) and of the R code for the creation of the maps are available among the shared datasets attached to this work as Supplementary Information ."
10.1038/s41597-022-01157-0,"The custom-made MATLAB toolbox for loading, viewing and processing of ndpi & ndpa files and for estimating the total tissue area and average cell density of a WSI can be accessed at: https://github.com/tovaroe/WSI_histology ."
10.1038/s41597-022-01156-1,"The code implementation was done in Python3. The scripts to perform the download, restructuring, validation and visualization of the data are available at the ISFH GitHub repository ( https://github.com/ISFH/WPuQ )."
10.1038/s41597-022-01150-7,"Script files were created using the R statistical programming to estimate missing values of ΔT l , ΔT l and ΔPr and draw Figs. 2 – 6 which are available in the figshare repository 24 ."
10.1038/s41597-022-01140-9,"Data, metadata descriptions, and R code are available in the Figshare data repository for public access (accessible from: https://doi.org/10.6084/m9.figshare.15025296 ) 50 ."
10.1038/s41597-022-01158-z,"Python-language-based codes for converting the JSON file to the original CHG file generated by VASP of each case are provided, which can be downloaded through Figshare 39 and our Carolina Materials Database ( http://www.carolinamatdb.org/ )."
10.1038/s41597-022-01143-6,"To develop and execute the analysis code, we used Python 2.7. The code is released as a single version, which was used for both data collection and analysis. All Python dependencies with their versions are captured in a text file requirements.txt at the root directory. All code files can be freely accessed on on GitHub at https://github.com/a trisovic/dataverse-r-study. The code is released under MIT license."
10.1038/s41597-022-01160-5,"Downloads of the Atomic Simulation Environment 29 (v. 3.21.1) and NumPy 43 (v. 1.20.1) packages for Python are freely available. Amsterdam Modeling Suite 31 (v. 2020.203, r92091) is a commercial software, for which a free trial may be requested at www.scm.com ."
10.1038/s41597-022-01162-3,"R functions and databases used for generating the sample metadata are specified in the method section. A customized pipeline used for quality filtering of the raw sequence data obtained from HTS, delineation into ASVs and taxonomic classification of ASVs as described in Herzog et al . 30 is available as a “ITS2.bash” file from the Zenodo repository 28 ."
10.1038/s41597-022-01167-y,No specific code was generated for analysis of these data.
10.1038/s41597-022-01166-z,"The codes used to process the along track measurements and for the Optimal Interpolation (OI) are not available for public use as Collecte Localisation Satellite (CLS) and the Centre National des Etudes Spatiales (CNES) are the proprietary owners. However, these codes are extensively described in 55 and 8 . The python code used for the comparison of the product with external sources of data are available at https://github.com/MatthisAuger/SO_SLA ."
10.1038/s41597-022-01177-w,"All data can be easily processed with publicly available tools such as json and pymatgen 35 . An example usage is provided with the data. The dataset was generated with VASP, the bash and python scripts to generate input files or manage the output files can be downloaded from github repository: https://github.com/hyllios/utils/tree/main/ht_pd_scan ."
10.1038/s41597-022-01152-5,The bioinformatic code used to process scat DNA sequence data are available in the “Example Bioinformatic Files.zip” folder on figshare along with example data 35 . Sample demultiplexing and sequence taxonomy assignment steps were performed using MacQIIME (version: 1.9.1-20150604) and the code used for these steps is in folder “1_Qiime_processing”. Example output files from MacQIIME processing steps containing 16S and COI sequence taxonomy assignments are in the folder “2_Qiime_output_R_input”. Those files were then further processed in R (multiple versions) to generate DNA sequence percentages using the code contained in folder “3_R_processing”. Note: Demultiplexing requires QIIME mapping files for each sample plate which are stored in the figshare folder 35 “QIIME_Mapping_Files” for sequence data stored the NCBI SRA database 36 . A complete list of the bioinformatic functions in MacQIIME is available online http://qiime.org/scripts/ .
10.1038/s41597-022-01165-0,The codes used in this article were deposited in https://github.com/LuChenLab/Neuron .
10.1038/s41597-022-01168-x,"The procedure of spatial sampling is executed in the ArcGIS Pro platform. The code of the deep learning model is available at https://github.com/ChanceQZ/RoofTopSegmatation . The program is described by Python3, packages of which are Pytroch, Numpy, and OpenCV mainly."
10.1038/s41597-022-01169-w,The scripts used to generate the R-CDL dataset are available in this GitHub repository: https://github.com/llin-csiss/RCDL .
10.1038/s41597-022-01154-3,"Python and C++ codes used to perform data curation, training workflow, and performance evaluation shown in this manuscript are publicly available on GitHub at https://github.com/Mengjintao/SolCuration or one can cite our code by 43 ."
10.1038/s41597-022-01159-y,"The code used to generate this dataset is publicly available on a GitHub repository ( https://github.com/elpidakon/CRESCENDDI ). This code was developed and tested using: OHDSI standard vocabulary version v5.0 18-JAN-19 ( https://athena.ohdsi.org/ ), which includes: RxNorm version 20181203, RxNorm Extension version 2019-01-17, and MedDRA version19.1. Database storage and operations were enabled using PostgreSQL 9.3. Drug and event mapping steps were performed using OHDSI Usagi version 1.2.7 ( https://github.com/OHDSI/usagi ). Web data extraction was performed using Python 3.6. Scores for the SDAs were calculated using Python 3.6 (Omega), SAS (delta_add) and R version 4.0.0 (IntSS, PRR, EBGM and BCPNN); AUC scores and CI estimates were calculated using MATLAB R2020b ( perfcurve function)."
10.1038/s41597-022-01183-y,The relevant code data used for calculation and analysis in this paper can be obtained from the supplementary information .
10.1038/s41597-022-01195-8,"The supporting data associated with this article (provided in. csv format), the list of articles proving data (presented in.ris and.bib formats), and the code used (in R language) to describe the data and generate maps from it are available from GitHub: https://github.com/fonturbel-lab/pollination_catalogue ( https://doi.org/10.5281/zenodo.4445125 ) 32 . A mirror repository is available from figshare ( https://doi.org/10.6084/m9.figshare.14743881 ) 31 , which is automatically updated from GitHub. All information is provided under a Creative Commons (CC0) license."
10.1038/s41597-022-01185-w,"All the data generated at the MP2/cc-pVTZ and the CCSD(T)/cc-pVQZ levels of theory were performed with the CFOUR software package. TBE and other data were obtained using various software packages (MOLPRO, CFOUR, MRCC) as described in the Methods section."
10.1038/s41597-022-01192-x,"The programs used to generate all the results were Python, Google Earth Engine (GEE) and ESRI ArcGIS (Pro 2.5). The scripts of data collection and preprocessing on GEE can be accessed on GitHub ( https://github.com/terryyangwhu/BH_China.git ). Furthermore, we have made the Si-GPR model’s source code publicly accessible on GitHub ( https://github.com/terryyangwhu/BH_China.git )."
10.1038/s41597-022-01181-0,"All scripts used to mine the extant literature corresponding to the CoRE MOF 2019 database are commented and are available on a public GitHub repository at https://github.com/hjkgrp/text_mining_tools . Manuscript copyrights are retained by the publishers, preventing the complete dissemination of full-length articles, but the mined data is provided with an open source CC-BY license and is available on Zenodo 69 (see also Data Records). The MOFSimplify website is located at https://mofsimplify.mit.edu . The code backend for the MOFSimplify website is available in a public GitHub repository at https://github.com/hjkgrp/MOFSimplify . The repository contains a user manual for the website."
10.1038/s41597-022-01196-7,The code used to perform all steps described here and shown in Fig. 1 is contained within a.zip archive named “Code.zip”. The code can be accessed via the Zenodo data repository 23 ( https://doi.org/10.5281/zenodo.5021361 ).
10.1038/s41597-022-01184-x,"All source code used in this analysis is available on Open Science Framework (OSF) 39 . This code was run using R 4.0.5, ArcGIS Pro 2.8 (with Python 3), and ArcGIS Desktop 10.7."
10.1038/s41597-022-01189-6,"The interferometric processing of Sentinel-1 data was performed with processing scripts using the commercial software developed by GAMMA Remote Sensing (v20201216). Usage of the software is subject to licensing ( https://www.gamma-rs.ch/software ). A open code repository for the core code developed for processing, data access, and visualization is available at the Earth Big Data LLC openSAR github repository 45 . Relevant code for this data set is available in the global_coherence folders in the code and notebooks sections at this repository. A python tool to mosaic and subset data from the tiles is available at: https://github.com/EarthBigData/openSAR/blob/master/code/global_coherence/global_coherence_mosaic_tool.py ."
10.1038/s41597-022-01163-2,All scripts used to process the data 52 are publicly available at https://www.github.com/psychoinformatics-de/fairly-big-processing-workflow ( https://doi.org/10.5281/zenodo.6019782 ). The recipe used to build the CAT Singularity container 53 ( https://doi.org/10.5281/zenodo.6021002 ) is publicly available at https://www.github.com/m-wierzba/cat-container .
10.1038/s41597-022-01180-1,"All scripts used for the processing of data and our preliminary analyses are available alongside the data at https://gin.g-node.org/NIN/V1_V4_1024_electrode_resting_state_data , in the ‘code’ folder. All experiment control scripts are listed in Table 2 and the data processing scripts are listed in Online-only Table 1 . Matlab version R2015b, Python version 3.7 and Snakemake version 5.8.1 were used. The only Matlab dependency was the NPMK toolbox (version 5.0, Blackrock Microsystems), a copy of which is included in the data repository. Direct Python dependencies include neo 0.9.0, nixio 1.5.0 elephant 0.10.0, odml 1.4.5 and odmltables 1.0. A full list of all Python dependencies and the specific versions used can be found in the Python environment specifications, along with the Python scripts (Online-only Table 1 )."
10.1038/s41597-022-01174-z,The code used to validate our data is available at https://github.com/alibatti/LaMobiliereDatasetCode in the form of Python scripts.
10.1038/s41597-022-01170-3,"Dataset preparation and validation was performed using MATLAB (R2018b). Computer programs are included in the dataset. They can be used for browsing through the dataset and plotting some of the results. Although MATLAB is a proprietary language, the.m files can be read with a text viewer."
10.1038/s41597-022-01190-z,"The source code to generate the dataset in this study is available on Zenodo 30 . The code is written in Ruby. Installing ParsCit ( https://github.com/knmnyn/ParsCit ) is required to run this program. This code is applicable to any language version of Wikipedia. We attached sample data of the revisions on the pages “Fair trade” and “Solomon Islands” as well as identifiers referenced on these pages of English Wikipedia to enable anyone to generate a part of the dataset. To generate the full dataset, the following preprocessing is needed: (1) download the dump data of Wikipedia and apply Step 1-1 to Step 1-5 described in building the basic dataset section. (2) obtain all revisions of the pages derived from (1) by applying Step 2-1 described in building the basic dataset section, and converting them to JSON lines format. After this preprocessing, the codes corresponding to Step 2-2 above are available by just running “main.sh”. If the type of each editor is needed, Step 2-3 above should be performed."
10.1038/s41597-022-01176-x,"All code used for pre-processing mRNA and miRNA sequencing data is publicly available on GitHub ( https://github.com/OncoRNALab/exRNAQC/blob/main/Preprocessing ) 61 . For circRNA detection, the CircExplorer2 manual was followed as described in the Methods section. Further downstream analyses (differential expression, GSEA, fusion gene detection, and variant analysis) was done following the guidelines of the different R packages and software tools as described (with the used versions) in the Methods section."
10.1038/s41597-022-01193-w,"Code used in this paper is available in the following locations: The github repository, tmilliman/sir_to_netcdf, ( https://github.com/tmilliman/sir_to_netcdf ) includes the python and bash scripts that: 1. Convert NASA SCP backscatter SIR images to NetCDF files of seasonal mean and standard deviation backscatter at 0.05° (lat/lon) for all land, and with an urban mask. Separate code for each sensor (ERS, QuikSCAT, ASCAT). A separate github repository, tmilliman/urban_backscatter, ( https://github.com/tmilliman/urban_backscatter ) includes scripts that: 2. Extract from NetCDF files in #1 the backscatter data for 11 × 11 grids around a lat-lon location and create CSV files from this data. 3. Create using the scripts in #2 CSV files with the mean backscatter for city-level grids around city centers and for also for the invariant regions shown in Fig. 3 . The github repository sfrolking/urban_backscatter_ERS_QSCAT_ASCAT ( https://github.com/sfrolking/urban_backscatter_ERS_QSCAT_ASCAT ) includes R scripts that: 4. Evaluate seasonal backscatter for invariant evergreen tropical forest sites and construct Fig. 2 . 5. use the CSV files from #3 above and correlate 2015 summer mean ASCAT backscatter with building volume data and construct Fig. 3 . 6. use the CSV files from #3 above to generate annual mean summer backscatter time series plots for sample cities and construct Figs. 4 – 6 ."
10.1038/s41597-022-01178-9,No custom code was used in assembling the dataset published herein. All steps of analysis and data processing are described in Methods as well as in Meinrenken et al . 18 .
10.1038/s41597-022-01182-z,"Microsoft® Excel was used to enter, store and quality check the collected data. Json file was used only to download WD data from the dashboard and was provided by Everion®. This file is not needed to access, use or process the here provided data sets. The R script used for quality checks of the provided data is available on Figshare 25 ."
10.1038/s41597-022-01161-4,The code used to normalize and analyze transcriptomic and metabolomic data similarly as disclosed in the Usage Notes section have been deposited in Github repository ( https://github.com/fle1/Scientific_Data ).
10.1038/s41597-022-01173-0,The code used to perform technical validation on the (i)BIDS dataset is available at https://github.com/UMCU-RIBS/ieeg-fmri-dataset-validation . We also provide a set of utility scripts to help new users get started with processing and visualizing the data ( https://github.com/UMCU-RIBS/ieeg-fmri-dataset-quickstart ).
10.1038/s41597-022-01191-y,"Along with the pooled data and metadata files as of mid-December 2021, users can find in figshare 6 an R script to retrieve the latest version of the pooled data and metadata files and update them. This script can also be used to merge the pooled data and metadata files."
10.1038/s41597-022-01179-8,"The Skew DB is produced using the Antonie DNA processing software ( https://github.com/berthubert/antonie2 ), which is open source. In addition the pipeline is fully automated and reproducible, including the retrieval of sequences, annotations and taxonomic data from the NCBI website. The software has also been deposited with Zenodo 23 , 24 . A GitHub repository is available for this article on https://github.com/berthubert/skewdb-articles , which includes this reproducible pipeline, plus a script that regenerates all the graphs and numerical claims from this paper."
10.1038/s41597-022-01222-8,"The database is available online within the OSF Repository, including the different imaging datasets, segmentation files, as well as metadata comprising patient characteristics and details on the available pulse sequences and used MRI systems per patient (Identifier: https://doi.org/10.17605/OSF.IO/QX5RT ) 52 . We additionally provide Python scripts to read and visualize the data by utilizing open-source image analysis libraries – SimpleITK ( https://simpleitk.org/ ) and NiBabel ( https://nipy.org/nibabel/ ), available within the repository wiki page."
10.1038/s41597-022-01203-x,"Python code is available at figshare within the same collection as the dataset: 14 https://doi.org/10.6084/m9.figshare.c.5645578.v1 . The code is available for preprocessing, visualization and attribute extraction."
10.1038/s41597-022-01236-2,"The R code files provided on Figshare contain complete the analyses of the ScBrAtlas study 9 (Table 2 ). Code files are also available from the GitHub repository https://github.com/yunshun/HumanBreast10X . All the bioinformatics analyses were performed in R 3.6.1 on x86_64-pc-linux-gnu (64-bit) platform, running under CentOS Linux 7. The following software packages were used for the analyses: Seurat v3.1.1, limma v3.40.6, edgeR v3.26.8, pheatmap v1.0.12, ggplot2 v3.2.1, org.Hs.eg.db v3.8.2 and vcd v1.4-5."
10.1038/s41597-022-01207-7,"The 3DSIM reconstructions, image deconvolutions and associated image registrations were completed using the proprietary software softWoRx 7.0.0 following the microscope."
10.1038/s41597-022-01202-y,"For the technical validation, we used code that is publicly available without restrictions (see methods section). In detail following functions/scripts were used: jqrs.m - QRS detector based on the Pan-Tompkins method ( www.physionet.org/content/pcst ) ada_f.m – adaptive filtering of heart beat time series ( www.tocsy.pik-potsdam.de/ada.php ) runrmssd.m – RMSSD estimation ( www.physionet.org/content/pcst ) prsa.m – deceleration capacity estimation ( www.physionet.org/content/pcst ) wfdb2mat.m – convertion of waveform database format to MATLAB file ( www.physionet.org/content/wfdb-matlab/0.10.0/mcode/wfdb2mat.m )."
10.1038/s41597-022-01240-6,"MatLab (R2017b), STATA (16), and ArcGIS (10.5) are the major applications used to obtain the energy consumption and efficiency data. The code for PSO-BP matching the relationship between the two sets of satellite nighttime light data and the inversion of city and county energy consumption is provided in the Appendix. Codes and datasets for the DEA method, which measures energy efficiency in 189 cities, are also provided in the Appendix."
10.1038/s41597-022-01230-8,No code was used in this study.
10.1038/s41597-022-01188-7,Our codes for data acquisition and data processing are provided on the following Github repository: https://github.com/sselab2021/FMS_human_3d-skeleton_by_Azure-Kinect .
10.1038/s41597-022-01213-9,"The Android project related to the mobile application used for the data acquisition from all sensors is available at https://github.com/impires/DataAcquisitionADL . In addition, the Java project used for the automatic measurement of the parameters of related to the different sensors is available at https://github.com/impires/FeatureExtractionMotionlessActivities . The code for preliminary data exploration and analysis is available as a Jupyter notebook at https://github.com/impires/JupyterNotebooksMotionlessActivities . The Jupyter notebook shows how the data can be loaded, and how the initial data exploration can be performed showing some charts and descriptive statistics. This will be more than sufficient to bootstrap future uses of the dataset."
10.1038/s41597-022-01225-5,No special code was used for analysis of the current dataset. All of the analyses were done with the following open access programs: FastQC version 0.11.9. ( https://github.com/s-andrews/FastQC ). Trimmomatic-0.39 program. ( http://www.usadellab.org/cms/?page=trimmomatic ). Cell Ranger version 2.1.1 ( https://github.com/10XGenomics/cellranger ). Seurat ( https://satijalab.org/seurat ). The Loupe Cell Browser v5.0.0 ( https://www.10xgenomics.com/products/loupe-browser )
10.1038/s41597-022-01210-y,"Scripts to load the datasets into Matlab and run the described fNIRS data quality assessment routine are available at https://github.com/borjablanco/RS_4months . The rest of the Matlab functions used to analyse this dataset come from fNIRS data analysis package Homer2 ( https://homer-fnirs.org/ ), and its more recent version Homer3 that can be found in ( http://openfnirs.org/software/homer/ ). Interested readers are also directed to the snirf repository ( https://github.com/fNIRS/snirf ), which provides information for working with fNIRS data structured in this format. Information on how to convert a directory of fNIRS files to a correctly formatted BIDS dataset can be found in ( https://github.com/rob-luke/fnirs-apps-sourcedata2bids )."
10.1038/s41597-022-01212-w,Matlab computer software used to produce GO-SHIP Easy Ocean product is available from the GO-SHIP Easy Ocean GitHub repository ( https://github.com/kkats/GO-SHIP-Easy-Ocean ). The present paper is based on version 1.4 https://doi.org/10.5281/zenodo.5527383 ). A user can substitute their preferred stations and gridding method by implementing their workflow in MATLAB and re-running the batch file. The guidelines to reproduce the work are found in the Procedure.md file in the software repository.
10.1038/s41597-022-01197-6,The R code to generate analyses and figures is available in GitHub ( https://github.com/StefanoMammola/Analysis_Global-Spider-News-Database ).
10.1038/s41597-022-01249-x,The country*waves files of the CAUCP dataset have been merged using STATA16. The Code is available on the figshare repository.
10.1038/s41597-022-01204-w,"The GCAM v5.2 and FLUS models can be freely downloaded in https://github.com/JGCRI/gcam-core/releases and http://www.geosimulation.cn/FLUS.html , respectively."
10.1038/s41597-022-01231-7,"ME-ICA uses AFNI and python, both of which are open-source software. ME-ICA processing code is available at https://github.com/ME-ICA/me-ica . As the code base is unmaintained, readers are also directed to https://tedana.readthedocs.io/ for additional multi-echo de-noising options."
10.1038/s41597-022-01233-5,"All data and code is available without restrictions under the terms of a Creative Commons Zero (CC0) waiver ( https://creativecommons.org/share-your-work/public-domain/cc0/ ). R code for retrieving and filtering data from POWO and GBIF, and for generating and evaluating Maxent models is available on GitHub ( https://github.com/jannebor/plant_range_estimates ). Any further requests can be directed to the corresponding author."
10.1038/s41597-022-01198-5,Data compilation and aggregation for all indicators was done using R. The complete dataset and R aggregation scripts for the conflict databases are available on the Harvard Dataverse 42 at: https://doi.org/10.7910/DVN/LDI5TK .
10.1038/s41597-022-01200-0,"Software applications used in the study are based on public open sources, and the codes used in this study are available at https://github.com/xdc-lab/ARGazeCodes . All the codes used in the preceding data processing and technical validation are available at https://github.com/xdc-lab/ARGazeCodes ."
10.1038/s41597-022-01226-4,"All custom code is freely available in several locations, depending on type. Code to work with the DMCC55B data records, including to reproduce the validation analyses, is in the DMCC55B Dataset Description component 17 of the DMCC OSF site 15 . Code elsewhere is not specific to the DMCC55B dataset, but used in the main DMCC project. Eprime task presentation scripts can be downloaded after completing the online form 25 . Processing and summary scripts used after data collection are in the dualmechanisms GitHub 36 or Docker Hub 82 repositories, depending on type."
10.1038/s41597-022-01244-2,"For the calcium trace extraction, suite2p v0.9.3 was used. Suite2p input parameters are specified in the form of ops.npy files. Technical validation scripts are available in “validation” folder in 10 . The analysis code, summarized in the interactive Fig. 5 , is available in the repositories https://github.com/OpatzLab/HanganuOpatzToolbox (v 1.0.0 Scientific Data release) and https://github.com/mchini/Yang_Chini_et_al (v 1.0.0 Scientific Data release). In addition to the code, Jupyter notebooks are provided to illustrate the calcium imaging validation procedure ( Validation_stability.ipynb ), and basic analysis approach at the level of individual neurons ( analysis_pipeline_for_concatenated_recordings.ipynb )."
10.1038/s41597-022-01241-5,"Software used in the generation of this project is third-party software as described in the Data Records section, i.e., Spectronaut and DIALib-QC."
10.1038/s41597-022-01187-8,"Data are generated using PYTHIA 8.240 40 , setting the collision energy at 13 TeV. Unless otherwise specified, all parameters were fixed to their default values. We set the beam parameters to produce proton-proton collisions at 13 TeV Beams:idA = 2212 ! first beam, p = 2212, pbar = −2212 \\ Beams:idB = 2212 ! second beam, p = 2212, pbar = −2212 \\ Beams:eCM = 13000 .! CM energy of collision \\ while the rest of the card is configured specifically for each process, as indicated in the PYTHIA manual 40 . For example, W → ℓ v decays are generated with the settings: WeakSingleBoson:ffbar2W = on! switch on W production mode 24::onMode = off! switch off any W decay 24:onIfAny = 11 13 15! switch on W- > lv decays. The signal-specific parameters for the four benchmark signal models are set as follows: • For A → 4ℓ: set the Higgs mass to 50 GeV, force the decay to Z * Z * final states, and force Z * → ℓℓ decays (ℓ = e , μ , τ ). • For LQ → bτ : set the LQ mass to 80 GeV and force its decays to a b quark and a τ lepton. • For h 0 → ττ : set the Higgs boson mass to 60 GeV and switch off any decay mode other than ττ . • For h + → τv : set the charged Higgs boson mass to 60 GeV and switch off any decay mode other than τv . We emulate the detector response with DELPHES 3.3.2 41 , using the default Phase-II CMS detector card. For simplicity, we avoid degrading the detector resolution to account for the coarser nature of L1T event reconstruction. This simplification does not affect the aim of the study, which is not focused on assessing the absolute physics performance but instead on comparing different algorithms and their resource consumption. We include the effect of parasitic proton collisions, sampling the number of collisions according to a Poisson distribution centered at 20. The Delphes outcome is processed by a custom Python macro to store the aforementioned physics content on HDF5 files, which are then published."
10.1038/s41597-022-01245-1,All the code used to generate the database is open-source and available at https://github.com/covid19datahub/COVID19 .
10.1038/s41597-022-01238-0,"A Github repository is publicly available ( https://github.com/omar-mohamed/CDD-CESM-Dataset ) which contains helper scripts to make training a DL model on the dataset easier like reading the annotations, pre-processing the images by resizing and normalizing, training different existing models, augmenting the images while training, and evaluating the different models and plotting the segmentation results. The scripts were written using Python 3.6 with Tensorflow 2.3 for the training process, and OpenCV 4.1 and Pillow 6.1 for the image processing."
10.1038/s41597-022-01217-5,The reported data were generated from experiments. No custom code was used to generate or process the data.
10.1038/s41597-022-01248-y,"In order to reproduce all the outputs provided in the PDF reports for all the datasets of the database: once downloaded, raw data can be uploaded in MOSAIC bioacc to perform the statistical analysis on-line. In addition, for each dataset, the corresponding R code can be retrieved from the download section of MOSAIC bioacc , if using the R software is preferred."
10.1038/s41597-022-01194-9,A code to extract the PDRMIP data is available at the storage of the data (see usage section) where all the PDRMIP data are freely available. The PDRMIP data are available through the World Data Center for Climate (WDCC) https://www.dkrz.de/up/systems/wdcc with https://doi.org/10.26050/WDCC/PDRMIP_2012-2021 29 .
10.1038/s41597-022-01237-1,"The bioinformatics tools used by the dataset are described under the Methods section. The parameters are as follows. BLAT(v. 36 × 5): -out = blast8 reference.fasta input.fasta out.blast8. MAFFT(v 7.453): --thread −16 --quiet in.fasta > out.ma. ANNOVAR(2019 Oct 24 version): convert2annovar.pl -format vcf4 -allsample -withfreq input.vcf > output.avinput, annotate_variation.pl -geneanno -dbtype refGene -buildver NC_045512v2 -out output.avoutput input.avinput annovar_ref/. The code used for data analysis are available on GitHub https://github.com/BioMedBigDataCenter/KGCoV/ ."
10.1038/s41597-022-01242-4,"The provided codes are modified versions of those published earlier 19 , 27 , 28 , 31 and custom in-house scripts. Modification and further distribution fall under the restrictions described by the authors. The codes can be found in https://gin.g-node.org/NeuroGroup_TUNI/Comparative_MEA_dataset/src/master/Codes 33 . MATLAB 2020a (MathWorks) and RStudio version 1.3.959 were used during the preparation of the current publication."
10.1038/s41597-022-01216-6,"MS 2 PIP is open source, licensed under the Apache-2.0 License, and hosted on https://github.com/compomics/ms2pip_c . The Jupyter notebooks used to generate Fig. 3(b,c,e,f) are available through Zenodo, under https://doi.org/10.5281/zenodo.5714380 48 ."
10.1038/s41597-022-01201-z,All calculations of daily SPI are based on the Python language and are available at GitHub: https://github.com/wangqianfeng23/DailySPI . Any updates will also be published on GitHub.
10.1038/s41597-022-01215-7,"The table extraction code is available at https://github.com/olivettigroup/table_extractor . It is written in Python3, and takes in a list of HTML/XML files (supplied by the user) and the corresponding DOIs, and then returns a list of tables extracted from the files as JSON objects."
10.1038/s41597-022-01205-9,The code used for technical validations is publicly available in the SWAQ repository on Github: https://github.com/giuliaulpiani/SWAQ .
10.1038/s41597-022-01186-9,The SQL and PHP code used for producing the exemplary distributions shown in Fig. 4 is provided in the file supplementary_code_for_article.zip and is publicly available 34 .
10.1038/s41597-022-01208-6,"The land simulation in this study was performed by the FLUS model software (GeoSOS-FLUS V2.4), which can be downloaded for free from http://www.geosimulation.cn/FLUS.html . Meanwhile, the tutorial on the operation of this software can be found in the user manual at this URL. The other spatial calculations and analyses in this study were performed by ArcGIS software as described in the Method section. The spatial data used for input are all publicly available online, with sources cited within the manuscript."
10.1038/s41597-022-01253-1,All code used for analysis of the data are freely available at: https://github.com/CXO531/ElectroMap .
10.1038/s41597-022-01172-1,All of the custom code used for the generation and analysis of this dataset is publicly available at the diyiyonghu GitHub repository at https://github.com/diyiyonghu/analysis-code.git .
10.1038/s41597-022-01209-5,"All python codes (python 3.9.6, https://www.python.org ) for creating urbanization level projections are stored in public repository Figshare 31 ."
10.1038/s41597-022-01171-2,"We used containerized versions of fMRIPrep 20.2.1 and MRIQC for data preprocessing and quality control. Example calling scripts for fMRIPrep, jupyter lab notebooks for figure recreation and R code for the example factor analysis are provided at https://github.com/adolphslab/ConteDataRelease . The code to reproduce resting-state and movie analyses are provided at https://github.com/adolphslab/rsDenoise . As outlined in detail in the source, this codebase can easily be adapted to run many different configurations of denoising decisions on the data."
10.1038/s41597-022-01251-3,"For the code analysis presented here, please check: https://github.com/psilantrolab/SUDMEX_CONN"
10.1038/s41597-022-01243-3,All generated code and data are hosted within Figshare repository https://doi.org/10.6084/m9.figshare.14975136.v1 22 .
10.1038/s41597-022-01229-1,The imputation of missing values in the raw 1 H NMR metabolite concentration data can be accessed using the following URL: https://metabolomics.cc.hawaii.edu/software/MetImp/ . The evaluation of each method used can be accessed on GitHub ( https://github.com/WandeRum/MVI-evaluation ) 12 . The R scripts for preprocessing of metabolites profile dataset and visualization are provided in the GitHub repository ( https://github.com/GIST-CSBL/CMTs ).
10.1038/s41597-022-01252-2,"The code used to collect and store the aggregated consumption data is available at https://gitlab.com/alspereira/EMD-SF . This project used the EMD-DF library to create the audio files, which is available https://gitlab.com/alspereira/EMD-DF . The code runs using Java 8 or higher on a Windows machine. The code used to collect the individual appliance consumption is available at https://gitlab.com/mikemx55/Plugwise-2-M-ITI . The code runs using Python 3 on a Ubuntu machine. Finally, the Python 3 code to reproduce the examples presented in this paper is available on the dataset repository at https://osf.io/jcn2q/ 36 ."
10.1038/s41597-022-01227-3,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01260-2,The GEE code and validation samples in this study are available in GitHub at https://github.com/IrisDudu/globalplantations .
10.1038/s41597-022-01206-8,"The most up-to-date data will be available and updated regularly at: https://doi.org/10.5061/dryad.g1jwstqs2 . The code to replicate all the analyses, as well as the raw data finalized as of August 2021 is openly available via: https://github.com/rdmanzanedo/MeadoWatch_SDATA_2022.git . The code to launch the Shiny app data exploration tool is available in the same GitHub folder and as a browser application here: https://explorations.shinyapps.io/MeadoWatch/ ."
10.1038/s41597-022-01235-3,"All the data and code used in this study are available open access from the ATN DAC Data Portal ( https://portal.atn.ioos.us ) and the Research Workspace DataONE member node ( https://search.dataone.org/portals/RW ) as well as at GitHub ( https://bit.ly/3noJCJD ). As we are providing the raw telemetry data and metadata from the platform manufacturer, the code we provide is for data visualization used to make the figures in this manuscript."
10.1038/s41597-022-01256-y,"The R package gmbaR provides a set of R functions to read and work with the GMBA Inventory 2.0 . This package is provided, explained, and continuously developed on Github ( https://github.com/GMBA-biodiversity/gmbaR )."
10.1038/s41597-022-01214-8,All the codes used in this study to construct the dataset were written in the MATLAB language and will be openly available at https://github.com/YuPeiHPU/ReconstructGlobalMODIS-LST.git under GNU Affero General Public License v3.0 after this work is accepted. The code used to implement the DINEOF method is openly shared by Azcarate 60 at https://github.com/aida-alvera/DINEOF.git .
10.1038/s41597-022-01218-4,"The code developed to process the OSM data is publicly available through the following GitHub repository: https://github.com/snirandjan/CISI 59 . The procedure for the developed CI dataset can be simulated using the main script, which is divided into three sections: (1) extraction of CI from OSM files in .PBF format, and reclassification; (2) estimation of amount of CI; and (3) calculation of the CISI. We also provide code for the validation procedure, and for the development of the figures and supplementary files. Detailed information per section and on the applied functions can be found on the repository, README file, and throughout the code."
10.1038/s41597-022-01250-4,Scripts to generate the results as Datalad 77 datasets are available in a G-Node GIN repository ( https://doi.org/10.12751/g-node.7is9s6 78 ).
10.1038/s41597-022-01175-y,No code was used to generate the data.
10.1038/s41597-022-01199-4,"The custom code used to produce the dataset, written in Matlab R2015a, is available within the data repository via https://eprints.soton.ac.uk/434946/ (see CHIROL_CREEK_EXTRACTION_DEMO zip folder and utilisation guide README_Creek_Extraction_Algorithm). Readers can also refer a previously published methodology paper 21 which details the algorithm’s functioning and validation process."
10.1038/s41597-022-01261-1,"QSAR calculations were performed using the ChemAxon’s Marvin suite of programs, version 21.14 63 . For QM calculations we used the Gaussian16 package, revision A.03 65 . The Amber18 package 80 was used for MD simulations and FF generation. We used simple bash scripts to iteratively extract descriptors from outputs and generate AB-DB data-files."
10.1038/s41597-022-01221-9,"The source code of the DHSVM model used for snow simulations can be freely downloaded at https://github.com/pnnl/DHSVM-PNNL . The R programming language was used for developing IDF curves, detecting trend and determining seasonality of annual maximum series, using the following packages: trend 46 , lmom 47 , circular 48 . Source codes that were used to develop and analyze the data are publicly available at https://github.com/Lizzy0Sun/NG-IDF-analysis-code/ ."
10.1038/s41597-022-01255-z,"Scripts used to generate the DICOM images for radiologists can be accessed from (‘ExampleScripts/fastmri-to-dicom.py’) in the open-source GitHub repository. The detailed method used has been specified in the Methods section. More open-source tools for reconstructing the original fastMRI dataset, including standardized evaluation criteria, standardized code, and PyTorch data loaders can be found in the fastMRI GitHub repository ( https://github.com/facebookresearch/fastMRI )."
10.1038/s41597-022-01234-4,No code was used to generate the data presented in this data paper.
10.1038/s41597-022-01258-w,The version and parameter of all bioinformatics tools used in this work are described in the Methods section.
10.1038/s41597-022-01257-x,"The Python code for detecting and filling the data gaps, as well as for modifying outlier values, is available at the dataset’s GitHub page: https://github.com/LBNL-ETA/Data-Cleaning ."
10.1038/s41597-022-01262-0,The code used for the technical validation is publicly available at https://github.com/Emognition/Emognition-wearable-dataset-2020 . The code was developed in Python 3.7. The repository contains several Jupyter Notebooks with data manipulations and visualizations. All required packages are listed in requirements.txt file. The repository may be used as a starting point for further data analyses. It allows you to easily load and preview the Emognition dataset.
10.1038/s41597-022-01224-6,"The code for the IMOS mooring toolbox used to quality control IMOS data can be accessed here: https://github.com/aodn/imos-toolbox . The code used to aggregate the mooring time-series data is available here: https://github.com/aodn/python-aodntools/tree/master/aodntools/timeseries_products . MATLAB, Python, and R tutorials have been created to help users download, load, plot, and export data (as CSV files) contained in the products. These are publicly available under a Creative Commons Attribution 4.0 International license (CC BY 4.0) on line at figshare 46 ."
10.1038/s41597-022-01254-0,"The code used for the cohort analysis is available on GitHub ( https://github.com/SciEcon/UTXO ). The GitHub repository is also archived by Zenodo 35 , with the code available in Python and written in Google Colab Notebook with Markdown. first release created on Github: 22 Apr 2021; license: GPL-3.0 License"
10.1038/s41597-022-01282-w,"Algorithms for data processing were coded with Matlab 2018b, and the flow surface velocities were computed with the open-source software Fudaa-LSPIV (version 1.7.3). For further details, see https://forge.irstea.fr/projects/fudaa-lspiv ."
10.1038/s41597-022-01228-2,"The IFS forecast model and the Meteorological Archival and Retrieval System (MARS) software are not available for public use as the ECMWF Member States are the proprietary owners. However, the CHE global nature run dataset and the MARS data extraction features are freely available through ECMWF API ( https://www.ecmwf.int/en/forecasts/access-forecasts/ecmwf-web-api ) following a registration step ( https://apps.ecmwf.int/registration/ ). The data can be accessed using python ( https://www.python.org ). The commands and steps required are detailed in the Supplementary Information file 1 (S 2) ."
10.1038/s41597-022-01266-w,"Data was analysed with CrystFEL 0.9.1. The CrystFEL 0.9.1 software suite is a free open source software available under the GNU Public License version 3 and can be downloaded from http://www.desy.de/twhite/crystfel/ . The AGIPD data was calibrated using the EuXFEL calibration pipeline, release 3.0.0-beta 29 . The raw data and calibration constants are also available for development of calibration algorithms."
10.1038/s41597-022-01223-7,"The programming scripts written (in MATLAB) to load in the data from across all the available spreadsheets, and output combined time-series of high and low water, for each site, are available from https://github.com/ivanhaigh/Thames-Sea-Level-Data ."
10.1038/s41597-022-01232-6,"The databases were prepared using the Fanosearch software library ( https://bitbucket.org/fanosearch/magma-core , commit 1ec4c69), which is freely available under a CC0 license. The commit hash in that reference records the precise version of the software used. Table 4 describes intrinsics (i.e. functions in the computational algebra system Magma 42 ) provided by that library that can be used to rebuild the database, or to perform the consistency checks described in the ‘Technical validation’ section above. Lairez’s original implementation of his generalised Griffiths–Dwork algorithm is available from GitHub ( https://github.com/lairez/periods ) under a CeCILL license."
10.1038/s41597-022-01273-x,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01263-z,All the code to create the dataset is available at github.com/GoogleCloudPlatform/covid-19-open-data . Jupyter notebooks to reproduce the analyses in this paper are available under the examples folder.
10.1038/s41597-022-01275-9,The annual maps of lakes larger than 10 km 2 from 1991 to 2018 were produced using GEE platform. Key JavaScript code developed for this work are openly shared with the scientific community at figshare repository 44 . GEE should be used to access and edit the code.
10.1038/s41597-022-01219-3,The code written in JAVA for the generation of the BC database and the analysis of the connectome including the database is available at https://neuroviisas.med.uni-rostock.de/neuroviisas.shtml .
10.1038/s41597-022-01270-0,The data file with XLSX format are accessible on ScienceDB. No additional code is used during the calculation of provincial process CO 2 emissions from China cement production. The installation of Microsoft Office or WPS Office is recommended to manage the data and reproduce the study results.
10.1038/s41597-022-01265-x,The FAIR Genomes code is available on GitHub at https://github.com/fairgenomes as Free and Open Source Software. The FAIR Genomes semantic metadata schema release v1.1 as described in this manuscript is available at https://github.com/fairgenomes/fairgenomes-semantic-model/tree/v1.1 . The codebase is also available as an archive at Zenodo 76 . Both code and schema are released under a CC BY 4.0 license.
10.1038/s41597-022-01276-8,MISSING
10.1038/s41597-022-01259-9,The authors do not have code specific to this work to disclose.
10.1038/s41597-022-01279-5,Climate Data Operators from Max Planck Institute of Meteorology was used in the pre-processing of the data. Then functions of the SPEI package in R programming Language were used to prepare the final code. The code files are provided to the journal as Supplementary Information .
10.1038/s41597-022-01271-z,"The formatted data and R code to produce the estimates in this paper are available online in the Figshare collection 19 . The data comprise three files for the input data: the bilateral migrant stock data, the demographic changes data and the population totals. The R code comprises a single R script that a) loads the input data, b) cleans the input data to a common set of countries in each period, c) derives native-born population totals in each country required for the demographic accounting methods and d) estimates migration flows using the six different methods. The estimation functions used were developed for the migest package 30 available on CRAN."
10.1038/s41597-022-01277-7,"Here, the most important information about the used software is given. More information is included in the Supplementary Tables 1 to 4 . For Ile-Ife supersite, the proprietary software EasyFlux DL v. 2.3 provided by Campbell Scientific for acquisition and analysis of the turbulent fluxes data is used ( https://www.campbellsci.com/easyflux-dl ). This software is written and supplied along with CSAT3/LI-7500 system. For Kumasi supersite, all codes produced to quality control and produce the data are stored under https://github.com/barbarabrooks/DACCIWA–Matlab . For example, the Matlab programme calculating the turbulent fluxes is stored under DACCIWA–Matlab/FL1/NC/NC flux estimates/write_NC_variables_FLUX_EST_v2.m. The other codes concern the automatic weather stations, Ceilometer, MRR, radiometer, rapid sondes, sodar, and radiosondes. At the Savè supersite, the turbulent fluxes of the KIT energy balance station were calculated with the TK3.11 software 19 , 53 available under https://zenodo.org/record/20349 . For the UPS energy balance station, the turbulent fluxes have been estimated with the EC method EddyPro® Software (Version 6.2.0) from LI-COR Environmental, available under https://www.licor.com/env/support/EddyPro/topics/whats-new.html . The vertical profiles of the wind deduced from the UHF wind profiler were calculated with a software developed in Laboratoire d’Aérologie 54 . This software has been used to process many previous campaign datasets 21 , 22 , 54 , 55 , 56 . The atmospheric quantities of the microwave radiometer at Savè were obtained with a retrieval algorithm provided by the University of Cologne 23 , 57 , 58 , 59 available under ftp://gop.meteo.uni-koeln.de/pub/loehnert/mwr_data_flow/ . The algorithm was trained on about 13000 radiosonde profiles measured at Abidjan, Ivory Coast, between 1980 and 2014. The cloud base height from ceilometer backscatter was retrieved applying the manufacturer’s algorithm (Manual - Lufft - CHM 15k. https://www.lufft.com/products/cloud-height-snow-depth-sensors-288/ceilometer-chm-15k-nimbus-2300/ , 2017). Cloud radar uses IDL software provided by Metek 60 ."
10.1038/s41597-022-01284-8,The programs used to generate all the results were Python (3.11) and ArcGIS (10.4). Analysis scripts are available on GitHub ( https://github.com/HaoweiGis/humanFootprintMapping/ ).
10.1038/s41597-022-01281-x,"The R code used to generate Figs. 4 , 5 , and 6 can be found in Zenodo 43 ."
10.1038/s41597-022-01272-y,"To allow for an easier usage of our data, a Python Jupyter Notebook is also included with the dataset (file preprocess_dataset.ipynb 24 ). The notebook has been tested with the following packages versions: pandas = 1.3.3, numpy = 1.20.3, pickle = 4.0. The code performs a series of data pre-processing operations, that include: • loading the Pickle file that encodes the dataset as a Pandas DataFrame; • printing some validation results, including the values presented in Table 1 ; • generating a sample machine learning-ready dataset, in the form of a set of Numpy arrays. The code provided significantly contributes to relieve the burden of data pre-processing, which typically absorbs a major part of time in the development and testing of machine learning solutions."
10.1038/s41597-022-01268-8,We used the Zen software (RRID:SCR_013672) to export TIFF images from the original slide scans. Nutil v.403 18 (RRID:SCR_017183) was used for preprocessing of images. QuickNII v2.2 20 was used for spatial registration of images (RRID:SCR_016854).
10.1038/s41597-022-01267-9,The data reported in this article are reported in standard Excel format without special code for manipulation.
10.1038/s41597-022-01239-z,"We provide MATLAB code that serves two purposes: (a) to give examples of how the data can be accessed; and (b) to perform some elementary data validation analyses. The code is available along with the data in the repository. We briefly summarize the MATLAB scripts that we provide along with the data. h5_extract. h5_extract() is a function that takes as its input the name of an HDF5 file, a cell array of strings that specify the parameters of the experiment that are to be extracted (for this dataset, this should always be event_value, and event_time), and a cell array of strings that specify the events that are to be extracted (typically the list of event names that can be extracted from the HDF5 itself, as demonstrated in the example scripts). The output of the function are two MATLAB structure arrays - event_value and event_time - which correspond to the groups in the HDF5 file of the same name. Each field name of the structure arrays is the name of an event and the content in the field corresponds to the values and times associated with the event, respectively. Once extracted, the data are ready for visualization and analysis. The scripts technical_validation, example_raster_fr, and example_trial_histogram demonstrate the use of the h5_extract() function to read in all events saved in an HDF5 file. technical_validation. The technical_validation script was used to perform all the plausibility checks described in the section “Technical Validation”. The script prints all the statements from that section of the manuscript that contain quantitative information about the data (such as, for example, average firing rate) to the console. example_raster_fr. To demonstrate how individual spike times can be accessed and visualized, the script creates a raster plot of spike trains and a scatter plot of firing rates per trial (for trials lasting at least 500 ms), colored by trial outcome, for one example file. The example file is specified in the first line of the script and can easily be changed by the user. The raster plot also includes the time of reward delivery and the end of each trial. example_trial_histogram. To demonstrate how multiple files can be accessed for population analyses, a histogram of the number of trials across recording sessions is produced for each of the three experiments and color-coded by monkey. example_probe_time. This script contains a function, probe_time(), which extracts the values of all events at a given time point, as well as some additional code that illustrates the use of the function. example_rc_stim_extraction. This script demonstrates how the direction and speed values of the RC stimulus can be extracted and in particular, how those segments that were masked in some of the files (see Methods) can be determined. example_eye_data. To demonstrate how the eye tracking data can be accessed, this function plots the x- and y-position of the monkey’s gaze as well as the size of the right and left pupil for the first n timesteps (where n can be specified by the user, default is 100). example_spatial_mapping_analysis and example_tuning_analysis. To demonstrate how spiking activity can be related to stimulus features, these two functions plot firing rate as a function of probe location in the Spatial Mapping experiment (example_spatial_mapping_analysis) or as a function of motion direction and speed in the Tuning experiment (example_tuning_analysis)."
10.1038/s41597-022-01211-x,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01290-w,"Geospatial processing was primarily conducted using pre-existing tools within ESRI ArcMap 10.5. However, data retrieval, specifically OSM data, was obtained using code within the OSM package of the R programming environment. Technically, the code primarily utilized pre-existing routine function calls supported within the OSM library, specifically determining bounding geographies (in our case, US states) for OSM feature retrieval and using search terms for “keys” (major groups of objects) and “values” (sub-classifications of keys). Keys and values were varied and dependent upon each energy life cycle. Reproducible examples of code are provided in Supplementary File 2 , Tables S2 - 3 ."
10.1038/s41597-022-01288-4,"Tutorials for loading the dataset and code for training 3D-based neural network models are publicly available without restriction ( https://github.com/learningmatter-mit/geom and https://github.com/learningmatter-mit/NeuralForceField ). CREST and xTB are both freely available online ( https://github.com/grimme-lab/crest/releases and https://github.com/grimme-lab/xtb/releases ). CREST version 2.9 was used with xTB version 6.2.3 to generate the initial CREs. CENSO 1.1.2 was used with Orca 5.0.1 87 and xTB 6.4.1 to refine the ensembles. Orca 5.0.2 was used for all single-point calculations. A race condition bug in version 5.0.1 meant that some CENSO energies were clearly incorrect (conformational energies above 1,000 kcal/mol), while some energy calculations failed to converge for reasonable geometries. Therefore, we discarded ensembles with failed energy calculations or conformational energy ranges exceeding 30 kcal/mol at any stage of the optimization. We also performed new single-point calculations on all converged CENSO geometries with Orca 5.0.2; 0.44% of the energies were found to be incorrect and were replaced."
10.1038/s41597-022-01280-y,Code to run our Jupyter Python dashboard and recreate all results in this paper can be found at https://github.com/BruntonUWBio/ajile12-nwb-data . We used Python 3.8.5 and PyNWB 1.4.0. A requirements file listing the Python packages and versions necessary to run the code is provided in our code repository. Our code is publicly available without restriction other than attribution.
10.1038/s41597-022-01287-5,The source code for the HORDB database website has been uploaded to GitHub: https://github.com/CPU-HORDB/HORDB .
10.1038/s41597-022-01283-9,"The dataset and the utility script (utils.py) are available on Zenodo 26 , an open data repository. The python3 script is also available on GitLab ( https://gitlab.com/comari/dax-ti ), in which the users might obtain static (dax-ti-static) or a rolling release version (dax-ti-sid) of the project. The rolling release version will be continuously updated based on external requests. Researchers are encouraged to contribute to the database through GitLab or via e-mail, sharing their published data to expand the dataset."
10.1038/s41597-022-01289-3,All generated sequencing raw reads have been deposited in the NCBI Sequence Read Archive under accession PRJNA751387. The following commands were used to identify the phylogenetic relationship between breeding line strains. <Adapter Trimming: Trimmomatic v0.39> java -jar trimmomatic-0.39.jar PE -threads 12 ILLUMINACLIP:<Adapter Fasta>:2:30:10:2:keepBothReads LEADING:3 TRAILING:20 MINLEN:125<Read Mapping: bwa-mem2 v2.1> bwa-mem2 mem -t 16 <reference_index> <sample_left_pair> <sample_right_pair> | samtools sort –o <sample_name>.bam – <Remove Duplicate: samtools v1.10> samtools rmdup <aligned_bam_file> <Remove_duplicated_bam_file> <Variant Calling: bcftools v1.10.2> bcftools mpileup -Ou –f <reference_file> -s <bam_list_file> | bcftools call -mv -Ov -o calls.vcf <Variant Filtering: Vcftools v0.1.16> vcftools --vcf calls.vcf --remove-indels --recode --max-missing 1.0--min-alleles 2 --max-alleles 2 --minQ 30 <InDel and SV calling: SvABA v1.1.3> svaba run –t <bam_file> -p 12 -L 6 -I –a <sample_name> -G GCF_014905235.1_Bmori_2016v1.0_genomic.fna <SNV annotation: SnpEff v5.0> Java -jar snpEff.jar Mori <SNV.vcf>
10.1038/s41597-022-01278-6,"The ARTwork pipeline, described in the WGS quality control section is publicly available at https://github.com/afelten-Anses/ARtWORK . The employed bioinformatics tools and their versions are specified in Table 5 ."
10.1038/s41597-022-01291-9,"The transformation matrices for time-dependent alignment were calculated using the module ICP in the software package OPALS 62 . The rigid alignment was calculated by minimizing point-to-plane distances using a search radius of 0.5 m for plane fitting and a sampling distance of 0.05 m. Stable parts were extracted with 0.5 m radius at locations of planar surfaces. Python and Matlab scripts are provided with the data, with basic functions to read and write the point cloud data from the LAZ files, to apply the rigid transformation matrices to the point cloud data for time-dependent alignment and georeferencing, and to read out information on scan settings from the metadata files of an epoch."
10.1038/s41597-022-01292-8,"The entire computational code employed in the SS analysis within this work is openly available at the GitHub repository github.com/simcomat/SS_2D_Materials . It is intensely built upon tools and methods from Pymatgen 44 and ASE 54 , and provide functions to identify, measure and classify SS effects that appear valence/conduction bands of 2D materials band structure calculations."
10.1038/s41597-022-01269-7,"The pymascdb package to manipulate the data and described in the previous section can be freely accessed at https://github.com/ltelab/pymascdb . Code documentation, examples and installation instructions are also available at https://pymascdb.readthedocs.io/en/latest/index.html ."
10.1038/s41597-022-01294-6,"All source code for this work is freely available under the MIT license. The source code used to generate the band gap database is available at Figshare 27 and https://github.com/QingyangDong-qd220/BandgapDatabase1 . The updated patch for the modified Snowball algorithm, which is now compatible with nested models, is available at https://github.com/QingyangDong-qd220/BandgapDatabase1/tree/main/chemdataextractor2 . A clean build of the ChemDataExtractor version 2.0 code is available at http://www.chemdataextractor2.org/download ."
10.1038/s41597-022-01293-7,USEEIO v2.0 was built in useeior v1.0.0 61 . The environmental and employment datasets were created with flowsa v1.0.1 26 as flow-by-sector data products. The flow-by-sector method names for the corresponding datasets are shown in Table 9 . The indicator characterization factors for all elementary flows were built in the LCIA Formatter v1.0.2 63 as LCIA data products. The USEEIO Modeling Framework for USEEIO v2.0 9 provides an overview of the source code along with links to useeior and supporting software packages.
10.1038/s41597-022-01295-5,The source code used to generate the database is available at https://github.com/JIUYANGZH/opticalmaterials_database . The code of ChemDataExtractor that was modified for database auto-generation in the optical-property domain is available at https://github.com/JIUYANGZH/opticalmaterials_database/tree/master/chemdataextractor . The source code of the website can be found at https://github.com/JIUYANGZH/aws_test .
10.1038/s41597-022-01264-y,Fully annotated Executable R scripts and R Markdown documents are available in our public RA-MAP GitHub in order to allow complete reproduction of our analysis workflow ( https://github.com/C4TB/RA-MAP ). All analyses were conducted in R version 4.0.5.
10.1038/s41597-022-01332-3,"To create the reference data set, including the control and validation data sets, we employed the Geo-Wiki application, which can be used to visually check available land cover and land use maps against very high-resolution imagery. Alternatively, users can employ the LACO-Wiki tool, which has similar functionalities and is openly available, but it requires users to upload the land cover and land use maps. All the geographical operations were done in QGIS 3.8.0, and the accuracy matrixes were calculated in R 3.6.1., using the raster 45 and dtwSat libraries 46 . The forest management layer was generated using the CGLS-LC100 collection 2 processing line developed by VITO NV on behalf of the European Commission Joint Research Centre (JRC). All code was written in Python 2.7. The regional models are available as Zenodo record 47 , and the related biome cluster are also stored as a separate Zenodo record 48 ."
10.1038/s41597-022-01308-3,"In this analysis, default parameters or parameters recommended by the developer were used. The options used for the processing of the 16 S rRNA sequencing were as follows: qiime dada2 denoise-paired ––i-demultiplexed-seqsdemux-paired-end.qza ––o-table table ––o-representative-sequences rep-seqs ––p-trim-left-f 6–-p–trim-left-r 6 ––p-trunc-len-f 250 ––p-trunc-len-r 250 . The options used for processing the metagenomic data in the CLC Genomics Workbench (ver. 6.5.1) is as follows: trimming: limit = 0.05, maximum two ambiguousnucleotides allowed"
10.1038/s41597-022-01312-7,The code used to pre-process the data set presented in this paper is available at: https://github.com/dotlab-brazil/arbovirus-dataset-brazil .
10.1038/s41597-022-01305-6,"Python scripts that implement model calibration, data assimilation, dataset generation, and mapping are available ( https://github.com/paperoses/CHN_Winter_Wheat_AGB ). Further questions can be directed towards Hai Huang (haihuang@cau.edu.cn)."
10.1038/s41597-022-01311-8,"The MATLAB code developed for this work is available from ZENODO 48 , together with a complete simple guide that illustrates fundamental steps to perform the kinematic analysis and graphical representation."
10.1038/s41597-022-01285-7,The code required for reproducing the figures and tables is freely available on GitHub ( https://github.com/XintongYao-96/Rat-microRNA-Bodymap ).
10.1038/s41597-022-01322-5,The programs used to generate all the results were MATLAB (R2017b) and ArcGIS (10.5). The PSO-BP codes for modelling the relationships between the national GDP and nighttime light data are publicly available under Figshare 43 .
10.1038/s41597-022-01296-4,The workflow consisting of several R scripts used to produce these datasets can be found in the Zenodo repository together with the data files and supplementary figures. The development of code can be tracked on Github here: https://github.com/jkumagai96/Marine_Habitat_protection . All the data presented in the figures in the manuscript can be reproduced with the available code written and run in R Studio (version 1.3.1073 – “Giant Goldenrod” Linux) with R version 3.6.0. The packages used are managed through renv with these associated files are available on Github. Please note that the workflow requires the user to download all the habitat data and the union of exclusive economic zones and country polygons from the data sources listed in Table 2 .
10.1038/s41597-022-01299-1,"All custom codes for data preprocessing and technical validation are available at https://github.com/BNUCNL/MEG_Gump . Preprocessing was performed using MNE-BIDS v0.8 ( https://mne.tools/stable/index.html ), MNE v0.22 ( https://mne.tools/stable/install/mne_python.html ), fMRIPrep v20.2.1 ( https://fmriprep.org/en/stable/ ), pydeface v2.0.0 ( https://github.com/poldracklab/pydeface ), and dcm2niix v1.0.20180622 ( https://github.com/rordenlab/dcm2niix )."
10.1038/s41597-022-01306-5,No code was used in the creation of this data. Titles should avoid the use of acronyms and abbreviations where possible. Colons and parentheses are not permitted.
10.1038/s41597-022-01310-9,No code was used in this study.
10.1038/s41597-022-01303-8,"A proprietary code, Academic Release CASTEP version 8.0 6 , 7 , 8 , 9 , 10 , 11 was used to perform DFT and core-loss spectra calculations. The configuration files used in the calculation is provided for reproducing the site C-K edge spectra at figshare 31 along with some of the output files for confirmation of calculation condition. For making input files, parsing output files, creating database and visualization, we have used the following python libraries: Numpy, Pandas, h5py, rdkit and Matplotlib. The Python code used for parsing the CASTEP input and output files is available at GitHub 32 under the MIT license. The Python code used for making the Gaussian smeared spectra dataset from the database HDF5 file of eigenvalues and dynamical structure factors is also available at GitHub 39 under the MIT license, and can be used for making spectra with arbitrary smearing parameters."
10.1038/s41597-022-01309-2,We used the MATLAB 2020b for data processing. The core codes for the study are available at https://github.com/BiWenjunnju/code_TL.git .
10.1038/s41597-022-01335-0,"The code to generate the dataset is not available, as it deals with privacy sensitive source data. Alongside the dataset itself we also provide a Jupyter notebook in Python which implements a machine learning pipeline on the dataset for the task of cross-sectional prediction. This code illustrates how to handle the data. The notebook, some helper code and the data are available at https://gin.g-node.org/JanYperman/motor_evoked_potentials ."
10.1038/s41597-022-01304-7,Codes used in this study were done using R-Studio with R version 4.0.4. The codes are available through the Github link https://github.com/bijoychandraAU/Temporal-disaggregation-of-precipitation .
10.1038/s41597-022-01326-1,"We provide code in R that will assist in reviewing the database and examining patterns ( https://afilazzola.github.io/GrazingDatabase/ ). We conducted data synthesis, technical validation, and visual quality assurance in R version 3.5.1 using tidyr , dplyr , ggplot 29 , and raster 30 packages. Within the repository, we provide code for separating nested observations within the same cell, such as the list of herbivores reported in each study that are currently separately by semicolons. We also provide code for joining each meta-data file to the master file. All code used in the technical validation of this study is provided in the repository. All the code is written in R, except the code used for the consolidation step in technical validation, which is written in JSON. All code is freely available under the Massachusetts Institute of Technology license."
10.1038/s41597-022-01337-y,"The REACH software, including JavaScript code, a sample R script for running REACH using the Python API, and a user guide, is accessible via the following GitHub archive: https://github.com/EcoGRAPH/epidemia_reach . Computer code for the REACH Python package is accessible via the following GitHub archive: https://github.com/EcoGRAPH/epidemia_gee ."
10.1038/s41597-022-01340-3,"All algorithms used in this paper can be freely obtained from open source Python packages. The image pre-processing pipeline and feature engineering (Canny edge detection, circular Hough transform, convex hull, local binary pattern) were performed by using the algorithms implemented in the scikit-image library 21 . The Principal Component Analysis was carried out with the Singular Value Decomposition using the numerical python library, NumPy 32 ( numpy.linalg.svd ). Hierarchical clustering tools made available by SciPy 25 in the module scipy.cluster.hierarchy were also used. The algorithm for minimax linkage has been published as an R package on the CRAN repository under the name protoclust ( https://CRAN.R-project.org/package=protoclust ). Independently, the authors of this paper developed the pyprotoclust Python package 17 (albeit with the retroactive incorporation of this previously established naming convention). Our Python implementation allows us to obtain seamless integration with the interface established by SciPy’s hierarchical clustering and the other Python-specific tools used in this study. Our implementation can also take advantage of multi-threading to improve algorithm performance. The open-source code for the pyprotoclust Python package can be obtained from the Python Package Index under the MIT license. The documentation is hosted publicly online by Read the Docs ( https://pyprotoclust.readthedocs.io )."
10.1038/s41597-022-01274-w,"The software/code works out of the box on a current Ubuntu system using Python, and all source data were derived from open and free datasets available for the whole of Sub-Saharan Africa. The Python software 25 , scripts and a virtual environment are provided with the data and requires minimal input from the user. The software is controlled by a configuration file. The GitHub repository contains an example configuration file, creation.cfg. In this file the following should be changed: • Path to input data sets • Names of input data sets should these have been changed. • Path to the location where the output files should be saved • Names of the output files. The user can also choose to change the following settings in the configuration file: • Walking speed reduction to account for children. As standard it is set to 0.78 to generate a 22% reduction in the travel speeds. If a user wishes to generate a cost allocation surface for adult walking speeds they should change the factor to 1.0. • Water speed can be changed in this file. It is set to NA in the landcover.csv and should remain as such. The water speed can also be changed in the configuration file and will appear in the water passable cost allocation surface. Additional steps required prior to running the code: • The roads from the standard OSM roads download and m apwith.ai download should be merged. We used the merge function in ArcMap 10.7.1 32 . The merged road data require a new ‘tag’ attribute to be created which contains the road type (name). • The user should check the roadcosts.csv file to ensure the road names match those in the shapefile. • The user can change the travel speeds assigned to each road type by editing the roadcosts.csv file. • The user can change the travel speeds assigned to each land cover type by editing the landcovercosts.csv file. If users want to use a different land cover map they can do so by ensuring that the land cover types in the landcovercosts.csv match those in the chosen land cover data. All of the analysis was conducted using Python-3 apart from merging the OSM roads with the MapwithAi roads. The code has been developed as an all-in-one software which can be downloaded from Zenodo 25 and GitHub [ https://github.com/ChildPovetyAccesstoServices/cpas/tree/v1.0 ] and is licensed under the GNU General Public License v3.0 only, meaning changes to the code are permitted as long as they are distributed under the same license. The code includes an example configuration file creation.cfg . This is the only file the user needs to alter to repeat the process. Within this file the user specifies the location of the input data and the names and paths for the output files. The user can also specify the weighting if they wish to consider children’s travel speed, if not they can set the reduction factor to 0 which will generate adult cost allocation surfaces. The code runs in two steps (1) calculate the cost allocation surfaces (2) use the least cost path analysis to estimate the travel time from every pixel to the nearest health centre. All rasters are read and written using the rasterio python package ( https://github.com/mapbox/rasterio ) together with xarray 62 . The steps and python packages used for pre-processing the roads data: • Walking speed values are assigned to an array based on land cover type. • A CSV file of OSM road categories and associated walking speeds was used to input the assigned walking speeds into a pandas Data Frame 63 . • A shapefile containing roads was rasterised using the walking speeds assigned to each type of road. The fastest walking speeds are used for pixels that contain roads of different types. There were a few steps to this which are detailed below: a. The road types of the CSV file were matched to the road types of the shapefile using a fuzzy match to ensure a walking speed was assigned to every road type in the shapefile. b. The road types were grouped by walking speed. Each group of roads was processed separately. c. Roads from the shapefile were filtered by road type and rasterised using the rasterize function of the rasterio.features module. The walking speed is used as value for the rasterised roads. The all_touched option was used so that all grid squares in which road segments existed were counted as road segments in the new raster. This was essential so the rasterization of the roads retained the connections. d. The resulting walking speed surfaces were merged taking the maximum value at each pixel for all road types. Pre-processing steps of the SRTM data required: • Re-sampled to 20 m resolution, using bilinear interpolationSlope is calculated using the formula \(slope=\sqrt{\left({\left(\frac{dH}{dx}\right)}^{2}+{\left(\frac{dH}{dy}\right)}^{2}\ast \frac{100}{111120}\right)}\) The factor 100 converts the gradient to a percentage slope and the factor 111120 coverts degree latitude/longitude to metres. This approximation is only valid close to the equator which is the case in this study. As Eq. 1 does not account for gradients over 100%, these were removed and replaced with a NA value. The cost allocation surface and health facilities locations were combined to calculate travel time from each cell in the array to the nearest health facility using the graph.MCP_Geometric() and find_costs() methods from the SciPy package 64 ."
10.1038/s41597-022-01330-5,All the data and python scripts used to generate coordination number based PES surface to analyze the data for each reaction channel is provided at https://doi.org/10.6084/m9.figshare.19601689 30 .
10.1038/s41597-022-01302-9,The analysis code (Matlab 2020a) used for the examples provided below is available in the GitHub repository https://github.com/Jinyuan1998/scientific_data_metagenome_shotgun . The script used for each figure is in a separate directory: • Figure 1: Figuer1/scFigure 1.m. • Figure 2: Figuer2/scFigure 2.m. • Figure 3: Figuer3/scFigure 3.m. • Figure 4: Figuer4/scFigure 4.m. • Figure 5: Figuer5/scFigure 5.m.
10.1038/s41597-022-01324-3,The lightweight processing needed to visualize and to decompress the bag files is described in the previous section. Processing code for data analysis is available on the supplementary material.
10.1038/s41597-022-01339-w,"Both the ISOR manual and the automated catalogues were produced using the SeisComP software package. While the core of SeisComP is open source and freely available at http://seiscomp.de , in our analysis we also used the scanloc module which is provided under license by Gempa GmbH http://gempa.de . The evscore and scrtDD modules are open-source and available at https://github.com/swiss-seismological-service ."
10.1038/s41597-022-01300-x,No specific code was generated for analysis of these data.
10.1038/s41597-022-01331-4,"All source code for sedat is made available through github under the academic free license, at the time of writing. The code is hosted at https://github.com/gyetman/DOE_CSP_PROJECT ."
10.1038/s41597-022-01328-z,"All anonymization steps were computed in Python 3.8.2 on a Windows 8.1 platform as described in the Methods section. We are not able to publicly share the actual code involved in the de-identification procedure, as patient information was processed. The de-identification procedure should be sufficiently reproducible based onto the presented information. We attached a Jupyter notebook that is able to preview images including tags and labeled objects ( “annotation_preview.ipynb” ). It can be found in the “notebooks” folder and online at Figshare under https://doi.org/10.6084/m9.figshare.19330688.v1 43 . The dataset is accompanied by a Jupyter notebook that is able to split files into different folders, based onto two columns of a CSV file ( “copy_files_by_csv.ipynb” ). It is intended to efficiently prepare datasets for image classification tasks with the included “dataset.csv” file. Image post-processing code is publicly provided as second Jupyter notebook with the dataset ( “image_conversion.ipynb” ), also located in the “notebooks” folder."
10.1038/s41597-022-01313-6,"All data processing and analysis of existing datasets and the generation of TWLs, DWLs, return periods, and probabilistic extreme water level impacts were carried out by a custom MATLAB code library developed specifically for this project. A repository of these codes and templates can be found at https://github.com/Climate-Shope/West_Coast_TWLs.git for access and download. Due to the complexity of the scripts and backing data, please contact Li Erikson at lerikson@usgs.gov for assistance in implementation if necessary."
10.1038/s41597-022-01354-x,All the Python scripts used for analysis in the Technical Validation section for analysis and figure generation are available in the G-Node repository.
10.1038/s41597-022-01357-8,The code implementation was done in R 4.0.5 using R studio. The scripts to perform data visualization are available in 28 .
10.1038/s41597-022-01318-1,The codes that were used to make the graphs presented in this paper are available at the following address: https://github.com/dbeillouin/Data_Paper_SOC#readme .
10.1038/s41597-022-01336-z,The raw data are available on the ESA website https://kelvins.esa.int/mars-express-power-challenge/ as provided by the MEX operations team at ESOC. These data are pre-processed using the above-described approaches.
10.1038/s41597-022-01385-4,"BUSCO: --evalue 1e-03, -sp Arabidopsis.LACHESIS:CLUSTER_MIN_RE_SITES = 30;CLUSTER_MAX_LINK_DENSITY = 2;CLUSTER_NONINFORMATIVE_RATIO = 2;ORDER_MIN_N_RES_IN_TRUN = 49;ORDER_MIN_N_RES_IN_SHREDS = 49.Software parameters of repeat annotation: default parameters for LTR_FINDER, RepeatScout, and PASTEClassifier. RepeatMasker: -nolow -no_is -norna -engine wublast.Software parameters of gene prediction: default parameters for Genscan, Augustus,GlimmerHMM, GeneID, SNAP, GeMoMa, Stringtie, TransDecoder, GeneMarkS-T, and EVM.Hisat:--max-Intronlen 20000, --min-intronlen 20. PASA: -align_tools gmap, -maxintronlen20000.GenBlastA: -e 1e-5.BLASTP: -e 1e-10.CandiSSR: perl CandiSSR.pl -i crl.file -o out_path/.Default parameters were used in other software unless otherwise specified."
10.1038/s41597-022-01338-x,"Our code is freely available at https://github.com/jaydevine/MusMorph . The scripts describe every stage of the MusMorph data acquisition and analysis, including image preprocessing (e.g., file conversion, image resampling and intensity correction), processing (e.g., atlas generation, non-linear registration, label propagation), and postprocessing (e.g., shape optimization, morphometric analysis). We developed and implemented the code with Bash 4.4.20, R 3.6.1, Python 3.6, and Julia 1.2.0 on Ubuntu. To facilitate MusMorph software installations, reproducibility, and data aggregation, we have created a comprehensive Docker image that can be downloaded as follows: $ docker pull jaydevine/musmorph:latest . Further information about running the Docker container is available on GitHub. All code is distributed under the GNU General Public License v3.0."
10.1038/s41597-022-01323-4,"All code, including custom scripts such as occ_cleaner, bind.tip2, SelectVariables , and autokuenm discussed above, are available as R scripts and summarized in Markdown format under the code directory in the GitHub project repository ( github.com/RhettRautsaw/VenomMaps ). We also provide functions to convert SDM outputs (R function: raw2clog, raw2log, log2raw ) and threshold models (R function: raster_threshold ) in the functions.R script."
10.1038/s41597-022-01315-4,All the code used to develop RF model is available in ‘source file’.
10.1038/s41597-022-01317-2,The scripts used to classify paragraphs and extract procedures as well as to perform the data analysis are home-written codes which are publicly available at the GitHub repository https://github.com/CederGroupHub/text-mined-solution-synthesis_public with acknowledgement of the current paper. The underlying libraries used in this project are all open-source: Tensorflow ( www.tensorflow.org ) Keras ( keras.iokeras.io ) SpaCy ( spacy.iospacy.io ) 60 NLTK ( https://www.nltk.org/ ) 62 gensim ( radimrehurek.comradimrehurek.com ) 59 scikit-learn ( scikit-learn.org ) 75 ChemDataExtractor ( chemdataextractor.org ) 40 Material Parser ( github.com/CederGroupHub/MaterialParser ) Borges ( github.com/CederGroupHub/Borges ) LimeSoup ( github.com/CederGroupHub/LimeSoup ).
10.1038/s41597-022-01321-6,"Scripts developed for the generation of this dataset as well as notebooks for example data analysis are available at https://github.com/CederGroupHub/text-mined-aunp-synthesis_public , along with an acknowledgement for this paper. The libraries use for this project are: ChemDataExtractor , SpaCy , scikit-learn , gensim , Tensorflow , Keras , PyTorch , and Simple Transformers ."
10.1038/s41597-022-01333-2,"Together with the collection of cycling safety datasets, we share the code used for curating the datasets. All code has been written for Python3. We present the code under Jupyter notebooks, which provide step-by-step instructions on how each dataset was curated. The code is available under the MIT license ( https://opensource.org/licenses/MIT ) and is available at https://github.com/U-Shift/cyclands ."
10.1038/s41597-022-01316-3,Annotated markdown R code used to generate the analysis is available in GitHub ( https://github.com/StefanoMammola/European-cave-spider-traits-1.git ).
10.1038/s41597-022-01350-1,The Python codes for the NER and RE experiments represented in Technical Validation can be accessed from https://github.com/DMCB-GIST/PPRcorpus .
10.1038/s41597-022-01377-4,"We used ArcGIS 10.4 ESRI 2017 to analyse the satellite images and create the boxes. ArcGIS 10.6 ESRI 2017 can also be used. Various pansharpening algorithm exists 22 . As we have used the ESRI pansharpening algorithm, we recommend using this one. The Gram-Schmidt is often preferred when monitoring wildlife from space 23 ; however, we have found that sometimes it may shift the pansharpened image compared to the panchromatic and multispectral images. Therefore, if a pansharpening algorithm other than ESRI is used, we recommend testing that it does not shift the image or to be aware of by how many pixels it has shifted the image."
10.1038/s41597-022-01341-2,The scripts for generating unbiased average templates are publicly available at https://github.com/vfonov/nist_mni_pipelines .
10.1038/s41597-022-01319-0,"The following software and versions were used for quality control and data processing: 1. Proteomic analysis: Perseus (version 1.6.0.7, MPI for Biochemistry) and within the R environment (R foundation for statistical computing). 2. Spectra identification and quantification: MaxQuant environment (version 1.6.0.16, MPI for Biochemistry). 3. Reference proteome: Homo sapiens reference proteome (UP000005640, 71567 entries, downloaded on August 28, 2017, from the UniProt database). 4. Transcriptomic data analysis: CLC Genomics Workbench (version 10.1.1, QIAGEN). Empirical Analysis of DGE (version 1.1, cutoff = 5) was used for multi-group comparisons and statistics. 5. Adapter trimming and demultiplexing: bcl2fastq tool. 6. RNA-seq mapping was done against the Homo sapiens (hg38) (May 25, 2017) genome sequence."
10.1038/s41597-022-01349-8,We share our codes for data processing and generation in GitHub 31 . The detailed description of the codes is in the README.md.
10.1038/s41597-022-01325-2,"Our R scripts, used for the analysis of the data, are available on GitHub ( https://github.com/martijnkersloot/perceptions_analysis )."
10.1038/s41597-022-01356-9,The Python code that we used to produce the datasets presented in this paper is published at https://github.com/AlexandreLab/UKERC-data .
10.1038/s41597-022-01352-z,Source code for the Whyis application framework can be found on Github at https://github.com/tetherless-world/whyis/ . Source code and documentation for the Vega-Lite project can be found on Github at https://github.com/vega/vega-lite . The W3C Recommendation for the SPARQL 1.1 Query Language can be found at https://www.w3.org/TR/sparql11-query/ .
10.1038/s41597-022-01342-1,"The experimental measurements were processed using a MATLAB ® library internally built by the Cobra probe manufacturer Turbulent Flow Instrumentation Pty. Inc. All custom scripts that were used to synchronize wind measurements across different experiment repetitions (“1_alignSignals.m”), to save the data in proper format (“2_saveData.m”), and to generate the database (“3_datasetWriting.m”) have been made available 36 . All scripts were created in the commercial software MATLAB ® (version R2017b)."
10.1038/s41597-022-01363-w,"The complete database of the rheological data based on literature studies has been reported in an Excel file, uploaded on figshare repository 65 . The presented rheological model has been implemented in an Excel file, uploaded on figshare repository 65 https://doi.org/10.6084/m9.figshare.16886155.v1 ."
10.1038/s41597-022-01344-z,The R source code used to process these data is open access and described in detail in The Comprehensive R Archive Network ( https://cran.r-project.org/ ). The USACE QCC Measurement Archive R code suite developed during this work is open access and is fully described in a USACE QCC Measurement Archive SOP document ( https://github.com/CandiceH-CHL/USACE_QCC_measurement_archive.git ).
10.1038/s41597-022-01343-0,"The open-source Python code of the fire tracking system, as well as sample scripts for reading the dataset, are freely available at the figshare data repository 33 , along with the 2012–2020 FEDS dataset. Versions and packages of the Python script include Numpy (1.17.5), Pandas (1.0.1), Geopandas (0.7.0), Xarray (0.15.0), Scipy (1.4.1), Shapely (1.7.1), Gdal (3.0.4), and Pyproj (2.5.0)."
10.1038/s41597-022-01372-9,"Custom codes for generation and processing of the data and the figures are presented in the repository 67 . A MATLAB script “eldbeta_convert.m” was provided for data processing in converting the raw data to the epoch data. The data preprocessing and technical validations were conducted in MATLAB R2018b and Python 3.6.10. A “README.md” file was used for a brief description of the code in the code repository. The Benchmark database and the BETA database as well as the classification algorithms can be found in their corresponding repositories related to the papers, and thus they are not provided in this data descriptor."
10.1038/s41597-022-01376-5,The code to reproduce the plots in the paper is publicly available on Figshare 24 .
10.1038/s41597-022-01374-7,No codes were developed for this research.
10.1038/s41597-022-01320-7,No custom scripts were used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-022-01361-y,The code is available on GitHub 20 . The data collection was performed in Central Standard Time of the United States which is 6 hours behind GMT.
10.1038/s41597-022-01358-7,"Code written by the Data Management Team in support of this project can be found on github.com under an open and permissive usage license. Gap filling of the drain flow data and visualization of the results were performed within RStudio 38 , 39 . The source code and corresponding input and output data are publicly available on GitHub 40 . The code is split into three pieces corresponding to individual phases of the gap-filling method. In addition, there are separate scripts for the visualization of the data. At drainagedata.org, users can visualize the data with customized tools, query based on specific sites and measurements of interest, and access site photographs, maps, summaries, and publications. The code used for the backend data services and frontend web interfaces can be found on GitHub 41 ."
10.1038/s41597-022-01346-x,The code can be found at https://github.com/CliMA/GriddingMachine.jl under the Apache 2.0 License. The exact version of the package used to produce the results presented in this paper is also archived on CaltechDATA along with the datasets 17 .
10.1038/s41597-022-01348-9,The code for extracting information from data sources and for the harmonization of CoV2K content is available on the project’s GitHub repository https://github.com/DEIB-GECO/cov2k_data_collector/ .
10.1038/s41597-022-01362-x,"In this study, we use the General Algebraic Modeling System (GAMS) to conduct the cross-entropy estimation, and MINOS solver is used to conduct the nonlinear optimization tasks. All codes used for analysis are available in the public GitHub repository: https://github.com/qianhaoqi/China-Industrial-Environmental-Database ."
10.1038/s41597-022-01364-9,"The observations database, all raw CSV files and the R scripts used to standardise and check the observations, as well as a sample script to aggregate the observations database into a species-trait data set are available in the auxiliary material 20 . The auxiliary material also contains README.txt files that describe the structure and usage of the data and scripts. The auxiliary material is managed as a GitHub repository ( https://github.com/animaltraits/animaltraits.github.io ). GitHub is also used to build and serve the website."
10.1038/s41597-022-01393-4,The NCL code used to generate the downscaled products can be found at https://github.com/bthrasher/daily_BCSD .
10.1038/s41597-022-01353-y,"The code used for the preprocessing, tomographic reconstruction, and postprocessing is available on GitHub ( https://github.com/HiPCTProject/Tomo_Recon )."
10.1038/s41597-022-01347-w,Python code for prepossessing the data and implementing the segmentation based on different classes are available online https://github.com/cruiseresearchgroup/InGauge-and-EnGage-Datasets .
10.1038/s41597-022-01360-z,"All of the source code for CDCDB database generation has been uploaded to GitHub: https://github.com/Omer-N/CDCDB , where it is maintained. We also provide the code for parsing and visualizing the data (see Usage Notes above)."
10.1038/s41597-022-01387-2,No custom code was used to generate or process the data.
10.1038/s41597-022-01389-0,"All code used in the experiments to generate results, plots and tables was written in Python and is available through our GitHub repository for EIPH analysis [ https://github.com/ChristianMarzahl/EIPH_WSI/ ] in the folder SDATA and is referenced on Zenodo 29 ."
10.1038/s41597-022-01371-w,No custom code was used to generate the datasets or processes described in the manuscript. The process is available at the DOI providing repository https://zenodo.org/badge/latestdoi/444480619 with no restrictions on access or use.
10.1038/s41597-022-01345-y,The LC-MS feature detection software (MassHunter®) used in this work is commercially available from Agilent®.
10.1038/s41597-022-01367-6,No custom code was used in this study.
10.1038/s41597-022-01366-7,"The GMT and GDAL routines used in the SEAHORSE workflow are Open Source and can be accessed on their respective webpages ( https://www.generic-mapping-tools.org/ and https://gdal.org/ ). All relevant code related to the main SEAHORSE workflow are available at https://github.com/SeaBed2030/IBCSO_v2_Dorschel_et_al_2022 . Data for the technical validation are hosted on figshare 33 . Since the SEAHORSE workflow was customised to fit the existing architecture of AWI’s high performance cluster, most of the code is specific and requires severe adjustments when moved to a different environment."
10.1038/s41597-022-01402-6,"The recording of all data was controlled by a procedure written with PsychoPy 3.2.4 that established a connection to the gamepad, BITalino, and camera and recorded the data streams and images. The raw data thus collected was then converted to CSV and JSON formats using custom scripts in Python 3.8 with libraries: pandas for data manipulation, heartpy for ECG signal processing, and neurokit2 for EDA signal processing. All analyses supporting technical validation of the dataset were performed using statsmodels and bioinfokit libraries for Python. Finally, level maps were visualized using the bokeh library. The whole code is available to interested researchers in a dedicated repository: https://gitlab.geist.re/pro/biraffe2-supplementary-codes ."
10.1038/s41597-022-01390-7,"All analyses were supported by the Python programming language (version 3.7.7) and its scientific software stack 50 . Molecular conformations were generated using RDKit ( http://www.rdkit.org , version 2020.03.3) and GFN2-xTB 26 , 27 , 28 , 29 (version 6.3.1). All quantum mechanical calculations were carried out with Psi4 37 (version 1.3.2). Molecular structure visualizations were created using PyMol 56 (version 2.3.5) and ChemDraw (version 19.1.1.32). The rclone ( https://rclone.org , version 1.54.0) WebDAV client was used for all data uploading purposes."
10.1038/s41597-022-01395-2,Scripts using R programming language are provided to produce figures. Additional code and related files are available at figshare repository 21 : https://doi.org/10.6084/m9.figshare.19105049.v1 .
10.1038/s41597-022-01403-5,"The Python code for reading the ECG data, attributes and diagnostic code dictionary, evaluating the signal quality, and dataset partition is available in figshare 18 ."
10.1038/s41597-022-01384-5,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-022-01379-2,No custom code was used in the data acquisition of these datasets.
10.1038/s41597-022-01411-5,"An open-source project facilitating the use of VitalDB dataset has been launched, and a lot of code written in C/C++, Python, Javascript, and R languages is currently available from Zenodo ( https://doi.org/10.5281/zenodo.6321507 ) 20 . Python codes that can be used as references of algorithm research are also available from Zenodo ( https://doi.org/10.5281/zenodo.6321522 ) 21 . The examples of sample codes for statistical analysis are as following: - General Characteristic: vitaldb_tableone.ipynb - Mortality: asa_mortality.ipynb - Acute Kidney Injury - mbp_aki.ipynb The examples of sample codes for artificial intelligence algorithms are as following: - Drug effect estimation using Long Short-Term Memory: ppf_bis.ipynb - Hypotension prediction using Long Short-Term Memory: hypotension.ipynb - Mortality prediction using Gradient Boosting Machine: predict_mortality.ipynb"
10.1038/s41597-022-01327-0,"The geomodels presented in this paper were constructed using the GeoModeller commercial package, which was developed by BRGM, the French Geological Survey, with the support of Intrepid Geophysics. This software is based on original methodologies for geological and geophysical modelling 25 , 26 , 45 . More information at: https://www.geomodeller.com . The following versions of GeoModeller were used to achieve the geomodels presented in this study. Los Humeros: GeoModeller Version: 4.0.7 Build Date: May 22 2019 Build Number: 27eee3dc31ba Acoculco: GeoModeller Version: 4.0.8 Build Date: Aug 04 2020 Build Number: d783d7694b8 The default parameters of GeoModeller were used for the interpolation of all the geomodel versions presented in this paper."
10.1038/s41597-022-01406-2,"The data collection was conducted using the application GetSensorData , which is available at the GitLab repository 17 . The processing of the raw data, displaying of the distribution of the subsets and the technical validations of the data collected was done in Matlab. The version required to run the code for processing the raw data is R2017a or above, and for the rest of scripts (display and technical validations) it is R2020a or above. Also, the Statistics and Machine Learning toolbox is required for the functions used in the technical validations. The source code is available at the Zenodo repository 20 ."
10.1038/s41597-022-01382-7,"The code is available as part of the data collection at the data repository of the Donders Institute for Brain, Cognition and Behaviour."
10.1038/s41597-022-01314-5,"This study did not use any computer codes to generate the indoor and outdoor temperature dataset. Online, code is available to calculate various heat indices such as the Wet Bulb Globe Temperature (WBGT) 38 based on the collected data."
10.1038/s41597-022-01413-3,All the scripts and brain templates involved for processing is publicly available as part of the recent CCS updates 57 and can be visited at GitHub ( https://github.com/zuoxinian/CCS/tree/master/projects/isybdemo ).
10.1038/s41597-022-01359-6,"The details and computational parameters of raw read processing for all panels and read mapping for panel TFS are expanded versions of descriptions in our related work 1 . AGL read processing Each sample was demultiplexed using bcl2fastq v2.20 (Illumina) 23 with the base mask Y150, I8, Y10, Y150 and all default settings except for mask-short-adapter-reads, which was set to 0. Adapters were trimmed using AGeNT Trimmer (Agilent) 24 . BRP read processing After demultiplexing using bcl2fastq v2.20 23 , sequence data were filtered using the Trimmomatic 0.36 25 with parameters “TRAILING:20 SLIDINGWINDOW:30:25 MINLEN:50”. IDT read processing IDT libraries were prepared with xGen Dual Index UMI Adapters—Tech Access which contain 9 bp degenerate unique molecular identifiers (UMIs) downstream of the i7 sample index. To demultiplex Illumina sequencing data containing UMIs, Illumina basecall (BCL) files were used to generate demultiplexed BAM files using Picard v2.9.0 ( http://broadinstitute.github.io/picard/ ) IlluminaBasecallsToSam. Prior to running Picard v2.9.0 IlluminaBasecallsToSam, Picard v2.9.0 ExtractIlluminaBarcodes was used to determine the barcode for each read using the read structure 151T8B9S8B151T (T = template, B = sample barcode, M = molecular barcode, and S = skip). The UMI bases were not used for downstream analysis since there was not enough raw sequencing depth for consensus analysis. After demultiplexing, FASTQ files were generated using Picard v2.9.0 SamToFastq. FASTQ files were downsampled to an equivalent read count per sample using seqtk v1.0 ( https://github.com/lh3/seqtk ). IGT read processing Raw reads were firstly quality trimmed with Trimmomatic 0.36 25 , using an 8-base-pair sliding-window algorithm with a quality score cutoff of 20, clipping off ends with at least one occurrence of a quality score below 20, and discarding reads that dropped below a length of 40 base-pairs. Adaptors (a1: GATCGGAAGAGCACACGTCT, a2: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT) were removed simultaneously. ILM read processing Pan Cancer sample testing analysis was performed using the standard TruSight Tumor 170 pipeline available on the BaseSpace Sequence Hub. Briefly, high level sequencing run metrics are evaluated to generate a Run QC Metrics report. Next, reads are converted into the FASTQ format using bcl2fastq v2.20 23 and adapters are trimmed. QGN read processing Raw sequencing data were demultiplexed and converted into the FASTQ format using bcl2fastq v2.20 23 in BaseSpace, with the library specific pairs of Qiagen custom read 1 primer sequences and the reverse complementary Illumina i7 primer sequences. QIAGEN’s UMI-aware variant caller smCounter2 26 can be used for variant calling. ROC read processing Two NovaSeq runs were provided by Illumina via BaseSpace (RUN1 = SeqC2_ROC1_ST_16_17_18_19; RUN2 = SeqC2_ROC1_ST16_17_18_19). BCL files were converted to unaligned BAM files using instructions provided by IDT ( https://www.idtdna.com/pages/products/next-generation-sequencing/adapters/xgen-dual-index-umi-adapters-tech-access ). Following the IDT tech note, Picard 2.18.3 ExtractIlluminaBarcodes and IlluminaBasecallsToSam were used to create the unaligned BAM files. The UMI for each fragment is stored in the RX tag in the BAM file. TFS read processing and mapping Signal processing and base calling were performed using Torrent Suite Software 5.8 ( https://github.com/iontorrent/TS ) using default parameters for the Oncomine Comprehensive Assay. The signal processing step consists of modeling the pH dynamics on the semiconductor surface taking into account the varying local pH in each individual sensor coming from the different reagent flows across the chip and from any nucleotide incorporation that may be happening over each sensor 27 . The base calling step consists of taking the estimated levels of nucleotide incorporation for each read and each nucleotide flow, and modeling the de-phasing process whereby some templates within each clonally-amplified population run ahead or behind in terms of their nucleotide incorporation. During the base calling process, sample-specific barcodes and 3′ adapters are annotated. After completion of primary analysis with Torrent Suite Software 5.8, reads were uploaded to Ion Reporter Software 5.6 28 for subsequent processing. Reads were aligned with the Torrent Mapping Alignment Program (TMAP, https://github.com/iontorrent/TS/tree/master/Analysis/TMAP ), which uses the BWA fastmap routine to map reads and applies post-processing of the alignments to optimize for technology-specific error patterns."
10.1038/s41597-022-01307-4,"The Dynamic World NRT dataset has been made available as an Earth Engine Image Collection under “GOOGLE/DYNAMICWORLD/V1”. This is referenced in either the Earth Engine Python or JavaScript client library with: ee.ImageCollection(‘GOOGLE/DYNAMICWORLD/V1’) . We provide a public web interface for rapid exploration of the dataset at: https://sites.google.com/view/dynamic-world/home . We also provide an example of accessing Dynamic World using the Earth Engine Code Editor in the following code snippet: https://code.earthengine.google.com/710e2ae9d03cd994c6e8dc9213257cbc . The Dynamic World model has been run for historic Sentinel-2 imagery and is being run for newly acquired Sentinel-2 imagery; users are therefore encouraged to work with outputs available in the NRT Image Collection available on Earth Engine. Nonetheless, to ensure reproducibility, we have archived the trained model, example code for running inference, and additional information on the model architecture in Zenodo at https://doi.org/10.5281/zenodo.5602141 44 ."
10.1038/s41597-022-01396-1,All analyses used to generate figures and summary statistics were performed in R (v.4.1.2) 164 . No custom computer code or algorithms were used to generate the data presented in the manuscript.
10.1038/s41597-022-01381-8,The code used to test the internal consistency of the database is publicly available at the GitHub repository https://github.com/LandSciTech/CANSARD .
10.1038/s41597-022-01301-w,"The code used to generate the four databases can be found at https://github.com/gh-PankajKumar/ChemDataExtractorStressEng . This repository contains the modified ChemDataExtractor 2.0, webscraping scripts and post-processing tools. The repository contains propertyExtractor.py , which was used to automatically extract yield strength and grain size and serves as an example as to how the herein modified version of ChemDataExtractor can be used for extracting engineering-material properties. Also, propertyExtract_Example.ipynb is an iPython notebook that walks through the basic steps to extract records from an input article. A static version of the repository is available to download from Figshare 17"
10.1038/s41597-022-01407-1,No custom computer codes were generated in this work.
10.1038/s41597-022-01388-1,"The PAPILA dataset 44 is publicly available at https://doi.org/10.6084/m9.figshare.14798004.v1 . As detailed in the composition of the dataset, the clinical data of both eyes of each patient and the corresponding diagnosis are stored in spreadsheet and plain text format. In addition, the folder named HelpCode contains sample code in Python to read, handle and process the dataset. Jupyter Notebooks are also provided to exemplify the use of the PAPILA features."
10.1038/s41597-022-01369-4,"Code to replicate all analyses presented here, the intermediate datasets and the exhaustive dataset are available at the publicly accessible Sciences-Po Dataverse: https://doi.org/10.21410/7E4/YLG6YR 51 , 52 ."
10.1038/s41597-022-01416-0,"The images and Django labeller annotation files (JSON) are available on the mentioned Zenodo repository 17 . Furthermore, the code to obtain JSON files in the MS COCO format is published alongside this dataset, and is made available online 15 ."
10.1038/s41597-022-01394-3,The source code of the ACovPepDB web interface has been shared on GitHub and Gitee .
10.1038/s41597-022-01398-z,The full repository containing all data and code folders described above is available through the Open Science Framework ( https://osf.io/c689u/ ) 45 . The analysis code and experimental task code are also available on Github ( https://github.com/air2310/FeatAttnClassification ).
10.1038/s41597-022-01351-0,Two literature tools have been made available in the Supplemental Information. The file FlameRetardantsFromMeSHMining.xlsm is the chemical – Flame Retardant co-annotation output from the process briefly described in the Methods. The second tool is a version of the PubMed Abstract Sifter populated with the resulting Flame Retardant Inventory 129 . The Abstract Sifter is tool for easy querying of PubMed with a list of chemicals. Both tools have ReadMe sheets with instructions for usage. An additional user guide is provided for the Abstract Sifter.
10.1038/s41597-022-01418-y,"The complete dataset is available as zip file at figshare 23 . The dataset includes all measured data and all post-processing and evaluation scripts. In addition, the publicly available wrapper app 17 is freely available in case the dataset needs to be updated or expanded."
10.1038/s41597-022-01438-8,"The CAMD code used to generate the data described herein is available at http://github.com/TRI-AMDD/CAMD . Scripts used to generate and analyze the dataset, as well as reproduce the figures in this manuscript are all included in the above data repository."
10.1038/s41597-022-01436-w,"The acquired metabolite profiles were processed by using ProteoWizard package ( http://proteowizard.sourceforge.net ), XCMS Online software ( https://xcmsonline.scripps.edu/ ), SIMCA 13.0 (Umetrics AB, Umea, Sweden) software and MetaboAnalyst plotform ( https://www.metaboanalyst.ca ), respectively."
10.1038/s41597-022-01370-x,"Codes for the demographic details of the sample, database with stimuli details and sources, and raw data outputs are freely available on FigShare at https://figshare.com/articles/figure/EaRTH_-_The_Environmental_Risks_To_Humans_database/14662173 5 . No custom code was used to generate, process or analyse the data presented in the manuscript."
10.1038/s41597-022-01427-x,"Data preprocessing tasks were performed in Python programming language. A Jupyter notebook (Geocode_cnet.ipynb) including the data preprocessing steps is provided alongside the paper 23 . Example figures (geographical distribution, co-occurrence networks, citation network) were constructed in the R program language. The R script is provided to produce and modify the figures based on the needs of the researcher."
10.1038/s41597-022-01380-9,"The complete open reanalysis pipeline description and documentation, workflows, container recipes, and custom code and visualisation scripts, as well as parameter input files are available through the GitHub repository at https://github.com/PRIDE-reanalysis/DIA-reanalysis ."
10.1038/s41597-022-01409-z,"The data presented in the database contains raw, full time-series EEG recordings and it is possible to analyse in any way. Nonetheless, for full transparency and replicability the complementary custom python code used for preprocessing (which was peer reviewed and beta-tested) as well as the code used for the neurophysiological validation is published together with the entire dataset on www.brainclinics.com/resources as well as on www.synapse.org ( https://doi.org/10.70303/syn25671079 ) 22 in one package and available under the same conditions described above. In addition, we have also published the TD_BRAIN_code on github: https://github.com/BCD-gitprojects/TDBRAIN/ ."
10.1038/s41597-022-01386-3,All tools used are open source and available at their respective references. The code for generating plots of quality control data (and the respective source data) can be found at https://github.com/TIGRLab/human_phantoms_plots .
10.1038/s41597-022-01430-2,"The R version 4.0.2 along with the R packages, dataRetrieval (v2.7.6), lubridate (v1.7.9.2), dplyr (v1.0.2), and data.table (v1.13.6) are used to download and quality control hourly gauge rainfall data. We used the 2020 17 Lidar Radar Open Software Environment (LROSE) to derive hourly radar rainfall. The python version 3.7.10 along with the python packages, pandas (v1.3.5), numpy (v1.21.2), netCDF4 (v1.5.8), xarray (v0.19.0), and matplotlib (3.5.0) are used for the validation process."
10.1038/s41597-022-01447-7,The R code used to identify cell subclusters and profile tissue-specific virus receptor expression and tissue-specific chromatin accessible regions are available online ( https://figshare.com/s/132dd4a1d364e459bac8 ) 41 .
10.1038/s41597-022-01419-x,The MATLAB codes for generating and processing data and the CN-NADC In situ ocean surface R n can be accessed at the figshare 80 .
10.1038/s41597-022-01426-y,"This project supports the open accessibility of viewing scientific data, however the project itself utilises licensed code which cannot legally be provided openly. No code from this project generates any scientific results or performs any form of scientific analysis. The code base consists of several components, namely three clients for Android, iOS, and webGL for viewing and interacting with augmentations. Additional components are integrated through reputable cloud service providers, such as file storage and scalable computation to provide image processing, file ingestion, conversion of uploaded data, and QR creation capabilities. A web server and database handles client requests, serves responses, and enables creation of new augmentations. The components of this project are inter-reliant, and do not function in isolation. Individual licensed and proprietary components handle many aspects throughout the codebase, including PDF handling and processing (‘PDF Renderer’ by Paroxe), webGL computer vision support (‘OpenCV for Unity’ by Enox Software), animated motion (‘iTween Native Extension’ by Pixelplacement), debug and error reporting (‘Lunar Mobile Console’ by SpaceMadness) and processing of input clicks/gestures (‘Fingers’ by Digital Ruby). Please contact Tyler.Ard@loni.usc.edu for specific requests or further information."
10.1038/s41597-022-01417-z,Figshare R Code DEG analysis: https://doi.org/10.6084/m9.figshare.19382594.v1 44 . Figshare Python Codes: https://doi.org/10.6084/m9.figshare.17031869.v2 38 . Figshare R Code for PCA analysis: https://doi.org/10.6084/m9.figshare.17031884.v2 41 .
10.1038/s41597-022-01415-1,The methods and tools applied in this paper use open-source tools detailed in respective publications Bakas et al . 12 publication. The python code for extracting PyRadiomics features from Rembrandt and the TCGA segmented data (Supplementary File 1 and 2 respectively) is provided here. https://github.com/ICBI/rembrandt-mri .
10.1038/s41597-022-01329-y,No custom codes or algorithms were used to generate or process the data presented in this manuscript.
10.1038/s41597-022-01463-7,Custom code (hAMRonization v1.0.3) was used to compare different AMR tools to predict AMR genes in genomic data and produce a standard report to compare results across tools (Fig. 2 .).This code is available at Github 29 .
10.1038/s41597-022-01424-0,No custom code was used for the generation or analysis of this data.
10.1038/s41597-022-01422-2,No custom code was made for the compilation and validation procedures in this dataset.
10.1038/s41597-022-01442-y,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01443-x,"The custom Mathematica (Wolfram Research, version 13.0) script used to subject the t stacks to a mean transformation has been described previously 18 (this function does not require any parameters)."
10.1038/s41597-022-01410-6,"Weekly water temperature and flow estimates at 5 arcminute were created using PCR-GLOBWB and DynWat. The PCR-GLOBWB model code is available at https://github.com/UU-Hydro/PCR-GLOBWB_model as well as https://doi.org/10.5281/zenodo.247139 32 , and the global input files are available through https://doi.org/10.5281/zenodo.1045339 33 . The DynWat code is available via https://github.com/wande001/dynWat . Monthly output from Wanders et al . 13 is available through https://doi.org/10.5281/zenodo.3337659 34 . Scripts used for figures in this paper are available through https://github.com/vbarbarossa/futurestreams_figures . Lastly, the repository https://github.com/JoyceBosmans/FutureStreams contains the scripts used to create the ecologically relevant derived variables, scripts to mask out grid cells with unrealistic values (see User Notes above), the script used to create Fig. 4 , an example configuration file for PCR-GLOBWB as well as a table with years in which warming levels are reached for each RCP and GCM combination. All the model runs were carried out on the Dutch national e-infrastructure Cartesius."
10.1038/s41597-022-01446-8,"The feature-tracking code used to derive ice velocities, GIV, is available on github and Zenodo 36 , 66 . All other code, including Google Earth Engine download scripts and the ice-thickness inversion code, is available on zenodo 67 ."
10.1038/s41597-022-01408-0,"Code used to generate the Technical Validation and Usage Notes figures is provided on the repository 49 , as indicated in section Data Records. R scripts were ran with R version 3.6.0 50 , using packages reshape2 51 , rgdal 52 , tidyverse 53 and viridis 54 ."
10.1038/s41597-022-01423-1,Not applicable to this dataset.
10.1038/s41597-022-01421-3,"The code availability for open access is given by the dataset 21 : (R codes) AHP analysis and result.html . These codes are written in R language (version 4.1.2, https://r-project.org ) to input the raw data (.xlsx), run the AHP algorithm and produce the final result summarized in Table 2 . For further description of the R codes, refer to the section Data Records."
10.1038/s41597-022-01433-z,Custom scripts were not used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-022-01440-0,"The creation of datasets was done with Matlab R2014b and R-4.1.2. Code used for data preparation and analysis was available at Jing, et al . 73 ."
10.1038/s41597-022-01437-9,The code used to produce our calculations is available at https://github.com/COVID-policy-response-lab/PPI-data .
10.1038/s41597-022-01431-1,"All CF-Seq code is open source and has been made available for use on GitHub under the MIT License [ https://github.com/samlo777/cf-seq.git ]. The application is also hosted on a server maintained by Dartmouth College and is accessible at the following web link [ http://scangeo.dartmouth.edu/CFSeq/ ]. In its current version, CF-Seq utilizes the following R package versions: shiny (1.6.0), shinydashboard (0.7.1), shinyjs (2.0.0), DT (0.19.1), plotly (4.9.4.1), ggplot2 (3.3.5), edgeR (3.34.1), KEGGREST (1.32.0), UniProt.ws (2.32.0), tidyverse (1.3.1), stringr (1.4.0)."
10.1038/s41597-022-01444-w,The code of the AnimalMetagenome DB has been uploaded to GitHub: https://github.com/boyNextDooooor/AnimalMetagenomeDB.git .
10.1038/s41597-022-01391-6,"We provide code in an open access repository that will assist in reviewing the database and examining patterns ( https://github.com/afilazzola/IcePhenologyDatabase ). Within the repository, we provide the Python code used for the creation of the dataset, quality assurance, and quality control. We included a set of visualization options for technical validation in Python. All Python code was conducted in Python Version 3.8.1 ( http://www.python.org ) using numpy 47 , pandas 48 , textract libraries. We also provide R code that was used as a second reviewer for technical validation. The code in the qaqc.r file was used to convert the dataset into “long” format where there is only one column each for ice-on and ice-off date. The same file may also be used by future users for a quick visualization of patterns within the lake ice database. All R code was conducted in R version 3.5.1 49 using the tidyr, dplyr, ggplot 50 , and broom packages. All code is freely available under the Massachusetts Institute of Technology license."
10.1038/s41597-022-01420-4,• Project name: EukaProt_in_PublicSoilMetag 31 • Project home page: https://github.com/CaroleBelliardo/EukaProt_in_PublicSoilMetag.git • Operating system(s): Platform independent • Programming language: Python3 • Other requirements: Python3.8 or higher • License: License: GNU General Public License v3.0
10.1038/s41597-022-01401-7,"The ATLAS v2 lesion segmentations were generated using ITK-SNAP version 3.8.0. Our protocols for lesion segmentation can be found on our Github ( https://github.com/npnl/atlas ). Code used to preprocess the dataset were adapted from the MINC-toolkit ( https://github.com/BIC-MNI/minc-toolkit ). T1w images were defaced using the “mri_deface” tool from FreeSurfer (v1.22) ( https://surfer.nmr.mgh.harvard.edu/fswiki/mri_deface ). PALS, our open-source software to perform lesion analyses, can be accessed at https://github.com/npnl/PALS . Finally, as part of the MICCAI ISLES 2022 challenge, we provide sample code on our Github ( https://github.com/npnl/isles_2022/ ) to assist users in getting started with the lesion segmentation challenge (e.g., code to obtain the data, load it, and save predictions in the format expected by our automatic evaluator)."
10.1038/s41597-022-01365-8,"All scripts used to obtain raw data, clean or process raw data, perform QA, and construct the database are available in the MMDB Processing Scripts folder at https://doi.org/10.23645/epacomptox.16674298 . Various versions of R and python were used in different project stages; the primary version for both data cleaning and building the database was R version 3.6.2. An example R script containing sample queries of MMDB by chemical and media is maintained in the Sample Queries folder. We will also maintain an SQL script (to be run in MySQL immediately after the MMDB dump file) to correct any identified curation mistakes in official MMDB releases in the MMDB Correction Scripts folder."
10.1038/s41597-022-01439-7,The scripts used to calculate firm level GHG emissions of wastewater treatment facilities are available in the Zenodo repository: https://doi.org/10.5281/zenodo.6052815 55 .
10.1038/s41597-022-01429-9,"The virtual research environment Kadi4Mat, its documentation and source code, can be found at https://kadi.iam-cms.kit.edu/ . The SurfTheOWL application (both source code and standalone executable) for deriving key-value pairs from the TriboDataFAIR Ontology is available at https://github.com/nick-garabedian/SurfTheOWL with its most up-to-date changes, while main updates are listed as versions in Zenodo 36 ."
10.1038/s41597-022-01432-0,The main software package underlying Lexibank is curated on GitHub ( https://github.com/lexibank/lexibank-analysed/tree/v0.2 ) and archived with Zenodo ( https://doi.org/10.5281/zenodo.5227817 ) 15 . Individual datasets belonging to the Lexibank wordlist collection are curated on individual repositories on GitHub (see our master list at https://github.com/lexibank/lexibank-analysed/blob/v0.2/etc/lexibank.csv ) and are also all archived with Zenodo (see https://zenodo.org/communities/lexibank/ ).
10.1038/s41597-022-01392-5,"Code of the post-refinement module, named MAGRE, is available at GitHub ( https://github.com/yosuken/MAGRE ). The options and parameters of all tools used for the analysis are described in the main text."
10.1038/s41597-022-01373-8,"Construction of the gridded data was performed using the R environment for statistical computing version 3.6.3. Python version 3.8.5 was also used. The code that describes the procedures (quality control, gap-filling, homogenization, spatial interpolation, and spatial downscaling) to obtain the gridded data of the meteorological subvariables and PISCOeo_pm is freely available at figshare 99 and GitHub ( https://github.com/adrHuerta/PISCOeo_pm ) under GNU public licence version 3."
10.1038/s41597-022-01445-9,"Code is available with the data records through Z enodo 38 (version 2). The contents provide access to the calculations integrated into R, including in- and output of the dataset with descriptors. Specifically, R scripts for charge balance calculations (.R), Full set of raw ion concentrations for the R scripts (.txt), Example of 25 raw ion concentrations for the R scripts (.txt), Full set of balanced outputs from the R script (.txt), Example of 25 balanced outputs from the R script (.txt). The code is available under a CC BY 4.0 license permitting redistribution and reuse with appropriate credit."
10.1038/s41597-022-01297-3,All custom codes used in this study have been uploaded to Github 93 .
10.1038/s41597-022-01405-3,We developed the heatmetrics R package to facilitate replication of these methods to other meteorological data sets. The package is available to download via figshare 48 .
10.1038/s41597-022-01355-w,"The code used to generate the two databases can be found at https://github.com/edbeard/pv_database . This code makes use of a new library that can be found at https://github.com/edbeard/dsc_db , which defines the logical processes used in the overall data-extraction algorithm. ChemDataExtractor-PV, the bespoke version of ChemDataExtractor, which contains additional models and parsers for the extraction of photovoltaic data, is located at https://github.com/edbeard/chemdataextractor-pv ."
10.1038/s41597-022-01435-x,Code to generate the summary statistics are available from the Github repository in the folder ‘notebooks’: https://github.com/OpenBioLink/ITO . The entire Github repository for the v1.01 release is archived on Zenodo 41 : https://doi.org/10.5281/zenodo.6566103 .
10.1038/s41597-022-01457-5,This study was supported in part through computational resources provided by the Institute for Cyber-Enabled Research at Michigan State University (ICER) . The following software was used to perform quality and expression analyses of the dataset: 1. FastQC v0.72 https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . 2. MultiQC v1.9 https://multiqc.info/ . 3. HISAT2 v2.2.1 http://daehwankimlab.github.io/hisat2/ . 4. SAMtools v1.9 http://www.htslib.org/ . 5. featureCounts v2.0.1 https://www.rdocumentation.org/packages/Rsubread/versions/1.22.2/topics/featureCounts . 6. DESEQ v2.11.40.6 https://bioconductor.org/packages/release/bioc/html/DESeq2.html . 7. R v3.6.3.
10.1038/s41597-022-01414-2,The presentation script is shared within the BIDS formatted database.
10.1038/s41597-022-01428-w,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-022-01400-8,"Peri-implant bone composition analyses were performed using MicroView 3D Image Viewer, an open source, dual-licensed, 3D image viewer ( http://microview.sourceforge.net/ ) and OsiriX 8.5 software ( https://www.osirix-viewer.com/ ) available commercially."
10.1038/s41597-022-01471-7,The data was processed in Python and ArcGIS. The VIC model code could be downloaded from http://uw-hydro.github.io/ .
10.1038/s41597-022-01441-z,"The code used for all automatic processes described in this paper, involving sampling, collection, processing, and validation of data, is available at https://github.com/ricbl/eyetracking 28 The software and versions we used were: MATLAB R2019a, Psychtoolbox 3.0.17 25 , 26 , 27 , Python 3.7.7, edfapi 3.1, EYELINK II CL v5.15, Eyelink GL Version 1.2 Sensor = AC7, EDF2ASC 3.1, librosa 0.8.0 40 , numpy 1.19.1 41 , pandas 1.1.1 42 , 43 , matplotlib 3.5.1 44 , 45 , statsmodels 0.12.2 33 , shapely 1.7.1 46 , scikit-image 0.17.2 47 , pyrubberband 0.3.0, pydicom 2.1.2 48 , pydub 0.24.1, soundfile 0.10.3.post1, pyttsx3 2.90, pillow 8.0.1 49 , scikit-learn 0.23.2 50 , nltk 3.5 51 , syllables 1.0.0, moviepy 1.0.3 52 , opencv 3.4.2 53 , Ubuntu 18.04.5 LTS, espeak 1.48.04, joblib 1.1.0, ffmpeg 3.4.8, and rubberband-cli 1.8.1."
10.1038/s41597-022-01460-w,Computer codes to fit the parameters of the TOPMODEL-based diagnostic model are publicly available on GitHub ( https://github.com/yixixy/Wetland_simulation_by_TOPMODEL ) 49 .
10.1038/s41597-022-01378-3,All the codes are published through BARI’s GitHub account (user: @BARIBoston; https://github.com/BARIBoston ).
10.1038/s41597-022-01476-2,The atlas is available at and all the image data used in this paper are available at https://www.nitrc.org/projects/adultatlas . Code for the atlas generation can be found at https://github.com/bieqa/AdultBrainAtlas .
10.1038/s41597-022-01399-y,The commercially available GeneData software ( https://www.genedata.com/ ) was used for LC-MS data analysis.
10.1038/s41597-022-01449-5,The data repository contains a jupyter notebook that provides the code to access the input GSW dataset and process it using the ORBIT framework.
10.1038/s41597-022-01462-8,"The statistical analysis and modeling employed for examples given in the Usage Notes was done in R 80 and included R packages developed within the Pandora & IsoMemo initiatives 59 , 60 , 61 , 82 , 83 . Source code for spatiotemporal models (AverageR, TimeR, OperatoR, KernelTimeR, and LocateR) is available for download at GitHub ( https://github.com/Pandora-IsoMemo/iso-app ) together with the source code for ReSources ( https://github.com/Pandora-IsoMemo/resources ). These can be run locally ( https://github.com/Pandora-IsoMemo/drat ) as Shiny apps 81 . For modeling reproducibility, a full description of model options is given in Supplementary Information S5 . The MATILDA data community where CIMA is stored is part of the Pandora data platform that is based on the CKAN open source data management system ( https://ckan.org/ ). This is hosted by the Max Planck Computing and Data Facility."
10.1038/s41597-022-01448-6,"The code implementation was performed in Python using a Jupyter notebook. The Python scripts to perform data preprocessing, visualization and technical validation are available at the sathorndata GitHub repository. ( https://github.com/EEM0N/sathorndata.github.io/blob/main/sathorndata.ipynb .)"
10.1038/s41597-022-01469-1,"The codes are provided in a figshare 39 repository and are also available in a public github repository https://github.com/SoccerNet/ ; the last also provides codes for different baselines for solving tasks, such as camera calibration or re-identification. Those data are used in the public SoccerNet Challenge 2022 for the tasks of camera calibration, pitch localization and player re-identification. Finally, all information about the dataset, the tasks and the challenges can be found on the https://www.soccer-net.org website."
10.1038/s41597-022-01383-6,"The data cleaning notebook and list of variables can be obtained freely here: https://doi.org/10.17605/OSF.IO/36TSD 31 . The data was imported and cleaned using the R software qualtRics, data.table, tidyverse, and multicon. Before analysing the data, it should be noted that invalid cases were excluded and the response options for some variables were recoded to numeric values measuring the degree of agreement (see data cleaning above for details). In some of these options, a neutral value was added to the response options and scored in two different ways. For data quality reasons, we also employed an attention check and filtered data in regard to this check."
10.1038/s41597-022-01412-4,"Python and R code and generated CSV files are available on HydroShare 35 . For the NEON data processing packages, refer to the NEONiso package 24 found at on CRAN or at https://doi.org/10.5281/zenodo.3836875 ."
10.1038/s41597-022-01455-7,"A step-by-step guidance and the source-code for dataset generation and machine learning benchmarks can be found on GitHub 28 . Specifically, we provide ready-to-use Pytorch data loaders with both data processing and splitting included, and also share the code of evaluators to support fair comparison among different ML-based algorithms, of which the dependencies and usage are also descibed on Github 28 ( https://github.com/tamu-engineering-research/Open-source-power-dataset )."
10.1038/s41597-022-01459-3,"Fortran code: getStat.f, getStatDir.f, getHsEx.f. The Fortran code developed to calculate the COWCLIP statistics can be requested via the COWCLIP website ( https://cowclip.org/data-access ). The code - as described in the Data Generation Method section, consists of a set of code commands (getStat.f, getStatDir.f and getHsEx.f) which can be compiled with a Fortran compiler, linked against netCDF4 and HDF5 libraries. The documentation for setup, usage and requirements for the code is described within the technical reports 29 , 30 which complement this data descriptor. These commands can be executed by COWCLIP contributors to generate the set of ocean wave statistics from their raw simulations. With the specific purpose of sharing in an open data format, and adhering to relevant data standards, the processed data is given in netCDF format, the global metadata attributes from the submitted netCDF data recorded, and additional information added where possible to ensure both CF Conventions & Attribute Convention for Dataset Discovery (‘ACDD’) standards compliance."
10.1038/s41597-022-01425-z,"All data processing steps were performed using native tools and/or customized batch processing within ESRI’s ArcGIS 10.4 software package 50 in a dedicated computing setup (64-bit processing). The two core tools applied were ‘Zonal Statistics’ and ‘Flow Accumulation’. To support repetitive tasks of this work, a multitude of adjusted batch routines were developed as needed, mostly defining input and output path names for the standard tools and to handle internal object IDs. No stand-alone programming code was created that allows automatic processing of new data into the format of LakeATLAS. This is in alignment with the premise of our work, i.e., to produce standardized data by applying tedious, individual, and customized GIS steps specific to every input dataset so that other users do not have to repeat these time-consuming manual iterations."
10.1038/s41597-022-01452-w,We did not use any custom code to process the data described in the manuscript.
10.1038/s41597-022-01451-x,NORA3-WP is created using MatLab (version 2018a). A short description of the functions used to create the variables in NORA3-WP can be found in Table 4 . The matlab scripts are permanently archived at zenodo: https://doi.org/10.5281/zenodo.6138696 .
10.1038/s41597-022-01467-3,All of the code used to create and extract the CO 2 -USA synthesis data set is maintained in an open access GitHub repository: https://github.com/uataq/co2usa_data_synthesis .
10.1038/s41597-022-01461-9,"The data processing and analysis were performed with Python 3. The code responsible for processing and analyzing the maps and the images is published under open access 40 . The used colormap for the temperature is individually defined and provided as well. It is suited to visualize temperature structures for Arctic winter conditions. In case of specific requests, please contact the corresponding author directly."
10.1038/s41597-022-01482-4,The source code is included as part of the dataset 30 . All code is implemented using MATLAB (MathWorks Inc).
10.1038/s41597-022-01465-5,All software used in this study was published in peer-reviewed journals. Additional information was described in detail in the Material and Methods section
10.1038/s41597-022-01487-z,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-022-01456-6,"No code was used to generate, test, or process the current dataset."
10.1038/s41597-022-01475-3,All the codes used to clean the raw datasets have been uploaded to GitHub for public use ( https://github.com/yapanliu/ashrae-ob-database ). The raw datasets are also open to the public on request.
10.1038/s41597-022-01450-y,"The code that exports the QuPath v0.1.2 manual annotations is accessible at https://github.com/qbeer/qupath-binarymask-extension . The required system memory is at least 8GB. The annotations are saved as binary masks for each corresponding category and at a predefined sub-sampling rate based on spatial size. All these metadata are stored in the filename with the location of the top-left pixel in the original whole slide at the highest resolution. The patch generating and CNN modeling code is available at http://github.com/patbaa/crc_data_paper . The code processes the whole slides in a sequential manner, patch-by-patch, and does the above-described filtering steps (Algorithm 1) while it also assigns all the corresponding labels to patches. The code for the baseline CNN training also can be found in the same repository."
10.1038/s41597-022-01490-4,The source code of CESM1.2.2 can be accessed at https://www.cesm.ucar.edu/models/cesm1.2 . The scripts used to generate the datasets and figures have been written using the NCAR Command Language version 6.6.2 (NCL6.6.2) 48 and are available in the Figshare repository 40 .
10.1038/s41597-022-01484-2,The custom Jupyter Notebook scripts for some exploratory plots in Technical Validation section can be found in the “Code” folder of our figshare repository 31 . The acquisition program can be found in https://github.com/xuhui-hu/SciData2021 .
10.1038/s41597-022-01458-4,"The data analysis methods, software and associated parameters used in this study are described in the Methods section. Default parameters were applied if no parameter was described. No custom scripts were generated in this work."
10.1038/s41597-022-01454-8,We did not use the custom code for the data management.
10.1038/s41597-022-01468-2,"The Interactive Data Language (IDL) software was used to ingest the GEFS forecast data and the CHIRPS rainfall estimates, and to create the CHIRPS-GEFS output. IDL is a proprietary scripting language with a well-developed library of functions for interacting with large raster data sets, statistical analysis, and intuitive visualizations. IDL code used in producing CHIRPS-GEFS is available through the CHIRPS-GEFS 47 data repository, at https://doi.org/10.15780/G2PH2M . Users can follow a link on that page to a directory that contains IDL pro scripts used in operational CHIRPS-GEFS production: (1) Retrieve GEFS GRIB forecast files from the NOAA NCEP GEFS website for days 1–10 from the 0.25 degree resolution output and days 11–16 from the 0.5 degree output; (2) Create 1-day and 16-day forecast precipitation accumulations from these files at 0.25 degree resolution; (3) For each pixel, identify the percentile rank of the current forecast compared to post-2000 reforecast and available “real time” GEFS for this time period, and then re-grid these to 0.05 degree to match CHIRPS resolution; (4) Produce a CHIRPS-unbiased version of the 16-day GEFS forecast by, at each pixel, sorting CHIRPS 16-day amounts for the same time period and identifying the CHIRPS amount matching the forecast percentile; and (5) Using these CHIRPS-GEFS 16-day forecast totals, produce 16 1-day totals based on the ratios of GEFS forecasts for each day versus the 16-day period."
10.1038/s41597-022-01477-1,The Matlab code used for the PSHG image sets fitting with the single-axis molecule model for collagen and the FIJI macros used to generate the entire dataset are available at: https://doi.org/10.17605/OSF.IO/K2Z8G 45 .
10.1038/s41597-022-01434-y,"The majority of data processing was done using Microsoft Excel 2010® version 14.0.4734.100. Python v3.8 was used for the calculations of the percentage size fractions of Chl-a, seasonal averaging ( i.e . binning values into a monthly average for each sampled depth) and scaling the figure colormaps. The specific code written for this manuscript can be found within the plotting script at the following open source GitHub repository: https://github.com/bjmcnabb/Saanich_Inlet ."
10.1038/s41597-022-01466-4,"No code has been provided because the 3D data were reconstructed using commercial SfM software package of Agisoft PhotoScan (Agisoft LLC, St. Petersburg, Russia), ver1.4.2."
10.1038/s41597-022-01480-6,The code used to generate the main data table is available at https://doi.org/10.6084/m9.figshare.c.5656993.v1 . The SQL script included with the data DOI demonstrates the steps that we took to produce this table from SafeGraph’s raw data after it has been read into a database for processing.
10.1038/s41597-022-01479-z,"The code used to generate figures is available at https://github.com/jbeninde/CaliPopGen . As the data was taken from the published literature manually, no additional code was used to extract data."
10.1038/s41597-022-01494-0,"The calculation of data is mainly done by Excel 2017, and there is no special code."
10.1038/s41597-022-01453-9,The R codes and packages used to create the plots in this paper can be accessed at github.com/cjabradshaw/AustralianSharkIncidentDatabase. These codes can be used to recreate summaries reported in this paper.
10.1038/s41597-022-01368-5,"The code for digitizing and extracting the data from the literature plots is the open-source code WebPlotDigitizer version 4.5 25 , which is freely accessible."
10.1038/s41597-022-01473-5,Code used to generate the figures that describe this dataset can be found on GitHub at https://github.com/RiesLabGU/LepTraits . All data are available from a FigShare repository 30 .
10.1038/s41597-022-01220-w,We have included the code necessary to reproduce all our work 68 . We have broken the code into three scripts: • 01_Biomass_density_estimation.R: Converts raw data files into tidy formats and generate summary estimates of species biomasses. Biomass density estimates were conducted in R 52 . • 02_Link_Assignment.R: Generates encounter and compatibility filters and assigns feeding links to all consumers. Resource filters were assigned with the tidyverse package 57 . • 03_Web_Assembly.R: Assembles food webs for each burn category into analyzable network format. Network assembly was conducted with the igraph package 98 . R scripts are in the Assembly_workflow folder available in the Dryad Digital Repository 68 : https://doi.org/10.5061/dryad.rv15dv47g .
10.1038/s41597-022-01503-2,There was no custom R code produced during the collation and validation of this dataset.
10.1038/s41597-022-01505-0,The source code of this work can be downloaded from https://github.com/Peiliang/HistoML .
10.1038/s41597-022-01509-w,"Project name: EEG dataset for RSVP and P300 Speller Brain-Computer Interfaces. Project home page: https://github.com/KyunghoWon-GIST/EEG-dataset-for-RSVP-P300-speller . Operating system(s): Windows, MAC. Programming language: MATLAB, Python. Other requirements: MATLAB r2020a or higher, Python 3.6 or higher. License: MIT License. We note that the results of the article were produced using MATLAB. We provide MATLAB and Python scripts, and users can use Python to extract features and evaluate P300 speller performance as well, but the result may differ slightly from MATLAB."
10.1038/s41597-022-01298-2,Our source code to generate the new versions of our knowledge graph is publicly available at https://github.com/dice-group/COVID19DS and is maintained in parallel with the knowledge graph.
10.1038/s41597-022-01514-z,No custom computer code or algorithms were used to generate or process the data presented in this manuscript.
10.1038/s41597-022-01511-2,All figures have been produced using R (version 4.1.2) and RStudio (version 2021.09.1 + 372). The scripts used are available in a GitHub repository 35 and are archived on Zenodo 29 .
10.1038/s41597-022-01512-1,"Statistics (one sample t-test) was performed using the Perseus software version 1.6.14.0 39 . PCA was performed and plotted using the python bioinfokit package 55 . Pearson correlation, hierarchical clustering analysis and subsequent similarity matrix were performed and created using the Phantasus software version 1.11.0 ( https://artyomovlab.wustl.edu/phantasus/ ). Volcano plots were created using VolcaNoseR (mirror R2) 40 . Network analysis was performed using the String application on Cytoscape version 3.9.0 41 . Heatmap and bar charts were created using GraphPad Prism version 9.2.0. Images were stored on Perkin Elmer Columbus server version 2.9.1. No custom code was used to process the data in this manuscript."
10.1038/s41597-022-01507-y,"We would like to release the following software, algorithms and trained vessel segmentation model that used in the construction process of RETA benchmark. They can be found from the “codes”, “models”, “software” and “supports” folders. • CARL software . It could help users to investigate annotation details of vascular structures in a zoomed and magnified view. Users are also able to visualize and evaluate segmentation results of retinal anatomical structures and lesions using CARL software. It can work as a model auditing tool and is available in the “software” folder. • Image transformation and enhancement . The “codes” folder contains essential MATLAB source code of our image preprocessing algorithm. Users can use “IDRiD_cropImage.m” to crop original colour images of the IDRiD challenge. “IDRiD_restoreImage.m” is used to upsample labelled vessel masks. A script file named as “CARL_image2mat.m” can transform users’ private images into our custom mat files. • Vessel segmentation model . A pretrained vessel segmentation model and corresponding model inference code are in the “models” folder. Please read usage documentation before executing model inference. It may take some time to set up the running environment. • Vessel centreline extraction . We borrowed a Python implementation of the skeletonization method from “scikit-image” Python package to extract vessel centrelines. The parameter “method” of the used skeletonize function is “lee” in our experiment. • Data augmentation . We utilised basic image rotation and flipping to create a large-scale image set for model training. A multi-thread image augmentation code implemented in Python programming language is saved in the “supports” folder."
10.1038/s41597-022-01495-z,The files containing the codes for data analysis are available at Figshare.com 28 .
10.1038/s41597-022-01510-3,"The script to generate PharMeBINet is available from Zenodo 52 and from GitHub https://github.com/ckoenigs/PharMeBINet . The repository contains python (3.8.5) programs and bash (4.4.20) scripts to execute all python programs, download databases, and execute the Neo4j Cypher tool. The website is available at https://pharmebi.net . The download of PharMeBINet is provided on the website."
10.1038/s41597-022-01513-0,No custom code has been used during the generation and processing of this dataset.
10.1038/s41597-022-01501-4,No code was used to generate this data.
10.1038/s41597-022-01483-3,See Usage notes part for information on code availability to compute or process the shared files.
10.1038/s41597-022-01508-x,"Custom scripts used for processing the sBLISS, RNA-Seq, and Hi-C datasets are available at https://github.com/BiCroLab/NatSciData_Neuro . Custom scripts used to analyze the immunofluorescence images are available at https://github.com/elgw/sci_data_20220516 ."
10.1038/s41597-022-01485-1,"No code is used in this study. Figures were produced using R statistical software 39 and packages 40 , 41 , 42 ."
10.1038/s41597-022-01496-y,"We used freely available code from the following packages: maptools 51 , obistools 56 and WoRMS 50"
10.1038/s41597-022-01489-x,No custom code was used to generate or process the data.
10.1038/s41597-022-01532-x,We developed a more comprehensive open-source KNIME workflow for the curve fitting calculations 17 .
10.1038/s41597-022-01492-2,"The code is available at https://github.com/MGEdata/auto-generate-corpus . When researchers extract their own corpus, they only need to write labeling functions that meet the characteristics of their own corpus in the framework we write, which is very simple to use."
10.1038/s41597-022-01474-4,"Python code to access, query, visualize and analyze the Agricultural SandboxNL database is distributed, with the dataset and accompanying documentation. GitHub repository to share all GEE/python scripts and information used to create Agricultural SandboxNL database. https://github.com/ManuelHuber-Github/Agricultural-SandboxNL ."
10.1038/s41597-022-01541-w,There is no customized code in generation or processing of datasets.
10.1038/s41597-022-01464-6,"The codes for eye-movement measure calculating, descriptive statistics and quantitative validation are available on OSF repository 31 . There were two R script files. The file named “Main.R” contained the R codes for data calculation and validation, and all of the functions used are contained in the file named “functions.R”."
10.1038/s41597-022-01530-z,"All analyses were conducted on Linux systems. The version and code and parameters of the main software tools are described below. (1) LongQC, version 1.2.0c, parameters used: default. (2) Filtlong, version 0.2.1, parameters used: min_length 1000, keep_percent 90, split 100, mean_q_weight 10. (3) KMC, version 3.1.1, parameters used: k21, ci1, cs10000. (4) GenomeScope, version 2.0, parameters used: ploidy 3, kmer_length 21. (5) MaSuRCA, version 4.0.5, parameters used: LIMIT_JUMP_COVERAGE = 300, CA_PARAMETERS = cgwErrorRate = 0.15, FLYE_ASSEMBLY = 0. (6) HaploMerger2, version 20180603, parameters used: default; hm.batchA and hm.batchB. (7) QUAST, version 5.0.2, parameters used: default. (8) BUSCO, version 5.2.2, parameters used: lineage_dataset sauropsida_odb10. (9) RepeatMasker, version 4.1.1, parameters used: engine ncbi, xsmall, Database: Dfam with RBRM. (10) RepeatModeler, version 2.0.1, parameters used: default, Database: The scaffolds assembled with MaSuRCA and HaploMerger2. (11) Augustus, version 3.4.0, parameters used: species = Database trained with BUSCO, alternatives-from-evidence = true, hintsfile = Output of RepeatMasker. (12) Diamond, version 2.0.13, parameters used: more-sensitive, max-target-seqs. 1, evalue 1e-5."
10.1038/s41597-022-01491-3,The Python library MarIQT is available as open source and under rapid development. Contributions in the form of issue submissions and merge requests are welcome. https://gitlab.hzdr.de/datahub/marehub/ag-videosimages/mariqt .
10.1038/s41597-022-01522-z,"Python code used to generate the irrigation maps is available from the figshare repository 60 . The software used in this work include: ∙ ArcGIS 10.2 ∙ Python 2.7, numpy 1.16.6, pandas 0.19.0, scipy 1.2.3, scikit-learn 0.20.3, matplotlib 1.1.1"
10.1038/s41597-022-01523-y,All commands and pipelines used for the genome and transcriptome analyses were performed according to those manuals and protocols of the applied bioinformatics software.
10.1038/s41597-022-01493-1,"Geospatial processing and analysis were done using ESRI ArcGIS Pro (version 2.9), ArcMap (version 10.8), Python (versions 2.7 & 3.6) programming language, and Microsoft Excel for further data analysis, graphic and presentation. The Python programming code 66 used to run the calculation of ET 0 and AI is provided and available online at: https://doi.org/10.6084/m9.figshare.20005589 ."
10.1038/s41597-022-01535-8,"The software used for the SPECT/CT data reconstructions was the commercial Dosimetry Toolkit Package (GE Healthcare, Milwaukee, USA). No custom codes have been designed and used for this study."
10.1038/s41597-022-01538-5,"The code used to preprocess the data and plot results is openly available on GitHub ( https://github.com/andlab-um/MT-EEG-dataset ). For more details about code usage, please refer to the GitHub repository."
10.1038/s41597-022-01549-2,"The code used for the N 2 O flux calculation is open-source and was published in the Comprehensive R Archive Network (CRAN). Each used script is mentioned in Table 8. All codes used to calculate NH 3 fluxes are stored together with the StaPlaRes-DB-Thuenen on the repository OA 20 . The model METVER was developed by Böttcher, et al . 44 from the German Weather Service (DWD). The model source code is published by Bach 45 (see Appendix) and can be downloaded from OA repository 20 as well."
10.1038/s41597-022-01529-6,"The code used to generate this data is freely available on GitHub under the MIT license 82 . Details on how to use the scripts to generate the data are provided in the Usage Notes. Some of the scripts utilize helpful components of the Reaction Mechanism Generator, such as RMG-Py, RMG-database, and the Automatic Rate Calculator (ARC) 83 . All related software is open-source under the MIT license and freely accessible on GitHub. For RMG-Py, checkout the qchem_parser branch, and for RMG-database, checkout AEC_BAC . The GitHub version commit string was ea2eb625fb1dcc6892ef6ddd5d7fdc96abf477e1 for ARC on the main branch."
10.1038/s41597-022-01470-8,"All parameters required for using Illumina BeadStudio software to generate summary probe profile file of the present dataset are publicly available in “The BeadStudio Gene Expression Module v3.4 User Guide” from Illumina 26 . It includes information about the data that can be exported from the “Group”, “Sample”, and “Control” Probe Profile tables. The probe profile tables include the average signal intensities (AVG_Signal) detected from all the beads of a given beadtype which target a specific sequence in the mouse genome. The ‘Detection Pval’ is the p-value that represents the chance that the target sequence signal was distinguishable from negative controls. Data analysis in this manuscript was performed by using Agilent 2100 Bioanalyzer 24 and Partek Genomics Suite v6.6 27 software, and their default protocols. Input files generated by BeadStudio and used for data analysis 41 and tables with analysis output data 42 are available at Figshare ."
10.1038/s41597-022-01548-3,The code used in this study can be accessed alongside the final dataset.
10.1038/s41597-022-01545-6,"The code that was used to analyze patient data, read EDF files, run baseline sleep stage classifier, and generate figures and tables in this paper is published at https://github.com/liboyue/sleep_study ."
10.1038/s41597-022-01502-3,"The code used for the data processing, the technical validation and the calculation and visualisation of the z-normalised demand is available on the repository Zenodo (ref. 16 ) at https://doi.org/10.5281/zenodo.6563114 under Commons Attribution 4.0 International (CC-BY 4.0) license ( https://creativecommons.org/licenses/by/4.0 ). All code is based upon the R programming language version 4.05 25 . The upload is structured in two folders named ‘01_Data’ and ‘02_R_code’. Within the folder ‘02_R_code’, the code is structured into three folders: ‘01_main’, ‘02_technical_validation’, and ‘03_additional’. The first folder contains the code necessary to process the original data as described in ‘Methods’ to obtain the complete equidistant dataset. The second folder contains the code used for the analyses described in ‘Technical Validation’ including their initial investigations. The third folder contains the code necessary for the in ‘Usage Notes’ mentioned calculation and visualisation of the z-normalised demand and the mentioned initial investigations of imputation methods. Each folder contains its own R project, the related code files, and a “readme.pdf” file describing the purpose of each R code file. The user must create a free API key to the weather data used for‘ Hotdeck’ and ‘RF’ imputation methods. The steps, therefore, are described here: https://confluence.govcloud.dk/display/FDAPI/User+Creation , the R code to obtain the weather data is provided. The R package renv 34 was used for package management for each R project. If the user has R and the package renv installed, the command renv::restore() installs all necessary R packages in the applied version. If the user wishes not to use the package renv, the file “renv.lock”, which can be opened with a common text editor, includes a list of all used packages, their version, and their dependencies."
10.1038/s41597-022-01528-7,"All scripts are published at Zenodo under the identifier: https://doi.org/10.5281/zenodo.6592148/ . To ensure that the code runs smoothly, use the updated versions of R and all Comprehensive R Archive Network (CRAN) packages declared in the repository. We used R version 4.0.3 40 ."
10.1038/s41597-022-01498-w,The code used for loading and processing DICOM images is based on the following open-source repositories: Python 3.7.0 ( https://www.python.org/ ); Pydicom 1.2.0 ( https://pydicom.github.io/ ); OpenCV-Python 4.2.0.34 ( https://pypi.org/project/opencv-python/ ); and Python hashlib ( https://docs.python.org/3/library/hashlib.html ). The code for data de-identification and outlier detection was made publicly available at https://github.com/vinbigdata-medical/vindr-cxr .
10.1038/s41597-022-01539-4,The software underlying Community Inclusion Currencies and the Sarafu implementation are available in public repositories at https://gitlab.com/grassrootseconomics . The data extraction and processing code described in the Anonymization section is available in a separate public repository at https://github.com/grassrootseconomics/dashboard (especially: sarafu_user_db_test.py ). The script invoked by staff at GE to run the data extraction and processing code is included in the documentation folder that accompanies the dataset via the UK Data Service ReShare repository 13 . Supplementary File 1 is a code notebook that provides a provisional labeling for disbursement/reclamation transactions and a procedure for noting known artefacts in the data. Supplementary File 2 reproduces the technical validation.
10.1038/s41597-022-01486-0,All the code used to generate the figures in this manuscript is available on the following GitHub repository: https://github.com/nussaibahrs/ARTD .
10.1038/s41597-022-01520-1,The code for generating the RTSIF is available at https://github.com/chen-xingan/Reconstruct-TROPOMI-SIF.git .
10.1038/s41597-022-01540-x,The source code for processing the DMSP-OLS Stable Light dataset to produce the CCNL dataset is available at https://doi.org/10.5281/zenodo.6100284 44 .
10.1038/s41597-022-01478-0,MISSING
10.1038/s41597-022-01533-w,"The code used to prepare the EPIC-QA dataset is provided at https://github.com/h4ste/epic_qa , and a Python script for computing the evaluation metrics reported in the technical validation section of this manuscript is provided with the dataset."
10.1038/s41597-022-01552-7,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01527-8,"All data contained in the IonSolvR database was generated with the DFTB+ program (v. 19.1) 33 . This code is freely available via the DFTB+ website ( https://dftbplus.org/download/dftb-stable ) and GitHub ( https://github.com/dftbplus/dftbplus ) via the GNU Lesser General Public Licence (LGPL-3). The 3ob-3-1 parameter set 44 , 45 , 46 can be freely downloaded from the dftb.org website ( https://dftb.org/parameters/download )."
10.1038/s41597-022-01547-4,"All the code and geoprocessing scripts used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) 50 from the repository www.github.com/fineprint-global/app-mining-area-polygonization 27 . The processing scripts were written in R 47 , Python 48 , and GDAL (Geospatial Data Abstraction Library 51 ). The web application to delineate the polygons was written in R Shiny 52 using a PostgreSQL 53 database with PostGIS 54 extension for storage. The full app setup uses Docker 54 containers to facilitate management, portability, and reproducibility."
10.1038/s41597-022-01550-9,"The mesoscale eddy automatic detection code is available to the public, and can be downloaded from https://doi.org/10.6084/m9.figshare.19802062.v1 55 . It should also be noted that the vector geometry eddy detection algorithm has regional differences, with results deteriorating in coastal zones and near islands, but is still highly accurate for the open ocean. All data processing and calculations in the process of constructing the dataset are completed by MATLAB R2016b ( http://www.mathworks.com ). Considering user-friendliness, the eddy automatic detection code is currently being converted into a Python format, but due to the huge workload, the code conversion work is still in progress, and the dataset will be uploaded and updated in the future."
10.1038/s41597-022-01542-9,"All Python code to re-run the technical validation described in this report can be found on our Github: https://github.com/neuralinterfacinglab/SingleWordProductionDutch . The code relies on the numpy 81 , scipy 82 , pynwb 83 , scikit-learn 84 and pandas 85 packages."
10.1038/s41597-022-01555-4,"In favor of transparency and reproducibility, but also in line with the scientific data principles of Findability, Accessibility, Interoperability, and Reusability (FAIR) 45 , we have made the tools used to generate the data for this study publicly available 38 . Specifically, the CaPTk platform 22 , 23 , 24 , version 1.8.1, was used for all the preprocessing steps. CaPTk’s source code and binary executables are publicly available for multiple operative systems through its official GitHub repository ( https://github.com/CBICA/CaPTk ). The implementation and configuration of the U-Net with residual connections, used in this study, can be found in the GitHub page of the Generally Nuanced Deep Learning Framework (GaNDLF), version 0.0.14 ( https://github.com/CBICA/GaNDLF ). Finally, ITK-SNAP 33 , was used for all the manual annotation refinements."
10.1038/s41597-022-01563-4,"Raw data, as well as R-code for cleaning, are freely available at: https://doi.org/10.17632/sv95c7ydpy 16 ."
10.1038/s41597-022-01543-8,"A static version of the SARS-ANI Dataset and related files that accompany the dataset (metadata, R code, archived reports) have been deposited on Zenodo 38 and are available at https://doi.org/10.5281/zenodo.6442730 , for public access. A live version of the dataset (together with the related files) is accessible on GitHub at https://github.com/amel-github/sars-ani . Please refer to the README file in the code release for further instructions."
10.1038/s41597-022-01544-7,All analytical code used for processing and technical validation is available on the Benayoun Laboratory GitHub repository ( https://github.com/BenayounLaboratory/Neutrophil_scRNAseq_2022 ). The provided R code was run and tested using R 4.1.2 27 .
10.1038/s41597-022-01526-9,The code used to produce and process these datasets was developed using Matlab. All of the routines used to develop the A1 and B1 data products are available in an open access Zenodo repository 40 .
10.1038/s41597-022-01500-5,There is no specific code developed to access these data. Users can use and process the data in software of their choice.
10.1038/s41597-022-01481-5,All collected data and related soil characteristics are provided online for reference and are available at https://doi.org/10.5281/zenodo.5547338 37 . The R code used to create the GSHP database is available on Github ( https://github.com/ETHZ-repositories/GSHP-database ).
10.1038/s41597-022-01506-z,"The ABC_BPMDS is available without any restrictions under the http://www.zenodo.org 28 URL ( https://doi.org/10.5281/zenodo.6384343 ). In addition, a detailed curation protocol including a GraphPad Prism file are provided under the https://www.zenodo.org 50 URL ( https://doi.org/10.5281/zenodo.6405752 ). All information is also available on the http://www.panabc.info website and the use is free of charge."
10.1038/s41597-022-01515-y,This study does not use custom code.
10.1038/s41597-022-01570-5,Python and R source code to download and process the PlanetScope imagery and generate the product can be obtained through a public repository at https://github.com/BU-LCSC/PLSP . R source code for generating the figures in the Technical Validation section is also available on the same repository.
10.1038/s41597-022-01562-5,"All custom software, including the python code to transfer csv-formatted data to JSON and to validate JSON formatted data, are freely available at https://github.com/Defining-Our-Research-Methodology-DORy/3D-Microscopy-Metadata-Standards-3D-MMS ."
10.1038/s41597-022-01566-1,"Data processing, visualization, and random forest development were conducted using pre-existing libraries, particularly the randomForest package 33 and ggplot2 package 37 (for density plots and stacked bar plots) in the R programming environment, whereas spatial mapping was conducted in ArcMap 10.7.1, as indicated in the methods and validation. Therefore, no custom code was used for this study."
10.1038/s41597-022-01559-0,"The code used for checking, cleaning and analysing the data is available in the open GitHub repository https://github.com/Between-the-Fjords/funcab_data , of which a versioned copy is available at Zenodo 54 ."
10.1038/s41597-022-01497-x,"The Matlab code provided with this article includes the code for fetching the fingertip force data of all the fingers and thumb displacement data from the dataset file. The nomenclature of the Matlab file is forceanddisp_dataplots.m. This code helps to average the normal forces, tangential forces of individual fingers separately and thumb displacement data across the trials and participants. Further, the code plots the averaged normal and tangential force data of the individual fingers and thumb. The Matlab software version used for this code was MATLABR2016b. We had also provided the code for preprocessing the dataset and it is named as filter_code.m. The Matlab code for the moment computation of the data collected from experimental handle-1 of the first experiment is given separately and it is named as momentcomputation.m. Matlab codes are available in figshare along with the raw dataset 21 ."
10.1038/s41597-022-01551-8,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01560-7,"In line with the scientific data principles of Findability, Accessibility, Interoperability, and Reusability (FAIR) 123 , the tools used throughout the generation of these data are publicly available. Specifically, we have used the Insight Toolkit (ITK) 124 to convert the raw DICOM files to the NIfTI file format 49 , and when this mechanism did not work we have used the dcm2niix software (version 1.0.20200331) 125 . All image registrations were performed using the ‘Greedy’ registration algorithm ( https://github.com/pyushkevich/greedy ) 52 , a CPU-based C++ implementation of the greedy diffeomorphic registration algorithm 53 . ‘Greedy’ is integrated into the ITK-SNAP segmentation software 120 , 121 ( https://www.itksnap.org/ , version: 3.8.0, last accessed: 27/May/2020), as well as into the Cancer Imaging Phenomics Toolkit (CaPTk) 35 , 59 , 60 ( www.cbica.upenn.edu/captk , version: 1.8.1, last accessed: 11/February/2021). For the defacing of the acquired scans we used the “mri_deface” tool ( https://surfer.nmr.mgh.harvard.edu/fswiki/mri_deface ), and for the brain extraction we used BrainMaGe version 1.0.4 58 ( https://github.com/CBICA/BrainMaGe ). In addition, the CaPTk platform 35 , 59 , 60 , 108 , 109 , version 1.8.1, was used for all the preprocessing steps, as well as for obtaining the perfusion derivatives, and generating the output DICOM files ( https://cbica.github.io/CaPTk/ht_utilities.html ) after defacing of the data. The implementation producing the diffusion derivatives will be available in CaPTk v.1.9.0. CaPTk’s source code and binary executables are publicly available for multiple operating systems through its official GitHub repository ( https://github.com/CBICA/CaPTk ). The implementation and configuration of the pre-trained segmentation models used in this study can be found in the GitHub page of the Federated Tumor Segmentation (FeTS) platform ( https://github.com/CBICA/FeTS ). Finally, ITK-SNAP 120 , 121 version 3.8.0 was used for all manual annotation refinements."
10.1038/s41597-022-01583-0,"No custom code was used in the generation or processing of datasets. All data is provided as ASCII text in CSV format and can be processed without custom code. An optional interactive Excel worksheet for convenient browsing, importing and resampling of the data is available upon request from the corresponding author."
10.1038/s41597-022-01572-3,"All code is open source and available at https://github.com/earthlab/firedpy . The data we produced is available at the Earth Lab Data collection at CU Scholar 20 . Links to each dataset are provided on the front page of the aforementioned github repository, and are provided here in Table S1 . The docker container with a custom software environment for running firedpy is at https://hub.docker.com/repository/docker/earthlab/firedpy ."
10.1038/s41597-022-01534-9,"Processing codes for the ISARIC COVID-19 database are openly available online, and contributions from the research community to share these codes are encouraged. For this reason, a public code repository has been created along with this manuscript to develop and share code collectively: https://github.com/ISARICDataPlatform/ISARICBasics.git . The content of this repository is under continuous development. Still, it has been seeded with code to generate patient-level datasets suitable for statistics and machine learning research, such as patient demographic, comorbid conditions at the time of admission, application of treatments, and severity scores, among others. It is possible for the research community to directly submit updates, improvements, and additions to the repository via GitHub. Moreover, a Jupyter Notebook containing the code used to generate the tables and descriptive statistics included in this paper is openly available on GitHub."
10.1038/s41597-022-01561-6,The source code of the anonymization tasks is publicly available as open source software 6 . The clustering and cluster validity methods were based on the open source scikit-learn Python library implementation 53 . The clustering & validity pipeline source code is publicly available at https://github.com/Farmerinpt/clustering-anonymization-utility .
10.1038/s41597-022-01569-y,The custom R scripts written to process the raw fragments counts are freely available ( https://github.com/prajkta9/bioinformatics-scRNA-seq ).
10.1038/s41597-022-01517-w,"All code for forecast data validation and storage associated with the current submission is available in the Forecast Hub GitHub repository, https://github.com/reichlab/covid19-forecast-hub-validations . Ensemble models are built with code in the covidEnsembles R package, https://github.com/reichlab/covidEnsembles . The code for forecast analysis is at https://doi.org/10.5281/zenodo.5207940 12 (covidHubUtils R package) and https://doi.org/10.5281/zenodo.5208224 7 (covidData R package). Any updates will also be published on Zenodo."
10.1038/s41597-022-01585-y,"To identify individuals from acknowledgement statements, the NLP software CoreNLP 31 was used. Script files were created using Python programming with version 3.9, to build a dataset that is available in Zenodo 53 ."
10.1038/s41597-022-01582-1,All analyses are implemented using the code available in the R package spider v.1.3 (Brown et al . 38 ).
10.1038/s41597-022-01525-w,"Multi-modal datasets are of great interest for evaluating the dataset-merging procedures for distortion correction and for correlative measurements analysis tools. The distortion correction algorithms and data analysis tools used in this study can be found at https://github.com/charpagne . Reindexing of the EBSD data with the dictionary indexing approach was performed with EMsoft version 4.2, the latest version of the code is available at https://github.com/EMsoft-org/EMsoft . The 3D voxelized dataset has been reconstructed using DREAM.3D, a software publicly available on GitHub ( https://github.com/BlueQuartzSoftware/DREAM3D ) or in the following link for direct download ( http://dream3d.bluequartz.net/Download/ . One version of a mesh structure was created with XtalMesh 21 , a publicly available code on GitHub ( https://github.com/jonathanhestroffer/XtalMesh ). The second version was created with the software suite from Simmetrix (SimModeler Voxel), a commercially available software ( http://www.simmetrix.com/ ). Digital image correlation were performed using the software Xcorrel HDIC. The authors should be contacted for further information on this in-house software."
10.1038/s41597-022-01578-x,"All the code for generating the dataset and figures is published as IPython notebooks on Github, https://github.com/sciosci/AFT-MAG . All the coding was completed using Python."
10.1038/s41597-022-01488-y,"All code used for downloading and processing the data used in this project, including the modelling and technical validation code, may be accessed at figshare 48 . To ensure that our work is reproducible, all code is written in open-source languages."
10.1038/s41597-022-01575-0,"The Matlab data variables can be obtained by exporting the EEG raw files (*.cdt) and ET raw files (*.edf) using MATLAB R2021a (MathWorks, Natick, MA, USA) and EEGLAB toolbox ( http://sccn.ucsd.edu/eeglab/ ). The codes for analyzing the ET and EEG data were compressed into a zip format file and shared with the dataset."
10.1038/s41597-022-01600-2,The custom R code is available in Figshare 20 .
10.1038/s41597-022-01595-w,"All software used in this work is in the public domain, with parameters being clearly described in Methods. If no detail parameters were mentioned for a software, default parameters were used as suggested by developer."
10.1038/s41597-022-01597-8,The full pipeline is available at https://github.com/morangershoni/BovEx . All software used in this study are freely available: SRA Toolkit: https://github.com/ncbi/sra-tools . BWA: https://github.com/lh3/bwa . Picard Tools: https://broadinstitute.github.io/picard/ . GATK software: https://github.com/broadinstitute/gatk/releases . VEP: https://m.ensembl.org/info/docs/tools/vep/script/vep_download.html . PLINK: https://zzz.bwh.harvard.edu/plink/download.shtml . KING: https://www.kingrelatedness.com/Download.shtml .
10.1038/s41597-022-01587-w,"Segmentation was performed using the commercially-available tool sliceOmatic v. 5.0 (Tomovision, Magog, Canada). The code for NIfTI file conversion of DICOM CT images and corresponding .tag format muscle/adipose tissue segmentations was developed using in-house Python scripts and is made publicly available through GitHub: https://github.com/kwahid/C3_sarcopenia_data_descriptor . Alternative code for converting .tag files to Matlab readable format can be located at: https://github.com/RJain12/matlab-tag-reader ."
10.1038/s41597-022-01574-1,The code used for data processing is available with the data set at the data repository. A publication of the processing routines on a maintained public repository is in preparation.
10.1038/s41597-022-01568-z,A code is written using statistical package (R.4.1) to process data. The code is available online 64 ( https://doi.org/10.6084/m9.figshare.16607912 ).
10.1038/s41597-022-01580-3,"To streamline the processing of the data, we provide various tools and scripts that are accessible at https://github.com/HRI-EU/multi_modal_gait_database . In particular, a Python script is available to join the CSV files into one single pandas data frame, which also supports filtering for specific tasks, participants, and data columns. Furthermore, we provide a visualization tool that jointly displays all three sensor modalities as illustrated by Fig. 6 . The tool allows the adjustment of current labels and the creation of custom labels or tags, enabling the generation of additional machine learning tasks."
10.1038/s41597-022-01573-2,"Some Matlab and Python scripts have been made available in the codes directory for the users to replicate some of the figures in this Data Descriptor: • plot_wificsi.m : This script is used to load the complex WiFi CSI data recorded by each NUC device and visualize the amplitude variations over time, as illustrated in Fig. 5(a) . The user can specify the start and stop timestamps and visualize the CSI stream in that time segment (for a given transmit antenna, receive antenna, and subcarrier index). Furthermore, for comparison purposes, the generated plots consist of the raw (unfiltered) CSI data and those which have been denoised using Discrete Wavelet Transform (DWT). • plot_uwb.m : This script is used to load the complex CIR data recorded by each passive UWB system, convert it into CFR using FFT and visualize the amplitude variations over a given time segment (between a given pair of UWB nodes), as illustrated in Fig. 5(b) . Furthermore, this script allows the users to plot the accumulated and aligned CIR measurements, as shown in Fig. 7 . • plot_uwb_fppow_crowdcount.m : This script is used to load the UWB data for the crowd counting experiment ( exp028 ) and plot the first path power level (in dBm) over time for each UWB system (between a given pair of UWB nodes), as illustrated in Fig. 6 . • plot_PWR_demonstration.m and plot_pwr_spectrogram.py : These scripts allow the users to visualize the PWR spectrograms from the three surveillance channels: “rx2” (as illustrated in Fig. 5(d) ), “rx3” and “rx4”, as a function of time and Doppler. • plot_kinect_data.m : This script allows the user to plot the motion capture data (as a function of velocity versus time) from one of the two Kinect systems, as illustrated in Fig. 5(c) . Furthermore, the users can visualize the stick (skeletal) representation of the kinect motion capture data as an animation over the specified time segment. For example, two frames of the stick representations when a person is standing and sitting are illustrated in Fig. 12(a,b) , respectively. • oddet.py : This python script allows the user to extract only the modalities and features needed rather than having to load the entire files and then stripping out unused features. With this python script, one can select the modality, experiment number and features needed through the command line interface. Additionally if a specific set of features are required, one can also specify all the columns needed through YAML configurations which will allow the user to curate the dataset to the format that more closely suits the usage. This python script is available at the following GitHub repository: https://github.com/RogetK/ODDET ."
10.1038/s41597-022-01564-3,No novel code used in the construction of FIVES dataset.
10.1038/s41597-022-01565-2,"Code, documentation, and other related sources used to produce VI data described in this work are available on GitHub at https://github.com/CSISS/NRT_VI . The usage and examples of requesting VI data through WMS/WCS are available on the Developer Guide page of VegScape at https://nassgeo.csiss.gmu.edu/VegScape/devhelp/help.html ."
10.1038/s41597-022-01598-7,The code used for creating the datasets is available at: https://github.com/lasigeBioTM/SeEn .
10.1038/s41597-022-01553-6,No custom code was used to generate or process the data presented in this manuscript.
10.1038/s41597-022-01589-8,Time-series images processing and crop mapping algorithms were implemented in MATLAB language. The processing code and related files are available at https://doi.org/10.6084/m9.figshare.14936052 . Datasets of cropping patterns in China and other countries/regions with finer resolutions (i.e. 10–30 m) can be produced based on publicly accessible time series images of Landsat and Sentinel-2 Multispectral Instrument (MSI) using the shared processing code 35 .
10.1038/s41597-022-01557-2,"The data pre-processing methods and procedures of validation mentioned in the technical validation section were carried out in Jupyter Notebook. Python version 3.5.8 was used throughout. The correlation analysis and distribution display are conducted using seaborn, sciki-learn 54 and pandas packages. The codes and a brief description(readme.md) have been uploaded."
10.1038/s41597-022-01554-5,The code used to validate the dataset and make the figures in this manuscript is available at the Figshare repository 51 .
10.1038/s41597-022-01601-1,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-022-01590-1,The source code of the web service and portal (CRITTERBASE Version 1.0.0) - for accessing quality-checked data sets - is released under the BSD 3 license and is publicly available via the following stable link: https://critterbase.awi.de/code . The software libraries and versions used are referenced in the source code. The Collector App for CRITTERBASE - the software for data quality checking and storage - is available as a free download under the open-source BSD 3 license ( https://doi.org/10.5281/zenodo.5724021 49 ). The software libraries and versions used are referenced in the README.MD file.
10.1038/s41597-022-01586-x,The above methods indicate the programs used for analysis within the relevant sections. The code used to analyse individual data packages is deposited at https://github.com/zhcosa/MAGs-from-cold-seep .
10.1038/s41597-022-01596-9,"All code used for the generation of the structural connectivity matrices from the raw diffusion and structural data is also available at https://doi.org/10.17605/OSF.IO/YW5VF 27 , under scripts/ . All scripts use FSL tools, version 5.0.7.0 21 . The four custom scripts are numbered in order of execution. • 1_dti_preprocess excludes DWI volumes with artifacts using excludeVols , performs preprocessing of DWI data using preprocess_dti_Takuya_2013 (also available at http://www.bic.mni.mcgill.ca/thayashi/dti.html 22 as dti_preprocess), and submits preprocessed data to the BEDPOSTX tool (estimation of diffusion parameters). Note that the optional averaging of data using dti_avg is not compatible with exclusion of volumes with artifacts, and was not included in our processing pipeline. • 2_dti_reg performs skull-stripping of raw T1 images, the registration of DWI and T1 images, and the registration to MNI space. • 3_dti_track2 prepares AAL ROI masks in subject DWI space and performs probabilistic tractography. It requires the AAL template ( ROI_MNI_V4.nii ) and a text file with the mask IDs of the 90 cortical ROIs to be represented in the structural connectivity matrix ( ROI_MNI_V4_90.txt ), provided under scripts/AAL/ . • 4_dti_get_conn2 creates the structural connectivity matrix."
10.1038/s41597-022-01604-y,No custom code was used to process or analyze the data.
10.1038/s41597-022-01581-2,"Initially, all the thematic layers and municipal boundaries were imported to a database created in the database management system PostgreSQL/PostGIS via the ogr2ogr , a command line tool 33 . Scripts were then developed to calculate the statistics. These scripts used regular SQL queries and the ST_AREA and ST_INTERSECTION spatial functions from PostGIS. We also used the aggs_for_arrays plugin, version 1.3.2 34 , which is an extension that offers various functions for optimized computation of different statistics on array of numbers. The calculation of the descriptive statistics for the 642 thematic classes/variables took more than three hundred hours of parallel processing on an Intel Core i7 desktop workstation equipped with 8 physical CPU cores and 64 GB of RAM. The following is an example of a script for calculating the maximum forest area statistics per municipality. This same script were used for all of them: By using similar PostGIS scripts with slightly different statistics functions we were able to calculate for all the 5,570 Brazilian municipalities the following statistics: count, minimum, maximum, mean, sum, standard deviation, 25 th percentile, 50 th percentile (median), and 75 th percentile, and the normalized counterparts as well (Table 3 ). All the statistics were stored in temporary tables, which were aggregated via the INNER JOIN function (using the geocode of each municipality as the junction field) in a final table. At the end of the execution, the per-municipality statistics for each layer were integrated and structured as a collection of datasets containing 11,556 attributes that represent the temporal and spatial plurality of the main socioenvironmental characteristics of the Brazilian municipalities."
10.1038/s41597-022-01608-8,"The BRAX Labeler code used for the extraction of labels from Brazilian-Portuguese radiology reports is available on Github ( https://github.com/edreisMD/BRAX-labeler ). To prevent the risk of patient re-identification, the anonymization code is not provided."
10.1038/s41597-022-01521-0,"All code used to generate, pre-annotate, and analyze the LCT corpus is freely available at https://github.com/uw-bionlp/clinical-trials-gov-data . The LCT annotation guidelines can be found at https://github.com/uw-bionlp/clinical-trials-gov-annotation/wiki ."
10.1038/s41597-022-01612-y,MISSING
10.1038/s41597-022-01602-0,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01594-x,"The Processing setup, Quan browser and Qual browser (Thermo Fisher Scientific, Les Ulis, France) in Xcalibur version 4.1 and Thermo Scientific TraceFinder (version 4.1) were used for collecting the HRMS library of volatile compounds. The structures of the volatile compounds were drawn using ChemDraw Professional 17.0 (Cambridgesoft, USA). High-resolution mass spectrums are plotted using Python (version 3.7)."
10.1038/s41597-022-01610-0,"The script used to process raw data into the data is included as supplementary data, with the proteomics data deposition at pride 16 and on the authors github page ( https://github.com/M-Russell/HalomonasProcessingRepository )."
10.1038/s41597-022-01591-0,The ZTRAX dataset was stored in relational databases using Safe Software Feature Manipulation Engine (FME) ( https://www.safe.com/ ). Code for this pipeline is available at https://github.com/johannesuhl/ztrax2sqlite2csv .
10.1038/s41597-022-01605-x,No code was used in this study.
10.1038/s41597-022-01613-x,"All software with respective versions and parameters used for producing the resources here presented (i.e., transcriptome assembly, pre and post-assembly processing stages, and transcriptome annotation) are listed in the methods section. Software programs with no parameters associated were used with the default settings."
10.1038/s41597-022-01499-9,"The dataset will be made publicly available for researchers, conservationist, policy makers, and solar developers to further explore conservation and solar energy development relationships, help inform policy decisions and minimize solar development effects in ecosystems at: https://researchlabwuopendata.blob.core.windows.net/solar-farms/solar_farms_india_2021.geojson . Source code with our model architecture implementation, trained models accompany with instructions on how to use it is available on GitHub at: https://github.com/microsoft/solar-farms-mapping for anyone to use under MIT open-source license."
10.1038/s41597-022-01614-w,"The JavaScript algorithms used for cloud-free image composition and water surface extraction based on the GEE platform are provided in Supplementary File 2 at Figshare 29 . The MATLAB algorithms used for the generation of skeletonized river networks, control points and vertical lines perpendicular to river centerlines and the associated.xlsx files are provided in Supplementary File 3 at Figshare 29 ."
10.1038/s41597-022-01579-w,"All code used to process and visualize the data, including information on software packages used, is freely available in the OSF project 30 ."
10.1038/s41597-022-01622-w,"The computer program to estimate the IAF can be found in 26 , and is described in 27 ."
10.1038/s41597-022-01592-z,"The R project associated with aggregating the raw datafile (FUBC_9_raw_data.csv) into crop categories for inclusion in the combined data file (FUBC_1_to_9.csv), and creation of results in Supplementary Table 4 are available at https://github.com/ludemannc/FUBC_1_to_9_2022 ."
10.1038/s41597-022-01537-6,"All supplementary material, and scripts used to process the data and generate resulting provenance are publicly available at https://github.com/RationAI/crc_ml-provenance/tree/new-auto-provenance ."
10.1038/s41597-022-01611-z,"All the results were generated using the basic formulas in Excel (version 2204). Along with results, all formulas are built into our Data Record files ( Domestic Extraction.xlsx , Domestic Processed Output.xlsx, Imports and Exports.xlsx, Balancing Items.xlsx , and China EW-MFA Accounts (1990 – 2020).xlsx ) at figshare repository 36 . The tool for checking data consistency is developed with the Office Visual Basic for Application (VBA), which can be built into any Excel file. The instructions and source codes are provided in Supplementary File 3 and Supplementary File 4 . These codes are publicly available."
10.1038/s41597-022-01599-6,For data collection we used the commercial software SPOTTERON by the company SPOTTERON GmbH ( https://www.spotteron.com ). The code for this software is not open source.
10.1038/s41597-022-01609-7,The code utilized to generate the individual fault maps in the visual verification and quality assurance of the database is written in Python 3.0 and is available at https://doi.org/10.5066/P9E3B8AG as a Jupyter Notebook 58 . This notebook is intended to share the plotting processes for how faults were visualized and can be manipulated by a user to prepare map images of specific faults or regions of choice.
10.1038/s41597-022-01593-y,"Perseus proteomics software version 1.6.2.2 ( https://www.perseus-framework.org ) was used for statistical analysis, where p values were calculated by Students t test. The volcano plots and heatmaps were also generated from the Perseus proteomics tools. The differentially expressed proteins were further subjected to Gene Ontology analysis using the Blast2GO ® software package (BioBam Bioinformatics Solutions, Spain) ( https://www.biobam.com/omicsbox ). These annotated genes were used for enrichment analysis of the gene ontologies of upregulated and downregulated genes by clusterProfiler version 4.2.2 ( https://bioconductor.org )."
10.1038/s41597-022-01627-5,"The original images were resized into 224 × 224 and 96 × 96 by using the web-based tools – https://teachablemachine.withgoogle.com by choosing a new image project with standard image model or embedded image model, respectively. There is no customized code in generation or processing of datasets."
10.1038/s41597-022-01626-6,"The contributed datasets were provided in Excel spreadsheets (Microsoft Office 2013), therefore no code is available for this step. Scripts to conduct taxonomic standardization using the LCVP, to plot environmental distribution, and trait representation in the plant phylogeny are available at FIGSHARE 22 . The scripts were developed in R."
10.1038/s41597-022-01524-x,The custom code used to create this dataset is available on https://doi.org/10.5281/zenodo.6620655 36 .
10.1038/s41597-022-01634-6,"No specific custom code was used to generate the dataset in the manuscript. The data, which are provided in ASCII text format, can be freely downloaded from the figshare data repository and analysed using any software. We used Generic Mapping Tools 32 to compile several figures."
10.1038/s41597-022-01644-4,"All codes for the experimental design, data organization, and technique validation are available at https://github.com/BNUCNL/WholebodySomatotopicMapping . Preprocessing was performed using fMRIPrep version 20.2.1 ( https://fmriprep.org ). ICA was performed using MELODIC version 3.15 ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MELODIC ), and IC classifications were manually performed using melview ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Melview ). Grayordinate-based (CIFTI format) brain activation analysis was performed by combining the Ciftify ( https://github.com/edickie/ciftify ) and HCP pipelines ( https://github.com/Washington-University/HCPpipelines )."
10.1038/s41597-022-01624-8,The custom code used to apply the atlas to new subjects is implemented in Python 3.8 and is available at the github repository https://github.com/connectomicslab/probconnatlas . This code needs the multi-scale probabilistic atlas files stored on the Zenodo repository 65 . The used and the current version of the software is 1.0. All the parameters employed to process the datasets are provided in the atlas files.
10.1038/s41597-022-01571-4,"Source code to generate quantitative maps in the example dataset are provided by qMRLab 44 , hMRI-Toolbox 43 and pymp2rage 64 . Each software provides extensive user documentation, which were followed to create derivative datasets."
10.1038/s41597-022-01649-z,"Two core tools applied in the study were ‘Spatial join’ and ‘Zonal Statistics’ provided by ESRI’s. ArcGIS 10.7 software package. In addition, the customized batch steps of reprocessing data, including lake area extraction and raster attribute extraction, were programmed using Python 2.7 scripts which were provided in our data set named ‘Lake_area_extraction.py’ and ‘Raster_attribute_ extraction.py’, respectively 41 ."
10.1038/s41597-022-01623-9,"MEG task paradigms, and scripts used for DICOM to BIDS format conversion and de-identification of structural MRI scans are available in the study git repository: https://github.com/nih-megcore/hv_protocol ."
10.1038/s41597-022-01654-2,"The code supporting this study’s findings is available on GitHub at https://github.com/CMDM-Lab/rsdb_publication . The scripts and packages used for the RSDB rely on open-source packages such as Ruby on Rails, MariaDB, ElasticSearch, Cytoscape.js 50 , and in-house Ruby scripts."
10.1038/s41597-022-01603-z,All R code required to reproduce the U.G.L.I data products and validation analysis is available at the following https://doi.org/10.5281/zenodo.5750665 33 .
10.1038/s41597-022-01630-w,ADTC-InSAR 53 has been developed in MathWorks MATLAB version R2019a 72 using TRAIN MATLAB software. It is possible to generate tropospheric corrections for the previously mentioned volcanoes using the Corr__ADTC_InSAR.m and Corr__ADTC_InSAR.py scripts.
10.1038/s41597-022-01662-2,The data processing workflow for establishing the spectral library is available in GNPS.
10.1038/s41597-022-01650-6,"We used R Version 4.0.3, an open-source programming language and environment for statistical computing, to implement our harmonization method. The full harmonization process, starting from the data retrieval to producing the final dataset, is provided in R scripts. All code, data inputs, the final dataset (SNAPD), and the intermediate flagged dataset (WQP_to_SNAPD_flagged) are publicly available on HydroShare here: https://doi.org/10.4211/hs.9547035cf37940eb9b500b7994a378a1 25 ."
10.1038/s41597-022-01632-8,The MATLAB code for calculation and creation of the dataset is available as supplementary information file.
10.1038/s41597-022-01633-7,"The studies used a combination of open-source and proprietary apps to collect observational data and deploy remote interventions via three apps (described above). The source code for tools used for the data collection, such as the passive data collection app (Survetory - used in the V2 study), iPST, and HealthTips (two of the three randomized interventions), is available on the study Github archive ( https://github.com/aid4mh/Brighten_Studies_Archive ). The gSCAP pipeline used to featurize the raw GPS data from the V2 study is available here ( https://github.com/aid4mh/gSCAP ). The data cleaning and curation pipeline is also available on GitHub ( https://github.com/apratap/BRIGHTEN-Data-Release )."
10.1038/s41597-022-01658-y,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-022-01625-7,"The code for LPP-fMRI corpus is publicly available at the OpenNeuro repository under code / subdirectory, and also at the following GitHub repositories: https://github.com/jixing-li/lpp_data , https://github.com/chrplr/lpp-paradigm . The code includes the presentation scripts for all three languages, the scripts used in technical validation and for preparing this data paper (e.g., compute_tsnr.py ), in addition to code for obtaining annotations (e.g. count_parser_actions.py ). Code for certain annotations like word embeddings and POS tagging is not included since there are several publicly available toolkits available to researchers."
10.1038/s41597-022-01645-3,Code associated with this manuscript can be found at OSF 70 .
10.1038/s41597-022-01660-4,Code sharing is not applicable to this article as the development distributed linked metadata (dbGaP variable ID linked to PhenX variable ID) in existing databases and features from PhenX and dbGaP.
10.1038/s41597-022-01643-5,We provide the raw csv data files obtained during the data collection structured by user and device identifier. We did not implement any custom code to generate or process the data.
10.1038/s41597-022-01636-4,The code is available at https://github.com/usc-sail/tiles-2019-dataset/ .
10.1038/s41597-022-01504-1,The source code of AiiDA is released under the MIT open-source license and is made available on GitHub ( github.com/aiidateam/aiida-core ). It is also distributed as an installable package through the Python Package Index ( pypi.org/project/aiida-core ).
10.1038/s41597-022-01657-z,"Python code for producing, reading and plotting data for any city in the dataset is provided at https://github.com/dh107/Carbon-Monitor-Cities/ ."
10.1038/s41597-022-01647-1,A script containing all the algorithms in this paper stored in ‘code.zip’ is provided with the dataset. All code is implemented in python (version python3.7 on Windows).
10.1038/s41597-022-01648-0,No specific code or script was used in this work. The commands used in the processing were all executed according to the manuals and protocols of the corresponding bioinformatics software.
10.1038/s41597-022-01651-5,Preliminary script for processing ISA-TAB files and estimating BLUEs is accessible at https://github.com/AbhishekGogna/GABI-WHEAT/tree/master/output_data .
10.1038/s41597-022-01661-3,R code used to generate harmonized sets of diatom count data and test the described dataset is archived together with Data Records at https://doi.org/10.23719/1524246 57 .
10.1038/s41597-022-01556-3,"The CDRC repository hosts both the output data files along with the R code and scripts used to clean the input data and reproduce the analysis – where based on openly available data. The series of files and code are available online through: https://data.cdrc.ac.uk/dataset/retail-centre-boundaries , https://github.com/ESRC-CDRC/Retail-Centre-Boundaries . Scripts are written and the analysis conducted in R version 4.1.1 25 . Core packages for replicating the generation of the dataset, including the spatial and network operations, include: h3 26 , sf 27 , data.table 28 , tidyverse 29 , tidygraph 30 , igraph 17 , and osmdata 16 ."
10.1038/s41597-022-01635-5,The Google Earth Engine JavaScript code to develop covariate layers and estimate the distribution of tidal flats globally is archived on Zenodo 58 .
10.1038/s41597-022-01615-9,"The BIDS validator code is available in the BIDS Validator repository on GitHub, https://github.com/bids-standard/bids-validator ."
10.1038/s41597-022-01637-3,"RNA-seq data analysis was performed with fastp ( https://github.com/OpenGene/fastp ) and HISAT2 version 2.0.5 ( https://daehwankimlab.github.io/hisat2/ ). For differential expression and functional enrichment analysis, DESeq2 version 1.16.1 and clusterProfiler version 3.4.4 were used: http://bioconductor.org/packages/3.15/bioc/ . Proteomic raw data were processed using Proteome Discoverer version 2.4 ( https://www.thermofisher.com/order/catalog/product/OPTON-30810#/OPTON-30810 ), NAguildeR ( https://www.omicsolution.com/wukong/NAguideR/ ), limma ( https://www.omicsolution.com/wkomics/passwd/hytestlimma/ ), STRING version 11.5 ( https://cn.string-db.org ), and cystoscope version 3.9.0 ( https://github.com/cytoscape/cytoscape/releases/3.9.0/ )."
10.1038/s41597-022-01666-y,Matlab codes for data analysis of calculating the dengue locations in subzones over weeks are available at the figshare repository 14 .
10.1038/s41597-022-01577-y,Public domain software tools were used in tRF & tiRNA-seq quality verification and standardized naming. These software tools are listed below: Sequencing quality are examined by FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . The standardized tRF & tiRNA ID for tRNA derived fragments consists of four parts: http://trna.ucsc.edu/tDRnamer/ .
10.1038/s41597-022-01631-9,"No custom code was used in the creation of this database. We used OnionNet 21 http://github.com/zhenglz/onionnet/ ML model to train on PLAS-5k dataset. Ambertools 52 , GaussView 48 , MODELLER 45 , and H++ server 46 were used for preparation of complex containing protein, ligand, and cofactor(s). MD simulations were carried using OpenMM 7.2.0 program 57 ."
10.1038/s41597-022-01640-8,"The source code used to produce this dataset is openly available in a GitHub repository at https://github.com/urbangrammarai/spatial_signatures and in the form of a website on https://urbangrammarai.xyz . Code is organized in a series of Jupyter notebooks and have been executed within the darribas:gds_dev:6.1 40 Docker container, unless specified otherwise in the individual notebooks."
10.1038/s41597-022-01638-2,The developed Matlab and Python codes to process the data are freely available on the first author’s github repository ( https://github.com/geisekss/motion_capture_analysis ). The MoCap Toolbox is freely available and extensively documented on the University of Jyväskylä website ( https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/materials/mocaptoolbox ).
10.1038/s41597-022-01656-0,"The described Z H calibration and correction procedures are available in the python packages wradlib 26 , Py-ART 27 , cluttercal ( https://github.com/vlouf/cluttercal https://github.com/vlouf/cluttercal ) and gpmmatch ( https://github.com/vlouf/gpmmatch ) and demonstration scripts for data visualization, processing and absolute calibration are provided as part of the data repository and the published relative calibration codes are also available on github (cluttercal)."
10.1038/s41597-022-01668-w,"Code files are available from the GitHub repository https://github.com/YMSen/Chicken_skeletal_muscle . All the bioinformatics analyses were performed in R 4.0.3 on x86_64-pc-linux-gnu (64-bit) platform, running under CentOS Linux release 7.9.2009 (Core). The following software packages were used for the analyses: STAR v2.7.0e, kallisto v0.44.0, tximport v1.20.0, tximeta v1.8.5, ggforce v0.3.3, limma v3.48.3, DESeq 2 v1.32.0, pheatmap v1.0.12, ggplot2 v3.3.6, RColorBrewer v1.1–3 and Rtsne v0.15."
10.1038/s41597-022-01653-3,"No custom code was used to generate or process the data. WebPlotDigitizer4.2 (GNU Affero General Public License v3.0, https://automeris.io/WebPlotDigitizer/ ) and OriginPro2018 (OriginLab Corporation, https://www.originlab.com/ ) were used to obtain numerical values of the data points and their uncertainties plotted in the graphs. The file containing the database was prepared in Microsoft Excel 2016 (Microsoft Corporation, https://www.microsoft.com/en-gb/microsoft-365/excel )."
10.1038/s41597-022-01674-y,"CatLC is available for download, along with all the necessary information and tutorials, on the following website 6 . There is a tutorial on how to manage the data in the following url: https://github.com/OpenICGC/CatLC/ . There is also the code to reproduce the training presented in the article. We provide the logs for the whole training that can be visualized using Tensorboard."
10.1038/s41597-022-01628-4,"The extrapolation can be performed by a straightforward implementation of the equations provided in the methods section. To aid the implementation of the equations, an example extrapolation is provided in spreadsheet (.xlsx) format in Figshare 22 . The illustration depicts the extrapolation method to obtain the \({S}_{2{n}_{10}}\) value in Table 1 , and the extrapolated S 2n value can then be used to obtain the corresponding unknown mass value."
10.1038/s41597-022-01663-1,"MilkyBase.xlsx and its technical description MBdescription.pdf as well as the mentioned macros in an MBmacros.xlsm file, are available from the Figshare repository 17 ."
10.1038/s41597-022-01671-1,"All software used in this study are in the public domain, with parameters being clearly described in Methods. If no detail parameters were mentioned for the software, default parameters were used as suggested by developer."
10.1038/s41597-022-01679-7,No custom code was made during the collection and validation of this dataset.
10.1038/s41597-022-01655-1,The code for the creation and deployment of the database can be found online at www.github.com/sharkipedia/sharkipedia .
10.1038/s41597-022-01685-9,No custom code was used to generate or process the data.
10.1038/s41597-022-01546-5,"Custom generated scripts used for processing OPT data including COM-AR 17 (alignment of axis of rotation during OPT scan setup), DFTA 17 (alignment of axis of rotation post-OPT scanning) and CLAHE 2 (improving islet segmentation) is compiled as a software package (together with video instructions on their implementation) at GitHub, Link “ https://github.com/ARDISDataset/DSPOPT ”."
10.1038/s41597-022-01675-x,"The global gridded population dataset was created using python 3.9.7 as well as ArcGIS 10.6 software platform, and the code of key steps can be available at Figshare. The code can be downloaded at Figshare ( https://doi.org/10.6084/m9.figshare.19609356.v3 ) 46 ."
10.1038/s41597-022-01620-y,"CJARS project code that does not contain sensitive information is made available at https://github.com/umcjars . This includes a template of our data production process, which can be found at https://github.com/umcjars/cjars_production_code_shell . In addition, code for the validation exercise presented in this article can be found at https://github.com/umcjars/cjars_bjs_validation . Code used for other research projects involving CJARS data conducted at the Census Bureau through the FSRDC system is available upon request via email at erd.cjars@census.gov. This code must be cleared first by the Census Bureau to avoid disclosure of any potentially sensitive information."
10.1038/s41597-022-01680-0,"The version and parameters of bioinformatic tools used in this study have been described in the Method section. If no parameter is described, the default is used."
10.1038/s41597-022-01665-z,"The workflow included several custom-made R and python scripts, which are accessible GitHub ( https://github.com/microsudaqua/usudaquadb )."
10.1038/s41597-022-01607-9,"The code used to convert the test-retest resting, and cognitive state EEG dataset for sharing with OpenNeuro was referred to the conversion of the EEG sedation dataset for sharing in BIDS, which is public available at https://www.fieldtriptoolbox.org/workshop/madrid2019/bids_sedation/ . The BIDS background is explained on http://bids.neuroimaging.io , details on the specification can be found on the paper titled “EEG-BIDS, an extension to the brain imaging data structure for electroencephalography”, which was published in Scientific Data."
10.1038/s41597-022-01670-2,"A Matlab software associated with this manuscript is licensed under MIT and published on GitHub at https://github.com/selipot/sst-drift.git and archived on Zenodo 37 . This software allows the user to fit model ( 2 ) to temperature observations and derive the resulting SST estimates and their uncertainties. Input arguments to the model fitting function include an arbitrary order for the background non-diurnal SST model and arbitrary frequencies for the diurnal oscillatory model. A sample of Level-1 data from drifter AOML ID 55366 is provided in order to test the routines and produce figures similar to Figs. 4 and 5 . Alternatively, the main code can also generate stochastic data for testing purposes."
10.1038/s41597-022-01516-x,All R code used for my analysis is available via GitHub: https://github.com/cohensimpson/smallnet_ScientificData . Permanent access to an archived version of this code is available via Zenodo 118 .
10.1038/s41597-022-01682-y,"All processing pipeline scripts are openly available. Code used to generate pre-processed outputs can be accessed via GitHub ( https://github.com/MICA-MNI/micapipe ). Documentation for the processing pipeline, including usage and detailed processing steps, can also be accessed via ReadTheDocs ( https://micapipe.readthedocs.io )."
10.1038/s41597-022-01667-x,The code used to generate the results of this analysis are available as supplementary material. Data analysis was carried out in STATA (version 16.1).
10.1038/s41597-022-01584-z,All data processing was performed in the R statistical language 60 . Code is organized into a workflow that is documented on the following website: https://land-4-bees.github.io/bee_tox_index/ .
10.1038/s41597-022-01689-5,All data and code are available without restrictions from figshare 33 .
10.1038/s41597-022-01659-x,"All code is written in the Python 3 programming language. All files, including Python files, necessary to the compilation of the dataset are available on the following GitHub repository: https://github.com/g-dolphin/WorldCarbonPricingDatabase ."
10.1038/s41597-022-01696-6,"All the code and processing scripts used to produce the results of this paper were written in Python, Jupyter lab. Links to scripts and data for analysis can be found in the GitHub repository ( https://github.com/Bob05757/Renewable-energy-generation-input-feature-variables-analysis )."
10.1038/s41597-022-01686-8,"No custom code was used to generate or process these data. All commands for QIIME2, phyloseq, ggplot2, and fantaxtic are available in r-markdown format in a single file named “Combined_Scripts.rmd” in the Dryad repository 20 ."
10.1038/s41597-022-01683-x,No custom code was generated.
10.1038/s41597-022-01677-9,"For the code used for quality control and technical validation, and preprocessing please see the Zenodo repository 39 ( https://doi.org/10.5281/zenodo.6554869 )."
10.1038/s41597-022-01618-6,The code used to generate the results in this paper is available at https://github.com/IulianEmilTampu/SPLIT_PROPERLY_OCT_DATA.git .
10.1038/s41597-022-01700-z,No custom code was used to generate this study.
10.1038/s41597-022-01698-4,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-022-01639-1,"The datasets are stored in an online repository and are accessible via links on the site. The largest of the datasets is over 500GB (compressed), the smallest being around 1.4GB (compressed). The QDataSet is provided subject to open-access MIT/CC licensing for researchers globally. The code used to generate the QDataSet is contained in the associated repository (see below), together with instructions for reproduction of the dataset. The QDataSet code requires Tensorflow > 2.0 along with a current Anaconda installation of Python 3. The code used to simulate the QDataSet is available via the Github repository 33 ( https://github.com/eperrier/QDataSet ). A Jupyter notebook containing the code used for technical validation and verification of the datasets is available on this QDataSet Github repository."
10.1038/s41597-022-01681-z,"The pipeline applications we provided contain all the processes described by Workflow Description Language (WDL, https://github.com/openwdl/wdl ) before R analyses. Please note that the parameters in them are fixed, and the sample files can be processed by direct invocation. The upstream analysis of WES and RNAseq is available at the GitHub repository ( https://github.com/fudan-tnbc ). The dockers used in the upstream analysis can be obtained in the Docker Hub ( https://hub.docker.com/u/chenqingwang ). The code for pre-processing the upstream result data and drawing the figures can be found in the GitHub repository ( https://github.com/fudan-tnbc/TNBC-Multiomics ). Quality control metrics for all data were collected in the metadata tables and visualized using R v4.1.2 ( https://cran.r-project.org/ , R development core team)."
10.1038/s41597-022-01701-y,"We used Google Earth Engine via Python to query Sentinel-2 images and to extract spectral indices and texture from the images. We performed all other steps, including image downloading, PCA, sample collection and filtering, and image classification in Python. Code is available at https://github.com/yangju-90/urban_greenspace_classification ."
10.1038/s41597-022-01703-w,"Software including their version information were listed in the method section. Custom scripts were provided at personal GitHub ( https://github.com/feifei/scripts_to_share ), including scripts to scaffold the contigs with optical maps (scaffolding_with_maps.py), to identify indel and SNPs using mapping information from Illumina DNA reads (base_change_from_pileup.py) and update reference sequence accordingly (base_change_incorporation.py), to call SNPs (snps.py), to analyze duplicated regions (duplicated_regions.py) and to generate the figures (optical_maps.R, circos.R, dotplot.R)."
10.1038/s41597-022-01692-w,"Code examples for training the segmentation and classification architectures can be found in the form of Jupyter notebooks in our GitHub repository ( https://github.com/DeepPathology/CanineCutaneousTumors ). Furthermore, we provide exported fastai learners to reproduce the results stated in this work. The datasets.csv file lists the train, validation, and test split on slide level. For network inference, we provide two Jupyter notebooks for patch-level results ( segmentation_inference.ipynb and classification_inference.ipynb ) and one notebook for slide-level results. This slide_inference.ipynb notebook produces segmentation and classification outputs as compressed numpy arrays. After inference, these prediction masks can be visualized as overlays on top of the original images using our custom SlideRunner plugins wsi_segmentation.py and wsi_classification.py . To integrate these plugins into their local SlideRunner installation, users have to copy the respective plugin from our GitHub repository into their SlideRunner plugin directory. Additionally, the slide_inference.ipynb notebook provides methods to compute confusion matrices from network predictions and calculate class-wise Jaccard coefficients and the tumor classification recall. As mentioned previously, we provide six python modules to convert annotations back and forth between MS COCO and EXACT, MS COCO and SQLite, and EXACT and SQLite formats. This enables users to extend the annotations by custom classes or polygons in their preferred annotation format. These modules can be found in the annotation_conversion directory of our GitHub repository."
10.1038/s41597-022-01699-3,"The source code (implemented in Python) for performing all the described figure analysis steps and generating the data entries is available at https://github.com/ViktoriiaBaib/curvedataextraction . The axis and legend detection step uses the TensorFlow2 Object Detection API and provides a fine-tuned CNN model. File “object_detection_axes_legend.py” performs object detection of legend, x-axis, and y-axis objects and generates PNG and JSON records for these objects. File “color_decomposition.py” performs clustering by color and produces PNG of color-isolated image, palette, as well as PNG and JSON records of separate color clusters in pixel coordinates. It uses methods from “posterization.py”. File “final-record.py” performs axes scale parsing and applies it to all the clusters, producing cluster records in units of measurement. It utilizes methods from “final_record_func.py”."
10.1038/s41597-022-01708-5,The scripts for preprocessing the MRI and MEG data as well as those used in the technical validation section are available at https://openneuro.org/datasets/ds004078 .
10.1038/s41597-022-01707-6,All the code produced for the present study is available from the following GitHub repositories: - http://github.com/fairsharing/jsonldschema - https://github.com/fairsharing/jsonschema-documenter - https://github.com/FAIRsharing/JSONschema-compare-and-view Supporting documentation is available from https://jsonldschema.readthedocs.io .
10.1038/s41597-022-01690-y,"This database is accompanied by a folder with all the scripts used to process and handle the data described. It is openly hosted in Zenodo 15 . Additionally, an extended code repository is available on Github ( https://github.com/ManuelPalermo/HumanInertialPose.git ) with updated code to not only process the data described, but also calculate kinematics, visualize and evaluate the resulting motions and offers extended support for general inertial pose estimation pipelines. All scripts are based on the Python programming language and, thus, open source. The code contains a permissive MIT license for unrestricted usage. We hope this dataset and associated code can further contribute to the development and evaluation of classic or data-driven inertial human pose estimation solutions, with applications, for example, in human movement understanding and forecasting, ergonomic assessment and gait/posture analysis."
10.1038/s41597-022-01697-5,Code used to process and analyze the data is available upon request. Some R scripts can be downloaded from https://github.com/odap-ico/colonomics .
10.1038/s41597-022-01706-7,"Raw data files of fluorescence images were analyzed by quantitative Image analysis using the GenePix image analysis software (GenePix Pro 6.0; https://axon-genepix-pro.software.informer.com/6.0/ ). In order to assign fluorescence signals to annotated genes gene array list (GAL) files were used ( https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE169361 ) 74 . Processing of the raw data obtained (GPR-files) was performed with the BioConductor R-packages limma version 3.14 ( https://bioconductor.org/packages/release/bioc/html/limma.html ) and marray version 3.14 ( https://bioconductor.org/packages/release/bioc/html/marray.html ) to achieve background correction of spot intensities, ratio calculation/normalization, and diagnostic-plot generation for array quality control. Statistical analysis steps were performed within our in-house microarray database as well as with Excel (Microsoft)."
10.1038/s41597-022-01716-5,"All analyses were generated with existing packages, and no original code was created. The packages associated with this analysis can be found below. Data analysis was performed using a variety of biostatistical platforms Perseus 1.6.15.0 ( www.coxdocs.org ), R ( www.r-project.org ) (version 4.0.4), Orange3 python package ( https://orange.biolab.si/ ) (version 3.31.0), and GSEA 4.1.0 ( https://www.gsea-msigdb.org/gsea/index.jsp ). Gene set enrichment analysis (GSEA) was used to define pathways enriched in each grouping. Heatmaps and clustering were performed using the R package ComplexHeatmap ( http://bioconductor.org/packages/release/bioc/html/ComplexHeatmap.html ) (version 2.9.3)."
10.1038/s41597-022-01664-0,"The code used for the geometry curation and standarization, and plotting can be found at https://github.com/je-santos/Large-simulation-dataset . The fluid flow simulations were performed using 71 . For all studies using custom code in the generation or processing of datasets, a statement must be included under the heading “Code availability“, indicating whether and how the code can be accessed, including any restrictions to access. This section should also include information on the versions of any software used, if relevant, and any specific variables or parameters used to generate, test, or process the current dataset."
10.1038/s41597-022-01718-3,We provide the code of the data conversion and processing pipeline under https://github.com/lab-midas/TCIA processing. The trained PET/CT lesion segmentation model is publicly available under https://github.com/lab-midas/autoPET/tree/master/ .
10.1038/s41597-022-01727-2,"Code and the script files used to convert the sounds files into meaningful format are published in ( https://github.com/tabarkarajab/Large-Scale-Audio-dataset -). We developed this code using Python and Pycharm Community software (Version 2021.3). The large-Scale Audio Dataset relies on the following dependencies: os, logging, traceback, shlex, and subprocess."
10.1038/s41597-022-01720-9,All source codes are available on our Gitlab repository: https://gitlab.com/nivall/guixreprodsci (Tag: v1.0-pre2) and the version at submission time is archived on Software Heritage at https://archive.softwareheritage.org/swh:1:rev:707f00afef8f6ef1f29a7a4c961dd714f82833f5 .
10.1038/s41597-022-01691-x,"Software code and scripts used in this study are available as Open Source in github 58 , 59 , 62 . Python scripts were tested under Python 3.7 and require the additional modules PIL, pandas, xml, csv, errno, sys, os, argparse, glob, pathlib and re. R scripts were tested in R 4.1.3 and require the additional packages parallel, foreach, and doMC. Shell scripts were tested using Bourne Again Shell (bash) 5.1.16."
10.1038/s41597-022-01704-9,"The code used to curate the data and produced the figures in this data descriptor are available at Zenodo 268 . All data manipulations were performed using the R statistical environment 6 (v. 4.1.3) and the tidyverse package 277 (v. 1.3.1). Figures were built upon the ggspatial 278 (v. 1.1.5), maps 279 (v. 3.3.0), ggtree 280 (v. 3.1.1), ggtreeExtra 281 (v. 1.3.3), ggnewscale 282 (v. 0.4.5), patchwork 283 (v. 1.1.1), and ape 284 (v. 5.5) packages."
10.1038/s41597-022-01722-7,"This database is accompanied by a folder with all the scripts used to process, handle, visualize, and evaluate the data described (available in PhysioNet 32 and GitHub 34 ). All scripts are based on the Python programming language and, thus, open source. The code contains a permissive MIT license for unrestricted usage. The dataset has also been used on a related publication, to develop and evaluate deep learning based algorithms for patient pose estimation using the robotic walker 35 . The authors hope it can further contribute to the development and evaluation of classic or data-driven vision-based pose estimation algorithms, applications in human detection, joint tracking, and movement forecasting, and gait/posture metrics analysis targeting solutions for motor rehabilitation."
10.1038/s41597-022-01726-3,"The code and information needed for training and testing the deep learning model presented in the technical validation section are published on GitHub, which can be accessed via https://github.com/acharoen/Rhode-Island-GI-VCE-Technical-Validation . The pre-trained model used to generate the predictions (InceptionResNetV2.h5) is stored in the same data repository 12 ."
10.1038/s41597-022-01725-4,The produced datasets elaborated in this work were constructed based on custom-built codes written in Matlab and ArcGIS 10.2. The codes and 1 km×1 km Chinese anthropogenic Hg emission data 36 can be obtained from zenodo or by contacting the designated staff at https://cgeed.net .
10.1038/s41597-022-01702-x,"The R package Met2Graph developed and used to generate the TumorMet datasets is publicly available at the Met2Graph Github repository ( https://github.com/cds-group/Met2Graph ). The package has a detailed tutorial to generate the networks. Met2Graph implements a flexible process flow to build graphs starting from a GSM and can be easily integrated with user-customized functions. It allows the creation of the three different types of graphs described, based on the selection of nodes, edges, and attributes: Metabolites-, Enzymes- and Reactions-based graphs. It allows integrating gene expression data into Metabolites-based graphs. It provides several options and parameters to customize the resulting graphs. To name a few: to create multiple or simplified edges (simplification is possible using three different methods), to remove recurring metabolites, to consider the double direction in case of reversible reactions, to generate graphs as directed or not, and to plot the networks. All the details and the different arguments are described in the package manual and “help” section of the related functions. The code to compute the distribution based distance measures and to obtain the simplified networks is also available at the GraphDistances Github repository ( https://github.com/cds-group/GraphDistances )."
10.1038/s41597-022-01713-8,"The preprocessing of EEG data was conducted with EEGLab. The codes of the preprocessing of EMG and ACC, labeling of raw data, and feature extraction are available with the data file 13 ."
10.1038/s41597-022-01731-6,We published the code used to extract the data on: https://github.com/EvacuationBehavior/Highway-Routing-Data-Processing . There are no restrictions to access and use/reuse the code.
10.1038/s41597-022-01734-3,All versions of third-party software and scripts used in this study are described and referenced accordingly in the Methods sections for ease of access and reproducibility.
10.1038/s41597-022-01733-4,"Plaque2.0 batch image analysis for infection scoring was performed by MATLAB (version R2016b, The MathWorks, Natick, USA) script AntiVir_batchprocessing.m used by UZH for image analysis is provided at IDR under idr0130/3-Screen/Analysis . It is based on the Plaque2.0 software available on GitHub: https://github.com/plaque2/matlab under GPLv3 open source license. To batch analyze the virus screening data by Plaque2.0, fork or download the Plaque2.0 AntiVir code from GitHub: https://github.com/plaque2/matlab/tree/antivir . Place the AntiVir_batchprocessing.m file from idr130/3-Screen/Analysis into the Plaque2/matlab folder and follow the instructions in AntiVir_batchprocessing.m . A MATLAB license is required. Hit filtering using R was performed in R (version 4.0.2.) script AntiVir_hitfiltering used by UZH for data processing and hit filtering is provided at IDR under idr130/3-Screen/Analysis ."
10.1038/s41597-022-01735-2,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01711-w,The annual area and RLV of lakes larger than 1 km 2 from 1989 to 2019 were produced using GEE API and Python. The code developed for this work are openly shared with the scientific community at Zenodo repository 48 . GEE should be used to access and edit the code.
10.1038/s41597-022-01693-9,We used the RStudio version (1.2.5019) for data processing. The codes for the study are available at GitLab https://git.ufz.de/batool/n_surplus .
10.1038/s41597-022-01673-z,All code used in this study has been deposited with the data at MetaboLights MTBLS4066 22 : Metabolite profiling of Arabidopsis mutants of lower glycolysis.
10.1038/s41597-022-01738-z,"All calculations were conducted in R 28 (version 4.2.0), using the forecast , psych , foreign , jtools , and lmtest packages 16 , 29 , 30 , 31 , 32 . The R code used to aggregate and calculate the statistical metrics of ecology is available on OSF at https://osf.io/r9msf/ . Code for the dendrogram example is also available on OSF and uses the circlize and dendextend packages 33 , 34 ."
10.1038/s41597-022-01621-x,"The NTB model code involved in generating GNV was developed using VC++, and the code is available in Figshare 65 . Usage methods and important parts of the code have been commented."
10.1038/s41597-022-01695-7,"To facilitate replicability, Jupyter notebooks 108 and Dockerfiles 109 necessary to reproduce the methods described herein are provided in the HBN-POD2 GitHub repository at https://github.com/richford/hbn-pod2-qc . The specific version of the repository used in this study is documented in 110 . Most of the code in this repository uses Pandas 111 , 112 , Numpy 113 , Matplotlib 114 , and Seaborn 115 . The make or make help commands will list the available commands and make build will build the requisite Docker images to analyze HBN-POD2 QC data. In order to separate data from analysis code 116 , we provide intermediate data necessary to analyze the QC results in an OSF 117 project 118 , the contents of which can be downloaded using the make data command in the root of the HBN-POD2 GitHub repository. The NIFTI-1 files and TFRecord files provided as input to the CNN models may be separately downloaded using the make niftis and make tfrecs commands, respectively. The remaining make commands and Jupyter notebooks follow the major steps of the methods section: 1. The cloudknot preprocessing function used to execute QSIPrep workflows on curated data was a thin wrapper around QSIPrep ’s command line interface and is provided in the “notebooks” directory of the HBN-POD2 GitHub repository in a Jupyter notebook with the suffix preprocess-remaining-hbn-curated.ipynb . 2. The expert rating analysis can be replicated using the make expert-qc command in the HBN-POD2 GitHub repository. 3. The Fibr community science web application is based on the SwipesForScience framework (swipesforscience.org), which generates a web application for community science given an open repository of images to be labelled and a configuration file. The source code for the Fibr web application is available at https://github.com/richford/fibr . 4. The images that the Fibr raters saw were generated using a DIPY 45 TensorModel in a cloudknot -enabled Jupyter notebook that is available in the “notebooks” directory of the Fibr GitHub repository. Fibr saves each community rating to its Google Firebase backend, the contents of which have been archived to the HBN-POD2 OSF project as specified in Table 3 . 5. The community ratings analysis can be replicated using the make community-qc command in the HBN-POD2 GitHub repository. Saved model checkpoints for each of the XGB models are available in the HBN-POD2 OSF project and are automatically downloaded with the make data command. 6. The input multichannel volumes for the CNN models were generated using DIPY 45 and cloudknot 46 and saved as NIfTI-1 files 119 . These NIfTI files were then converted to the Tensorflow TFRecord format using the Nobrainer deep learning framework 107 . The Jupyter notebooks used to create these NIfTI and TFRecord files are available in the “notebooks” directory of the HBN-POD2 GitHub repository, with suffixes save-b0-tensorfa-nifti.ipynb and save-tfrecs.ipynb , respectively. 7. We trained the CNN models using the Google Cloud AI Platform Training service; the HBN-POD2 GitHub repository contains Docker services to launch training (with make dl-train ) and prediction (with make dl-predict ) jobs on Google Cloud, if the user has provided the appropriate credentials in an environment file and placed the TFRecord files on Google Cloud Storage. Further details on how to organize these files and write an environment file are available in the HBN-POD2 GitHub repository’s README_GCP.md file. To generate the figures depicting the deep learning QC pipeline and results, use the make deep-learning-figures command. 8. We provide a Docker service to compute integrated gradient attribution maps on Google Cloud, which can be invoked using the make dl-integrated-gradients command. This step also requires the setup steps described in README_GCP.md . 9. We provide a Docker service to conduct the CNN-i site generalization experiments on Google Cloud, which can be invoked using the make dl-site-generalization command, which, again, requires the setup steps described in README_GCP.md . Similarly, theThe XGB-q site generalization experiments can be replicated locally using the make site-generalization command, which will also plot the results of the CNN-i experiments. 10. The tractometry pipeline was executed using pyAFQ and cloudknot in a Jupyter notebook provided in the “notebooks” directory of the HBN-POD2 GitHub repository with the with suffix afq-hbn-curated.ipynb. with suffix afq-hbn-curated.ipynb , provided in the HBN-POD2 GitHub repository in the “notebooks” directory. The pyAFQ documentation contains a more pedagogical example of using pyAFQ with cloudknot to analyze a large openly available dataset ( https://yeatmanlab.github.io/pyAFQ/auto_examples/cloudknot_example.html ). 11. The bundle profile and age prediction analyses can be replicated using the make bundle-profiles and make inference commands, respectively."
10.1038/s41597-022-01747-y,No computer code was used to generate this data set.
10.1038/s41597-022-01728-1,"We used the following packages to generate this dataset: • Open Ephys acquisition GUI 25 , version 0.4 • Kilosort 27 , version 1 • Phy 28 , version 2.0a1 • pynwb 12 , version 2.0.0 • ndx_pose, https://github.com/rly/ndx-pose • pose-tensorflow 17 , 18 , https://github.com/cxrodgers/PoseTF , lightly modified fork of https://github.com/eldar/pose-tensorflow • whisk 16 , https://github.com/cxrodgers/whisk , lightly modified fork of https://github.com/nclack/whisk • ffmpeg-python, https://github.com/kkroening/ffmpeg-python • ffmpeg, version 4.0.2 • ipython 29 , version 7.22.0 • pandas 30 , version 1.2.4 • numpy 31 , version 1.19.2 • scipy 32 , version 1.6.2 • scikit-learn 33 , version 0.24.1 • scikit-image 34 , version 0.18.1 • statsmodels, version 0.12.2 • pyglmnet 35 , https://github.com/glm-tools/pyglmnet • matplotlib 36 , version 3.3.4"
10.1038/s41597-022-01724-5,"All the software programs used in this article ( de novo transcriptome assembly, pre and post-assembly steps, and transcriptome annotation) are listed in the Methods paragraph. In case of no details on parameters, the programs were used with the default settings."
10.1038/s41597-022-01684-w,The code used in this article were deposited at https://github.com/evotools/CattleOManalyses .
10.1038/s41597-022-01740-5,The programs used to generate the data were GWR4 and ArcGIS (10.7). The code is available on GitHub ( https://github.com/Yue121/0.01deg-population-China-2018 ).
10.1038/s41597-022-01737-0,"All code for OSeMOSYS Global is provided in a public GitHub repository ( https://github.com/OSeMOSYS/osemosys_global ) under a MIT license. Instructions on how to install and run OSeMOSYS Global can be found in the repository or on the OSeMOSYS Global website ( https://osemosys-global.readthedocs.io ). While a basic understanding of Python and OSeMOSYS will be beneficial, it is not required to run the workflow. All users and readers are invited to contribute to OSeMOSYS Global following the contribution guidelines outlined on the repository."
10.1038/s41597-022-01710-x,No code is shared in this paper.
10.1038/s41597-022-01736-1,"The heatmap shown in Fig. 5 was obtained using an in-house written code (R), available at github ( https://github.com/Juliette92/histone_atlas ). The PeptidoformViz tool (Shiny App) is available at github ( https://github.com/statOmics/PeptidoformViz )."
10.1038/s41597-022-01723-6,No code was used to generate this dataset.
10.1038/s41597-022-01751-2,The dataset presented in this manuscript includes raw data that have not been processed by any script or software-based procedure.
10.1038/s41597-022-01732-5,The repository https://efcdata.cref.it/ contains the Jupyter notebooks written in Python 3 necessary to reproduce the reconstruction and the Fitness and Complexity algorithm.
10.1038/s41597-022-01739-y,"All code and notebooks in the Atlas repository are available at https://github.com/IPCC-WG1/Atlas and on Zenodo 6 under the Creative Commons Attribution license, CC-BY 4.0. The new notebook reusing the Atlas repository dataset to produce GWL plots (Fig. 3 ) is available at https://github.com/SantanderMetGroup/2022_Iturbide_FAIRprinciplesIPCC and also on Zenodo 27 ."
10.1038/s41597-022-01694-8,"The described dataset was generated using open-source software packages. The structural images were converted and organized into the BIDS using dcm2bids (v2.1.6) 35 , facial information was removed with PyDeface (v2.0.0) 34 , and IQMs were generated using MRIQC (v0.16.1) 26 . Each software provides extensive user documentation, which were followed to create the dataset."
10.1038/s41597-022-01717-4,No custom code has been used to generate or process this dataset.
10.1038/s41597-022-01676-w,The R code used in the analysis of the Visium data is available on GitHub ( https://github.com/yuGithuuub/Normal_liver_visium ). This R code is also available at figshare 26 .
10.1038/s41597-022-01741-4,No code is used in this manuscript.
10.1038/s41597-022-01688-6,The R code to read the plot-species-abundance file (ReSurveyGermany.csv) and combine it with the header data (Header_ReSurveyGermany.csv) is provided on https://github.com/idiv-biodiversity/Read_ReSurveyGermany .
10.1038/s41597-022-01714-7,The source codes for the Immune Signatures Data Resource and all data are available in ImmuneSpace ( https://www.immunespace.org/is2.url ) and in Zenodo 60 ( https://doi.org/10.5281/zenodo.5706261 ) and FigShare 45 : ( https://doi.org/10.6084/m9.figshare.17096978 ). Pre-processing code and supplementary data in full detail can be found in the ImmuneSignatures2 R package hosted on Github ( https://github.com/RGLab/ImmuneSignatures2 ).
10.1038/s41597-022-01729-0,"Data used for the production of the Italian SMR database are available from ISTAT (see the paragraph Data Source ). The elaborations have followed the procedure described in the Methods section and can be implemented in whatever numerical computing environment (e.g., R, Matlab, Python). In our case, the algorithms used were created in Python 3.7 and are available on Zenodo 14 ."
10.1038/s41597-022-01761-0,"The codes we developed for crop yield computation and crop yield dataset generation are available at https://doi.org/10.5281/zenodo.6444614 88 . The code was programmed using Python 3.9. In this code, we used the sklearn library for calling machine learning algorithm and GDAL library for raster data reading and processing. Moreover, the band calculation tool of ArcGIS 10.4 was used for crop water productivity dataset generation."
10.1038/s41597-022-01758-9,"Most of the processing of image data, including image registration, segmentation and technical validation (i.e., calculation of Dice similarity coefficients) was accomplished using commercially available software (MIM v. 6.9.7, MIM Software Inc.). Calculation of 95-percentile Hausdorff distance for technical validation used open-source software for image computation (Plastimatch 14 v. 1.7.0)."
10.1038/s41597-022-01778-5,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-022-01760-1,"The data set processing process and usage method can be obtained from Figshare 22 . We believe that in the case of serious data missing or large data differences, the effect of using the most basic mathematical method is more effective and reliable. Our basic methods can be realized by using the conventional functions in Excel without new code. Please refer to the methods section. Spatial processing content ran in ArcGIS Desktop (V10.2 or later). The interpolation and extrapolation is processed in Microsoft Excel. All software needs to be installed in the Windows 10."
10.1038/s41597-022-01777-6,"Serum enzyme activity, antioxidant capacity, and immune parameter were statistically processed and analyzed using R version 4.0.3, default parameters or parameters recommended by the developer were used."
10.1038/s41597-022-01781-w,No custom code was used to generate or process the data described in this paper.
10.1038/s41597-022-01709-4,"The LGS dataset and the lignin structure generator tool are available on Figshare 23 and GitHub ( https://github.com/sudhacheran/lignin-structure-generator ) respectively. Moving forward, the database and accompanying tools will be periodically updated and extended. The latest version of the LGS dataset can be downloaded from: https://labs.wsu.edu/pmrg/resources/lgsdataset/ ."
10.1038/s41597-022-01744-1,"All code for generating the datasets, including image scraping, preprocessing, inpainting and SliceGAN , can be accessed openly at the MicroLib github repository, https://github.com/tldr-group/microlib , which includes in-depth instruction to ensure reproducibility."
10.1038/s41597-022-01754-z,"Our implementation of the DOS fingerprint is part of the NOMAD project 23 and can be obtained as a stand-alone Python package hosted at Github 29 ( https://github.com/kubanmar/dos-fingerprints ). Additionally, we provide a tutorial that explains how to use the DOS descriptor to find in the NOMAD Repository 1 the most similar materials to a given reference. This tutorial is implemented in the NOMAD AI Toolkit 30 and can be accessed online ( https://nomad-lab.eu/aitoolkit/tutorial-dos-similarity ). In this work, we choose to focus our DOS descriptor on an energy region around the Fermi energy. To this end, we set the parameters Δ ε min = 0.05 eV, Δ ε max = 1.05 eV, N = Δ ε max /Δ ε min = 21, ε ref = 0 eV, and W = 4 eV (see Eq. 2 ). W H = 4 eV, N ρ = 512, ρ min = N ρ Δ ρ min = 0.25, ρ max = 2.75, and N H = ρ max / ρ min = 11 (see Eq. 4 ). An implementation of our clustering algorithm is available at Github 31 ( https://github.com/kubanmar/similarity_threshold_clusterer )."
10.1038/s41597-022-01773-w,Code 67 used to perform the data preparation and analysis for this paper is available on GitHub at https://github.com/casmlab/personal-data-survey and through Zenodo with the identifier https://doi.org/10.5281/zenodo.6807258 .
10.1038/s41597-022-01752-1,"The code used to automatically generate the database is available at https://github.com/odysie/thermoelectricsdb , along with examples, code for cleaning and aggregating the database, and supplementary information about the database and the data extraction process."
10.1038/s41597-022-01742-3,Fortran Code to generate the WGS 84 geoid undulations using spherical harmonic synthesis of EGM2008 is available from the NGA 15 and is available for download at https://earthinfo.nga.mil/index.php?dir=wgs84&action=wgs84 and is linked from the repository. C++ Code to generate the WGS 84 geoid undulations of EGM 84 and EGM 96 is part of the GeographicLib project 45 and is available for download https://geographiclib.sourceforge.io/html/GeoidEval.1.html . The Java code to repeat the volume and mean depth calculations is deposited in the GitHub Tinfour repository located at https://github.com/gwlucastrig/Tinfour and is linked from the repository.
10.1038/s41597-022-01776-7,"A MATLAB script has been made available in the dataset directory for the users to replicate some of the figures in this Data Descriptor: • plot_uwb_signals.m : This script can be used to load the two files background_CIR and target_CIR (either in .csv format or .mat format) and plot the Channel Frequency Response (CFR) of the UWB data for a desired time segment (between a chosen pair of UWB nodes), as illustrated in Fig. 3 . Furthermore, this script allows the users to plot the first path power level (in dBm) and ground truth trajectory, as depicted in Fig. 4 , and the aligned CIR measurements as demonstrated in Fig. 5 ."
10.1038/s41597-022-01779-4,Code associated with the randomization of antibody designs is available on GitHub ( https://github.com/mit-ll/Insilico_Ab_Variant_Generator ). Code used for sequence analysis is functionally similar to code that has previously been released 18 . This code can be accessed on GitHub ( https://github.com/dyounger/yeast_synthetic_agglutination ).
10.1038/s41597-022-01743-2,The labelled corpus in our experiments are publicly available in a Dataverse repository: https://doi.org/10.18167/DVN1/YGAKNB 12 . The whole classification and evaluation pipeline was performed using the scikit-learn library (Python): https://scikit-learn.org/ 31 .
10.1038/s41597-022-01763-y,All data records files and R scripts used for the data collection procedure are schematized in Fig. 4 and available at online repositories 85 : https://github.com/lguillier/SACADA_Database and https://zenodo.org/record/6572948#.YouM3ajP3tR . The database (bibliographic references) was also extracted as RIS and BIB files for open-source software.
10.1038/s41597-022-01749-w,"Not applicable, we submitted to the original sequences, therefore no code or software was used."
10.1038/s41597-022-01782-9,The code used to analyze the data in the current study is available at: https://github.com/nliulab/mimic4ed-benchmark .
10.1038/s41597-022-01767-8,No custom code has been created or used during the generation/unification and processing of this dataset.
10.1038/s41597-022-01792-7,During this study no custom code was used.
10.1038/s41597-022-01746-z,Access to the charge density data provided by the Materials Project API ( https://github.com/materialsproject/api ) and grid transforms of the charge density is done using the pyRho python package. See the [sec:usage]Usage Notes section for more information. The scripts used to generate the validation data can be access at along with the direct download of the validation dataset 24
10.1038/s41597-022-01753-0,The pipeline used for gene structural and functional annotation is available in details at https://github.com/andreaminio/AnnotationPipeline-EVM_based-DClab .
10.1038/s41597-022-01787-4,"As stated previously, all processing steps including the generation of the dataset, most of the figures and the manuscript are script based. All required source code 56 can be found on Hydroshare ( https://doi.org/10.4211/hs.8ea376970c904c6698fc8cfe392689de ) as a static code repository. Due to the procedure of the reviewing process, this static code repository only contains the status of the code before the last reviewing iteration. The final code used for submitting the reviewed manuscript can be found in this separate code release on Github ( https://github.com/MxNl/macro_mohp_feature/releases/tag/v013.1.1.0 ). The actively developed code can be also found in the same repository on Github ( https://github.com/MxNl/macro_mohp_feature ). We encourage interested users of this dataset to report errors in the code or to give hints on further methodological or programming improvements through opening an issue in the Github repository or contacting the corresponding author via E-mail 13 , 55 ."
10.1038/s41597-022-01764-x,"All code for dataset anonymization is available at https://github.com/Datalab-AUTH/LifeSnaps-Anonymization . All code for reading, processing, and exploring the data is made openly available at https://github.com/Datalab-AUTH/LifeSnaps-EDA . Information about setup, code dependencies, and package requirements are available in the same GitHub repository."
10.1038/s41597-022-01786-5,"The Python code used to generate the MSRs along with all their metadata, including hourly profiles, as well as the code to perform screening and clustering, is openly available on https://github.com/bhussain89/Model-Supply-Regions-MSR-Toolset ."
10.1038/s41597-022-01783-8,"The bioinformatics pipeline (reads trimming, mapping, quantification and differential expression) with a detailed list of command lines, and data and source code of DRGProfile shinyApp, were deposited on the repository Figshare 83 ."
10.1038/s41597-022-01794-5,All programs and commands used to present this data are contained in Table 2 .
10.1038/s41597-022-01800-w,The following software and versions were used for quality control and data processing: 1. FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc (version 0.11.9). 2. TopHat2: http://ccb.jhu.edu/software/tophat/index.shtml (version 2.1.1). 3. Cufflinks: http://cole-trapnell-lab.github.io/cufflinks (version 2.2.0).
10.1038/s41597-022-01745-0,"A large code-base was required to translate the original raw data into organized and structured datasets. This suite of state-specific files, written variously in Python, R, and Stata, is stored and retained internally. The scripts that we used to clean the data from each state in 2018 and 2020 are all available on a public GitHub repository 47 ."
10.1038/s41597-022-01804-6,No specific code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-022-01801-9,"All commands and pipelines used for data processing were according to the instruction manuals of the bioinformatics software cited above, and the parameters are clearly described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used, as suggested by the developer."
10.1038/s41597-022-01759-8,"No custom code was used to process the dataset presented in this article. To visualize microbial structure and composition, the diversity data provided in this dataset have been processed through the FROGS pipeline 59 version 3.2.3 ( http://frogs.toulouse.inra.fr/ ), available on the Galaxy server ( https://vm-galaxy-prod.toulouse.inrae.fr/galaxy_main/ ) of the Genotoul bioinformatic platform ( http://bioinfo.genotoul.fr/ ) under GNU GPLv3 licence."
10.1038/s41597-022-01810-8,No specific codes were used to produce the data presented.
10.1038/s41597-022-01791-8,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01802-8,The following GitHub repository contains the custom MATLAB script (R2020a) for loading and visualizing robot trajectory and motion capture data shown in Fig. 7 : https://github.com/UF-ISE-HSE/UF-Retail-HRI-Dataset .
10.1038/s41597-022-01748-x,"As part of the Integrated Multisector, Multiscale Modeling (IM3) project (funded by the U.S. Department of Energy’s Office of Science; see “Acknowledgements”), we are committed to delivering all research products as open-source to benefit those who may have interest in reproducing or building off of our work. For the reader’s convenience, we provide the open-source code as well as step-by-step documentation for how to reproduce RectifHyd in the following meta-repository: https://github.com/immm-sfa/turner_voisin_nelson_2022_scientific_data ."
10.1038/s41597-022-01806-4,Code for the TVB Brain Tumor pipeline is available at https://github.com/haerts/The-Virtual-Brain-Tumor-Patient . Code for retrieving the voxelwise resting state hemodynamic response function is available at https://github.com/bids-apps/rsHRF . Proof-of-concept notebooks for the introduction of region- and subject-specific HRF in TVB are presented at https://github.com/AmoghJohri/TVB-Tests . The TVB processing pipeline is available at https://github.com/BrainModes/TVB-empirical-data-pipeline and https://search.kg.ebrains.eu/instances/Software/71265c9f-5fe3-40e3-a7e4-b2bb45b5ea6e for cloud computing.
10.1038/s41597-022-01768-7,"To aid in usage of the dataset, code to generate sample plots and verification figures contained herein are publicly available on GitHub ( https://github.com/yiwenff/WUS-SR-data-descriptor )."
10.1038/s41597-022-01558-1,"Source curated data and mapping files (cell types, vaccine components, the NCBI gene file used, etc.), as well as the R code for the processing pipeline used to create the Dashboard submission files, are available on GitHub at https://github.com/floratos-lab/hipc-dashboard-pipeline . Supported web browsers - The HIPC Dashboard has been tested on recent versions of • Chrome (Version 93.0.4577.63 (Official Build) (64-bit)) • Firefox (Version 92.0 (64-bit)) • Edge (Version 93.0.961.38 (Official build) (64-bit))"
10.1038/s41597-022-01771-y,MISSING
10.1038/s41597-022-01775-8,All used scripts to implement or use our dataset and links to the GEE stored assets are available in the following Github repository 54 ( https://doi.org/10.5281/zenodo.5638409 ) repository with guidelines stored in a README file explaining all instructions about their execution.
10.1038/s41597-022-01790-9,Code used for calculating monthly and annual emission is written in Fortran and available from Zenodo 46 . The Spatial Allocator version 4.4 and SMOKE version 4.7 are used for data processing which can be obtained from CMAS webpage 40 .
10.1038/s41597-022-01789-2,"The implementation of the analysis results data model is available on Github 15 . This repository exemplifies how to construct the data model and the respective schema, as well as shows how to query the underlying database. Furthermore, we provide three output examples to visualize the results."
10.1038/s41597-022-01805-5,The code and documentation for pISA-tree is available at http://github.com/NIB-SI/pISA-tree . R packages and documentation are available from https://github.com/NIB-SI/pisar and https://github.com/NIB-SI/seekr . The code is openly available under MIT license terms.
10.1038/s41597-022-01803-7,"The code scripts used to build the models under R software are available at the supplementary material (see Supplementary Information S102 ) and open-source, archived at GitHub, with Zenodo, at: https://doi.org/10.5281/zenodo.7139034 34 ."
10.1038/s41597-022-01797-2,Code in R language to replicate the database creation have been made available from Figshare 25 .
10.1038/s41597-022-01796-3,"Python code for producing, reading and plotting data in the dataset is provided at https://github.com/dh107/Carbon-Monitor-Cities/ ."
10.1038/s41597-022-01712-9,"The three AI models introduced in this article, along with the Jupyter notebook and scientific software needed to reproduce our results have been released through DLHub 36 , 37 , 38 . Scientific software to train these models may be found in an open source GitHub repository 33 ."
10.1038/s41597-022-01793-6,The codes are fully operational under R 4.1.3. The package stplanr v0.4.0 15 was referenced to implement the modified radiation; the visualization of the trade flows was implemented with the package sf v1.0-4 16 . The codes for the modified radiation model and the estimation are available on Github: https://github.com/MathGISer/Poultry-Trade-Network .
10.1038/s41597-022-01809-1,Code and data used to reproduce all the figures and tables are available in the GitHub repository https://github.com/tanio003/GOPOPCORN_Data_Codes and archived here ( https://doi.org/10.5281/zenodo.6967484 ) 44 .
10.1038/s41597-022-01823-3,"All data analyses were completed on the Linux system using standard bioinformatic tools. The codes of the main procedures are described below. (1) Sequencing data mapping bwa mem -t 16 <ref_dir>/hg19.fa <sample_blood>_1.fq.gz <sample_blood>_2.fq.gz | samtools view -@ 16 -Shb -o<sample_blood>.bam samtools sort -@ 16 <sample_blood>.bam <sample_blood>.sorted picard MarkDuplicates INPUT = <sample_blood>.sorted.bam OUTPUT = <sample_blood>.sorted.dedup.bam METRICS_FILE = <sample_blood>.metrics.txt picard BuildBamIndex I = <sample_blood>.sorted.dedup.bam O = <sample_blood>.sorted.dedup.bam.bai (2) Coordinate sorting and duplicate marking bwa mem -t 16 <ref_dir>/hg19.fa<sample_tissue>_1.fq.gz<sample_tissue>_2.fq.gz | samtools view -@ 16 -Shb -o<sample_tissue>.bam samtools sort -@ 16<sample_tissue>.bam<sample_tissue>.sorted picard MarkDuplicates INPUT = <sample_tissue>.sorted.bam OUTPUT = <sample_tissue>.sorted.dedup.bam METRICS_FILE = <sample_tissue>.metrics.txt picard BuildBamIndex I = <sample_tissue>.sorted.dedup.bam O = <sample_tissue>.sorted.dedup.bam.bai (3) Identification of somatic mutations <savi_dir>/savi.py --memory 16 --superverbose --bams <sample_blood>.sorted.dedup.bam, <sample_tissue>.sorted.dedup.bam --names NORMAL,TUMOR --ref <ref_dir>/hg19.fa --outputdir <sample_dir>"
10.1038/s41597-022-01766-9,The forcing dataset for this century-long eddy-resolving simulation is available from https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era-20c (last accessed: 29 May 2019). The source code of LICOM3 is available at https://github.com/yongqiangyu/FGOALS.git .
10.1038/s41597-022-01772-x,The codes used for Global Daily-scale Soil Moisture Fusion Dataset (GDSMFD) are available in National Tibetan Plateau Data Center ( https://doi.org/10.11888/RemoteSen.tpdc.271988 ) 61 .
10.1038/s41597-022-01808-2,"All code is publicly available on our GitHub repository for the 50 stateSimulations project, https://github.com/alarm-redist/fifty-states . The package alarmdata 29 is a user-facing R package to download and work with our plans on the repository and our Dataverse."
10.1038/s41597-022-01762-z,"All reference genomes and scripts for mapping, assembly, genome coverage estimation, subsampling and correlation calculations associated with tables and figures are available at https://forgemia.inra.fr/metagenopolis/benchmark_mock ."
10.1038/s41597-022-01784-7,"The software code related to the Health Gym project is publicly available at https://github.com/Nic5472K/ScientificData2021_HealthGym . Our code is mainly written in Python 90 using the PyTorch 91 package for deep learning. In order to replicate our results, users will need access to the MIMIC-III 21 , 22 and EuResist 23 databases. MIMIC-III is a restricted-access resource; and users must complete the data use agreements on PhysioNet (see: https://physionet.org/content/mimiciii/1.4/ ). The EuResist Integrated DataBase (EIDB) can be accessed for scientific studies once a proposal for analysis has been approved by EuResist’s Scientific Board (see: http://engine.euresist.org/database/ ). Additional code for the data pre-processing for acute hypotension and sepsis can be found in the repository of Komorowski et al . 10 at https://gitlab.doc.ic.ac.uk/AIClinician/AIClinician/-/tree/master/ . This includes a combination of code in SQL 92 , Matlab 93 , Python, and their extension packages 94 , 95 , 96 , 97 . For more details, see explanations and usages in the Supplementary Materials."
10.1038/s41597-022-01795-4,"Two separate MATLAB programs were used for flattening the captured OCT images and for A-scan profiling. MATLAB version 8.3 (R2014a) was installed for running the script and there was no special requirement for this analysis. First, the OCT image needs to be loaded in the image flattening program, and then the mouse pointer needs to be dragged from the right to the left of the image to get the desired flattened image. The ROI of the flattened section can be adjusted. After saving the flattened OCT images, the A-scan profiling program needs to be applied to the saved flattened images to get their A-scan profiles. The details documentation of the MATLAB script will help to reuse the code. From A-scan profiles, the width and height of UE, PP, SP layer intensity, and leaf thickness can be measured to estimate and set the threshold for pre-identification of leaf abnormalities. The leaf layer intensity and thickness measurement process has already been discussed in the ‘leaf inspection algorithm for persimmon and apple’ section in the methods part. The MATLAB program files named ‘Image_flattening.m’ and ‘A_scan_profiling.m’ have been uploaded to Figshare 63 ."
10.1038/s41597-022-01818-0,All the code is freely available in Github at https://github.com/EuropeanSocialInnovationDatabase/ESID_V2 .
10.1038/s41597-022-01815-3,The software described in this paper is available for execution at http://metadatacenter.org . All source code is available from the Metadata Center landing page at https://github.com/metadatacenter .
10.1038/s41597-022-01715-6,The MATLAB code used to design the two phantoms presented in this work is available in the Figshare “ Phantom-Creator ” repository 16 . The MATLAB version 2020a and the Image Processing Toolbox are required to run the code.
10.1038/s41597-022-01785-6,The code for the LSTM-based soil simulations can be downloaded at https://github.com/osungmin/SciData2022_SoMo_EU .
10.1038/s41597-022-01606-w,We have made code available which enables file-level metadata extraction 51 for files that adhere to the reporting format.
10.1038/s41597-022-01825-1,"Since bioinformatics tools and pipelines are continuously updated, we urge researchers to start their analysis from the raw data and use up to date bioinformatics approaches. Therefore, we did not supply the specific code used during the HeCaToS project. Appendix II & III contain custom R-scripts for microRNA parsing and proteomics QC respectively."
10.1038/s41597-022-01798-1,"No custom code was used, and all analysis was performed using standard software."
10.1038/s41597-022-01821-5,"The data analysis methods, software and associated parameters used in present study are described in the section of Methods. If no detail parameters were described for software used in this study, default parameters were employed. No custom scripts were generated in this work."
10.1038/s41597-022-01834-0,The application is publicly available and can be accessed via the following link: https://jaafarhadi.users.earthengine.app/view/runoff-from-gcn250 . The code for the application 40 is available here on figshare in a text file (Code.txt): https://doi.org/10.6084/m9.figshare.19596157 .
10.1038/s41597-022-01819-z,Code to recreate the database is included in our Figshare upload 38 and can also be found in a dedicated repository https://github.com/wschuell/repo_datasets . The software is written in the Python programming language. The database can be created as either PostgreSQL or SQLite database. Version requirements are recorded in the project’s Readme file.
10.1038/s41597-022-01817-1,Software is available on GitHub: github.com/timvanderzee/human-walking-biomechanics.
10.1038/s41597-022-01812-6,Code used to determine statistics for the WCVP: Fabaceae checklist is available on Github ( https://github.com/jhnwllr/The-World-Checklist-of-Vascular-Plants–Fabaceae ).
10.1038/s41597-022-01813-5,"The source code for all the analyses presented in this paper, and also the web dashboard, can be found on these GitHub repositories: github.com/thegraphnetwork/COVID-CH-dashboard and github.com/thegraphnetwork/epigraphhub_py . The first one for the web dashboard, and the second for the all the data collection and analyses. The documentation can be found on this link: epigraphhub-libraries.readthedocs.io ."
10.1038/s41597-022-01788-3,All codes produced by this work for data analysis are available in the GitHub repository ( https://github.com/BioRDM/COVID-Wastewater-Scotland ) with rights to access under the Creative Commons (CC) BY License. The software environment used for data analysis including data normalisation and the heatmap and map figures (Figs. 2 – 3 ) was R (v4.1.2).
10.1038/s41597-022-01827-z,"For generating the catalogues, the IpoP code 31 , 32 , the Complete Automatic Seismic Processor (CASP 40 ) and RSNI-Picker2 41 , 42 are available upon request. All of the other codes are all open access: NonLinLoc software 35 used for CAT1 and CAT3; HypoDD 36 , 38 for CAT2, CAT4 and CAT5; PhaseNet picker 14 , (REAL) package 48 , Velest code 49 and HypoInverse software 50 used for generating the dataset of CAT5. The performed processing (Table 1 , Figs. 3 , 4 , and 6 ) are common statistical representations of the data and do not require custom codes; M c Lilliefors was calculated with the Python class of Herrmann and Marzocchi 57 . The Generic Mapping Tools ( www.soest.hawaii.edu/gmt ) were used for creating Fig. 1 , the Python graphing library plotly ( www.plotly.com/python ) for creating Figs. 2 – 5 , and Matlab ( www.mathworks.com ) for creating Fig. 6 ."
10.1038/s41597-022-01799-0,"The code for data processing and analysis was written in Python (version 3.6.9), using ArcPy in an ArcGIS Notebook with ArcGIS Pro (version 2.5.1) 51 . The DigitizeAfrica building footprints 52 used in creating the output index for urban areas in sub-Saharan Africa are available for humanitarian purposes on request from Ecopia ( https://www.ecopiatech.com/ ). Similar datasets for some countries are openly-available, such as Microsoft building footprints or Google Africa Open Buildings. The code used to create the spatial units and calculate the index values described in this paper is available to download from GitHub, in the following repository: https://github.com/heatherchamberlain/SocDistIndex ."
10.1038/s41597-022-01807-3,The source code for the Bioregistry is available at https://github.com/biopragmatics/bioregistry under the MIT License. The source code specific to the version of Bioregistry used in this article (v0.5.132) is archived on Zenodo 40 .
10.1038/s41597-022-01780-x,Python is used for all the analyses and implementations. The code to create the features for each city and to run the baseline experiments is available on our GitHub project ( https://github.com/zhu-xlab/So2Sat-POP ).
10.1038/s41597-022-01831-3,No code was used in this study.
10.1038/s41597-022-01820-6,All custom-written MATLAB scripts used for EEG processing and calculation of neurophysiological measures are included in the ‘code’ folder of the provided dataset.
10.1038/s41597-022-01824-2,"The processing steps to obtain the dataset were carried out in R and Rmarkdown and are reproducible (except for one step of the harmonization of DHS regions and variables that had to be done manually). All the code is available on the git repository of this article: https://gitlab.pik-potsdam.de/belmin/livwelldata-paper . The source code for the companion R package livwelldata is available on the git repository of the package: https://gitlab.pik-potsdam.de/belmin/livwelldata . The following R packages were central to the development of LivWell: tidyverse 41 , knitr 42 , rdhs 27 , DHS.rates 32 and survey 31 ."
10.1038/s41597-022-01822-4,"No custom code was used to generate, process, or analyse the data presented in the manuscript."
10.1038/s41597-022-01840-2,"The stimuli presentation, data preprocessing, and technical validation scripts are available at https://openneuro.org/datasets/ds004301 4 ."
10.1038/s41597-022-01755-y,"Software used for data generation and analyses are described in detail in the methods sections for each data type and in the references cited in those sections. The project utilized iRODS ( https://irods.org ) for data storage and organization and an Oracle relational database for supporting mathematical modeling and deposition of data to public repositories. Our relational database implemented the Scientific Knowledge Extraction from Data (SKED) schema 83 , 84 . An in-house custom tool, xport4sub, was developed to help prepare submissions to pubic repositories. Each public repository’s submission requirements and templates were unique. xport4sub was designed to be configured for a specific repository’s submission templates and to populate those templates automatically from MaHPIC’s relational database. Both the xport4sub tool and the SKED schema are published under MIT license and are available on MaHPIC Github repository ( https://github.com/mahpic/ )."
10.1038/s41597-022-01769-6,The pollution detection algorithm described in Beck et al . 30 to identify and flag periods of primary polluted data in remote atmospheric time series is available at 64 : https://doi.org/10.5281/zenodo.5761101 . The TERN peak fitting tool implemented in Igor Pro used for the analysis of the GC-MS/FID chromatograms is available at https://sites.google.com/site/terninigor/ .
10.1038/s41597-022-01730-7,"The web scraping and data-basing code used in this paper are available from https://github.com/erinlmartin/figshare_geoscrape.git . Instructions for use are available with the code. All data presented in this study are previously published and available on the Figshare repository. DOIs for the data are included within the GEOSCRAPE 22 database hosted with GEOROC at: https://doi.org/10.25625/FWQ7DT . This database will be updated by ELM until 2027. Table S1, SQLite template files 35 and metadata table available from Figshare: https://doi.org/10.6084/m9.figshare.16870603.v3 ."
10.1038/s41597-022-01833-1,"All code used to process the database is publicly available online. References to software are provided in Table 6 . The pipeline to evaluate new candidate tractograms along with the test set is available here: https://github.com/scil-vital/TractoInferno/ , and should be used with the same reference atlas: https://zenodo.org/record/4630660#.YJvmwXVKhdU 62 ."
10.1038/s41597-022-01835-z,SURE 2.0 database is available at Zenodo 3 .
10.1038/s41597-022-01830-4,"COVID-19 SeroHub uses custom code to store manually extracted data without processing. As described above, these data are downloadable by users via spreadsheet or API. Tools for visualizing extracted data were produced using HTML 5.0 and ECMAScript/Javascript and tested with Chrome Browser 90.x, Safari 14.x, and Firefox 88.x. The Apache eCharts 5.1.1 library was used to produce the Studies Map, Interactive Seroprevalence Tool, and Individual Study Page data visualizations. API code was produced using the Python version 3.8 on AWS Lambda, AWS API Gateway, and Elasticsearch 7.9. All code and cloud resources are secured in Federal Information Security Modernization Act (FISMA) compliant environments."
10.1038/s41597-022-01841-1,"This study utilized Mathwork Matlab 2020a to parse and export the ASF/AMC and BVH files. The open-source code used for parsing these files can be obtained from https://github.com/lawrennd/mocap . This study utilized Python 3.7.6 and extended a 17-joint BVH conversion package, video-to-pose3D ( https://github.com/HW140701/VideoTo3dPoseAndBvh ), to generate BVH files. The newly developed package can transform 15 or 20-joint models’ JSON files into BVH files. The developed code can be accessed with the following URL: https://github.com/YUANYUAN2222/GIT_json_to_BVH . Meanwhile, the code could be used to retag and process different datasets (i.e., Resampling and Skeletal structure alignment) is made public on the GitHub ( https://github.com/YUANYUAN2222/Integrated-public-3D-skeleton-form-CML-library ), which allow all readers and potential users to process the source dataset by themselves."
10.1038/s41597-022-01849-7,All the SAR acquisitions were processed by means the commercial software GAMMA© developed by the Swiss corporation GAMMA Remote Sensing AG (see https://www.gamma-rs.ch/ ). Annual updating of CSK InSAR products can be accessed via the INGV Iridium portal or upon request to the corresponding author.
10.1038/s41597-022-01828-y,GPPCY-IN was developed on Google Earth Engine (JavaScript) and calibration/validation was performed in MATLAB in the local system. A code for GPPCY-IN calculation can be found here: https://github.com/Prasun-G/GEE-code-for-Agricultural-GPP-India .
10.1038/s41597-022-01832-2,"All classical and quantum chemical calculations have been performed by using the SMSS 24 , which is a proprietary software package. The solubility predictions have been made by using the AqSolPred 20 , which is a freely accessible tool. In addition, the in-house developed Python scripts that have been used to parse the calculation outputs and to convert them into relational database formats, are openly accessible at https://github.com/ergroup/RedDB ."
10.1038/s41597-022-01845-x,No custom computer codes were generated in this work.
10.1038/s41597-022-01846-w,"We have listed the names and versions of the software used for data analysis. FastQC, version 0.11.3, was used for the quality check of the raw FASTQ sequencing file. https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . FASTP, version 0.23.1, was used to remove low-quality reads. https://github.com/OpenGene/fastp#per-read-cutting-by-quality-score . PEAT, version 1.2, was used to remove the adaptor sequence. https://github.com/jhhung/PEAT . STAR, version 2.6.1 was used for mapping. https://github.com/alexdobin/STAR . featureCount, SUBREAD, release 1.6.5 was used for expression counting. http://subread.sourceforge.net R package, version 4.0.3, was used for downstream gene analysis. https://www.r-project.org iDEP was used for the identification of DE genes. http://bioinformatics.sdstate.edu/idep/ TCC-GUI tool was used for downstream gene analysis. https://github.com/swsoyee/TCC-GUI ."
10.1038/s41597-022-01847-9,"The data directly describe the patients’ answers given in a numerical format and no analyses were performed, therefore no custom code was necessary to generate or process the data."
10.1038/s41597-022-01842-0,All code for the web server backend and frontend are publicly available for download on GitHub: https://github.com/grimmlab/HeliantHome .The web-application can be accessed via: http://www.helianthome.org .
10.1038/s41597-022-01837-x,"The bioinformatics software used for this study is Partek® Flow® software (Version 10.0.21.0302, Copyright © 2021, Partek Inc., St. Louis, MO, USA) and the link of online REVIGO tool used is: http://revigo.irb.hr/ ."
10.1038/s41597-022-01836-y,"The custom codes used for reading the signals of the database was created in MATLAB R2017b and is freely accessible at Physionet and IEEE Dataport 20 , 21 . To implement the codes, the users will need a MATLAB License • A readme file (readme.txt) with instructions about how to run the code in a 2017b or higher MATLAB version. • A Matlab script (fileread.m) with a simple example about how to read WFDB files and convert them to .mat format. • A text file (MotionSequence.txt) which provides the gesture sequence, and thus can be used to assign class labels to input data. • A Matlab script (feature_extraction.m) allows a simple example to extract frequency features using featiDFTI.m and segmentEMG.m functions. • A Matlab function (featiDFTI.m) for generating frequency division technqiue features from sEMG Data. • A Matlab script (segmentEMG.m) for implementing windowing of sEMG Data."
10.1038/s41597-022-01844-y,"Within the repository, we also provide code for extracting climate data of each subbasin from the Climatic Research Unit at the University of East Anglia ( http://www.cru.uea.ac.uk/ ) and the Global Precipitation Climatology Centre ( https://climatedataguide.ucar.edu/climate-data/gpcc-global-precipitation-climatology-centre ) in the code folder. ♦ The shp folder contains 218 subbasin boundary shp files. ♦ The downloaded input data are stored in 3 nc files with annual average temperature, annual precipitation, and annual potential evaporation data (1901–2019) in yearmean_cru_ts4.04.1901.2019.tmp.dat.nc , yearmean_GPCC_1901–2019_05.nc , and yearmean_cru_ts4.04.1901.2019.pet.dat.nc , respectively. ♦ The code for extracting data from the .nc file to the .xlsx file was written in Python, and extract_tmp-nc_to_xlsx.py , extract_precip-nc_to_xlsx.py , and extract_pet-nc_to_xlsx.py were used to extract temperature, precipitation, and evaporation data, respectively. ♦ The output data will be stored in .xlsx format ( multi_yr_tmp_subbasins6_1901–2019.xlsx , multi_yr_precip_subbasins6_1901–2019.xlsx , multi_yr_pet_subbasins6_1901–2019.xlsx for temperature, precipitation, and potential evaporation data, respectively) in folders tmp_clip_sub6 , precip_clip_sub6 and PET_clip_sub6 after running the code."
10.1038/s41597-022-01861-x,Custom code was not used in the generation of any of the files and data that are available for public access.
10.1038/s41597-022-01854-w,"UATD dataset is published in a figshare repository 19 . Furthermore, the annotation tool OpenSLT is published alongside the dataset, archived as “UATD_OpenSLT.zip”. OpenSLT is developed based on Qt 5.9. The tool worked well on Ubuntu 18.04/20.04 environment during our annotation work. In addition, we provide an example (a small dataset with several sonar image files and corresponding CSV files) with the tool for users to test. The file README.md along with the tool plays the role of guidance for the users."
10.1038/s41597-022-01814-4,NWChem is distributed as open-source under the terms of the Educational Community License version 2.0 ( https://www.nwchem-sw.org ). The RDKit software is freely available under the BSD license ( http://www.rdkit.org ). The Open Babel software is freely available under GNU GPL ( http://openbabel.org/wiki/Main_Page ).
10.1038/s41597-022-01863-9,Codes in developing the lake ice phenology dataset are freely available. Codes of the modified air2water model for lake surface water temperature simulation are available at: https://github.com/Siyu1993/ModifiedModel . Codes for lake ice phenology identification are written using Matlab and are available at: https://github.com/Siyu1993/MatlabCode.git .
10.1038/s41597-022-01839-9,The R code used for the analysis of scATAC-seq data is available on GitHub ( https://github.com/fangli0909/scATAC-seq_sa ).
10.1038/s41597-022-01848-8,The python manufacturing classification code and machine learning samples are available at https://doi.org/10.6084/m9.figshare.19808407 26 .
10.1038/s41597-022-01851-z,"Preprocessing code for the dataset and few-shot learning algorithms to verify its quality can be found here: https://github.com/Brain-Cog-Lab/N-Omniglot . The code uses Python3 and PyTorch platforms, and the Torchvision package version is expected to be higher than 0.8.1. Please refer to the Usage Notes section and ReadMe file to run the code."
10.1038/s41597-022-01864-8,"The data provided in this paper was processed on a desktop computer running Scientific RedHat Linux 6 in the tcsh shell. The open-source conversion programs runpkr00 (available at UNAVCO via Trimble, www.unavco.org ) and TEQC (also available at UNAVCO) were used to convert native receiver file formats to RINEX2.11 files when RINEX2.11 files were not available from surveyors. Vim, an open-source visual editor, was used to edit headers of RINEX2.11 files and to view summary files created during quality checking. Data quality checking was also completed using TEQC. Technical figures in this paper were created using the open-source program Generic Mapping Tools (GMT 26 )."
10.1038/s41597-022-01843-z,"The dataset has been acquired using the Multiple Azure Kinect GUI software, whose source code and the corresponding installer are available at “ https://gitlab.com/roberto.marani/multiple-azure-kinect-gui ”. This software is based on the Azure Kinect Sensor SDK v1.4.1 and Azure Kinect Body Tracking SDK v1.1.2 19 . In particular, the Azure Kinect SDK provides an API to record device data in a Matroska (.mkv) file containing video tracks, IMU samples, and device calibration. In this work, IMU samples are not considered. The Multiple Azure Kinect GUI software processes the Matroska file and returns the different types of data: RGB images, RGB-to-Depth-Aligned (RGB-A) images, Depth images, IR images, and Point Cloud. At the same time, exploiting the Azure Kinect Body Tracking SDK, skeletal data are stored in the corresponding TXT files. Along with the dataset, a Matlab demo code (.m file) is also provided to plot the skeletons onto the corresponding RGB images."
10.1038/s41597-022-01838-w,The code to produce the AOH is derived from code produced by Rondinini et al . 6 . AOH maps are produced reclassifying a base map that contains information on elevation and land cover. The geographical range maps are used to mask the areas outside the distribution of the species. Each species has a reclassification file that indicates which land cover classes and elevations are suitable. To transform the habitat information into land cover we used the translation table 12 . The code is both in GRASS and R.
10.1038/s41597-022-01874-6,"The data production algorithms have been introduced in the manuscript, which were conducted in GEE with simple implementation of water index methods. The code which is used to detect urban waterbody in GEE is available with the dataset at the figshare repository ( https://doi.org/10.6084/m9.figshare.20583558 ) 70 ."
10.1038/s41597-022-01862-w,"The .csv files composing the dataset were extracted directly from Tobii Pro X3–120 Eye Tracker, as well as images such as heatmaps. The original raw-data files and the python code used to generate the .csv files, figures and statistics here are available at Figshare 66 ."
10.1038/s41597-022-01852-y,"Source code is available at https://github.com/gav-sturm/Cellular_Lifespan_Study which includes data preprocessing, modeling, and analysis scripts. We have further included source code of the associated ShinyApp."
10.1038/s41597-022-01859-5,"All software and pipelines used in this study were executed according to the manual and protocols of the published bioinformatic tools. The versions of software have been described in Methods. The parameters of software/programs are as follows: Jellyfish: count -C -m 21. Genomescope: Rscript genomescope-1.0.0/genomescope.R NGS_reads.histo 21 150 NGS_reads.genomescope. Canu: genomeSize = 636 m -nanopore-raw. minimap2: -x map-ont. purge_dups: −2 -T cutoffs -c PB.base.cov. BUSCO: -m prot -f -l eukaryota_odb9. ProtTest: -all-distributions -F -AIC -BIC. Default parameters were used for Porechop, Pilon, Racon, RepeatMasker, RepeatScout, RepeatScout, Augustus, Genemark, Prothint, Braker, hmmsearch, OrthoFinder, MUSCLE and Gblock."
10.1038/s41597-022-01865-7,The image dataset was tagged by using the labelImage 44 graphical tool.
10.1038/s41597-022-01856-8,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01826-0,"Figures 4 – 8 were produced in Matlab. Example code and auxiliary functions to (1) reproduce Figs. 4 – 6 and (2) run the automated QC checks on the database are available on GitHub ( https://github.com/EJJudd/SciDataSupplement ). The paleocoordinates used to produce Figs. 7 , 8 were estimated using the plate model of Scotese and Wright 707 , implemented in G-Plates (Version 2.2.0) 729 ."
10.1038/s41597-022-01860-y,"Beyond the raw data files, we have also included code in Python, JavaScript (via the Node.js framework), and Matlab to read the JSON file into a native format (e.g., a Python dictionary)."
10.1038/s41597-022-01774-9,Does not apply.
10.1038/s41597-022-01576-z,The source code to generate the technical analysis has been uploaded to GitHub: https://github.com/BioAITeam/COVID19-Detection/tree/main/Cov-Caldas%20Dataset .
10.1038/s41597-022-01872-8,All of the code used to generate the figures and perform the analyses are included with the public lumos-ncpt-tools repository described above: https://github.com/pauljaffe/lumos-ncpt-tools/tree/v1.1.0 . See the README file for instructions on how to reproduce the figures and analyses. The software underlying the cognitive tasks themselves is proprietary and consequently cannot be shared at this time.
10.1038/s41597-022-01853-x,"The following open access software and versions were used for quality control and data analysis as described in the main text: 1. FastQC, version 0.11.5 was used for quality analysis of raw FASTQ sequencing data: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ 2. MultiQC, version 1.11 was used to aggregate and visualize FastQC and HISAT2 data outputs: https://multiqc.info/ 3. HISAT2-index-align, version 2.2.1 was used to index and align sequencing reads to the human hg38 genome: http://daehwankimlab.github.io/hisat2/ 4. featureCounts, version 2.0.3 was used to assign sequence reads to features in the human genome: http://subread.sourceforge.net/ 5. DESeq2, version 1.36 was used to quantify differentially expressed transcripts across various time points and replicates: http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html All code and scripts used for quality assessment and data analysis in this study is available at: https://github.com/WahlinLab/Organoid_RNAseq_SciData22 ."
10.1038/s41597-022-01866-6,The version and parameters of all of the bioinformatics tools used in this work are described in the Methods section.
10.1038/s41597-022-01875-5,"In order to facilitate future users of this dataset to get familiarized with the images, we have released the following ISLES 2022 Github repository: https://github.com/ezequieldlrosa/isles22 . The repository contains scripts to read the images, visualize them, and to quantify the algorithmic results performance with the same metrics used in the challenge to rank participants. Besides, we have also released the image container that is used during the challenge in order to evaluate the participants’ algorithm submissions ( https://github.com/ezequieldlrosa/isles22_docker_evaluation )."
10.1038/s41597-022-01871-9,WGS: https://gitlab.com/smlce/unideb_wgs RNA-seq: https://gitlab.com/smlce/unideb_rna-seq ChIP-seq: https://gitlab.com/smlce/unideb_chip-seq
10.1038/s41597-022-01867-5,The images data are available at Figshare repository 15 and data augmentation code are available using OpenCV library. Data annotation tool using LabelImg is available online 16 .
10.1038/s41597-022-01868-4,No specific code was developed for this work. The data analyses were performed according to the manuals and protocols provided by the developers of the corresponding bioinformatics tools that are described in the Methods section together with the versions used.
10.1038/s41597-022-01879-1,These simulations were conducted using PCTRAN-PWR3LP ( https://github.com/thu-inet/NuclearPowerPlantAccidentData/tree/main/Simulator/ ). The data processing step was performed using scripts written in the Python 3.10 programming language. More about this dataset can be found on the dataset’s GitHub page ( https://github.com/thu-inet/NuclearPowerPlantAccidentData ).
10.1038/s41597-022-01877-3,The raw data collected and stored in a MySQL database was then converted to CSV formats using custom scripts in Python 3.8 with the library pandas for data manipulation. All plots supporting the fault descriptions were performed using Plotly library.
10.1038/s41597-022-01869-3,No specific code was generated for analysis of these data.
10.1038/s41597-022-01884-4,"The R code for estimating photosynthetic capacities, calculating the timing and seasonality of precipitation, and extracting soil and vegetation information are available in the open GitHuB repository ( https://github.com/lpice/code-CPTDv2-.git ) The SPLASH code, in four programming languages (C++, FOR- TRAN, Python, and R), is available on an online repository under the GNU Lesser General Public License ( https://bitbucket.org/labprentice/splash )"
10.1038/s41597-022-01888-0,"The codes to construct the GRNWRZ V2.0 data set are available in Figshare 33 . There are two files and a folder: 01_Data Prepare.tbx – Prepare hydrological data, including flow direction and flow accumulation. 02_RNWRZ – Contain Fortran files, which are used to generate the RN and WRZ at levels from 3 to 7. 03_extract_RNWRZ.py – Generate codes for the RN and WRZ. The ArcGIS Model(.tbx) and Python(.py) scripts are run in ArcGIS Pro (v2.4 or above vision). The remaining folder contains all Fortran files. It was compiled by Intel Parallel Studio XE 2018, installed in Visual Studio 2013. All software needs to be installed in the Windows 10 OS."
10.1038/s41597-022-01881-7,The code used for processing this dataset is publicly available in our GitHub repository ( https://github.com/ysuter/gbm-data-longitudinal ). The Python and Bash scripts are available to reproduce and customize the extraction of radiomics features.
10.1038/s41597-022-01892-4,The code used to pre-process the data set is publicly available on GitHub and is accessible through the link: https://github.com/dotlab-brazil/tuberculosis_preprocessing .
10.1038/s41597-022-01873-7,"No code was used to generate this dataset. The codes to reproduce Figs. 1 , 2 , 3 , 4 , and 6 shown in the article are publicly available in the GitHub ( https://github.com/zliangocean/DOPv2021 ). Figure 5 was plotted by using Ocean Data View 36 software ( odv.awi.de )."
10.1038/s41597-022-01857-7,R (4.1.2) was used to generate the PAD-US-AR 48 dataset and results. QGIS (3.18.3) was used to create the maps. Scripts and source data to reproduce results are available on OSF ( https://osf.io/pwdsg/ ).
10.1038/s41597-022-01887-1,"All software used in this study were executed according to the manual and protocols of the published bioinformatic tools. FastQC, version 0.11.3, was used for the quality check of the raw FASTQ sequencing files. https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ . The versions and parameters of the transcriptome assembly and expression analysis software described in the methods section are as follows: Trimmomatic, version 0.36, LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50. Trinity, version 2.4.0, --seqType fq--SS_lib_type RF. hisat2, version 2.2.1.0, --rna-strandness rf –fr. cufflinks, version 2.2.1, --library-type fr-firststrand. htseq-count, version 0.9.1, -s reverse DESeq, version 1.18.0, pvalue < 0.05, |log2FoldChange| >1"
10.1038/s41597-022-01858-6,No code was used in the generation of this data. No code is required to access or analyse this dataset.
10.1038/s41597-022-01669-9,The code developed for the NAPKON public dataset is available as open-source software 30 .
10.1038/s41597-022-01756-x,"The program codes for data cleaning and temporal network reconstruction are shared along the dataset in an open repository. The codes have been developed in Python language using only standard or open licensed packages, and they are shared as iPython notebooks at 26 ."
10.1038/s41597-022-01870-w,"There are download scripts and data loaders available in the repositories https://gitlab.com/matschreiner/Transition1x and https://gitlab.com/matschreiner/QM9x . See the repositories and README for examples and explanations of how to use the scripts and datasets. All electronic structure calculations were computed with the ORCA electronic structure packages, version 5.0.2. All NEB calculations were computed with ASE version 3.22.1. Scripts for calculating, gathering, and filtering data can be found in the Transition1x repository. scripts/neb.py takes reactant, product, output directory, and various optional arguments and runs NEB on the reaction while saving all intermediate calculations in an ASE database in the specified output directory. scripts/combine_dbs.py takes an output path for the HDF5 file and a JSON list of all output directories produced by running the previous script and combines them in the HDF5 file as described in the paper. See the repository for how to install, specific commands, options, and further documentation."
10.1038/s41597-022-01876-4,"GeoModeller GeoModeller is a proprietary software developed by the French Geological Survey (BRGM) in collaboration with Intrepid Geophysics. It has been designed for three-dimensional interpretation and modelling based on the integration of various types of data, such as field observations and geophysical measurements. The calculation of the model is based on geological rules. These rules are defined in a geological pile that manages the relationships between the geological formations to be modelled 54 . Data are associated with formations and faults to constrain their geometry. The modelling of these geological objects relies on the interpolation of the input data. GeoModeller’s interpolation method produces a scalar field where iso-values represent the geological interfaces. The scalar field is interpolated by co-kriging two types of data: (1) 3D points that are the location of observed or interpreted geological interfaces and faults, and (2) orientation data that are 3D vectors representing the dip of geological formations. These two types of data are respectively associated with the iso-values and the gradients of the scalar field to be interpolated 54 , 60 . GeoModeller also enables direct and inverse geophysical calculations to improve and refine the geomodels 61 . For further information, see: https://www.geomodeller.com The GeoModeller version used for this study is the following: GeoModeller Version: 4.0.7 Build Date: May 22, 2019 Build Number: 27eee3dc31ba The default parameters of GeoModeller were used for the interpolation of the geomodel presented in this paper."
10.1038/s41597-022-01878-2,"The code to (1) create the raw CloudSEN12 imagery dataset, (2) download assets associated with each ROI, (3) create the manual annotations, (4) build and deploy cloudApp, (5) generate automatic cloud masking, (6) reproduce all the figures, (7) replicate the technical validation, (8) modify cloudsen12 Python package, and (9) train DL models are available in our GitHub organization https://github.com/cloudsen12/ ."
10.1038/s41597-022-01910-5,No specific script was used in this work. The codes and pipelines used in data processing were all executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-022-01855-9,"Code and data splits for baseline algorithms are available at Github, in https://github.com/qccData/qccCorpus ."
10.1038/s41597-022-01883-5,"Code for the spectral identification matching routine as well as the generation of figures for this paper are located on Github: https://github.com/emilymiller/spectra_microplas_reflib 34 . The script “user_instructions.txt” orients users to the spectra processing scripts titled, “cleaning_code.R” and “preprocessing.R”, followed by the spectral matching routine script, “technical_valid_weathered.R”."
10.1038/s41597-022-01894-2,Code for the pyCANON library and each compliance test is available in https://github.com/IFCA/pycanon . The library documentation can be found in https://pycanon.readthedocs.org .
10.1038/s41597-022-01891-5,"The custom awk script for filtering of VCF files is available at e!DAL-PGP and can be accessed here 82 . Custom R codes for phenotypic parameter estimations are included within the respective ‘R_code’ subfolder of each dataset 71 , 72 , 73 , 74 deposited into e!DAL-PGP. In addition, custom R codes to assess the genomic-phenotypic data interoperability and the computation of Roger’s distances as well as their needed inputs and expected outputs were also deposited into e!DAL-PGP and can be accessed here 70 ."
10.1038/s41597-022-01885-3,"The code used to generate the dataset is released under the BSD license and is available at Zenodo 24 . The repository has a directory with invoice templates in DOCX format and another directory with the dictionaries described above. The code of the pipeline was implemented in Python 3. For the first step, we used standard functions for simulating the data, based on the random, time, math, os , and string libraries. We also used the json library for working with JSON files. We used the docxtpl library to replace fields in the DOCX templates with the JSON data in the second step. Internally, this library relies on two packages: python-docx for reading, writing, and creating documents; and jinja2 for managing tags in the template. The DocxTemplate function allows replacing the text between braces with its corresponding value in the JSON file. The result is written in a DOCX file. Finally, this file is converted to PDF using the docx2pdf library. The program can be executed from the command line as python3 main.py . In order to generate the final version of the database, the program was executed in an Intel Core i9 CPU with 14 cores and 32.0 GB RAM, under Windows 10. The size of the database is approximately 30 GB in the disk. Each invoice may have between 2 and 4 pages, and the size of each PDF file on disk is between 160 KB and 1.68 MB. The size of JSON files is between 3 KB and 4 KB. The database contains 135 000 files with six and nine directories in the training and test sets, respectively. The test set has 45 000 invoices in PDF format and the training set has 90 000 files: 30 000 invoices in PDF format; 30 000 annotated invoices in PDF format; and 30 000 JSON files with the value of labels. In the non-distributable database, we have also included annotated invoices and labels for the test set, which amounts to 45 000 PDF and 45 000 JSON files, respectively. Therefore, the total number of files in this version is 225 000 files, which occupy 49.7 GB of disk. Regarding the execution time for the generation of the database, we calculated the average time of each step in the pipeline for each invoice; see Fig. 6 . This average time was calculated after eight runs with 500 invoices. The results in seconds per bill are as follows: • Simulation process: 0.036 s • Generation of the invoice file in DOCX format: 0.544 s • Conversion from DOCX to PDF: 2.053 s Therefore, the average time for creating one invoice was 2.633 seconds. The slowest step was the generation of PDF files (77.98% of the total time), followed by the creation of invoices (20.67%), and, finally, the simulation process (1.35%). The total run time was approximately 197 475 seconds, or 54.85 hours. The process was executed in several batches."
10.1038/s41597-022-01897-z,"The source codes contain various intellectual properties of our team members and we are not at liberty to distribute them. However, institutions may request for their own deployment of the system. Uers may contact the corresponding author (Jie He, email: hejie@ustb.edu.cn, Xiaotong Zhang, email: zxt@ies.ustb.edu.cn) for such requests."
10.1038/s41597-022-01895-1,"The instances of control software that produced DS 1, as well as the source code necessary to execute them–that is, the design methods, the simulator and the dependencies, the configuration files of the missions, together with scripts to compile the sources, generate necessary files, and execute the instances of control software–are available in a public repository 38"
10.1038/s41597-022-01900-7,"Each instrument is controlled either by code developed by the institution operating it or by code developed by the manufacturer and therefore often closed source or not even freely available and bundled with the instrument. Code used in the post-processing of the data has been developed by each institution, compiled in a package, and made available 114 . For the basic acquisition system of the Polar 5 aircraft and the KT-19, Werum Software & Systems AG has developed the software to communicate with the instruments and store the data. MiRAC-A radar, MiRAC-P, and HATPRO have been operated with software of the manufacturer Radiometer Physics GmbH. A LabView program by AWI controls AMALi and Nikon. The cloud particle probes CAS, CDP, CIP, and PIP are operated by a software from the manufacturer Droplet Measurement Technologies (DMT), where as for the 2DS it is Spec. Inc. and a LabView based program for the Polar Nephelometer. The spectral imager data acquisition software was developed by the manufacturer Specim, Spectral Imaging Ltd. Data evaluation was performed using the ENVI image analysis software. SMART is controlled by a LabView based software developed by Enviscope GmbH. The dropsonde system AVAPS has been post-processed with the Atmospheric Sounding Processing ENvironment (ASPEN, Version 3.4.4) 104 , which is publicly available. The ac3airborne package and tools developed within the project are written in python, open source, and publicly available on github 23 ."
10.1038/s41597-022-01917-y,No custom code was generated or used for analysis of the data presented.
10.1038/s41597-022-01906-1,The developed Python code for tagging and labelling the images is available through the Zenodo repository 49 . Another device that can be used for tagging fishes is the public Label Image tool ( https://github.com/tzutalin/labelImg ).
10.1038/s41597-022-01899-x,The entire build process cannot be made publicly available due to the inclusion of sensitive patient information. Our deidentification approach combined rule based approaches with a neural network. The rule based approach is published and publicly available 27 . The neural network is also publicly available and has been described elsewhere 18 . Online documentation describing the database in detail is available online at https://mimic.mit.edu . The source code to generate this documentation is open for contributions 28 .
10.1038/s41597-022-01909-y,MISSING
10.1038/s41597-022-01880-8,"The STATA do-file with codes used to create tables needed for creation of the presented dataset (FCCGD 29 ) from FHD is available at https://doi.org/10.17605/OSF.IO/MJFZ3 . The process of harmonisation and merging of the country’s individual datasets in order to create FHD has been recorded in a STATA do-file and is available upon request from Anna Kurowska. We used STATA 17 for this purpose. Due to country-specific policy restrictions, the FHD cannot be made available to the public."
10.1038/s41597-022-01893-3,"The up-to-date data are available from the general repository of the Ministry of Science of Chile at: https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto33/IndiceDeMovilidad.csv (IM indeces), and https://github.com/MinCiencia/Datos-COVID19/blob/master/output/producto29/Cuarentenas-Activas.csv (quarantines). The code to download the up-to-date data automatically and to reproduce the analysis in our paper is available at 58 ."
10.1038/s41597-022-01882-6,The scripts used in the generation of the SPICE dataset are available online at https://github.com/openmm/spice-dataset . The trained equivariant transformer models and the hyperparameters used to train them are available at https://github.com/openmm/spice-models/tree/main/five-et .
10.1038/s41597-022-01902-5,"No custom code was used to generate or process the data described in the manuscript. Rather, a combination of open software tools was applied. However, the supplementary material published at the repository 34 contains customized SPARQL commands to query the inventory."
10.1038/s41597-022-01916-z,"Figures were prepared using R Statistical Software (v4.1.2; R Core Team 2021) 53 , packages ggplot2 54 , treemap 55 ,funguild 51 and fungarium 56 . The dataset was shared via GBIF.org using Integrated Publishing Toolkit (IPT, www.gbif.org/ipt ) 57 . No original code was created to generate the dataset."
10.1038/s41597-022-01904-3,"Software to transform text files into ESRI-GRID ASC files is openly available on the GitHub 40 . Software to calculate Habitat Representativeness Score and PCA loadings is also openly available on the GitHub 92 and through a Web interface in the D4Science e-Infrastructure (RPrototypingLab VRE) 93 . R scripts to calculate cross-correlation and parameter statistics are available on our public Figshare repository 54 , in the “Script and Related Software” dataset."
10.1038/s41597-022-01920-3,"Code scripts to pre-process PLABA, reproduce the benchmark results of the experiments, and train and test additional models can be found at https://doi.org/10.5281/zenodo.7429310 , a Zenodo DOI 49 containing a static release of our GitHub repository."
10.1038/s41597-022-01924-z,The code used for analyses in this study can be found at https://github.com/JHKim36/FunctionalDiversisty/blob/main/Herpetofauna_code . ENM results and additional model information can be found at https://hyunkim36.users.earthengine.app/view/enmherptilekor .
10.1038/s41597-022-01925-y,No special code was used for analysis of the current data-set. All of the analyses were done with the following open access programs: FastQC version 0.11.9. ( https://github.com/s-andrews/FastQC ). Cell Ranger version 3.1.0 ( https://github.com/10XGenomics/cellranger ). Seurat. Version 3.1.4 ( https://satijalab.org/seurat ).
10.1038/s41597-022-01918-x,All the Python scripts used to generate the secondary data (binary images by Otsu’s thresholding) are provided at https://github.com/RajithaRanasinghe/Automatic_Thresholding .
10.1038/s41597-022-01922-1,All processing steps including data compilation and georeferencing were realized with ArcGIS Pro 2.9.5 software from ESRI 37 . The open-source software QGIS version 3.18 was used by assistants as additional software to georeferenced power plant locations 38 . We used the ArcGIS ‘ Edit – Create’ function and the QGIS ‘A dd feature’ function to manually assign power plant locations in cases where maps but no coordinates were accessible. Additional information is described in detail in the Material and Methods section. No stand-alone programming code was created.
10.1038/s41597-022-01912-3,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-022-01890-6,"The algorithm generating all possible cis and trans spliced peptides was originally described by Liepe et al . 63 . InvitroSPI method has been implemented with Snakemake in the Conda environment and is available at GitHub ( https://github.com/QuantSysBio/invitroSPI ). The analysis scripts (written in R) and implementation of invitroPB are available on Figshare online repository 70 . Analyses were carried out in R v4.1.1. Figures have been generated in R and postprocessing was done with Adobe Illustrator v25.2.3. The new in vitro TSN2 and TSN89 digestion samples were measured on Fusion Lumos Orbitrap, and acquired using Xcalibur v4.4."
10.1038/s41597-023-01937-2,Genome annotation: (1) RepeatMasker: parameters: -e ncbi -a -nolow -no_is -norna (2) TE-class: parameters: all parameters were set as default (3) Braker2: parameters: all parameters were set as default (4) PASA:–ALIGNERS blat (5) EvidenceModeler: parameters: all parameters were set as default Genome assembly: (1) CCS: parameters: all parameters were set as default (2) HiFiasm: parameters: hifiasm -u -o genome.asm–h1 R1.fq.gz–h2 R2.fq.gz ccs.fa.gz Gene family identification and phylogenetic analysis: (1) RAxML: parameters: -f a -m PROTGAMMAAUTO (2) MCMCTREE: parameters: all parameters were set as default The parameters of other not mentioned analysis modules were used as default parameters. The other custom codes used in this analysis were mentioned in the methods sections.
10.1038/s41597-022-01687-7,Computer code used for data analyses of this manuscript are previously published and referenced in the Methods section.
10.1038/s41597-022-01915-0,Code for the creation of CDFLOW is available as a series of R scripts via public repository on Figshare 19 ( https://doi.org/10.6084/m9.figshare.19787326 ).
10.1038/s41597-023-01939-0,"We automated all data processing and curation in the free and open software R (4.2.1, current version for windows). The data resources described in this paper, including R codes, can be accessed with no restrictions on the Synapse repository at https://doi.org/10.7303/syn26453964 . Anyone can browse the content on the Synapse website, but you must register for an account using your email address to download the files and datasets. Please see Table 3 and references 6 , 7 , 9 for details and links to the data resources."
10.1038/s41597-023-01931-8,Not applicable.
10.1038/s41597-023-01932-7,"We provide an example python script for loading the processed motion capture and vision data, named ‘main.py’ in the directory ‘data_loading_example’ on the Github repository. In addition we provide scripts for synchronization and frame-dropping, and examples of loading into pytorch machine learning pipeline. All code is available on ( https://github.com/abs711/The-way-of-the-future )."
10.1038/s41597-022-01921-2,All code for the development and implementation of the GWAS Explorer is available at GitHub in the following repository: https://github.com/CBIIT/nci-webtools-dceg-plco-atlas . All code is in a public repository with no restrictions to access.
10.1038/s41597-022-01719-2,"The scripts for frame extraction, annotation merging, and statistical analysis, as well as the results of the statistical analysis are made public on https://gitlab.com/nct_tso_public/dsad and via https://zenodo.org/record/6958337#.YvIsP3ZBxaQ . All code is written in python3 and freely accessible."
10.1038/s41597-023-01935-4,No code was used in dataset development.
10.1038/s41597-022-01889-z,An option to visualise the dataset is to use the software Climate Analysis Toolbox (Cate) see https://climate.esa.int/en/explore/analyse-climate-data/ ).
10.1038/s41597-023-01938-1,The BASH shell script used for merging of source atlases has also been made available on the GitHub repository as merge_code.bash . Investigators who wish to replicate the workflow will need to download the original source atlases and adjust file paths in the shell script to point to them. The script requires a recent version of the AFNI suite 70 .
10.1038/s41597-023-01933-6,"The codes used to preprocessing the data, calculation of numerosity properties and plot results are openly available on the OSF repository 33 . For more details about code usage, please refer to the OSF repository."
10.1038/s41597-022-01903-4,"All software used in this work are in the public domain, with parameters described in the Methods section. If no parameters were mentioned for a software tool, default parameters were used as suggested by the developer."
10.1038/s41597-023-01928-3,"The POSDA Tools used by TCIA for technical validation of this DICOM collection, are openly available, and contributions from the research community are encouraged: https://wiki.cancerimagingarchive.net/display/Public/Submission+and+De-identification+Overview For further detail on the curation of DICOM CT and SEG files using Posda Tools within TCIA, please refer to https://posda.com . CNN architecture and codes used in this manuscript can be found in our GitHub repository ( https://github.com/fuentesdt/livermask )."
10.1038/s41597-022-01905-2,"The final step in preparing the geometries (Step 5 in Fig. 1 ) utilized programming in MATLAB, which is published elsewhere 12 and freely available open source in the GitHub repository: https://doi.org/10.48550/arXiv.2209.06948 .In summary, a novel algorithm utilizing an iterative process of mesh reduction, over-closure/gap detection, radial basis function network (RBF) training, and nodal adjustment automatically adjusted overclosure between meshes to a desired gap distance."
10.1038/s41597-023-01936-3,"The code of the MADAP 15 package is publicly available on https://github.com/fuzhanrahmanian/MADAP and the documentation can be found in https://fuzhanrahmanian.github.io/MADAP/ . A stand-alone windows executable can be downloaded from the GitHub repository as well. Furthermore, MADAP 15 can be installed by running pip install madap . The analysis results presented in this article are generated using MADAP 15 version 1.0. Contributions are welcome, but should follow the common guidelines for group software development, which can be found in the CONTRIBUTION section of the MADAP 15 the repository. The code is developed for the Python version 3.9 and above and should use the following packages and versions: attrs > = 21.4.0, matplotlib > = 3.5.3 39 , numpy > = 1.22.4 40 , pandas > = 1.4.2 38 , pytest > = 7.1.2, scikit_learn > = 1.1.2, and impedance > = 1.4.1 22 . For running the GUI, PySimpleGUI > = 4.60.3 26 is required additionally."
10.1038/s41597-022-01911-4,"The code for digitizing the data from the literature plots is the open-source code WebPlotDigitizer version 4.5 27 , which is freely accessible."
10.1038/s41597-023-01940-7,"The code for the analysis of the datasets and the generation of the figures and tables can be accessed in the Figshare repository 40 , which is a JUPYTER notebook named “data_analysis.ipynb”. The script can be executed with Python 3.6 and allows for reproducibility and code reuse."
10.1038/s41597-023-01950-5,All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in Methods.
10.1038/s41597-022-01721-8,The data API and evaluation script in Python is available at https://github.com/MedMNIST/MedMNIST . The reproducible experiment codebase is available at https://github.com/MedMNIST/experiments .
10.1038/s41597-023-01959-w,The Python codes needed to reproduce the dataset are available from Github: https://github.com/fmidev/resiclim-climateatlas .
10.1038/s41597-023-01930-9,"The presented data structure and the related example subjects were created in MATLAB R2021b. Example of the data structures in Matlab are available (see Data Availability Section). To illustrate how to standardize raw data, we provide the Matlab code to standardize the raw data of the example subject from the ICICLE dataset (see Data Availability). The code is in the “MATLAB code for standardization” folder in the following GitHub repository (release version v1.0.0 upon article submission): https://github.com/luca-palmerini/Procedure-wearable-data-standardization-Mobilise-D . The code was used to obtain the standardized data.mat and infoforalgo.mat of the corresponding example subject. In the same repository, in the “Python code for access” and in the “R code for access”, we provide code to access the standardized data with Python and R programming languages, respectively."
10.1038/s41597-022-01919-w,"R code used for formatting, quality control, removing duplicates, and breakpoint detection are publicly available under https://github.com/elinlun/Hclim . The data are available at PANGAEA 15 ."
10.1038/s41597-023-01934-5,"In order to read the “*.bin” files, a python code is provided in the zenodo deposit. The program “PSD_estimation.py” gives some routines to read and calculate the PSD of a given velocity component at a given point. Python required version is written as a comment in the python code."
10.1038/s41597-023-01941-6,"Stata code used to create the Harmonized LASI-DAD dataset is available on the Gateway to Global Aging Data website at https://g2aging.org/ . Users must first register to access the Gateway website. Next, users are required to sign a Data Use Agreement form to request access to the data. Once access has been approved, the user can download the Harmonized LASI-DAD data file, the code to generate the dataset, the associated codebook, and the venous blood specimen (VBS) data file."
10.1038/s41597-023-01929-2,"All the figures presented in this manuscript may be generated using computational notebooks provided ( https://github.com/CoastTrain/CoastTrainMetaPlots ). Utilities for npz file variable extraction and class remapping are provided in the Doodler 44 and Segmentation Gym 48 software packages. All labels were created with Doodler 44 . Imagery was downloaded using CoastSat ( https://github.com/kvos/CoastSat ) and Geemap ( https://github.com/giswqs/geemap ) functionality. For more information, please see the Coast Train project website ( https://coasttrain.github.io/CoastTrain/ )."
10.1038/s41597-023-01956-z,"Pre- and post-processing tasks were carried out using the R-packages of the Climate4R project, extensively described in Bedia et al . 18 and Iturbide et al . 49 . This framework was developed to address the needs of different climate-impact studies and includes a roll of R-packages to access, pre- and post-process, and visualize climate data. All the packages and documentation, including tutorials and example-notebooks, are available through the following Github link: https://github.com/SantanderMetGroup/climate4R ."
10.1038/s41597-023-01953-2,Software versions used are all listed in the method descriptions. The data analysis pipeline for this study can available in GitHub ( https://github.com/YongxinLiu/EasyAmplicon ). And the custom code script of the Tail statistic analysis can be found in the Supplementary File 1 .
10.1038/s41597-023-01952-3,The code for establishing the database was available on GitHub: https://github.com/zh-zhang1984/ZhejiangProvinceICU/blob/main/ZhejiangProvinceICU.md
10.1038/s41597-022-01886-2,"The BDSO is generated using a dedicated ontology build pipeline, built as an extension to the Ontology Development Kit 20 , but released as a component of the PCL, with all terms having PCL IDs. Previous releases of PCL 3 , 11 , 30 represented some of the same cell types as the current release but used a different, less formal schema and a different ID system 3 , 11 , 30 . We have obsoleted these terms and provided a mapping, within PCL, to replacement terms allowing continued support for previous work annotated using PCL terms. The BDSO’s code base is available at GitHub ( https://github.com/obophenotype/brain_data_standards_ontologies ) including documentation of the full technology stack and details of the approach. The latest release of the ontology is available for download from http://purl.obolibrary.org/obo/pcl/bds/bds.owl and is hosted on the EMBL-EBI ontology lookup service (OLS) 36 at https://www.ebi.ac.uk/ols/ontologies/pcl . OLS provides ontology search, browsing, visualisation capabilities and enables web services driven programmatic access to the BDSO."
10.1038/s41597-022-01926-x,"Gaussian 09 14 , Phonopy 21 , and OCLIMAX 7 are used to generate the datasets. Gaussian 09 is commercial software that requires the users to purchase a license. Phonopy and OCLIMAX are freely available to the public. All parameters used in the calculations are provided in the database as input files."
10.1038/s41597-023-01965-y,"The code used to derive the final data product from the raw input data file is available under the licence GNU General Public License v3.0 (GPL-v3) from the GitHub repository www.github.com/fineprint-global/compilation_mining_data . All processing scripts were written in R 25 , and geoprocessing was conducted with the R package sf 26 ."
10.1038/s41597-023-01948-z,The Hi-C data analyses were performed using public tools. The following softwares were used to perform Hi-C data analysis: 1. FastQC v0.11.9 https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ 2. HiC-Pro v2.11.3 https://github.com/nservant/HiC-Pro 3. Juicer tools v1.14.08 https://github.com/aidenlab/Juicebox 4. HOMER V2.0 http://homer.ucsd.edu/homer/interactions2/index.html 5. bedtools v2.29.2 https://bedtools.readthedocs.io/en/latest/ 6. Gviz V1.40.0 https://bioconductor.org/packages/release/bioc/html/Gviz.html 7. ChIPseeker v1.32.0 http://bioconductor.org/packages/release/bioc/vignettes/ChIPseeker 8. HiCPlotter v0.6.6 https://github.com/akdemirlab/HiCPlotter 9. multiHiCcompare v1.14.0 https://dozmorovlab.github.io/multiHiCcompare/ 10. MACS v2.2.6 https://github.com/macs3-project/MACS 11. R v3.6.2 https://cran.r-project.org/ 12. limma v3.1.2 https://bioconductor.org/packages/release/bioc/html/limma.html
10.1038/s41597-023-01957-y,All the emission accounting is performed in Excel.
10.1038/s41597-022-01850-0,"To further describe the details of data processing in our method, we also provide code and instructions for reproducing the presented results 25 . In general, files that end with “.py” are supporting python module files, other files with “.ipynb” are written as Jupyter Notebook instruction, and the files under the folder “measurement” are the source code of the resampling software. The instruction files demonstrate the whole data processing workflow in Fig. 1 , including trip measurement, trajectory reconstruction, virtual traffic flow detection, and data validation. These files can be used to better understand the modeling and validation steps. This study proposes a resampling method of vehicular trajectories using the LPR data. A city-scale holographic unbiased trajectories dataset is reconstructed. Then it is validated by the consistency with other data sources on travel time results and demonstrated with the macroscopic characteristics of the fundamental diagram. The correlative coefficient of travel time is about 0.688 to 0.749. Moreover, with the anonymous interactive measurement, users can acquire multiple traffic data from the individual level without the risk of personal information abuse. This dataset and the tool could support relative research goals such as data fusion, patterns of mobility recognition, and sensor network optimization."
10.1038/s41597-022-01913-2,All mycorrhizal root biomass maps and their associated products were created in R statistical computing environment 39 . The code consists of five main interconnected scripts that are stored in Github repository ( https://github.com/milimdp/Fine-resolution-global-maps-of-root-biomass-C-colonized-by-AM-and-EcM-fungi -). 1. “Spatial_units_map”: Creates raster map of spatial units 2. “Colonization_spatial_units”: Calculates the mean percentage of colonization per spatial unit 3. “Mycorrhizal_roots_biomass_stapial_unit”: Calculates mean mycorrhizal root biomass per spatial unit 4. “Final_maps”: Creates final raster maps 5. “Quality_index”: Calculates quality index per spatial unit and creates maps
10.1038/s41597-022-01908-z,"The complete source code used for candidate retrieval and text alignment is openly accessible and permanently available on GitHub ( https://github.com/webis-de/scidata22-stereo-scientific-text-reuse ). The data processing pipeline is written in Python 3.7, utilizing the pyspark framework. The compute cluster on which we carried out the data processing and our experiments run Spark Version 2.4.8. The text alignment component is written in Go 1.16 and can be used as a standalone application. Detailed documentation about each pipeline component, recommendations for compute resources, and suggestions for parameter choices are distributed alongside the code to facilitate code reuse."
10.1038/s41597-023-01951-4,"Our public repository accessible at this URL https://github.com/gabrielkasmi/bdappv contains the code to generate the masks, filter the metadata and analyze our results. Interested users can clone this repository to replicate our results or conduct analyses."
10.1038/s41597-023-01964-z,"The source code of our custom programs in the Matlab and Python languages is publicly available at figshare, under the link to the dataset 22 . These programs allow users to read data from files, to visualize measurements and patterns, to extract samples related to the stance phase of gait, and to perform exploratory analyses of data. The source code has been designed so as to form a programming library, which could be used to develop more advanced applications related to signal processing and pattern recognition. In particular, the Matlab programs are as follows: 1. runMeasurementDemo.m : Demonstrates how to use the remaining programs to access and validate raw measurements and participant data. 2. runPatternDemo.m : Demonstrates how to use the remaining programs to access and validate processed data and participant data. 3. loadParticipants.m : Loads participant data from a CSV file into a structure of arrays. Each array describes one feature of all participants. 4. summarizeParticipants.m : Shows histograms and statistics of distributions of ages, weights, and heights of participants. 5. loadMeasurements.m : Loads raw data of gait measurements from CSV files into an array of structures of vectors. Each structure describes all signals of one measurement. 6. showMeasurement.m : Plots all signals of a given measurement, pointing out their fragments related to the stance phase of gait, between the foot-strike and foot-off events. Creates windows like that in Fig. 2 . 7. findStance.m : Determines initial estimates of indexes of raw samples related to the foot-strike and foot-off events. Estimation is based on analyzing the temporal slope of the vertical GRF. 8. cutStance.m : Extracts the stance-related fragment (wave) of a signal of the vertical GRF. Can use extrapolation to refine indexes and values of samples related to the foot-strike and foot-off events, so as to smooth the endings of the fragment. 9. extractStancesFromMeasurements.m : Cuts out samples between the foot-strike and foot-off events from raw GRF signals. 10. summarizeStances.m : Shows histograms, maximums, and minimums of distributions of indexes and values of boundary samples of stance-related fragments of GRF signals. Allows one to identify trends and potential outliers among measurements. 11. showStances.m : Plots stance-related fragments of GRF signals. 12. checkLeftLegFirst.m : Checks whether the first of two series of stance-related samples of the medio-lateral GRF is related to the left leg, while the second is related to the right leg. 13. convertStancesIntoPatterns.m : Time-normalizes stance-related fragments of GRF signals and assigns them to legs. Forms processed data so that they can be directly used as patterns in experiments in recognizing persons and shoes. 14. savePatterns.m : Saves processed data into CSV files. 15. loadPatterns.m : Loads processed data from CSV files into a structure of arrays. Each array describes values of one feature of all files. 16. showPatterns.m : Plots processed data of one or many measurements. Allows one to detect potential outliers among measurements. 17. summarizePatterns.m : Shows histograms, maximums, and minimums related to processed data. Allows one to identify trends and potential outliers. Each file begins with extensive comments that explain its purpose and way of usage. The Python programs have the same names and functionalities as the Matlab ones, and are as well commented, so it would make little sense to list and describe them herein. Only two remarks on them seem to be necessary. Firstly, the programs have been combined into a single module, the gait.py file, so users that would like to develop applications based on our library can import all functions by writing one, simple statement. Secondly, some of our programs show plots in windows, so users should have the Tk GUI back-end installed and associated with the matplotlib library."
10.1038/s41597-023-01949-y,"Code for removing technical variation from the NMR biomarker data and for re-computing composite biomarkers and ratios is available through the R package ukbnmr, which also provides tools for extracting and processing NMR metabolite biomarker data and associated quality control tags from UK Biobank. The ukbnmr R package is available to download from CRAN at https://cran.r-project.org/web/packages/ukbnmr/ or GitHub at https://github.com/sritchie73/ukbnmr/ and is permanently archived by Zenodo 33 at https://doi.org/10.5281/zenodo.7515459 . Code underlying this paper are separately available at https://github.com/sritchie73/UKB_NMR_QC_paper/ . This repository and specific release for this paper are permanently archived by Zenodo 34 at https://doi.org/10.5281/zenodo.7310524 ."
10.1038/s41597-023-01976-9,"The data presented in this study, as well as the list of tools employed, are publicly released: • AerialWaste toolkit: Instructions and scripts to visualize it and draw basic statistics are published in the following repository https://github.com/nahitorres/aerialwaste . • AerialWaste model: the models and the code to execute them are released on https://github.com/nahitorres/aerialwaste-model . • Inspection Tool: the Jupyter notebook-based tool is also publicly released on https://github.com/nahitorres/demo-inspection-tool . After the execution of the models in new areas, the predictions can be visualized with the inspection tool. • ODIN: the tool used for the dataset annotation process and for the model evaluation phase is a previous work, available at https://nahitorres.github.io/odin-docs/ ."
10.1038/s41597-023-01975-w,The code that was used to produce the Caravan dataset is available at https://github.com/kratzert/Caravan/ .
10.1038/s41597-023-01967-w,R code used in the analyses is included in the NMBU Open Research Data record 22 .
10.1038/s41597-023-01977-8,The code described in the previous section is freely available in GitHub under the MIT license ( https://github.com/coleygroup/dipolar_cycloaddition_dataset ). Further details on how to use it is provided in the associated README.md file.
10.1038/s41597-023-01962-1,"Health and demographic data harmonization scripts, coded in R, are available at https://github.com/Trajetorias-Sinbiose/Trajetorias_dataset . Please refer to the readme file for further instructions. Information on the script developed to produce the MPI-Trajetorias indicators is also found there."
10.1038/s41597-023-01960-3,"The PrimeKG’s project website is at https://zitniklab.hms.harvard.edu/projects/PrimeKG . The code to reproduce results, together with documentation and tutorials, is available in PrimeKG’s GitHub repository at https://github.com/mims-harvard/PrimeKG . In addition, the repository contains information and Python scripts to build new versions of PrimeKG as the underlying primary resources get updated and new data become available. PrimeKG data resource is hosted on Harvard Dataverse under a persistent identifier https://doi.org/10.7910/DVN/IXA7BM 102 . We have deposited the knowledge graph and all relevant intermediate files at this repository."
10.1038/s41597-023-01970-1,"The code is fully operational under Python 3.6, and the Python scripts used to implement the gridded grazing dataset can be obtained from https://github.com/nanmeng123456/Grazing-spatilization.git . Further questions can be directed to Nan Meng (nanmeng_st@rcees.ac.cn)."
10.1038/s41597-023-01963-0,"Python code for producing, reading and plotting data in the dataset is provided at https://github.com/xinyudou820/GRACED2021 ."
10.1038/s41597-023-01943-4,No custom code was used.
10.1038/s41597-022-01901-6,"All questionnaires were implemented in Qualtrics, tasks were implemented using the JsPsych toolbox 83 (Version 6.3.1 and 6.0.5). Data processing, analyses and visualizations were conducted in Python 3.7 84 , 85 and R 63 . All Qualtrics qsf and pdf files and task code, as well as preprocessing and visualization code is publicly available via github ( https://github.com/adolphslab/CVD-DYN_datarelease ). Additional Public Resources a) Web-based data explorer: http://coviddynamicdash.caltech.edu/shiny/coviddash/ b) Summary of COVID-19 Psychological Studies: https://coviddynamic.caltech.edu/resources/other-covid-studies c) 2020 Timeline: https://coviddynamic.caltech.edu/resources/timeline-2020-world-events ."
10.1038/s41597-022-01923-0,No costume code was used.
10.1038/s41597-023-01987-6,Code used for creating the Lagrangian sea ice parcel database were created using R coding software. This code can be accessed here: https://doi.org/10.5281/zenodo.7554521 . Ice Mass Balance Buoy data which were used to assess the trajectories can be found here: https://imb-crrel-dartmouth.org/ . Buoy Langrangian trajectories made with the database can be found here: https://doi.org/10.5281/zenodo.7554521 93 .
10.1038/s41597-023-01958-x,The software used to process the dataset consists of software for converting video data into a collection of images using VLC ( https://www.videolan.org/index.id.html ). Software used for labelling and making image bounding boxes using Darklabel is provided by https://github.com/darkpgmr/DarkLabel . Software used for converting labelled data into data that is ready for input in processed modelling with a deep learning is the Roboflow ( https://roboflow.com/ ) and the software used for data validation is the Python program.
10.1038/s41597-023-01981-y,To help users with the evaluate the generalizability of detection and segmentation method a code is available at: https://github.com/sharib-vision/EndoCV2021-polyp_det_seg_gen . The code also consists of inference codes that to assist in centre-based split analysis. Benchmark codes of the polypGen dataset with provided training and validation split in this paper for segmentation is also available at: https://github.com/sharib-vision/PolypGen-Benchmark . All the method codes are also available at different GitHub repositories provided in the Table 1 .
10.1038/s41597-022-01914-1,"Programming languages such as Python 3 and Java 8 are used for modeling, analyzing, and developing the framework. The code is deposited in the repository 69 alongwith the dataset."
10.1038/s41597-023-01978-7,All the shared and previously described datasets were generated by the MATLAB programs. MATLAB Version: 9.9.0.1467703 (R2020b) was used. All MATLAB codes were published through the Gitlab repository 50 . There is no restriction to accessing this public repository of the source code.
10.1038/s41597-023-01966-x,"R and Python code developed for the Eco-ISEA3H database 37 was committed to a public GitHub repository, and may be accessed via the following URL: https://github.com/mechenich/eco-isea3h ."
10.1038/s41597-023-01986-7,"The versions, settings and options of software tools used in this work are described below, and more detailed explanation is described in the Supplementary Notes section. (1) MaSuRCA: v4.0.9, default parameters; (2) Juicer: v1.6, default parameters; (3) 3D-DNA: v180922, default parameters; (4) Juicebox tool: v1.22.01, default parameters; (5) RepeatModeler: v2.0.1, default parameters+; (6) RepeatMasker: v4.1.2-p1, parameters: -xsmall -gff; (7) BRAKER: v2.1.6, parameters:–species = Benincasa_hispida–softmasking–prg gth–gth2traingenes–AUGUSTUS_ab_initio–gff3; (8) minimap2: v2.23-r1111, parameters: whole-genome alignment: -ax asm5 –eqx; mapping PacBio SMRT reads: -ax map-pb; (9) STAR: v2.7.9a, default parameters; (10) samtools: v1.7, parameters: view command: -bS, sort command: -O BAM; (11) GenomeThreader: v1.7.0, default parameters; (12) GeneMark-ES: v4.69_lic, default parameters; (13) AUGUSTUS: v3.4.0, default parameters; (14) InterProScan: v5.56–88.0, parameters: -dp -f tsv; (15) PANNZER2: web server version, default parameters; (16) eggNOG-mapper: v2.1.7, default parameters; (17) TBtools: v1.098726, creating genome dot plot: Quick Genome Dot Plot plugin: evalue 1e-3 Num of BlastHits 5, creating genome circus plot: Advanced Circos (input data prepared via Fasta Stats, One Step MCScanX, Text Merge for MCScanX and Transformat for Micro-Synteny View) with default paremeters; (18) SyRI: v1.6, default parameters; (19) bwa: v0.7.17-r1188, parameters: mapping reads: mem -M; (20) bcftools: v1.8, parameters: mpileup -Ou -q 30 -Q 20 –p; call -m -Ov; (21) VCFtools: v0.1.16, parameters:–remove-filtered-all–remove-filtered-geno-all–max-missing 1.0–min-alleles 2–max-alleles 2; (22) Qualimap: v.2.2.2, parameters: bamqc; (23) BUSCO: v5.2.2, parameters: -m genome -c 40; (24) Hicexplorer: v3.7, parameters: hicBuildMatrix:–binSize 10000, hicPlotMatrix:–dpi 600."
10.1038/s41597-023-01989-4,The fully reproducible codes are publicly available at GitHub: https://github.com/MichaelChaoLi-cpu/JapanPop .
10.1038/s41597-023-01988-5,"VASP 34 , 35 used in all DFT calculations is a proprietary software. For the database, dimensionality analysis, and web app, we used Atomic Simulation Environment (ASE) and Atomic Simulation Recipes (ASR) 36 , 37 , both released under GNU Lesser General Public License (LGPL). Phonopy 31 used in calculating the eigenvectors and performing symmetry analysis is released under New Berkeley Software Distribution (BSD) License. The workflow is defined as a part of Atomate code package 38 with FireWorks 39 for defining, managing, and executing jobs which both are released under a modified BSD license and free to the public. Pymatgen (Python Materials Genomics) used for producing inputs parameters and custodian 40 for performing error checking are both open-source packages under Massachusetts Institute of Technology (MIT) license. To store results and task parameters, MongoDB NoSQL database was used with the Server Side Public License (SSPL). All the information for prescreening and phonon calculation extracted from Phonon Database 17 , 31 and from Materials project 32 , 48 are both released under Creative Commons Attribution 4.0 International License. Fitting analysis of the experimental spectra was performed by Least-Squares Minimization fitting (LMfit) 49 python package released under New Berkeley Software Distribution (BSD) license."
10.1038/s41597-023-01990-x,No custom code was used.
10.1038/s41597-023-01972-z,"Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE). This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098."
10.1038/s41597-023-01994-7,"All software used in this paper have been described in the Methods and are freely available online. A copy of our workflow (bash, python, R code files) is also available at Figshare 41 ."
10.1038/s41597-022-01898-y,"The dataset is only comprised of raw data, enabling researchers to work on their pipelines with their custom preprocessing steps for instance. Therefore, no code has been needed for generating these data. Regarding the technical validation, it was performed using Matlab and JASP software. The code for the Subjective Questionnaires is available on GitHub ( https://github.com/Marcels-2-Neurons/Subjective_Questions/tree/main )."
10.1038/s41597-023-01961-2,"The code for modelling, prediction and validation is publicly available via the project GitHub repository 75 . The code was written and ran in R version 4.0.4, and it is dependent on the R package INLA. Further documentation regarding the scripts can be found in the README file within the GitHub repository. Instructions and code for constructing reproductive, maternal, newborn, child, and adolescent health and development indicators using NFHS surveys and DHS data which were used as input data can be found on the DHS Programme GitHub repository ( www.github.com/DHSProgram )."
10.1038/s41597-022-01811-7,All code can be found at https://osf.io/gvw56/ .
10.1038/s41597-023-01983-w,All code used in this analysis is available in the cited github repository 33 . This has been annotated to make it easy for users to see what portion of the NEON stream discharge data product is being used and to easily alter the code to analyze subsets of the data specific to their intended use. Code was run using the neonUtilities R package version 2.1.2 32 and R version 4.1.2 34 .
10.1038/s41597-023-01954-1,"All code used is open source and available at https://github.com/donmctaggart15/ternary_f_cathodes . The datasets are provided on the same repository. We recommend reading the Simmate, Materials Project API, and pymatgen documentation to follow filtering syntax."
10.1038/s41597-023-01999-2,No custom code was used in this study.
10.1038/s41597-023-02001-9,The code for the data pre-processing and technical validations are available on GitHub ( https://github.com/RIKEN-CFCT/hep_methyl_data ).
10.1038/s41597-023-02006-4,"NextDenovo software v.2.3.0: read_cutoff = 1k, seed_cutoff = 26,457, sort_options = -m 20 g -t 10 -k 40, minimap2_options_raw = -x ava-ont -t 10, correction_options = -p 10, minimap2_options_cns = -x ava-ont -t 10 -k17 -w17, and nextgraph_options = -a 1. NextPolish software v.1.2.4: sgs_options = -max_depth 200, lgs_options = -min_read_len 1k -max_read_len 100k -max_depth 100, and lgs_minimap2_options = -x map-ont. ALLHiC software v.0.9.14: “−NonInformativeRatio 0”, “−minREs 50”, “−MaxLinkDensity 3”, “−shortest_ 150”, “−longest_ 800”, “−format_ Sanger”, “−enz DpnII”, and “−CLUSTER 29”. TblastN v.2.2.26: E-value ≤ 1e−5. Swiss-Prot: E-value ≤ 1e−5. Nr: E-value ≤ 1e−5. KEGG: E-value ≤ 1e-3. GO: E-value ≤ 1e-10. Pfam: E-value ≤ 0.01. BLAST to predict rRNAs: E-value ≤ 1e−5, identify ≥ 85%, and match length ≥ 50 bp. BLASTP: E-value ≤ 1e−5. Other commands and pipelines used in data processing were executed using their corresponding default parameters."
10.1038/s41597-023-01942-5,"As mentioned previously, all code required to process dicoms to the final scalar maps is available: https://doi.org/10.20383/103.0594 45 . The code is also available publicly through GitLab: https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: https://osf.io/5eusw/ , and the source code is available upon reasonable request 46 ."
10.1038/s41597-023-02004-6,All the code to produce the results of this paper is accessible at: https://github.com/MeatYuan/MOS2.We all use Python and jupyter notebook.
10.1038/s41597-023-01998-3,"All the data annotations with the chemical properties were obtained from density functional theory calculations performed with the Gaussian 09 program 43 . The set of molecular geometries used as input for these calculations was generated using the Newton-X CS (version 2.2-B08) package 53 for Wigner sampling together with a Python code for geometry interpolations [ https://github.com/virtualzx-nad/geodesic-interpolate ] as described in the Methods section. Both programs are open access. A custom Python script was written to extract the relevant information from the Gaussian 09 output files, and it is publicly available in the GitHub repository https://github.com/maxjr82/QCDP . Finally, the Python script to perform the dimension reduction of the molecular geometries with the PCA method is available to download from https://github.com/maxjr82/PCA-for-WS22 ."
10.1038/s41597-023-01997-4,The codes for trip generation algorithms and the synthetic dataset validation are available via the GitHub repositories 32 .
10.1038/s41597-022-01907-0,"Most of the data analysis was performed using software running on the Linux system, and the version and parameters of the main software tools are described below. (1) SMRTlink: Version 10.1, parameters: No Polish: TRUE, min_zscore: −10 (Default) min_passes 3, Min_predicted_accuracy 0.99. (2) Arrow: parameters: bin_size_kb 1 hq_quiver_min_accuracy 0.99, qv_trim_3p 30, bin_by_primer false. qv_trim_5p (Ignore) qv_trim_3p (Ignore) bin_by_primer false. (3) CD-HIT-Est: Version 4.8.1, parameters: -c 0.96 –n 10 -G 0 - aL 0.00 -aS 0.99. (4) TransDecoder: Version 3.0.1, parameters: -G universal, -m 100. (5) BUSCO: Version 5.3.2, default parameters. (6) BLASTx: Version 2.10.1, parameters: -outfmt 6, -evalue 1e-5. (7) BLASTp: Version 2.10.1, parameters: -outfmt 6, -evalue 1e-5. (8) Metascape: Version 3.5, default parameters. (9) EggNOG: Version 2.1.8, parameters: -m diamond,--itype proteins,--sensmode more-sensitive,--go_evidence non-electronic. (10) PLEK: Version 1.2, parameters: -minlength 200, -isoutmsg 0, -isrmtempfile 1. (11) CPC: Version 2, default parameters. (12) MISA: Version 2.1, default parameters."
10.1038/s41597-023-01973-y,The code to conduct the quality control flagging described in the section Technical validation is written in R 95 and available on Zenodo 96 . The 3C code is available at https://gitlab.com/pgroetsch/Rrs_model_3C . The code for QWIP is on Zenodo 97 .
10.1038/s41597-023-02011-7,All the codes used in this study to generate the dataset were written in the Javascript in Google Earth Engine and are available through GitHub ( https://github.com/AliciaPython/GSSM1km ). The GSSM1 km dataset can be accessed at: https://code.earthengine.google.com/?asset=users/qianrswaterr/GlobalSSM1km0509 .
10.1038/s41597-023-02000-w,"We used SAGA GIS, ArcMap, LiDAR360 software to process all the remotely sensed datasets. We performed all other steps including biomass modelling, data analysis, and figure generations in R. All the codes along with input datasets can be found in in the following link: https://github.com/qinmaNNU/NYCtrees . This github folder includes codes developed for: 1. carbon storage prediction and uncertainty estimation (CarbonDensityUncertainty_Fig. 7Fig. 8Table 1.R); 2. tree structural distribution analysis over regions and their correlations among structural features (NYtree_Fig. 2RegionViolin_Fig. 3CorrMatrix.R); and 3. validation of tree structural estimation using field plot measurements (NYtreesVali_Fig. 5.R)."
10.1038/s41597-023-01992-9,"Direct use of our provided datasets is available on Zenodo 55 The source code used for data collection, processing and analysis is also on Gitlab 17 . The data processing is performed using Python 3.9 and the necessary toolboxes, such as Pandas and Geopandas. The data collection process is fully described in the paper. By open-sourcing the code, we aimi to provide the most relevant information for integrating the dataset into energy system models. Although step-by-step tutorials could also be helpful for this purpose. However, we think such information is best conveyed through the source codes 17 . We regret that we cannot provide scripts for the vRES potential data. The data of vRES potential is created by the EnDAT framework, which is in the process of being open-sourced and only available within DLR. For those data for which the license is “citation”, we have been permitted to redistribute the data after modifying it for this paper. We do not, however, have permission to publish their original data. We will continue to update this dataset and apply this dataset to further energy system studies. We encourage readers to contribute to fill in the gaps and improve the hypotheses of this dataset mentioned in the paper."
10.1038/s41597-023-01995-6,"The codes for calculating and evaluating computational extension scores are available in the subfolder named “Code” under the folder named “Supplimentary_Data” at https://doi.org/10.17605/OSF.IO/N5VKE 16 . Specifically, to estimate the semantic ratings of Chinese words, we first used “train_decode.py” to learn a mapping function from the Chinese embedding space to semantic ratings based on the 17,940 subjective ratings with their corresponding word embeddings. We then utilized “predict.py” to generate the semantic ratings of all extensional Chinese words. To estimate the semantic ratings for English words, we need to align the mapping relations between the English and Chinese embedding spaces beforehand. To achieve that, we first used “extract_en.py” and “extract_zh.py” to extract word representations that are in the Chinese-English bilingual lexicon and then used “match.py” and “train_align.py” to learn the mapping function from English to Chinese word representations. Finally, based on the two mapping functions, including English-Chinese mapping and Chinese to semantic ratings, we used “predict.py” to project English embedding peace to that of Chinese to generate the semantic ratings of all extensional English words. To validate the above results, we used “corr_binder.py” and “corr_binder_cn.py” to compute correlations between the extensional ratings and corresponding scores in Binder et al . 1 ."
10.1038/s41597-023-01985-8,"A link to the dataset, along with Python codes that are used to create the dataset, statistical analysis and plots, is released and publicly available at https://github.com/MaxKinny/TabRecSet ."
10.1038/s41597-023-01968-9,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub ( https://github.com/Hughes-Lab/niaid-schema-publication ) and archived on Zenodo ( https://doi.org/10.5281/zenodo.6816052 ): https://zenodo.org/record/681605292 86 . Code used to harvest metadata via Metadata Crawler ( https://crawler.biothings.io/ , https://github.com/biothings/biothings.crawler ) and Metadata Plus ( https://metadataplus.biothings.io/ , https://github.com/biothings/metadataplus ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub ( https://github.com/biothings/discovery-app ). Code to dynamically generate File 5 is available at https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd ."
10.1038/s41597-023-02003-7,We provide two model weights using Pytorch as a deep learning framework to detect the 24 chromosome categories for both object detection and single chromosome object detection. Our neural network model is based on YOLOv4. We recommend using argusswift’s code ( https://github.com/argusswift/YOLOv4-pytorch ) and provide a py file that converts xml files to coco format (xml2coco.py).
10.1038/s41597-023-02019-z,The device and software used to generate the 3D scanning models are Wiiboox white light raster 3D scanner and Reeyee_v2.5.0. The software used to generate the parameters of 3D scanning models are Geomagic Studio 12 and Microsoft Excel 2016.
10.1038/s41597-023-02014-4,"All software used in this study are in public and their parameters are clearly described in Methods. If no detail parameters were mentioned for the software, default parameters were used as suggested by developer."
10.1038/s41597-023-01982-x,"Several scripts are made available to allow users of all backgrounds and experience levels to access this data. After downloading the raw sequencing data, users can use the following code 35 to perform clonotyping and obtain a list of alpha and beta chains from each sample, as well as a report file with details on the success of the process. Note that the user should have a recent version of MiXCR installed, as detailed in the 18 MiXCR documentation. When using the clonotype data as input, either after following the step above or by downloading the clonotyping data made available here, the user can use an R script 36 to gather some statistics and perform an initial analysis of the clonotypes. Note that the user should have a recent version of the coding language R installed, and the appropriate packages up to date. The code also demonstrates performing sampling of the data, to prevent bias caused by samples with a larger number of clonotypes."
10.1038/s41597-023-02008-2,"Code 54 for replicating results in this article is publicly available on Zenodo ( https://doi.org/10.5281/zenodo.6784716 ). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine ( https://earthengine.google.com ) to obtain dam catchment data and conduct analysis."
10.1038/s41597-023-01955-0,"The source code used to create the ICS-209-PLUS dataset, the spatiotemporal linkage, and the ICS-FIRED linked database are publicly accessible on GitHub 39 , 40 , 41 ."
10.1038/s41597-023-02013-5,"Code is available on GitHub at https://github.com/healthy-liveable-cities/australian-national-liveability-study . The project was conducted between 2018 and 2020 using Python 2.7 with PostgreSQL 9.6, PostGIS 2.4, the ArcGIS 10.6 arcpy python library and network analyst extension. The code also draws heavily on the psycopg2, sqlalchemy, pandas and osmnx libraries. The code was developed across the duration of the project to meet evolving stakeholder needs for data and indicators. Unfortunately, across this period, software versions also evolved, and when a newer version of ArcGIS was installed in 2020 following expiry and renewal of institutional licences this required the use of Python 3. While this initially provided impetus to re-factor and update the code, project priorities within our research group changed and it became apparent this code would not be used in future projects, and there was not funding or scope to complete final code re-factoring. The exception to this was the ‘highlife’ project branch which contains code developed to create built environment measures targeting 2019 for the separate High Life study; this was the branch with the most recent and complete development efforts, and was therefore set as the default branch for the repository. An incomplete re-factoring for Python 3 is located on the ‘python3_2020 branch’; and the final main working branch of the overall project is the one titled ‘main’. Many lessons were learnt about managing large code projects through the course of the study 8 . The code for this project would ideally be re-factored but no team members had capacity to do so for this completed project. Project experiences meant that the team had broad desire to move towards more open source software solutions, for which the methods developed for this study were adapted and applied in other projects 4 , 66 , 67 ."
10.1038/s41597-023-01996-5,Supplementary files 5 and 6 were deposited in GitHub on 2022/12/20 33 . They can be found at the URL: https://github.com/IoannisKonstantinidis/RRHP_Code .
10.1038/s41597-023-02015-3,"The code used to generate the triplets and compare the embeddings is made available at https://osf.io/at8cs/ . The code to generate triplets [code/compare_triplets] requires, at a minimum, three inputs: the list words to be used as anchor, words concreteness ratings, and the pre-trained embedding to be used to define word distances. This code can be used to generate novel triplets fitting other experimental goals, for instance triplets at fixed distances between target words or triplets with only abstract (or concrete) terms. The code to compare embeddings [code/generate_triplets] requires the generated triplets and the embeddings one wishes to use to solve the triplet task. It can easily be adapted to test novel embeddings (e.g., with different training samples or vocabulary sizes). We also provide a notebook to perform basic exploration of the dataset including all the analysis here reported: ColabNotebook_3TT (available on the OSF repository as well). All analysis can be reproduced with the data directly available on the OSF repository safe for the two requiring individual subject data: Results_Demographics_1322.csv and Results_Responses_1322.csv will be accessible only after proper registration with the CNeuromod databank ( https://docs.cneuromod.ca/en/2020-alpha2/ACCESS.html ) due to ethical considerations. The OSF repository includes also the datasheet for the dataset 54 : https://osf.io/echny . Data and code are released under Creative Commons Attribution 4.0 International Public Licence (CC-BY 4.0 44 )."
10.1038/s41597-023-02005-5,"The source code used the Python language. The source code contains five sections: data_loader5_shanxitezhengqu_LP.py, unet_2d.py, data_preprocess.py, train_shanxitezhengqu_LP.py, Config_shanxitezhengqu_LP.py. The source code can be downloaded at https://github.com/LYHTTUCAS1/code ."
10.1038/s41597-023-02032-2,The R code used in the analysis of the data is available on GitHub ( https://github.com/wangliTeam/data-and-code ).
10.1038/s41597-023-02023-3,The code implementation was done in Python3 (3.8.3) using Jupyter notebook. The script that describes the step by step process followed is available at 25 .
10.1038/s41597-023-02009-1,"No custom code was used to generate or process the data presented in this manuscript. All software used in this work is in the public domain, with parameters being clearly described in Methods. If no detailed parameters were mentioned for a software, default parameters were used as suggested by the developer."
10.1038/s41597-023-02025-1,Code for data cleaning and analysis is provided as part of the replication package. The code is uploaded to the Github platform: https://github.com/scutbioinformatics/CMMD .
10.1038/s41597-023-02037-x,"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories: https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/ and https://pypi.org/project/waves-utilities/ ."
10.1038/s41597-023-02022-4,The following code and outputs are available on-line 27 : • The R code and outputs of the efficacy and performance of the models employed to estimate the global Olsen phosphorus concentration from the predictor variables. • Python code describing the filtering and post-processing steps involved in the use and analysis of the raw data and predictor variables to estimate global soil Olsen phosphorus concentration values and stocks.
10.1038/s41597-023-01979-6,No custom code was used to generate the dataset.
10.1038/s41597-023-02024-2,All code used to generate the figures and analysis of this paper is available in the Zenodo repository 31 .
10.1038/s41597-023-02028-y,No custom code was used to generate this work.
10.1038/s41597-023-01947-0,"Our datasets and the technical validation results are available as part of our data archive on (1) GitHub https://github.com/jaeyk/validated_names (2022), (2) OSF 34 https://doi.org/10.17605/OSF.IO/AHPVQ (2022), and (3) Harvard Dataverse 33 https://doi.org/10.7910/DVN/LP4EAR (2022). The archived GitHub repository (replication code) is available on zenodo 54 https://doi.org/10.5281/zenodo.7460488 (2022)."
10.1038/s41597-023-02002-8,"Custom code was not used to generate the data. EEG files were exported from proprietary format to EDF files using the associated EEG reviewing software for the NicoletOne and Neurofax EEG machines. Details on how to view the EEG data and import it into programming environments is described in the Usage Notes section. To assist with computer-based analysis of the EEG, we provide freely-available code to downsample the EEG to a lower and uniform sampling rate. For quantitative or machine-learning analysis, the neonatal EEG is often downsampled to a lower sampling rate, as the majority of the power is typically below 10 to 20 Hz. For example, Fig. 5e shows that 95% of spectral power is below 25 Hz. The processing routines include an anti-aliasing filter before downsampling. Both Matlab and Python versions of the code are included at https://github.com/otoolej/downsample_open_eeg (commit: 22e92db)."
10.1038/s41597-023-02016-2,"The code for creation of this dataset, usage examples and evaluation code used in the challenge is available on GitHub ( https://github.com/Ramtingh/ASOCADataDescription ). Figures 1 – 3 were created with data included in the dataset. A copy of the raw data used is included in the repository under the corresponding folder to maker recreating these figures easier. 3D Slicer (version 4.3) was used in the preparation of the dataset and Figs. 1 and 2 . Vascular Modelling Tool Kit (version 1.4) was used to calculate centerlines and generate Fig. 3 ."
10.1038/s41597-023-02031-3,"Examples are provided for information processing with this dataset, this code is freely available in the following repository: https://github.com/VicenteMorell/KIMHu ."
10.1038/s41597-023-02007-3,"The input data processing, dataset generation and validation were conducted using Matlab software (R2018b version). Code is available on Github: https://github.com/panpanyao/NNsm-FY-code ."
10.1038/s41597-023-02035-z,All code and instructions needed to reproduce all representations and results in the paper can be found at: https://github.com/csabaiBio/RBD-AlphaFold2-structures-and-phenotypic-information The GitHub repository contains scripts for: • https://github.com/csabaiBio/RBD-AlphaFold2-structures-and-phenotypic-information/blob/main/data_usage_scripts/aa_changes_vs_RMSD.ipynb Exploring simple amino acid variation statistics. • https://github.com/csabaiBio/RBD-AlphaFold2-structures-and-phenotypic-information/blob/main/interface_exploration/interface_importance.ipynb Exploring the importance of the interface with respect to ACE2 binding values. • https://github.com/csabaiBio/RBD-AlphaFold2-structures-and-phenotypic-information/blob/main/projections/UMAP_all_vars_structs.ipynb Visualizing embeddings. • https://github.com/csabaiBio/RBD-AlphaFold2-structures-and-phenotypic-information/blob/main/disorder_analysis/iupred_notebook-Analyses_mod.ipynb Investigating how pLDDT (b-factor) output of the AF2 files correlate with state-of-the-art disorder estimations.
10.1038/s41597-023-02044-y,"The LocalView dataset is publicly available at https://doi.org/10.7910/DVN/NJTBEM 41 . Code to replicate the main and supplementary analyses in this paper is available at https://doi.org/10.7910/DVN/KHUXIN 48 . More information, including a codebook and related research, is linked on our companion website at https://localview.net ."
10.1038/s41597-023-02059-5,The code used in this study can be found at https://doi.org/10.6084/m9.figshare.21940490.v1 . This code may be updated over time 56 .
10.1038/s41597-023-02042-0,Not applicable.
10.1038/s41597-023-02045-x,"The processing of IceLines has been carried out in the Calvalus system and GPU-cluster available at DLR’s Earth Observation Center by means of proprietary software and dedicated Python (v3.6 and v3.7) scripts. Given the use of proprietary tools, the implemented processing pipeline cannot be openly released to the public. However, all processing steps can be accessed and reproduced as follows: The pre-processing of the Sentinel-1 imagery can be replicated with the open source ESA SNAP Toolbox 8.0 and the processing steps described in Fig. 2 . The code of the HED-Unet based on Pytorch (v1.7) is available at https://github.com/khdlr/HED-UNet and the final post-processing script (Python v3.7) is available at https://download.geoservice.dlr.de/icelines/files/icelines_auxiliary_v1.zip . Additionally, this folder includes data used for the accuracy assessment, a Python script for bulk download (bulk-download-icelines.py) and an exemplary script (‘display-icelines-gee.js’) to display IceLines data with the corresponding Sentinel-1 scenes in Google Earth Engine."
10.1038/s41597-023-01991-w,The codes used in this study will be available at https://github.com/zhengchaolei/GlobalSSMGapfillDownscaling.git after this work is accepted.
10.1038/s41597-023-02033-1,"The Jupyter Notebook interactive document for data post-processing is freely available in ROHub, the Research object management platform 47 ."
10.1038/s41597-023-02048-8,"We provide the Python code to separate multichannel and time series 2PFM image volumes into single volumes, which are easier to manipulate. Multichannel XY, XYZ, XYT, and XYZT images are supported. For multichannel images, the user will be asked to select the channel of interest to export. For images with multi-T volumes (XYT and XYZT), the user has the option of exporting each T-stack separately, or as a single file. We also provide sample code for the image pre-processing tools described above. All code can be accessed at the MiniVess Github repository https://github.com/ctpn/minivess ."
10.1038/s41597-023-02034-0,"To replicate our work, downloadable helper functions can be used to filter and merge data from different sources. The code used to extract and process all the datasets are developed using Matlab and R. The codes are available at https://github.com/Agrimonia-project/Agrimonia_Data.git , with the user instructions included in the respective ‘README.md’ files."
10.1038/s41597-023-02050-0,"The code in this study is fully operational under Python 3.8.8, and the key packages were contained in the sklearn.Ensemble.RandomForestRegressor and the sklearn.model_selection.StratifiedKFold toolkit 41 in Python 3.8.8. The code can be found on GitHub ( https://github.com/NingZhan1978/High-resolution-livestock-seasonal-distribution-data-on-the-Qinghai-Tibet-Plateau-in-2020.git )."
10.1038/s41597-023-01974-x,"Project website for G raph XAI is at https://zitniklab.hms.harvard.edu/projects/GraphXAI . The code to reproduce results, documentation, and tutorials are available in G raph XAI ‘s Github repository at https://github.com/mims-harvard/GraphXAI . The repository contains Python scripts to generate and evaluate explanations using performance metrics and also visualize explanationa. In addition, the repository contains information and Python scripts to build new versions of G raph XAI as the underlying primary resources get updated and new data become available."
10.1038/s41597-023-01971-0,"The SPI Github repository contains the raw data used to produce our indicators, code to reproduce the values for all of our indicators and overall scores written in R , which is an open source statistical language, and a final data set available in CSV and Stata format. A detailed Readme file is available detailing how to use the code. The repository is licensed under the Creative Commons Attribution 4.0 International License, which means users are free to share or adapt any of the materials available, so long as appropriate credit is given to the SPI team. The Github repository also contains the version control history, which documents every change in the data and code of the entire project dating back to July 2020 to build confidence and transparency. The vast majority of code in this repository is written in the R language. The R version used was 4.0.3 (2020-10-10). This repository contains several files from the R package “ renv ” 45 . The renv package helps manage specific package versions used to produce the results in this repository. Because package version conflicts can make code that runs on one system not run on another system, it is important to have a list of the specific package versions used and a workflow for accessing these specific packages. The renv package provides this. In general, the renv::restore() command should install all packages found in the renv.lock file in this repository, so that version conflicts do not cause errors. An SPI Interactive Dashboard is available for a more detailed exploration of the data. In this application, it is possible to map any of the 51 Statistical Performance Indicators, to explore a country report for each of the 174 countries, and use a tool to see how the SPI overall scores change when alternative weights are applied to the dimensions and pillars. Version update This paper builds on but significantly expands the earlier analysis in the (unpublished) working documents by Dang et al . (2021) 14 and in Dang et al . (2021) 21 . All the sections are new, except for the three subsections “Conceptual Motivations”, “Construction of the SPI”, and “Issues for Further Considerations”, which are revised and updated with more recent references. This paper analyzes new SPI data for 2020, while the two cited references analyze data up to 2019. The appendix materials in the Supplementary Information are mostly based on these two cited references, except for the updated discussion related to the 2020 SPI data."
10.1038/s41597-023-02043-z,"The version of YARP used to generate the data is freely available through GitHub under the GNU GPL-3.0 License 43 . The code used to load the reaction dataset, parse the reaction features and DFT energies, and reproduce Figs. 2 , 3 is also freely available 40 . Further details on how to use these scripts are given in the Usage Notes."
10.1038/s41597-023-02040-2,All the code used in this study is available on Github as a release: https://github.com/ai4up/eubucco/releases/tag/v0.1 29 . It is free to re-use and modify with attribution under the MIT license .
10.1038/s41597-023-02012-6,"The associated files are made available in non-proprietary formats via EBRAINS 102 , 103 ( https://ebrains.eu/ ). Section images are shared in standard TIFF format, compatible for display and analyses through a range of tools. The pre-processing and image registration software, Nutil 98 (RRID: SCR_017183), QuickNII 101 (RRID: SCR_016854) and VisuAlign (RRID: SCR_017978), as well as the Waxholm Space atlas of the Sprague Dawley rat brain 96 , 97 , 99 , 100 (WHS rat brain atlas, RRID: SCR_017124), are available for download via NITRC, NeuroImaging Tools and Resources Collaboratory ( https://nitrc.org/ ). Spatial relationships between images and the WHS rat brain atlas, available in JSON files, can be inspected and manipulated with the QuickNII tool presuming that PNG versions of the images is present in the same folder as the related JSON file."
10.1038/s41597-023-02021-5,"All code required to analyse the data and format it for the package and web site are incorporated in a software container available at http://github.com/PNNL-CompBio/srpAnalytics or at Docker hub. To create a new data package with additional code, we can run the docker image with additional files in the prescribed data format. Command-line arguments to the docker will format the data to the schema on our github site so that it can be added to the repository."
10.1038/s41597-023-02030-4,"The python scripts (python 3.10) developed for the generation and validation of the synthetic dataset are publicly and freely accessible on Zenodo 57 . The scripts use the following python packages: pandas (1.4.4), numpy (1.23.2), pyreadstat (1.19), scipy (1.9.1) for the Pearson’s correlation coefficient computation, scikit-learn (1.1.2) for the RMSE computation, and matplotlib (3.5.3) to generate the charts. All these python packages are available from the Python Package Index: https://pypi.org/ . The humanleague package (2.1.10) providing the QISI and IPF implementations is available from the Python Package Index and on Zenodo 42 ."
10.1038/s41597-023-02020-6,The data preprocessing scripts and machine learning algorithm are publicly available via GitHub at https://github.com/inventec-ai-center/bp-benchmark . The custom code used for data visualization is available from the corresponding authors upon request.
10.1038/s41597-023-02052-y,No code was used to generate or process the current dataset.
10.1038/s41597-023-02065-7,"The source code of the baseline, as well as a direct link to the VirtualStaining Dataset, is available in the GitLab repository 12 . The installation of Python and Jupyter using the virtual environment is recommended, with the necessary technical instruction supplied in the “ReadMe.md” inside the repository."
10.1038/s41597-023-02051-z,"The data analyses were performed according to the manuals by the developers of corresponding bioinformatics tools and all software, and codes used in this work are publicly available, with corresponding versions indicated in Methods."
10.1038/s41597-023-02054-w,The CIED datasets are available in the form of XLSX files. No custom code is used in the construction of the datasets.
10.1038/s41597-023-02067-5,"Software parameters of genome assembly: default parameters for HiC-Pro and BUSCO. Falcon: length_cutoff = 13000 length_cutoff_pr = 14000; pa_HPCdaligner_option = -v -B188 -M24 -t12 -e.75 –k18 -w8 –h280 –l2800 -s1000, ovlp_HPCdaligner_option = -v –B128 –h180 -e.96 –k17 –l2800 -s1000. Wtdbg:–tidy-reads 5000 -fo dbg -k 0 -p 19 -S 3 -E 5–rescue-low-cov-edges–aln-noskip; wtdbg-cns -c 3; kbm-1.2.8 -k 0 -p 19 -S 4 -O 0; map2dbgcns; wtdbg-cns -k 13 -c. SMRTlink:–bam–bestn 5–minMatch 18–nproc 6–minSubreadLength 1000–minAlnLength 500–minPctSimilarity 70–minPctAccuracy 70–hitPolicy randombest–randomSeed 1. LACHESIS: CLUSTER MIN RE SITES = 100; CLUSTER MAX LINK DENSITY = 2.5; CLUSTER NONINFORMATIVE RATIO = 1.4; ORDER MIN N RES IN TRUNK = 60; ORDER MIN N RES IN SHREDS = 60. Software parameters of genome annotation: default parameters for SNAP, GeneMark-ET, MAKER, Trimmomatic, HISAT2 and StringTie. exonerate:–model protein2genome –percent 50–score 100–minintron 20–maxintron 20000. Augustus:–species = nasonia–noInFrameStop = true–gff3 = on–strand = both. RepeatModeler: -engine ncbi -LTRStruct. RepeatMasker -a. blasp: -e 1e-5. HMMER: -E 1e-5. BlastKOALA: Eukaryotes for taxonomy group and family_eukaryotes for KEGG GENES database file to be searched. Software parameters of orthologue and phylogenetic analyses: default parameters for OrthoFinder, MCMCTree. MAFFT:–auto. trimAl: -automated1. IQ-TREE: -m JTT + F + R6 -B 1000 -T AUTO. Cafe: -p. Software parameters of gene evolution analyses: default parameters for codeml. ParaAT: -f paml -m mafft. Software parameters of annotation and phylogenetic analysis of olfactory receptor genes: default parameters for the InsectOR pipeline. MAFFT:–maxiterate 1000–localpair. trimAl: -automated1. IQ-TREE: -m JTT + F + G4 -B 1000 -T AUTO. Software parameters of venom protein identification: default parameters for MaxQuant. RSEM:–bowtie2. Custom scripts were provided at personal GitHub ( https://github.com/xiaoshan40/scripts ), including scripts to retrieve the longest protein and CDS sequences for each gene (get_longest_protein_and_cds.pl), to concatenate aligned protein sequences to a supergene sequence (concatenate_aligned_sequences.pl), to automatically perform gene evolution analyses (run_codeml.pl) and to extract results from the original branch and branchsite model output files by codeml (get_paml_branch_result.pl and get_paml_branchsite_result.pl)."
10.1038/s41597-023-02062-w,"Segmentations were performed using the commercially-available ProKnow (Elekta AB, Stockholm, Sweden) software. The code for NIfTI file conversion of DICOM CT images and corresponding DICOM RTS segmentations, along with code for consensus segmentation generation, was developed using in-house Python scripts and is made publicly available through GitHub: https://github.com/kwahid/C3RO_analysis ."
10.1038/s41597-023-02055-9,"The code used to create and process the presented data is provided in 39 or is part of open-source repositories 48 , 49 , 50 , 51 , 52 , 53 ."
10.1038/s41597-023-02053-x,"All code is written in Python, the analysis is conducted using Columbia University high performance computing clusters (Ginsburg), and is available at https://github.com/os2328/CASM-dataset ."
10.1038/s41597-023-02036-y,The code to process the MedVidCL and MedVidQA datasets 13 and reproduce the results of the experimental benchmarks (with hyperparameters values) can be found at https://github.com/deepaknlp/MedVidQACL .
10.1038/s41597-023-02046-w,A Python Jupyter notebook for getting started with reading the data and comparing them with other reanalyses datasets at matching times (as outlined in the Usage Notes Section) is available at the AODN GitHub repository ( https://github.com/aodn/imos-user-code-library/blob/master/Python/notebooks/SAR_winds/SAR_winds_getting_started_jupyter_notebook/ausar_winds_getting_started_notebook.ipynb ).
10.1038/s41597-023-02017-1,"A git repository is publicly available at https://github.com/IRC-SPHERE/sphere-challenge-sdata/ . In this repository a number of scripts for visualisation, bench marking and data processing are available. (All subsequent sensor images were generated using these scripts)."
10.1038/s41597-023-02061-x,"The data records can be used straightforwardly as single files or as a set. A step-by-step example to extract the numerical data from each HDF5 file, along with a library developed for this purpose is available on GitHub: https://github.com/alges/hidsag . Along with the library, a Python notebook with sample usage is included containing: 1. The import of a data record from a path 2. The selection of a single sample 3. Slicing of a specific band and position for numerical data extraction 4. Visualization of the resulting slice 5. The list of each sample on the data record, its associated variables and metadata"
10.1038/s41597-023-02060-y,"The Python codes to generate the dataset are publicly available through the GitHub repository ( https://github.com/0oshowero0/HealthyCities ). Detailed instruction for software environment preparation, folder structure and commands to run the provided codes is available in the repository."
10.1038/s41597-023-02083-5,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-023-02074-6,"The main computational tools used in this study are R language based. Seurat 8 was used for data pre-processing, integration, and label transfer between reference and validation datasets. ProjecTILs 30 was used for interpretation of T cell states, and Monocle3 7 was used for pseudotime trajectory analysis. The R codes used for pre-processing of the used datasets, reference and validation datasets analysis, and pseudotime trajectory analysis can be found at figshare as “NSCLC_data_reanalysis_codes” file ( https://doi.org/10.6084/m9.figshare.c.6222221.v3 ) 79 ."
10.1038/s41597-023-01993-8,All data and standard operating procedures are released under Creative Commons Attribution 4.0 International (CC BY 4.0). All code was released under the MIT License and can be accessed in GitHub at https://github.com/hubmapconsortium/ccf-ontology . ASCT+B APIs: API Endpoint (includes interactive documentation): https://mmpyikxkcp.us-east-2.awsapprunner.com API Documentation: https://hubmapconsortium.github.io/ccf-asct-reporter/docs/api OpenAPI specification: https://mmpyikxkcp.us-east-2.awsapprunner.com/asctb-api-spec.yaml CCF-API: API Endpoint (includes interactive documentation): https://ccf-api.hubmapconsortium.org API Documentation and OpenAPI specification: https://ccf-api.hubmapconsortium.org API Database backend is N3.js: https://github.com/rdfjs/N3.js Code to instantiate/use CCF Database: https://github.com/hubmapconsortium/ccf-ui/tree/main/projects/ccf-database CCF-User Interfaces: https://github.com/hubmapconsortium/ccf-ui Validation Tools: https://github.com/hubmapconsortium/ccf-validation-tools
10.1038/s41597-023-02088-0,The R codes used to query the GBIF and iNaturalist databases (“scr_down_o_gbif_inat.R”) and for technical validation (“scr_tech_val.R”) are available at https://doi.org/10.6084/m9.figshare.21155419 .
10.1038/s41597-023-02068-4,"BioASQ has created a lively ecosystem, supported by tools and systems that facilitate the creation of the benchmarks. All software is provided with open-source licenses ( https://github.com/BioASQ ). In addition, the data produced are open to the public 15 ."
10.1038/s41597-023-02010-8,"No explicit code has been developed in conjunction with the data-set. However, several resources are necessary to use the data. For replaying, using and developing the raw data and to perform sensor fusion such as individual tasks, it is recommended to install ROS 18 (robot operating system) Melodic middleware documented at http://wiki.ros.org/Documentation with all necessary download links. For our presented camera calibration, which is the base for several tasks, Puzzlepaint camera calibration 19 is used with the codebase under https://github.com/puzzlepaint/camera_calibration . For the depth estimation in this research-work we simply used the self-supervised approach from Godard et. al 25 . The software is available under https://github.com/nianticlabs/monodepth2 . The localization algorithm A-LOAM from Zhang et. al 30 , available at https://github.com/HKUST-Aerial-Robotics/A-LOAM , was used for the localization task. We used Voxblox 31 for path planning ( https://github.com/ethz-asl/voxblox )."
10.1038/s41597-023-02047-9,No custom code was used to generate or process the GEOGLAM-BACS masks. The software used in the assembly of the masks was ArcMap version 10.6.
10.1038/s41597-023-01984-9,"The primary version of AutoDock used to generate the primary dataset is available from https://github.com/jvermaas/autodock-gpu . As noted in Fig. 4 , this is not recommended for new docking calculations. Instead, new projects should use current versions of AutoDock-GPU 15 such as 1.5.3, available from https://github.com/ccsb-scripps/AutoDock-GPU . To generate the Vina data for Fig. 4 , we used Vina 1.2.3 52 from https://github.com/ccsb-scripps/AutoDock-Vina . As described in the ‘Data Generation Protocol’ section, several custom software packages were developed and used in this project, including 1. https://code.ornl.gov/99R/launchad/-/tags/v1.2 2. https://code.ornl.gov/99R/pymapreduce 3. https://code.ornl.gov/99R/pmake 45 4. https://github.com/frobnitzem/mpi_list 5. https://github.com/frobnitzem/sars_docking 47"
10.1038/s41597-023-02063-9,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software. No specific code has been developed for this study.
10.1038/s41597-023-02041-1,The R Statistics code used to perform all methods described here can be accessed via the GitHub repo at the following link: https://github.com/jonesmattw/National_Warming_Contributions.git .
10.1038/s41597-023-02089-z,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository https://github.com/kg4sci/electrocatalytic_db . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor ( chemdataextractor.org ) 20 , gensim ( radimrehurek.com ) 49 , PyMuPDF( https://github.com/pymupdf/PyMuPDFPyMuPDF ), Pytorch ( www.pytorch.org ) and scikit-learn ( scikit-learn.org ) 50 ."
10.1038/s41597-023-02079-1,The data reported here were generated via synchrotron experiments and did not require any processing of datasets beyond trivial binning of the two-dimensional data into one-dimensional spectra and calibration of the energy loss.
10.1038/s41597-023-02091-5,"In order for this dataset to be fully reproducible and expandable in the future, we have open-sourced all the Python code used to generate and validate the resource in the following code repository ( https://github.com/deepsolar/pynlfff ) and can be downloaded directly via pip as pip install pynlfff . The code can be divided into three parts, dataset generation code, label generation code and dataset Toolkit code. The dataset generation code is for generating the dataset, label generation code is for labeling flare information to nlfff data list, and dataset Toolkit code is for manipulating the data. The whole process of code usage is shown in Fig. 12 to explain this usage more clearly. The tools and examples for getting original Bp, Bt and Br fits can be found at ( https://github.com/mbobra/SHARPs ). Dataset generation code The code of dataset generation mainly consists of three different components. The first component contains the preparing boundary conditions programs. This utility uses Bp.fits, Bt.fits and Br.fits of “hmi.sharp_cea_720s” to generate “boundary.ini”, “mask.dat”, “grid.ini” and “allboundaries.dat” for the next step. This code is multi-threaded for computing efficiency, allowing the users set the number of threads. Note that if the raw data file is corrupted or with the quality problem, the boundary conditions file may not be generated properly. The corrupted raw files may report an error when operating them, e.g., the quality problem of raw data may cause generated “mask.dat” file with “NaN”. The second component is for magnetic field calculation, consisting of Python code for computing flow control and magnetic field extrapolation module provided by Wiegelmann’s team 22 . The Python code is responsible for scheduling and controlling core computing, specifying the number of running processes, binding tasks and cores, adaptively assigning cores according to the task, maximizing the use of computing resources, quality control, and logging, etc. The third component is magnetic field calculation written by C language program. It is not included in our published “pynlfff” package since its copyright is owned by Wiegelmann’s team 22 . The C code needs to be compiled beforehand, and “pynlfff” provides bash scripts to automatically compile and generate single-process and multi-process programs. In addition, we have rewritten multi-grid bash scripts to perform magnetic field extrapolation for each layer separately. Python and C should be implemented together, using single-process C programs for small tasks and multi-process C programs for large tasks. In addition, we allocate computing cores according to the task size and employ core binding technique to maximize the use of computing resources. Dataset Toolkit Code After getting the dataset file, you can implement your own program to read the product file “Bout.bin” based on the storage structure of the product file “Bout.bin” which has been described in subsection NLFFF Data Format, and we provide a toolkit for python implementation to help you with the reading operation. Flare label generation code As shown in Fig. 4 , pynlfff already implements these processes and has updated the label information in the project website 39 , if there is any other information that needs to be customized, it can be done through pynlfff or by modifying the pynlfff code."
10.1038/s41597-023-02077-3,"Matlab codes used to preprocess data, compute joint centers and identify gait events are shared in open access through dedicated gitlab repositories (respectively: https://gitlab.unige.ch/KLab/preprocessing_toolbox , https://gitlab.unige.ch/KLab/fusion_biplane_xrays_motion_capture and https://gitlab.unige.ch/KLab/gev ). The Biomechanics Toolkit (BTK) is freely available on the following repository: https://github.com/Biomechanical-ToolKit/BTKCore . The IMUs data reader is freely available on the following website: https://physilog.com/ )."
10.1038/s41597-023-02075-5,"During data collection, raw CSV files were generated from the data stream accessed with SMI’s provided ET API within Unity. These raw CSV files were later converted to the format described in Table 5 using custom Python code. The code used to convert the raw files to the final format, along with the code used to generate Figs. 1g – 4 and the data for Tables 1 , 2 , is available on figshare 25 . This code was developed using Python 3.7.11 with the following main packages: numpy 1.21.6, pandas 1.3.5, openpyxl 3.0.9, and matplotlib 3.2.2. This repository also contains other supplementary material including a manual for the ET-HMD, a pamphlet with manufacturer-provided technical specifications for the ET-HMD, and the video clips used for the VID task."
10.1038/s41597-023-02093-3,"The codes used for calculation and analysis in this study are available in figshare 74 , 75 , 76 , 77 , 78 ."
10.1038/s41597-023-02056-8,"Specific functions for data pre-processing and spline fitting are freely available in the R package ithir 31 . Codes associated to the model fitting, cross-validation and mapping are freely available from https://github.com/AusSoilsDSM/SLGA/tree/main/Production/DSM/SoilOrganicCarbon . All analyses were performed in in the R programming language (version 4.1.0)."
10.1038/s41597-023-02090-6,The code used to evaluate the quality of the database is in Open Science Framework ( https://osf.io/djc69/ ).
10.1038/s41597-023-01946-1,The code used for the portal is available on https://github.com/CONP-PCNO/conp-portal and a version of the code is available on Zenodo 37 .
10.1038/s41597-023-02095-1,"The codes for two bias correction methods (LS and DT) and three neural network data fusion methods (BP, LSTM and CNN) are available at https://doi.org/10.5281/zenodo.7306199 . The codes were programmed using MATLAB version 2022a and Python 3.8."
10.1038/s41597-023-02049-7,"The CLM5 hydrological datasets are available to the public at https://doi.org/10.57931/1922953 in comma-separated value (.csv) and netcdf (.nc) formats. This experiment used a modified version of CLM5 designed to allow easier parameterization and support machine-specific compilation. The modified source code is available at https://doi.org/10.5281/zenodo.6653704 61 , forked from https://github.com/ESCOMP/CTSM/tree/branch_tags/PPE.n11_ctsm5.1.dev030 . Source codes that were used to develop and analyze the data are available at https://doi.org/10.5281/zenodo.7039118 62 . The MetSim disaggregation code is available at https://github.com/UW-Hydro/MetSim ."
10.1038/s41597-023-02107-0,"The MPS database was generated using DBgen (v1.0.0a7) ( https://github.com/modelyst/dbgen ), an open-source framework for building scientific databases and pipelines available at https://github.com/modelyst/dbgen . A python API, a command-line interface (CLI), and a Jupyter notebook with example queries are available in the Materials Provenance Store Client repository ( https://github.com/modelyst/mps-client )."
10.1038/s41597-023-02076-4,"A software suite accompanying the resource is available on https://github.com/latur/SWaveform . The repository contains scripts for a) database and GUI deployment on the SQLite platform and b) a toolkit for DOC profile and SV data processing and management. The toolkit contains scripts for generation of DOC profiles corresponding to breakpoint loci from alignment files (SAM, BAM or CRAM format) and annotated VCF files, as well as DOC profile conversion into BCOV format. In addition, we provide tools for profile clustering, motif discovery and a script for subsequent motif detection in DOC profiles."
10.1038/s41597-023-02085-3,"The codes used to convert the HydroSHEDS river database to a routable dataset are freely available 28 . The code was written in R programming language, version 3.6.2. Beyond R, there is no need for any special software or program to replicate our results."
10.1038/s41597-023-02071-9,The dataset is easy to access by Microsoft Excel or other software like R 23 or Python 24 . The complete dataset can be obtained from GitHub ( https://github.com/AnabelleLaurent/onfant.dataset ).
10.1038/s41597-023-02108-z,"Annotations were manually performed using ImageJ, there is no accompanying code as the software is publicly available (ImageJ 1.52p)."
10.1038/s41597-023-02073-7,"We provide scripts to transform our annotations to the frame-wise labels and also the source code of some baseline models that use standard deep learning techniques to detect CVS criteria using our database for interested users. All these scripts were coded using Python 3.8.11 and Pytorch as the machine learning framework. All scripts were tested on Linux Machines. The repository README file contains detailed instructions to ease the use of the repository and brief descriptions of all files. The code is publicly available at https://github.com/ManuelRios18/CHOLEC80-CVS-PUBLIC , licensed under MIT OpenSource license. Therefore, permission is granted free of charge to copy and use this software and its associated files."
10.1038/s41597-023-02038-w,DICOM format MRI data was converted to a BIDS compatible dataset using HeuDiConv 125 (v0.9.0; https://github.com/nipy/heudiconv ). Facial features were removed from structural scans using BIDSonym 126 (v0.0.5; https://github.com/peerherholz/bidsonym ) and the tools flirt and fslmaths in FSL 127 (v5.0.1; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/ ). Code used in data organisation and defacing is available online ( https://github.com/QTAB-STUDY/dicom-to-bids ). Head movement and outlier metrics for diffusion scans were calculated using the tool dwipreproc in MRtrix3 104 (v3.0_RC3; https://www.mrtrix.org ). Task and resting-state fMRI quality checking metrics were calculated using MRIQC 108 (v0.16.1; https://github.com/nipreps/mriqc ).
10.1038/s41597-023-02096-0,"The ASI Catalogue code is open-source and can be found at https://github.com/awesome-spectral-indices/awesome-spectral-indices and Zenodo 205 . The spyndex Python package is open-source and can be found at https://github.com/awesome-spectral-indices/spyndex . It is also available through PyPI ( https://pypi.org/project/spyndex/ ) and conda-forge ( https://anaconda.org/conda-forge/spyndex ). The catalogue in CSV format can also be downloaded from the Espectro Streamlit web app, which is available at https://share.streamlit.io/davemlz/espectro/main/espectro.py . The Espectro Streamlit web app code is open-source and can be found at https://github.com/awesome-spectral-indices/espectro ."
10.1038/s41597-023-02111-4,We relied on open source tools to perform data analysis. Custom code performed in R used in this analysis have been published in the following repository: https://github.com/GuerreroP/FISHRECAP-ATAC-RNA .
10.1038/s41597-023-02097-z,"Code underpinning the pipeline (BAGPIPE) used for aligning sequence data and calling variants BAGPIPE is available at https://bitbucket.org/renzo_tale/bagpipe/ . Detailed code outlining the various analyses and parameters is available without restriction at https://bitbucket.org/gibberwocky/amelhap . All analyses have been performed with freely available software. These include: BWA-MEM v0.7.17; SAMtools v1.9; GATK v4.0.11.0; vcftools v0.1.13; bcftools v1.13; Plink v1.90p; R 87 v4.1.3; R packages (vcfR 88 , alstructure, ggplot2, ggpubr, ggdist, tidyverse, readxl, reshape2, ComplexHeatmap, circlize, paletteer); and SHAPEIT v4.2. With the exception of plotting data, all analyses were conducted on the Edinburgh Compute and Data Facility (ECDF), a high performance computing cluster (HPC) running a Linux operating system."
10.1038/s41597-023-02086-2,"Table 3 provides links to all models, data, versions and DOI’s used to generate this dataset."
10.1038/s41597-023-02113-2,No custom code was used to generate or process the data presented in this manuscript.
10.1038/s41597-023-02110-5,All calculations of hydroclimate reconstruction for eastern China are based on the MATLAB language and are available at GitHub: https://github.com/baimengxin2016/Hydro_EC . Any updates will also be published on GitHub.
10.1038/s41597-023-02109-y,"For ease of access and reproducibility, the Methods and Technical Validation sections contain the command parameters and referencing for all versions of third-party software and scripts used in this study."
10.1038/s41597-023-02101-6,"To assist users in utilising the updated PDBx/mmCIF files and SIFTS annotations, a Google Colab notebook is available at https://colab.research.google.com/github/PDBe-KB/sifts_data_analysis/blob/main/sifts.ipynb or via GitHub at https://github.com/PDBe-KB/sifts_data_analysis . This notebook provides information on how to parse, extract and filter SIFTS annotations from the updated PDBx/mmCIF files. Additionally, the notebook demonstrates how users can compare various numbering schemes of a given residue across different PDB structures of the same protein."
10.1038/s41597-023-02115-0,The functions used to analyse the dataset were deposited in the following repository https://github.com/Luisiglm/proteomics_R .
10.1038/s41597-023-02081-7,The source codes for identifying outliers used in this paper are available at https://github.com/BGM-USD2020/GAPAD_codes.git .
10.1038/s41597-023-02112-3,All programs used in this study were published in peer-reviewed journals. Additional information was detailed in the Materials and Methods section.
10.1038/s41597-023-02105-2,The data processing procedures that we perform on the dataset are those mentioned above. 1. The Python code used to extract the frame number of CSV files to be found at https://doi.org/10.6084/m9.figshare.20579541.v1 27 (Csv and Bvh are the raw data and angle is the corresponding parsed data part. The csv file initials have been updated to the participant experiment sequence number.) 2. Web-based download: the BVH files to calculate durations can be found at https://physionet.org/content/kinematic-actors-emotions/2.1.0/ .
10.1038/s41597-023-02087-1,"We make our code available at https://github.com/barbrakr/NMRI225.git as NMRI225_run.m, NMRI225_run.py and nmri_functions, under a CC BY license. We used MATLAB 2018b to run NMRI225_run.m and Python 3.8 for running NMRI225_run.py. We have summarized the packages of the conda repository in Supplementary Materials."
10.1038/s41597-023-02123-0,"We provide the code used to extract the features with PyRadiomics at https://github.com/ysuter/OpenBTAI-radiomics . For reproducibility and convenience in case any user wants to customize the extraction, all the.py files needed and a “readme” file are available."
10.1038/s41597-023-02057-7,"The MSConvert installer and documentation is available from https://proteowizard.sourceforge.io/ . EncyclopeDIA is available at https://bitbucket.org/searleb/encyclopedia/ . The Skyline-daily installer and documentation are available from https://skyline.ms/skyline.url . The source code for both the MSConvert and Skyline projects are available as part of the Proteowizard project https://github.com/ProteoWizard/pwiz . All code used for the analysis of the data matrix including data QC, normalization, pre-processing, visualization, and figure generation is available online at https://github.com/uw-maccosslab/ADBrainCleanDiagDIA ."
10.1038/s41597-023-02128-9,"For information on software packages and versions used for pre-processing, see methods section. No additional custom code was implemented for CARDIO:DE."
10.1038/s41597-023-02106-1,"Figure 10 shows the architecture of MouseBytes. This platform is based on ASP.NET WEB API framework in which Angular 5.0 was used in the client side, and .NET Core 2.0 was used in the server side. Angular is a Javascript framework consisting of HTML templates, TypeScript components, modules, and services. The client communicates with the server through the Angular services to call the API endpoints in the server. In the server side, there are API controllers which are used by client services for communicating data, and they handle the business logic of the application along with the services and models. All of these elements are connected to the database (Microsoft SQL Server 2017) through the data access layer. The code repository for MouseBytes is available in Github ( https://github.com/Rodent-Cognition-Core/CBAS ) under the GPL 3.0 license."
10.1038/s41597-023-02119-w,No custom computer code or algorithms were used to process or generate the data presented in this manuscript.
10.1038/s41597-023-02094-2,"The generated datasets and the codes for producing the datasets are available from https://github.com/KowComical/CM_Power_Data and figshare 29 . The most up-to-date, continuously updated data can be visualized and uploaded from https://power.carbonmonitor.org . Codes are available upon reasonable requests."
10.1038/s41597-023-02132-z,"Fastp (version 0.12.4) fastp -i Read_1.fastq.gz -f 12 -o Read_1_fastp_trim.fastq -I Read_2.fastq.gz -F 12 -O Read_2_fastp_trim.fastq Trinity (version 2.13.2) Trinity–seqType fq–max_memory 96 G–samples_file Hilsa_samples–SS_lib_type RF–CPU 20–no_bowtie Hilsa_samples file is provided in Figshare repository 23 . CD-HIT (version 4.8.1) cd-hit-est –I Hilsa_RNA_Trinity.fasta –o Hilsa_TSA.fasta –T 20 –M 1400 –c 0.95 TransDecoder (version 5.5.0) TransDecoder.LongOrfs -t Hilsa_TSA_v2.fasta Then, TransDecoder.Predict -t HILSA_TSA_v2.fasta–retain_blastp_hits Hilsa_peptides_BLASTp.txt BLAST (version 2.12.0+) Database Preparation: makeblastdb –in uniprot_sprot.fasta -dbtype prot -parse_seqids -out uniprot_sprot_fasta.db BLASTp RUN-1: blastp -query Hilsa_peptide_transdecoder.fasta -db uniprot_sprot_fasta.db -outfmt 6 -max_target_seqs 1 -num_threads 16 -evalue 1e-5 -out Hilsa_peptides_BLASTp.txt The output file ‘Hilsa_peptides_BLASTp.txt’ was used to run ‘TransDecoder.Predict’ program. BLASTp RUN-2: blastp -query HILSA_TSA_v2.fasta.transdecoder. fasta -db uniprot_sprot_fasta.db -outfmt 6 -max_target_seqs 1 -num_threads 16 -evalue 1e-5 -out Hilsa_final_BLASTp.txt BLASTx RUN: blastx -db uniprot_sprot_fasta.db -query Hilsa_TSA.fasta -max_target_seqs 1 -outfmt 6 -num_threads 16 -evalue 1e-5 > Hilsa_Transcripts_Blastx.txt Trinotate Build_Trinotate_Boilerplate_SQLite_db.pl Trinotate Then, Trinotate Trinotate.sqlite init–gene_trans_map Hilsa_TSA_v2.fasta.gene_trans_map – –transcript_fasta Hilsa_TSA_v2.fasta–transdecoder_pep Hilsa_peptide_transdecoder.fasta Loading BLASTx and BLASTp results Trinotate Trinotate.sqlite LOAD_swissprot_blastx Hilsa_Transcripts_Blastx.txt And then, Trinotate Trinotate.sqlite LOAD_swissprot_blastp Hilsa_ final _BLASTp.txt Generating Annotation Report Trinotate Trinotate.sqlite report > Annotation_report.xls"
10.1038/s41597-023-02120-3,No specific code or script was used in this work. The commands used in the processing were all executed according to the manuals and protocols of the corresponding bioinformatics software.
10.1038/s41597-023-02124-z,"No specific code was developed in this work. The parameters of all commands and pipelines used for data processing are described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used, as suggested by the developer."
10.1038/s41597-023-02117-y,The code to run the model is available on Zenodo 21 .
10.1038/s41597-023-02078-2,"From the BlueBio PostgreSQL database, the .CSV dataset was extracted via queries through the free software environment R 14 , using the dbConnect function from the RPostgreSQL package 15 , a database interface and ‘PostgreSQL’ driver for R. The related R code (dbconnect_csv.R) is freely available through figshare 11 . The maps in Figs. 2 , 5 , 7 , 8 were created using several R packages like tidyverse 16 for data handling, rgdal 17 package providing bindings to the “Geospatial Data Abstraction Library”, sf package , a standardised way of encoding spatial vector data in R and the package ggplot for graphical visualisation. The R code (scidata_workflow.R) and the additional data layer for the creation of all the reported figures and summary statistics are deposited in the figshare 11 public repository. This could facilitate data re-use and analysis."
10.1038/s41597-023-02154-7,No specific code was used to produce the data described in this manuscript.
10.1038/s41597-023-02145-8,"All software used in this study are in the public domain, with parameters being clearly described in Methods and this section. If no detail parameters were mentioned for the software, default parameters were used as suggested by developer."
10.1038/s41597-023-02126-x,All custom codes used to analyze the data are shared on GitHub: https://github.com/Orset-Thomas/Squirrel_monkey_DWI.git .
10.1038/s41597-023-02066-6,"The data processing code is available in the tools folder of https://pegasus.ac.cn . The code is written in Python. The functions of the tools are as follows: (1) The tools/devtoolkit/labelTransformer.py is to convert oriented bounding boxes to standard bounding boxes and generate the dataset, (2) The tools/devtoolkit/visualization.py is to visualize images with bounding boxes, (3) The tools/output/voc2yolo.py is to generate the label files with the YOLO format to help users train the YOLO, which is the representative object detection algorithm."
10.1038/s41597-023-02118-x,"The Matlab version used for this work is MATLAB R2022b. The custom Matlab code and functions used to generate several of the supplementary files associated with this work are described in the Matlab Files sub-section of the Data Records section. The Matlab code and functions are described in further detail in the ‘Explanation’ files, which are publicly accessible via the Illinois Data Bank 37 . The Matlab code and functions are contained in the ‘Matlab Files’ folder (File 7), and the underlying data set is contained in the ‘SoyFACE 1-Minute Fumigation Data Files’ folder (File 9). The R statistical programming language and openair package are required in order to use the windRose function (Figs. 2 , 3 ). The R version used for this work is Rx64 4.1.2, which is free for all users to download. The underlying data set is contained in the ‘SoyFACE 1-Minute Fumigation Data Files’ folder (File 9)."
10.1038/s41597-023-02151-w,"The source code of MASNUM-WAM is available to the public and can be downloaded from https://doi.org/10.57760/sciencedb.02893 70 . The dataset can be regenerated by using the wind parameter files, i.e., files with <para_id> of ‘windx’ and ‘windy’, archived in ScienceDB 59 as wind forcings."
10.1038/s41597-023-02116-z,"The Jupyter notebooks providing the panel visualisation of the isotherm curves, enthalpy of adsorption data, IAST-based multicomponent mixture isotherm, and the t-SNE + DBSCAN analysis of the chemical and geometric properties of MOFs are distributed alongside the database in the Zenodo 59 repository. A fully automated workflow that is capable of recreating the dataset was made available as an open-source project (v1.0.0) on GitHub ( https://github.com/st4sd/nanopore-adsorption-experiment )."
10.1038/s41597-023-02143-w,"The data gap were filled in Python 3.8, using the following libraries: pandas 1.3.4, numpy 1.18.5, statsmodels 0.13.2, sklearn 1.0.2, scipy 1.7.3. The full custom Python script is provided on the open-access online dataset Figshare 26 [ ‘Estimated code.ipynb’ ]."
10.1038/s41597-023-02125-y,All code is available on on Github https://github.com/Cassie07/SNOW-Dataset .
10.1038/s41597-023-02146-7,"In addition to the CSV file, several components to work with the reported bilayer materials can be found in figshare 43 and in GitHub ( https://github.com/rkb12/BMDB-databaseBMDB-database ). In the stacking-pattern-PYTHON-code-for-each-class zipped directory, a code that obtains all possible stacking patterns (following the naming notation in Fig. 2 ) are given. Inside each subdirectory, a prototype example of monolayer structure and the corresponding stacking code are given. Executing the code in the same directory will create the stacking pattens for the respective class of bilayer. In the zipped soc-bandstructure-code, an in-house code generating the band structure with SOC is provided. The code reads a VASP output file and generates a graphical representation for the band structure. In the zipped transport-cal-code, an in-house code generating the transport properties such as conductivity ( σ / τ ), Seebeck coefficient (S), and power factor (S 2 σ / τ ) as function of chemical potential at different temperature for each bilayer material. The code reads VASP and BoltzTraP output files and generates a graphical representation for the transport properties. In the zipped soc-bandstructure-figures and transport-figures a graphical representation for the energy band structure (with and without SOC) and transport properties for the ground state stacking compositions of all bilayer materials are also given."
10.1038/s41597-023-02129-8,Code can be accessed at this link: osf.io/kzrc4
10.1038/s41597-023-02122-1,"A MATLAB script and the necessary functions used for processing the GRACE data, as summarized in Fig. 1 , are freely available for downloading on figshare at the following link: https://doi.org/10.6084/m9.figshare.20524035 28 . An example is provided to help the users with the overall steps. Nevertheless, due to computational limitations, the example is limited to 3-by-3 arc-degree resolution for observations and 1 arc-degree for parameters over the land areas. If desired, those with more robust computational resources can use inputs with a finer resolution. The users can use the available codes 28 to process the data sets themselves if a more extended period is desired. This depends on the availability of the Level-2 products (e.g., COST-G solution). Alternatively, the users could use Level-2 products from different processing centers if desired."
10.1038/s41597-023-02082-6,Python and Matlab scripts used to de-identify the .c3d and .mat files and validate the selected joint angles are available on Github: https://github.com/Graham-Lab1/3D_MoCap_Data_of_a_Movement_Screen . No custom code was used in addition to the Visual3D software to process the dataset.
10.1038/s41597-023-02138-7,"Details about codes that generate the dataset as well as the usage notes on accessing, downloading and pre-processing the datasets are made available on the homepage of the dataset on the Deep Blue Data system of University of Michigan 30 . Future updates of the codes and dataset will be made available on this website as well. Please contact the corresponding author for data request and questions. Additionally, our users can explore our interactive database dashboard ( https://vista-tec.shinyapps.io/VISTA-Dashboard/ ) for more technical details and run the VISTA algorithm live."
10.1038/s41597-023-02144-9,MISSING
10.1038/s41597-023-02072-8,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-023-02147-6,"The code used for the data normalization and merging steps was created and run in Stata/MP 17.0 and is made available in the data repository 90 . The acquisition and processing scripts are not shared publicly because EDGAR servers may block the simultaneous use of the same acquisition script based on the user agent in request headers. However, downloading regulatory filings via HTTP follows a standard procedure, and parsing XML files in Python using the lxml library is also well-documented."
10.1038/s41597-023-02130-1,"Parameters to software tools involved are described below: FastQC : version 0.11.8 FASTQ Toolkit (BaseSpace): version 2.2.5, parameters: trim of first 12 bp, Quality cutoff (Phred score <30), adapter removal Trinity : version 2.0.6 ( N. plumchrus ) and version 2.4.0 (all others), parameters: –seqType fq–CPU 32–max_memory 200 G –min_contig_length 300 –normalize_max_read_cov50 Bowtie : version 2.1.0, default parameters BUSCO : version 5.3.2, dataset: arthropoda_odb10 (Creation date: 2020-09-10, number of genomes: 90, number of BUSCOs: 1013). Sequence Manipulation Suite (SMS) : ( https://sites.ualberta.ca/~stothard/javascript/rev_comp.html ) TransDecoder : version 3.0.0, default parameters (open reading frame > 100 amino acid) Transeq : Search and Sequence Analysis Tools Services from EMBL-EBI in 2022 https://europepmc.org/ MAFFT : version 7.511 ( https://mafft.cbrc.jp/alignment/software/ ) BLASTp : (local Beowulf Linux computer cluster) against NCBI Swiss-Prot database: version February 2015, parameters: E-value of 10 −6 cutoff UniProt : ( http://www.uniprot.org/help/uniprotkb ), February 2021, parameters: Gene Ontology and the KEGG (Kyoto Encyclopedia of Genes and Genomes; https://www.kegg.jp ) databases OrthoVenn2 : version 2018. Inflation value 1.5; E-value 1e-15 except as indicated. https://orthovenn2.bioinfotoolkits.net/home MrBayes : version 3.2, parameters: lset rates = gamma ngammacat = 4; prset aamodelpr = fixed(wag); mcmc ngen = 10000000 relburnin = yes burninfrac = 0.25 printfreq = 1000 samplefreq = 1000 nchains = 4 savebrlens = yes RAxML : version 8.2.12, parameters: 1,000 bootstrap replicates (-# 1000), WAG substitution model and gamma distribution of rates (-m PROTGAMMAIWAGF) FigTree : version 1.4.4 (url: http://tree.bio.ed.ac.uk/software/figtree/ )"
10.1038/s41597-023-02121-2,"Brain/MINDS Data portal 23 , BrainSuite18a 34 , ANTs (Advanced
                Normalization Tools) 16 , Mrtrix3 21 , SPM12 (Statistical
                Parametric Mapping package) 35 , CONN (the functional connectivity
toolbox) 36 ."
10.1038/s41597-023-02102-5,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 ( https://www.python.org/ ); Pydicom 1.2.0 ( https://pydicom.github.io/ ); OpenCV-Python 4.2.0.34 ( https://pypi.org/project/opencv-python/ ); and Python hashlib ( https://docs.python.org/3/library/hashlib.html ). The code for data de-identification was made publicly available at https://github.com/vinbigdata-medical/vindr-cxr . The code to train CNN classifier for the out-of-distribution task was made publicly available at https://github.com/vinbigdata-medical/DICOM-Imaging-Router . The VinDr Lab is an open source software and can be found at https://vindr.ai/vindr-lab .
10.1038/s41597-023-02139-6,"The commercial software Spectronaut generated a spectrum library, and free software DIALib-QC evaluated the constructed spectrum library, which is described in the Data Records section."
10.1038/s41597-023-02127-w,"We used the pymatgen python package, which is open-source software under the Massachusetts Institute of Technology License, for materials analysis as well as the generation of VASP inputs and CIF files. The VASP DFT code used is accessible under a paid license, copyrighted by the University of Vienna, Austria. Initial structures were generated with SPuDS DOS version >2.20.08.06 ( https://www.unf.edu/~michael.lufaso/spuds/ ) using a custom high-throughput python wrapper available on GitHub ( https://github.com/zaba1157/PySPuDS ). Perovskite/non-perovskite classifications were performed using a custom python package that is available on GitHub ( https://github.com/rymo1354/crystal_motifs ) and based on the pymatgen and NetworkX graph network python packages."
10.1038/s41597-023-02134-x,The code of our XML parser is provided in the Supplementary_File_1.zip on our data repository: https://doi.org/10.13012/B2IDB-4353270_V2 .
10.1038/s41597-023-02104-3,"The code used to calculate the node-specific metrics, network-wide metrics, as well as static and interactive visualizations of each of the 40 AMC websites can be found at https://github.com/davidchen0420/Academic-Medical-Center-Topology . The Jupyter notebook AMC_Topology_Metrics.ipynb describes the steps used to calculate the metrics as comments. To run the Jupyter notebook, installation of the Anaconda distribution of Python 3.8.0+ and required scientific packages listed in the notebook is needed. Example input data and expected output results are provided in example_data.zip in the GitHub. The example input data is a subset of 3 AMC website nodes and internal edges that can also be found in the Figshare repository (see Data Records)."
10.1038/s41597-023-02150-x,MISSING
10.1038/s41597-023-02160-9,"The firmwares of the instruments v2018 and v2021, as well as the binary data decoder scripts, are fully available on the corresponding github repositories: https://github.com/jerabaul29/LoggerWavesInIce_InSituWithIridium , https://github.com/jerabaul29/OpenMetBuoy-v2021a . The scripts to plot the data from the netCDF files are available on the main Github repository: https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022 , together with the raw data. All code is developed in modern python (version 3.8 or higher), or C++, or Matlab, unless specified otherwise. The netCDF datafiles are following the netCDF4 standard, with CF attributes conventions. We also provide a mirror of the netCDF data files on the THREDDS server of the Norwegian Meteorological Institute in the context of the Arctic Data Center repository, at the following address: https://doi.org/10.21343/AZKY-0X44 . We will offer reasonable support regarding the data and its use through the Issues tracker of the data repository at https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022 , and we invite readers in need of specific help to contact us there. In addition, we plan on releasing extensions to this dataset periodically as more data are collected. We invite scientists who own similar data and are willing to release these as open source materials to contact us so that they can get involved in the next data release we will perform. In addition, we discovered, in the context of the present work, that there are already some openly available data about sea ice drift and waves in ice (for example, 75 , 76 , 77 , though there may be more such data available that we do not know of); however, these are scattered across the internet, and may be difficult to find. Therefore, in addition to the data release intrinsic to this dataset, we have started to maintain an index of similar open data at https://github.com/jerabaul29/meta_overview_sea_ice_available_data . We invite the reader aware of additional open datasets to notify us so that these can be added to our index, which we will keep extending in the future. We hope that these data, together with a variety of datasets that have been recently gathered 78 , 79 , will be a significant contribution towards building large, well sampled datasets of in situ observations of the MIZ and sea ice dynamics."
10.1038/s41597-023-02164-5,"No specific codes or scripts were used in this study. All software used is in the public domain, with parameters clearly described in the Methods section."
10.1038/s41597-023-02157-4,The code to create the R Shiny App is available on GitHub ( https://github.com/J4SJA/ClimBatsApp ).
10.1038/s41597-023-02142-x,"No special codes or scripts were used in this work, all data processing commands and parameters were executed according to the manuals and protocols of the corresponding bioinformatics software."
10.1038/s41597-023-02159-2,"The code used to produce the CMIP6-VN dataset by downscaling the CMIP6 GCMs is publicly available at: https://github.com/quanta1985/Bias-Correct-and-Spatial-Dissaggregation . The code is written using Bash, R, and CDO scripts."
10.1038/s41597-023-02156-5,No custom code was used to generate or process the data described in this paper.
10.1038/s41597-023-02148-5,"The geodatabase was created in ArcGIS Desktop 10.5 and 10.7 software by Environmental Systems Research Institute (ESRI). Transfer volume unit conversions were performed in Microsoft Excel. Figures were created using ArcGIS Desktop, X-Ray for Geodatabases 2018.3.12 by Vertex3, Microsoft Visio Professional 2016, and R. The data pre-testing used the WaSSI model which is available as an online tool at https://web.wassiweb.fs.usda.gov/s . We customized the model code in R, and this code is available at https://github.com/ln1267/IBT_Flow_check ."
10.1038/s41597-023-02131-0,MATLAB code used in generating the dataset and figures in this manuscript is available through open access 46 .
10.1038/s41597-023-02149-4,"The complete workflow, from mapping to the generation of figures is available at the GitHub repository ( https://github.com/Balays/MPOX_ONT_RNASeq )."
10.1038/s41597-023-02158-3,"Custom code generating figures and tables including correlation plots, volcanoes, Venn diagram, annotated heatmap, statistics tables, and ROC curves is available for download with registration for a free account on synapse.org. The code is available as R scripts in the Analysis folder deposited on Synapse 25 . These scripts were run as provided on R version 4.0.2 with the two provided input files to generate outputs."
10.1038/s41597-023-02171-6,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatic software.
10.1038/s41597-023-02173-4,The code repository with the scripts of data preparations and technical validations (models and pre-trained checkpoints) is available at https://github.com/simulamet-host/visem-tracking . The original YOLOv5 code is available at https://github.com/ultralytics/yolov5 .
10.1038/s41597-023-02140-z,"Processing code is publicly available and can be found at https://github.com/Helmholtz-AI-Energy/TBBRDet . The software is licensed under the Revised Berkley Software Distribution (BSD-3) license ( https://opensource.org/licenses/BSD-3-Clause ). All scripts are implemented with the Python (v.3.6.8) programming language 43 and utilize the PyTorch (v.1.10.2) machine learning framework 44 . Conceptually, the software provides the following functionalities: VGG annotation to COCO JSON converter implementing fully automatic conversion from the annotation format generated during the manual labeling process into the COCO JSON format archived on Zenodo. Dataset mappers for the Detectron2 and MMDetection libraries implementing random-access collections to individual images and corresponding annotations. These are necessary for enabling the loading of five-channel images in each library. Data may be augmented by arbitrary transformations during the loading procedure. Model configuration for all Detectron2 and MMDetection experiments performed in related works. Training/evaluation scripts for performing training and evaluation of neural networks for both Detectron2 and MMDetection . Dataset/experiment utilities for exploring the dataset, calculating image normalization coefficients, combining model scores, and calculating SLURM workload manager system 45 statistics (consumed energy, runtime, etc.). For creating, updating, and validating the FAIR DOs, the Typed PID Maker was used. This is a component of the FAIR DO Lab for working on FAIR DO tasks, which is found at https://github.com/kit-data-manager/FAIR-DO-Lab ."
10.1038/s41597-023-02190-3,This work did not utilize a custom script. Data processing was carried out using the protocols and manuals of the relevant bioinformatics software.
10.1038/s41597-023-02181-4,"To facilitate users of this dataset, we have released the following Github repository: https://github.com/mylyu/M4Raw . The repository contains Python examples for data reading and deep learning model training, and the trained model weights to reproduce the results in Figs. 2 – 6 ."
10.1038/s41597-023-02172-5,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02163-6,"Initially, the data were organised in a relational PostgreSQL database with separated tables for metadata, calibration lists, and spatial characteristics. The final dataset was generated in pgAdmin 4 using PostgreSQL 12 including all relevant information from these previously related tables. This included the following steps: 1. Metadata and spatial characteristics of locations joined to original data, secondary data for Ouagadougou added to the dataset, missing information on Loumbila checkpoint added (lean season 2014); duplicates removed. 2. Unit volume, product density, and specific product-unit combinations joined to the dataset. 3. Selected products extracted and livestock quantities adjusted (from number of animals to kg). 4. Missing quantity computed and adjusted quantities calculated. 5. Distance calculated and geometry added. The database structure as well as the auxiliary tables are available on GitHub 21 , however, they are not needed for reusing the data."
10.1038/s41597-023-02133-y,No custom code was generated for this work.
10.1038/s41597-023-02176-1,The codes were written in C++ and Python 3. They rely on Python DBSCAN package and on MPI ( https://www.mpi-forum.org ) for parallelization. The code is available from the data site 9 .
10.1038/s41597-023-02175-2,"All software used in this work are in the public domain, with parameters described in the Methods section. The commands used in the processing were all executed according to the manuals and protocols of the corresponding bioinformatics software."
10.1038/s41597-023-02080-8,"All raw and cleaned data—as well as the R-code—used for standardising national-teams data, merging, and cleaning them are available at https://doi.org/10.17605/osf.io/tfsza 56 ."
10.1038/s41597-023-02182-3,All scripts used to create the data splits and the final results are provided in a GitHub repository ( https://github.com/software-competence-center-hagenberg/Blastocyst-Dataset ).
10.1038/s41597-023-02170-7,The MVSE R package can be installed from GitHub ( https://github.com/TaishiNakase/MVSE ). The MVSE package tutorial and the R Markdown document that provides example code for the visualization of the Index P maps can also be found on GitHub ( https://github.com/TaishiNakase/Index-P-estimation-and-applications ).
10.1038/s41597-023-02196-x,"The code written for the development of the database is available upon request from the authors, but it is not open to the external users through the website to protect the database. The desired main functions of the database were created in Python and Django framework. Django was used for creating the model for the backend and Javascript for filtering functions. For the front-end, HTML, CSS, and JQuery were used. The database is available through website https://biomechlab.iyte.edu.tr/en/homepage/38 and the public repository Database covering the previously excluded daily life activities | Aperta (ulakbim.gov.tr) 38 , 44 ."
10.1038/s41597-023-02179-y,"All software and pipelines used for data processing were executed according to the manuals and protocols of the bioinformatics software cited above, and the parameters are clearly described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used. The version of the software has been described in Methods."
10.1038/s41597-023-02185-0,"As we are presenting collected survey data, no custom code was used or is necessary to generate or work with this data. Complete codebooks describing the questionnaires are available here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GTNEJD 6 ."
10.1038/s41597-023-02100-7,The codes used in this study were made publicly available. The scripts used for loading and processing DICOM images are based on the following open-source repositories: Python 3.8.0 ( https://www.python.org/ ); Pydicom 1.2.0 ( https://pydicom.github.io/ ); and Python hashlib ( https://docs.python.org/3/library/hashlib.html ). The code for data pseudonymization and stratification was made publicly available at https://github.com/vinbigdata-medical/vindr-mammo .
10.1038/s41597-023-02153-8,The ECG features directly correspond to the outputs of the respective algorithms up to minor harmonization. We provide code to apply the predefined SNOMED CT mappings to the labels in the dataset ( apply_snomed_mapping.py released as part of the data repository 24 ). Links to code samples facilitating the usage of the dataset are described under Usage Notes and are released in a dedicated code repository 19 .
10.1038/s41597-023-02177-0,The code utilized for the Monte Carlo simulations in this study is provided in the Supplementary Codes 1 – 3 .
10.1038/s41597-023-02162-7,All the codes for processing the NHDplus data and generating the watershed carbon yield maps were developed using MATLAB version 2020b and archived at Github: https://github.com/qhgogogo/Spatially-distributed-riverine-organic-carbon .
10.1038/s41597-023-02192-1,Code for calculation of the drought indices as well as all statistical analysis within this publication is publicly available at the zenodo repository 85 . The mesoscale Hydrologic Model is an open source model and is available at https://mhm-ufz.org/ .
10.1038/s41597-023-02191-2,"The software required for data processing and analysis and image generation in this study are accessible, the software versions as follows: 1. Trimmomatic (v0.39, http://www.usadellab.org/cms/index.php?page=trimmomatic ) 2. BWA(v0.7.17, http://bio-bwa.sourceforge.net ) 3. Megahit ( http://i.cs.hku.hk/~alse/hkubrg/projects/idba_ud/ ) 4. MetaGeneMark (v3.25, http://exon.gatech.edu/meta_gmhmmp.cgi ) 5. CD-HIT ( http://www.bioinformatics.org/cd-hit/ ) 6. SOAPaligner ( http://soap.genomics.org.cn/ ) 7. BLASTP (BLAST v2.2.28+, http://blast.ncbi.nlm.nih.gov/Blast.cgi ) 8. MSDIAL (v4.7, http://prime.psc.riken.jp/compms/msdial/main.html ) 9. SIMCA-P (v14.1) 10. Fastp (v0.20.0, http://opengene.org/fastp/fastp ) 11. Samtools (v1.10) 12. Sambamba (v0.7.1) 13. Mosdepth (v0.2.9) 14. Picard Tools (v2.0.1) 15. Bcftools (v1.939) 16. GATK (v4.1.6) 17. Plink (v1.9, Complete flag index - PLINK 1.9 (cog-genomics.org)) 18. Admixture (v 1.3.0) 19. VCFtools (v0.1.13) 20. FarmGTEx TWAS-server ( http://twas.farmgtex.org/ )"
10.1038/s41597-023-02193-0,"The codes used for data processing, analysis, and generating results are available at GitHub repository www.github.com/yadupokhrel/Tiwari_Mekong_DataSynthesis_SciData ."
10.1038/s41597-023-02187-y,The EchoviewR package 35 that was used to automate the Echoview processing is available from a Github public repository: https://github.com/AustralianAntarcticDivision/EchoviewR .
10.1038/s41597-023-02214-y,"Data pre-processing was performed in Python 3.6 26 with custom scripts that are freely available on GitHub ( https://github.com/colleenjg/OpenScope_CA_Analysis ) and were developed using the following packages: NumPy 27 , SciPy 28 , Pandas 29 , Matplotlib 30 , Scikit-learn 0.21.1 31 , and the AllenSDK 1.6.0. ( https://github.com/AllenInstitute/AllenSDK ). Stimuli were generated by Python 2.7 32 custom scripts based on PsychoPy 1.82.01 33 and CamStim 0.2.4. The code is freely available (along with instructions to reproduce the stimuli, and example videos) on GitHub ( https://github.com/colleenjg/cred_assign_stimuli ). Dendritic segmentation was run in Matlab 2019a using a robust estimation algorithm 17 , 18 ( https://github.com/schnitzer-lab/EXTRACT-public ). Pupil tracking was performed using DeepLabCut 2.0.5 22 ( http://www.mackenziemathislab.org/deeplabcut ). ROIs were matched across sessions using a custom-modified version of the n-way cell matching package developed by the Allen Institute ( https://github.com/AllenInstitute/ophys_nway_matching ). Code for estimating photon conversion statistics on the raw imaging stacks is available on GitHub 25 ( https://github.com/jeromelecoq/QC_2P/blob/master/Example%20use%20of%20QC_2P.ipynb )."
10.1038/s41597-023-02188-x,No novel code was used in the construction of MSHF dataset.
10.1038/s41597-023-02183-2,"The computer code used to generate the IPIO database with three types of firm ownership for mainland China is based on GAMS and MATLAB. The computer code used to process firm-level micro data and trade statistics is based on STATA. All these codes with detailed instructions have been uploaded in figshare provided by Scientific Data 46 . All codes will also be available at https://github.com/abumazan/Interprovincial-IO-database/tree/main after publication. All of the data files used to generate the IPIO tables, except the firm-level data and detailed trade statistics at the product level, are available for public access at figshare ."
10.1038/s41597-023-02152-9,"GeoMAP v.2022-08 has been generated for ArcGIS (10.8.1) and QGIS (3.4) as geodatabase and geopackage material, using a GCS WGS 1984 geographic coordinate reference and WGS 1984 Antarctic Polar Stereographic projection. Data were developed manually, then stored in a GIS database developed, web-delivered and maintained by GNS Science in New Zealand. Software ArcGIS® has been used to create the GIS database 24 , but data can be exported in a variety of formats and compatible with most other GIS software. ArcGIS data are available from the PANGAEA data archive 24 , an ArcGIS REST service ( https://gis.gns.cri.nz/server/rest/services/SCAR_GeoMAP/ATA_SCAR_GeoMAP_Geology/MapServer ), or viewed through a webmap ( https://data.gns.cri.nz/ata_geomap/index.html ). A series of QGIS and Google Earth KMZ files, exported from the ArcGIS geodatabase layers, are also available from the archive 24 . The original data have been segmented into ten regions to keep KMZ files at a reasonable (<25 Mb) and useable size. GeoMAP documentation ( https://geomap.readthedocs.io/en/latest/ ) has been generated using code deposited on GitHub ( https://github.com/selkind/GeoMap )."
10.1038/s41597-023-02174-3,"The code we have used to generate this dataset has multiple sources, albeit all being publicly available. As defined previously, the data is collected through a monitoring infrastructure called ExaMon, which we have developed and deployed on the CINECA infrastructure. The code is publicly available and can be used in different supercomputing facilities ( https://github.com/EEESlab/examon ). The libraries and software modules used to process the data streams and obtain the dataset presented here are the following: • examon-client • NumPy (1.23.1) • Pandas (1.5.1) • PyArrow (9.0.0) In addition, in the source code repository ( https://gitlab.com/ecs-lab/exadata ) it is possible to recover the list of software modules used to access the data and to perform the analysis described in Sec. “Technical Validation”."
10.1038/s41597-023-02212-0,"The MATLAB dataset files, provided in addition to the coma-separated values format (available at figshare 12 ) were prepared via MATHWORKS MATLAB® (release 2022a). The MATLAB code used to obtain the empirical distributions shown in this paper from the data, is freely available in the same repository as the data."
10.1038/s41597-023-02207-x,Code used to train the molecular language model as well as the trained model used for natural product-like molecule generation is available from GitHub at https://github.com/SIBERanalytics/Natural-Product-Generator .
10.1038/s41597-023-02219-7,"The calculation of data is mainly done by Excel 2017, and there is no special code."
10.1038/s41597-023-02226-8,No custom code was made for the compilation and validation procedures in this dataset.
10.1038/s41597-023-02218-8,All software and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and code/parameters of software have been detailed described in Methods.
10.1038/s41597-023-02167-2,"• FAIRplus Github organisation: https://github.com/FAIRplus • FAIR Maturity Model: https://github.com/FAIRplus/Data-Maturity (MIT license) • FAIR Wizard: https://github.com/FAIRplus/FAIR_wizard (Apache-2.0) • FAIR Cookbook: https://github.com/FAIRplus/the-fair-cookbook (CC BY 4.0) • IMI Data Catalog: https://github.com/FAIRplus/imi-data-catalogue (AGPL-3.0 for code, CC BY-NC-SA 4.0 for data)"
10.1038/s41597-023-02195-y,iOBPdb GitHub source code can be accessed online here: https://github.com/sshuklz/iobpdb_app .
10.1038/s41597-023-02202-2,"The underlying voter files are proprietary and sourced from L2, Inc. The data can be directly purchased from L2, Inc. Otherwise, the count files generated herein can be recreated by acquiring voter files from each state. Among the six states we have considered, data from North Carolina is the most straightforward to obtain, as the voter file can be downloaded for free from the State Board of Elections website. Data processing code is straightforward and involves iterating through each voter file and tallying names by race. Initial processing code was written in Python (version 3.6), while a small amount of post-processing was done in R (version 4.1.2). These steps are described in the Data Processing subsection. Sample data processing code are available alongside the count files in the Harvard Dataverse 13 . Code to recreate each of the figures can be found in a separate repository in the Harvard Dataverse 22 ."
10.1038/s41597-023-02166-3,The code is open source and available in a dedicated public repository on GitHub: https://github.com/FAIRplus/the-fair-cookbook .
10.1038/s41597-023-02210-2,"The gridded bathymetries (ASCII raster and netCDF files) are interpolated from raw sand elevation data by means of the code file ‘BMF_Gridded_bathymetry.m’ (included in the data repository folder 36 ). Code is written in MATLAB (R2018b) and is fully commented. Although MATLAB is a proprietary language, the ‘.m’ files can be read with a text viewer."
10.1038/s41597-023-02220-0,The custom data analysis of the present paper as well as the paper figures have been performed using RStudio Markdown and Bioconductor. A pdf version of the report including the R-Code and figures is available as Supplementary Information.
10.1038/s41597-023-02206-y,The code used to check species names and taxonomic authorities is available in the R package ‘plantlist’ version 0.8.0 (Zhang 28 ).
10.1038/s41597-023-02169-0,The ‘iratebirds.app’ application code can be found on GitHub https://github.com/luomus/iratebirds 42 . R code used to generate the modeled ratings (Haukka_et_al_iratebirds_Scientific_Data_Data_Modelling.R) as well as figures (Haukka_et_al_iratebirds_Scientific_Data_Figure.R) and tables (Haukka_et_al_iratebirds_Scientific_Data_Tables.R) in this manuscript and the supplementary file are available in Figshare alongside the data set 31 .
10.1038/s41597-023-02213-z,"Our Python source code is publicly available online as a GitHub repository 64 ( https://github.com/CompNet/FoppaInit ). It is designed to be applied to the raw TED tables 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , and leverages the Hexaposte 39 and SIRENE 40 databases mentioned in the Methods section. It performs the integrality of the processing described in this section, and produces the FOPPA database. When performed in parallel on 10 NVIDIA GeForce RTX 2080 Ti GPUs, this processing requires approximately 6 days."
10.1038/s41597-023-02098-y,"The codes to reproduce the baseline results presented in the Usage Notes section is available at https://github.com/simonMadec/VegAnn . We recommend users to start with th custom PyTorch dataloader to run easily for instance the training/evaluation with the crop-specific approach and the VegAnn generic approach, more information can be found in the associated ReadMe file."
10.1038/s41597-023-02224-w,The code 51 of processes is available at Zenodo dataset ( https://zenodo.org/record/7266229#.Y19tWHZBwuU ). The code needs to be run in the version of Matlab after 2018.
10.1038/s41597-023-02231-x,"All figures of this study were generated by Origin (2022), all data were processed by using Microsoft Office Excel (Excel 2019), and the generated datasets have been stored as xlsx files and shared in Figshare 39 ."
10.1038/s41597-023-02200-4,"The implementations of the DFT and MTP genetic algorithms used to search for low-energy structures are available via GitLab: https://gitlab.com/muellergroup/cluster-ga . The scripts and code for managing QCD, for example merging new clusters into QCD, updating existing clusters with DFT calculations using updated parameters, generating metadata of clusters listed in Table 3 , are also open-sourced at https://gitlab.com/muellergroup/qcd_mgmt ."
10.1038/s41597-023-02223-x,"A Matlab ® script ( Code_Availability.m available in figshare 28 ) and a Python script ( Code_Availability.phy available in figshare 28 ) are provided to demonstrate how the dataset can be accessed and how to visualize an sEMG signal. The provided code has a menu where the user can select a specific signal to be visualized. Another code ( Example_Classification.m available in figshare 28 ) is provided to access the dataset and obtain the FFT of the dataset. This code can be used to generate an input file to implement a classification algorithm using the Neural Net Pattern Recognition toolbox in Matlab ® , as explained in the first example of the Technical Validation Section. Also, the code ( Example_Identification.m available in figshare 28 ) is provided, this code help to the user to implement an identification algorithm with the signals in the dataset."
10.1038/s41597-023-02064-8,The code for parsing YouTube and Reddit networks is available under an MIT license at https://github.com/ChillsTV/AffectiveStimuliScraper .
10.1038/s41597-023-02221-z,"The code for climate variability calculation, EC post-processing, random forest algorithm used for gap-filling can be obtained with the flux dataset 3 ."
10.1038/s41597-023-02239-3,PCA analysis: R prcomp in software (3.1.1). Differential analysis: R package DESeq 2 1.6.3. Veen: https://jvenn.toulouse.inrae.fr/app/example.html .
10.1038/s41597-023-02189-w,No custom code has been used during the generation and processing of this dataset.
10.1038/s41597-023-02235-7,No specific code or script was used in this work. Commands used for data processing were all executed according to the manuals and protocols of the corresponding software.
10.1038/s41597-023-02199-8,Our source code to generate the new versions of Ingrid KG is publicly available at ( https://github.com/dice-group/Ingrid ) and will be maintained in parallel with the knowledge graph. We provide our source code under the software license of GPL 3.0 ( https://www.gnu.org/licenses/gpl-3.0 ).
10.1038/s41597-023-02245-5,"The open-sourced code used for performing the light cone correction, contrast enhancement, color normalization transformations, as well technical validation can be accessed publicly through this online Gitlab repository 22 : ( https://git.geomar.de/open-source/AI-SCW )."
10.1038/s41597-023-02244-6,No custom code or scripts were used for the curation and validation of the dataset.
10.1038/s41597-023-02228-6,The WAVEWATCH III setup files are available in the data link ( https://data-dataref.ifremer.fr/ww3/GLOB1D_CMIP6/ ) 24 for the purpose of replicating the data described in this paper. The “wavesetup.env” file can be found in the following repository of the above link: GLOB1D_ exp_id/ 0r/year/. Please refer to Data Records section for more description of the repository.
10.1038/s41597-023-02240-w,The programs used to generate the datasets and all the results were ESRI ArcGIS (10.8.1) and Google Earth Engine (GEE). The scripts of data collection and pre-processing on GEE can be accessed on GitHub ( https://github.com/XinYijie/GlobalBuildingHeight ).
10.1038/s41597-023-02161-8,No custom code has been written for this dataset.
10.1038/s41597-023-02135-w,"Code for filtering data is available in the file Functions4ASE.R at https://github.com/ec-jrc/airsenseur-calibration . The whole QC/QC filtering is carried out using the function Filter_Sensor_Data() in the Function4ASE.R file. Filter_Sensor_Data() includes the flagging of data during warming using function Warm_Index(), the flagging of data outside temperature and relative humidity using function TRh_Index, the flagging of outliers using function Outliers_Sens() and the flagging of invalid data using function Inv_Index(). Code for calibrating AirSensEUR sensor box is available in file CompareModels.R at https://github.com/ec-jrc/airsenseur-calibration/tree/master/Auto_Calibration . Explanation is given to use this script in the AirSensEUR Guidance report 7 ."
10.1038/s41597-023-02246-4,The computing progress was completed in the ArcGIS 10.6 software. No custom code was used.
10.1038/s41597-023-02236-6,No custom code was used to generate this dataset.
10.1038/s41597-023-02253-5,"All R code supporting the conclusions of this study can be accessed and downloaded via Github 33 ( https://doi.org/10.5281/zenodo.7915783 ). The main computational tools used in this study are R language based. The scripts used in this study include “GBD_Incidence_region.R,” which calculates incident cases, deaths, ASR, and EAPC for BC worldwide during 1990 and 2019. Another script, “GBD_Incidence_map.R” is employed to generate visualizations of BC incidence and mortality using GBD data from 204 countries and regions around the world. Additionally, “GBD_cluster.R” was used to perform hierarchical cluster analysis to identify countries with similar annual increases in BC incidence and mortality. To calculate the percentage of major risk factors globally attributable to BC mortality, “GBD_Percent.R” was utilized. The correlation between EAPC and ASIR, ASDR, and HDI was analyzed using “GBD_COR.R”. Finally, “Global_BAPC_prediction.R” was implemented to predict the future burden of BC using GBD data."
10.1038/s41597-023-02222-y,"The scripts used to calculate the number of SAGs, the Bray-Curtis Dissimilarity Matrix, conduct the hierarchical cluster, and generate the Figs. 1b , 3 , 4 , Supplemental Figures S4 – S8 written under R version 4.1.3. These scripts utilize the following R packages: tidyverse, egg, vegan, dendexted, sf, rnaturalearth, and rnaturalearthdata will produce the tables and figures presented in this paper. Direct link to relevant software and specifications can be found online at the Hallam Lab Github repository https://github.com/hallamlab/OMZ_SAG_Compendium_Figures . Additional software used, including version numbers, adjustable variables and other parameters include the following: Trimmomatic 0.35 108 : -phred33 LEADING:0 TRAILING:5 SLIDINGWINDOW:4:15 MINLEN:36 ILLUMINACLIP:Trimmomatic-0.35 108 : /adapters/TruSeq. 3-PE.fa:2:3:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 SPAdes 3.0.0-3.10 74 : --careful--sc--phred-offset 33 ProDeGe v2.3.0 76 CheckM v1.2.1 75 : checkm lineage_wf --tab_table -x.fna --threads 8 --pplacer_threads 8 CheckM v1.2.1 75 : checkm qa -o 2 --tab_table GTDB-Tk v2.1.0 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 : gtdbtk classify_wf --genome_dir --out_dir -x.fna --cpus 8 Nucleotide-Nucleotide BLAST 2.9.0 + 59 : blastn -query -db -outfmt “6 qacc sacc stitle staxid pident bitscore” -max_target_seqs. 1 -num_threads 4 -out Nucleotide-Nucleotide BLAST 2.9.0 + 59 : blastn -query -db -outfmt “6 qacc stitle pident bitscore” -max_target_seqs. 1 -num_threads 4 -out Anvi’o v5 62 : anvi-gen-contigs-database -f -o -n Anvi’o v5 62 : anvi-run-hmms -c Anvi’o v5 62 : anvi-get-sequences-for-gene-calls -c -o Anvi’o v5 62 : $CENTRIFUGE_BASE/p + h + v/p + h + v gene-calls.fa -S centrifuge_hits.tsv Anvi’o v5 62 : anvi-import-taxonomy-for-genes -c -p BWA v 0.7.17-r1188 79 : bwa index BWA v 0.7.17-r1188 79 : bwa mem Samtools v 1.6-19-g1c03df6 (using htslib 1.6-55-gb065a60) 80 : samtools view -b F 4 Samtools v 1.6-19-g1c03df6 (using htslib 1.6-55-gb065a60) 80 : samtools index file.sorted.bam Anvi’o v5 62 : anvi-profile -i -c --min-contig-length 2000 --output.dir --cluster-contigs Anvi’o v5 62 : anvi-merge path_to_profile1/PROFILE.db path_to_profile2/PROFILE.db -o --skip-concoct-binning Anvi’o v5 62 : anvi-interactive -p Anvi’o v5 62 : anvi-summarize -c -p -C Anvi’o v7 62 : anvi-gen-contigs-database -f -o Anvi’o v7 62 : anvi-run-hmms -c --num-threads 8 Anvi’o v7 62 : anvi-get-sequences-for-hmm-hits barrnap 81 : barrnap --kingdom bac --threads {threads} --outseq {working_dir}/barrnap/*rRNA.fasta {input.fasta_dir}/$g.fasta > {working_dir}/barrnap/$g.rRNA.gff barrnap v0.9 82 : barrnap --kingdom arc --threads {threads} --outseq {working_dir}/barrnap/*rRNA.fasta {input.fasta_dir}/$g.fasta >{working_dir}/barrnap/$g.rRNA.gff tRNAscan-SE v 2.0.11 82 : tRNAscan-SE -B -o {working_dir}/trnascan/$g.output.txt -m {working_dir}/trnascan/$g.stats.txt -b {working_dir}/trnascan/$g.bed -j {working_dir}/trnascan/$g.gff -a {working_dir}/trnascan/$g.trna.fasta -l {working_dir}/trnascan/$g.log --thread {threads} {input.fasta_dir}/$g.fasta"
10.1038/s41597-023-02217-9,"All the software programs used in this article (de-novo transcriptome assembly, pre- and post-assembly steps and transcriptome annotation) are listed with the version in the Methods paragraph. In case of no details on parameters the programs were used with the default settings."
10.1038/s41597-023-02203-1,"The code used to prepare the CHQ-SocioEmo dataset is provided at https://github.com/Ashwag1/CHQ-SocioEmo- , and the source code for the benchmarked experiments can be found with the dataset."
10.1038/s41597-023-02205-z,"The repository includes three scripts (also see section ‘Data Records’). ‘STC_calculation.R’ contains all steps from loading the raw trade data to calculating the STC. For legal reasons, we cannot share the raw data underlying the script. Table B1 in the Supplementary Information indicates the statistical offices we contacted to obtain the labour surveys. Trade data stems from the United Nation’s Comtrade database for trade in goods 35 and the OECD-WTO’s BaTIS database for trade in services 36 . ‘Paper_replication.R’ includes the code to replicate all figures and analyses presented in this article. ‘Mock_calculation’ provides an illustration with fictious data to demonstrate each steps outlined in ‘STC_calculation.R’ with concrete data."
10.1038/s41597-023-02184-1,"The experimental data are processed using MATLAB R2019b software to generate the FSPMI dataset. The software code and supporting files that generate the dataset are packaged into “FSPMI generation code.zip” and deposited to the figshare 28 . The file labeled as “FSPMI_generation_main.m” is the main program to process the experimental data to further generate the dataset. The file labeled as “text2trans.m” is the function required to run the main program, implementing the interpolation of the various spectral responses to the same wavelengths. Spectral response files (.txt) for all devices named by their items are provided in the folder “Optical devices”, which is also called when running the main program. In addition, the toolboxes named “curvefit”, “eml” and “matlab” are necessary to run the main program."
10.1038/s41597-023-02225-9,"The source code of the platform and the STM32 devices are available under the GPL-2.0 license at https://github.com/servinagrero/SRAMPlatform . Online documentation on the platform and guidance on custom station set up can be found at https://servinagrero.github.io/SRAMPlatform . The full list of python dependencies is available in the pyproject.toml file in GitHub repository. PostgreSQL ( https://www.postgresql.org/ ) is needed to store data, a message broker is necessary to communicate with the station, RabbitMQ ( https://www.rabbitmq.com/ )in our case, and Grafana is used to monitory metrics and sensors in a dashboard."
10.1038/s41597-023-02103-4,Scripts used to collate the ASE database and perform structural analyses are located alongside the dataset. All scripts are written in Python (v3.9) and are described in the accompanying README.md.
10.1038/s41597-023-02178-z,All calculations were done in ESRI ArcGIS 10.5 and no other computer code was used.
10.1038/s41597-023-02255-3,"All calculations of a global long term (1981–2021), high resolution (4 km) improved vegetation health index (VHI) dataset are based on MATLAB, and relevant step codes can be obtained from Github: https://github.com/BNUJingyuZeng/A-new-global-VHI-dataset-code.git ."
10.1038/s41597-023-02252-6,"The scrips used to generate regular vine matrices in P ython are included in the 4TU data repository under the P ython data collection 37 (see the Methods section). The files containing regular vine matrices on up to 8 nodes for P ython are compressed files in pbz2 format. In order to use these files, these need to be decompressed. For future users of the dataset a specific script get_matrices.py is available together with the files in the repository 37 . This script provides an example, contains subroutines and the P ython tree-equivalent class definition for each one of the matrices of interest. Roughly, what the get_matrices.py script will do is get the matrices from files in a user specified directory for the specified number of nodes. By default, an array is returned with all matrices as a P ython class, containing the tree-equivalent class (tree sequence type), index and matrix. For convenience, a user can also specify parts of the dataset based on the tree-equivalent class, which relates to the files names of the dataset."
10.1038/s41597-023-02204-0,"1. Real Time Analysis software: https://www.illumina.com/search.html?filter=support&q=RTA%20download&p=1 2. Bcl2fastq Conversion: https://support.illumina.com/downloads/bcl2fastq-conversion-software-v2-20.html 3. Cutadapt, https://github.com/marcelm/cutadapt/releases/tag/v1.18 4. Fastx_clean software, http://www.genoscope.cns.fr/fastxtend 5. FASTX-Toolkit, http://hannonlab.cshl.edu/fastx_toolkit/index.html 6. SortMeRNA v2.1, https://github.com/biocore/sortmerna 7. fastx_estimate_duplicate software, http://www.genoscope.cns.fr/fastxtend 8. fastx_mergepairs software, http://www.genoscope.cns.fr/fastxtend 9. Usearch, https://www.drive5.com/usearch/"
10.1038/s41597-023-02197-w,"The Modelica Buildings Library and EnergyPlus are freely available for download 39 , 40 . EnergyPlus runs on Windows, Mac OSX, and Linux operating systems. A Windows or Linux-based computer and Dymola solver are required to run Modelica, and Dymola can be licensed from Modelica Buildings Library. HVACSIM + is also freely available, upon request from NIST, and has no operating system requirements. The Modelon air conditioning library that was used to model the faults in the RTU refrigerant side, was accessed from Modelon’s library suite 34 . The Modelica-based library is used to design, analyze and optimize air conditioning systems. A custom Python-based script was developed to create Brick model .ttl files for each system in the dataset, following the process described in Method of Brick schema model development . The .ttl files are included in the data repository."
10.1038/s41597-022-01757-w,"The different codes used to process the different datasets are indicated within the text and are repeated here and includes: -Inline optical processing ( https://github.com/OceanOptics/InLineAnalysis ) -Satellite products used 38 , 40 , 41 , 42 , 43 , 44 , 45 -Mercator products 46 , 47 , 48 , 57 used. -Astronomical almanac to calculate sun/moon position and day-nights parameters from sites positions and time 49 , 50 . -Additional parameters of the carbonate system were calculated with CO2SYS.m v3.1.1 33 using in situ temperature, total alkalinity, total dissolved inorganic carbon, salinity, phosphate and silicate concentrations as inputs together with recommended parameters 34 , 35 , 36 , 37 (K1K2 = 10; KSO4 = 3; KF = 2; BOR = 2). -Ecotaxa 64 server github ( https://github.com/ecotaxa/ecotaxa ). -EcoTaxa data processing ( https://github.com/ecotaxa/ecotaxatoolbox ) -Morphological qualitative annotations 65 ."
10.1038/s41597-023-02198-9,"The source code for data selection and curation, data linkage, and metrics calculation is available at https://github.com/kellogg-cssi/SciSciNet ."
10.1038/s41597-023-02070-w,Convenience functions for navigating the MOSAIC database are included in the supplemental material (S5) and on the MOSAIC website https://mosaicdatabase.web.ox.ac.uk and associated GitHub repository https://github.com/mosaicdatabase/Rmosaic . All code is open access without restrictions on access.
10.1038/s41597-023-02247-3,"No specific custom codes were developed in this study. All commands and pipelines used for data analyses were conducted according to the manuals or protocols provided by the corresponding software development team, which are described in detail in the Methods section. Default parameters were employed if no detailed parameters were mentioned for the software used in this study. Supplementary Table 1 lists the versions, settings, and parameters of the relevant software used in this study."
10.1038/s41597-023-02251-7,"All software with respective versions and parameters used for producing the resources here presented (i.e., transcriptome assembly, pre and post-assembly processing stages, and transcriptome annotation) are listed in the methods section. Software programs with no parameters associated were used with the default settings."
10.1038/s41597-023-02250-8,No custom code was generated.
10.1038/s41597-023-02254-4,MISSING
10.1038/s41597-023-02233-9,"The following software and versions were used for quality control and data analysis: 1. FastQC, version 0.11.8 and MultiQC, version 1.7 were used for quality analysis of raw FASTQ sequencing data: http://www.bioinformatics.babraham . ac.uk/projects/fastqc/ 2. HISAT2 was used for mapping of sequence reads to the human GRCh38.13 genome assembly: http://www.ccb.jhu.edu/software/hisat/index.shtml 3. HTSeq, version 0.9.1 was used for calculating the gene counts: http://bioinf.wehi.edu.au/featureCounts/ 4. edgeR, version 3.34.1 was used for normalization and visualization of differential gene expression analysis output: https://bioconductor.org/packages/release/bioc/html/edgeR.html Software and codes are open source and readily available."
10.1038/s41597-023-02180-5,"No custom code was used in the generation of this dataset. The data were analyzed using the Julia programming language and plotted using matplotlib 27 , 28 ."
10.1038/s41597-023-02261-5,"The shared dataset is prepared based on the default setting of the UrbanPatch container and D-radius. If users want to customizable this dataset with different settings, they can use the shared UrbanPatch generation package ( https://github.com/ruirzma/UPTO ). There are four files included in the package: • “ConPatchForTile.py”: construct UrbanPatch individuals for UrbanTile objects when changing the receptive radius. • “ConPatchForBuilding.py”: construct UrbanPatch individuals for Building objects for a given receptive radius. • “GenMicroclimate.py”: generate the UrbanTile-scale microclimate. • “GenIDF.py”: generate UrbanTile-scale EnergyPlus IDF file."
10.1038/s41597-023-02229-5,The code used for generating preprocessed images and lung masks from the original CXR images is available on GitHub ( https://github.com/ZAEDPolSl/PolCovid ).
10.1038/s41597-023-02248-2,"We implemented an Android smartphone data collection application and used it to collect the K-EmoPhone dataset, which is available at https://github.com/Kaist-ICLab/K-EmoPhone_Logger . This application is intended to be run on smartphones with an Android API level of 21 or above. However, smartphones with an API level of 26 or above may not demonstrate the intended behavior owing to new privacy policies and deprecated data classes. In addition, our data exploration and machine-learning processes were written in a Jupyter notebook, which is available at https://github.com/Kaist-ICLab/K-EmoPhone_SupplementaryCodes ."
10.1038/s41597-023-02243-7,The file converter from original .mat files to formatted .csv files is available on figshare 26 . The code for the computation of the PIs is available on https://github.com/bmislab/DECODED .
10.1038/s41597-023-02262-4,"Short scripts used for extracting useful information from the VASP output files, such as the XAS and energies, are provided with the database. The workflow is available on GitHub ( https://github.com/atomisticnet/xas-tools/releases/tag/v0.1.0 )."
10.1038/s41597-023-02269-x,"The code supporting the users with data retrieval and submission is freely available at https://github.com/pangaea-data-publisher . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used."
10.1038/s41597-023-02265-1,The spectra data were processed and analyzed using Matlab 2016a. All used scripts to implement are available at Github: https://github.com/hanyyuan/groundObjectSpectrum.git .
10.1038/s41597-023-02264-2,No custom code was used.
10.1038/s41597-023-02155-6,"No customized code was produced to prepare or analyze the dataset because software that works in a GIS environment is required to open and process the LaICa dataset. Specifically, we used and suggest the open-source QGIS software available in different releases at https://qgis.org/it/site/ . However, further specific software (open-source or licensed) is required to open and process CSV files associated with the LaICa database."
10.1038/s41597-023-02242-8,DataLad and GIN are freely available. The manuscript contains all code to reproduce the workflow.
10.1038/s41597-023-02092-4,"A snapshot version of our custom analysis tools at the time of initial data preparation is included in the “ supporting_analyses ” directories within each experiment type. These include μCT (Python), DIC (Matlab), and DMA (Python and Excel) analyses, with documentation. Settings used for μCT analysis are summarized in Table 2 and settings for DIC analysis are in Table 3 . All first-party data and code referenced here are available free and open source per the included NIST license file; however, certain commercial formats and analysis software are also used. Example code for access, visualization, and filtering of data are provided free and open source (MIT License) as a Git repository, as described in the Data interfaces section."
10.1038/s41597-023-02215-x,A code example of IRIS is available at Zenodo 35 . The methodology is described in detail in our regional study 2 .
10.1038/s41597-023-02278-w,No specific code was developed for this work. The data analyses were performed according to the manuals and protocols provided by the developers of the corresponding bioinformatics tools in the methods.
10.1038/s41597-023-02209-9,The R script allowing to reproduce the entire study is available on Github ( https://github.com/MarceauQuatredeniers/Meta-analysis-of-healthy-human-kidney-single-cell-transcriptomics ).
10.1038/s41597-023-02263-3,"A set of codes for reading, pre-processing of sEMG, splitting of sEMG into windows of various sizes, extracting of sEMG features (including 20 kinds of features 33 that can be combined into Du’s feature set 31 , Hudgins’s feature set 34 , and novel time-domain feature set 35 , 36 ), normalization of extracted features, generation of sample data, and making log files are provided for easy handling of the data. In addition, some programs in the technical validation section were also included, which can be found from on GITHUB via the following URL: https://github.com/WH-Wei/SIAT-Lower-Limb-Motion-Dataset-Codes.git ."
10.1038/s41597-023-02058-6,"Sub-daily data generation code The technical details of the statistical model used to generate the 6-hourly H s from SLP predictors are included in the corresponding reference paper 16 which allow for the reproducibility of the presented dataset. Additionally, the corresponding Fortran and R codes are publicly available in the Government of Canada Open Data Portal, together with the d4PDF-WaveHs dataset (DOI https://doi.org/10.18164/d68361d0-8141-48b9-a25e-a9bc98d71438 ). Computation of statistics/indices: getStat.f , getHsEx.f To be consistent with COWCLIP2.0 8 , the H s statistics and indices were computed with getStat.f and getHsEx.f , after a slight modification to account for missing data (see Methods Section). The original Fortran code 25 was developed as part of the COWCLIP community framework and can accessed via the COWCLIP website ( https://cowclip.org/data-access ). The code can be compiled with a Fortran compiler, with netCDF4 and HDF5 libraries. Additional attribute information to account for CF conventions and ACCD standards, was added to this Fortran code output with standard NetCDF operators (NCOs) for file manipulation, such as the “ncatted” command."
10.1038/s41597-023-02259-z,The algorithms used for processing and segmenting the raw grayscale images are available as Python code at: https://github.com/IBM/microCT-Dataset . The code repository contains Jupyter Notebooks for simplifying data processing and visualization along with usage guidance.
10.1038/s41597-023-02283-z,The code of CN-P is archived at the Zenodo repository: https://doi.org/10.5281/zenodo.7460564 .
10.1038/s41597-023-02276-y,"The source code used to clean, unify, aggregate, and merge the different data components from all sources will be available on GitHub at https://github.com/CSSEGISandData/COVID-19_Unified-Dataset ."
10.1038/s41597-023-02285-x,The lake water extraction for this study was performed on the GEE platform. The GEE JavaScript code can be downloaded at https://github.com/GISLandsat/water-research1.git . GEE should be used to access and edit the code.
10.1038/s41597-023-02208-w,"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub ( https://github.com/oacore/ ). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode."
10.1038/s41597-023-02279-9,"No custom code was generated or applied for analysis of the genomic data presented. All software tools were referenced and used with default settings unless otherwise noted. Non-default parameters were as follows: FastQC (v0.11.9), multiqc (1.14): Quality check of mRNA-seq and sRNA-seq reads. Cutadapt (1.9.1): Adapter trimming of sRNA-seq reads (first pass parameters: -a AGATCGGAAGAGCACACGTCT -n 1 -e 0.2 -O 5 -m 1 --match-read-wildcards; second pass parameters: -g GTTCAGAGTTCTACAGTCCGACGATC -n 1 -e 0.125 -O 8 -m 1 --match-read-wildcards). After adapter trimming, reads were size-selected (using standard linux command line tools) to keep 18–25 nt reads. sRNAnalyzer pipeline: Alignment of sRNA reads (kit: NEB; min-length: 8; alignment type: multiple) to the A. thaliana sRNA locus database 19 . Salmon (1.7.0): Quantification of mRNA-seq reads has been performed with fragment GC bias correction (--gcBias). Duplicate reads were filtered out (default behaviour) for protein coding and noncoding genes. Duplicate reads were kept for transposable elements (--keepDuplicates). DESeq2 (1.24.0): Adjusted p-value < 0.05 for differential gene expression analysis, adjusted p-value < 0.05 for differential transposable element expression analysis, adjusted p-value < 0.05 for differential sRNA expression analysis. HISAT2 (2.1.0): mRNA-seq data alignment (reference: TAIR10) for visualization. bowtie2 (2.3.5): sRNA-seq data alignment (reference: TAIR10) for visualization. deepTools (3.3.0) bamCoverage: RPKM normalized coverage files of mRNA-seq (--binSize 1 --normalizeUsing RPKM) and sRNA-seq (-bs 15–smoothLength 45 --normalizeUsing RPKM) data for visualization in JBrowse."
10.1038/s41597-023-02284-y,Python code for producing data for 27 EU countries and the United Kingdom in the dataset is provided at https://github.com/kepiyu/Carbon-Monitor-Europe/blob/main/CM_EU_v2.py .
10.1038/s41597-023-02249-1,MISSING
10.1038/s41597-023-02241-9,MISSING
10.1038/s41597-023-02271-3,The real data used as the input of CTGAN is unavailable due to regulations around consumers’ privacy 18 . Others wishing to repeat the work or perform studies with the raw data should approach Watts A/S 13 . The code for data validation and analysis is available in the public repository of Figshare 51 .
10.1038/s41597-023-02282-0,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at https://doi.org/10.5281/zenodo.7556953 63 . All necessary input data are archived at https://doi.org/10.5281/zenodo.6783023 62 . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository 66 , https://doi.org/10.4121/collection:HANZE ."
10.1038/s41597-023-02267-z,Open-source software packages were used to process data and generate data products. Software versions and non-default parameters are specified where required.
10.1038/s41597-023-02257-1,The software used for quality control and data processing of GepLiver are as follows. 1. FastQC version 0.11.9 2. Trimmomatic version 0.33 3. STAR version 2.5.3a 4. StringTie version 1.2.3 5. FeatureCounts version 1.6.3 6. CIRI2 version 2.0.6 7. CellRanger version 6.0.0 8. R version 4.1.2 9. Seurat version 4.1.0 10. UCell version 1.3.1 11. inferCNV version 1.10.1 12. Monocle version 2.22.0 13. CytoTRACE version 0.3.3 14. CellPhoneDB version 3.1.0 15. STEM (Short Time-series Expression Miner) version 1.3.13 Custom code used for data processing and technical validation was provided in File “Custom R Scripts” deposited at figshare 14 .
10.1038/s41597-023-02288-8,No custom code was used in the collection and validation of this dataset.
10.1038/s41597-023-02286-w,The code to preprocess the raw bimodal dataset as well as the two stimulation protocols (one for each modality) are publicly available at: https://github.com/LTU-Machine-Learning/Inner_Speech_EEG_FMRI .
10.1038/s41597-023-02268-y,"The custom code that we used for taxonomic identification is available as Supplementary File 1 . Additionally, an example code for reading our database in R is provided at 39 ."
10.1038/s41597-023-02289-7,The R code used to perform differential expression analysis is available in FigShare File 3 22 .
10.1038/s41597-023-02272-2,"The codes include preprocessing of physiological signals, annotation synchronization, facial expression detection, and technical validation available at the Repository for the AKTIVES Dataset 2022 GitHub repository https://github.com/hiddenslate/aktives-dataset-2022 . The Python 3.9 version has been utilized for the development of algorithms. In the requirements.txt file, all necessary packages are mentioned. Uploaded codes can be helpful guidelines to preprocess and analyze the AKTIVES dataset."
10.1038/s41597-023-02237-5,"The code used for Technical Validation is public code and can be found in the original paper. The “Dataset and code 14 ” dataset is available in our uploaded dataset folder, and the remaining two cases of the original adult dental dataset are available for download on Archive 16 Panoramic radiography Database 15 . In addition, the annotation software referenced to produce the dataset is open source on EISeg 1 and LabelMe 2 , respectively."
10.1038/s41597-023-02291-z,"All bioinformatic tools used in this study were executed according to the corresponding manual and protocols. The version and code and parameters of the main bioinformatic tools are described below. (1) Trimmomatic v0.38, parameters used: “PE -phred33 ILLUMINACLIP:TruSeq. 3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:30 MINLEN:40”. (2) jellyfish v2.2.0, parameters used: “-C -m 21”. (3) GenomeScope v.2.0, parameters used: ploidy 2 and kmer_length 21. (4) nextDenovo v2.5.0, parameters used: default. (5) Purge Haplotigs v1.1.2, parameters used: default. (5) MetaBAT v 2.12.1, parameters used: default. (6) BLASTn v2.11.0+, parameters used: “-evalue 1e-20 -max_target_seqs. 1”. (8) BUSCO v5.4.5, parameters used: lineage_dataset eukaryota_odb10 (255 BUSCOs) and metazoa_odb10 (954 BUSCOs). (9) Norgal v1.0, parameters used: default. (10) MAKER v3.0, parameters used: default. (11) RepeatMasker v4.1.2-p1, parameters used: “-e rmblast -s -gff”, Database: Dfam v3.1 and RepBaseRepeatMaskerEdition-20181026. (12) RepeatModeler v 2.0.3, parameters used: “-LTRStruct”. (13) Trinity v2.5.1, parameters used: default. (14) Augustus, version 3.4.0, parameters used: species = Database trained with BUSCO. (15) SNAP v2006-07-28, parameters used: default. (16) EVidenceModeler v1.1.1, parameters used: default settings in Maker3. (17) PASA v2.4.1, parameters used: “-C -R -T–ALIGNERS blat”. Augustus, version 3.4.0, parameters used: species = Database trained with BUSCO, alternatives-from-evidence = true, hintsfile = Output of RepeatMasker. (18) Diamond v2.0.13.151 BLASTp, parameters used: “-ultra-sensitive -max-target-seqs. 1 -evalue 1e-5”. (19) HISAT2 v2.1.0, parameters used: default. (20) StringTie v1.3.4d, parameters used: default. (21) FEELnc v0.2.1, parameters used: default. (22) fastp v0.20.0, parameters used: “length_required = 18, max_length = 35, unqualified_percent_limit = 30, n_base_limit = 0”. (23) miRDeep2 v2.0.1.2, parameters used: default. (24) miRanda v3.3a, parameters used: “-sc 140 -en -5 -strict”. (25) OrthoFinder v2.5.4, parameters used: “-S diamond_ultra_sens”. (26) IQ-TREE v2.1.3, parameters used: “-m TEST -bb 1000”. (27) MCMCtree implemented in PAML v4.9 h, parameters used: Tree topology from IQ-TREE result, fossil records in Fig. 4 , burn-in: 10000000, sample frequency: 1000, and number of samples: 10000. (28) CAFÉ v4.2, parameters used: default. (29) QUAST v5.2, parameters used: default. (30) bbmap v39.01, parameters used: bbsplit.sh and mapPacBio.sh with default settings. (31) SAMtools v1.15.1, parameters used: command = coverage, depth, with default settings."
10.1038/s41597-023-02227-7,"The code used for calculating agricultural, forestry residues, and energy crops is written in Python and available from https://github.com/Rui-W-A/biomass-resource-China ."
10.1038/s41597-023-02287-9,"The Psychopy files to compile the experiment are stored on the Github repository https://github.com/hWils/Semantics-EEG-Perception-and-Imagination . Also on this repository are the Python processing and technical validation scripts. Users can directly use the Python code provided 1) to compute preprocessing as described in this paper, and 2) to reproduce the experimental results presented in the technical validation section."
10.1038/s41597-023-02260-6,"Custom codes to reproduce the results of data quality check are available from the GitHub repository ( https://github.com/Junichi-Ushiba-Laboratory/pj-hd-smrbmi ). To load the edf file format, EEGLAB package and corresponding add-on packages are required 53 . Detailed information can be found in “readme.md” at the repository."
10.1038/s41597-023-02292-y,No custom code was made during the collection and validation of this dataset.
10.1038/s41597-023-02277-x,The MIAGIS Python package is available on GitHub and the Python Package Index with comprehensive user documentation on GitHub.io: https://github.com/MoseleyBioinformaticsLab/miagis https://pypi.org/project/miagis/ https://moseleybioinformaticslab.github.io/miagis/
10.1038/s41597-023-02293-x,All analytical code used for processing and technical validation is available on the GitHub Repository ( https://github.com/christopherjin/SGBS_3T3-L1_differentiation_scRNASeq ). The provided R code was run and tested using R 4.2.2.
10.1038/s41597-023-02281-1,The miagis Python package is available on GitHub and the Python Package Index with comprehensive user documentation on GitHub.io: https://github.com/MoseleyBioinformaticsLab/miagis https://pypi.org/project/miagis/ https://moseleybioinformaticslab.github.io/miagis/
10.1038/s41597-023-02299-5,All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in Methods.
10.1038/s41597-023-02275-z,No custom code was developed for this work and data set.
10.1038/s41597-023-02304-x,"Shape characterisation of particles and calculation of shape descriptor parameters including surface area, volume, elongation, flatness, sphericity, and convexity are performed utilising the SHAPE code by Angelidakis et al . 1 publicly available from the link below: https://github.com/vsangelidakis/SHAPE . Image analysis is performed using the open-source software Fiji-ImageJ (1.53c) 7 and MATLAB (R2021a)."
10.1038/s41597-023-02295-9,"Most steps were completed base on the public-domain software, except for the calculation of differential genes. All analytical code of DEGs is available on the GitHub repository ( https://github.com/tangaode/Plasma-exosomes ). The provided R code was run and tested using R 4.1.0. The name and the links of all database depositories was showed in Table 4 ."
10.1038/s41597-023-02307-8,Software and their versions used for RNA-seq analysis were described in Methods. No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-023-02266-0,The code used for technical validation is available at https://github.com/iiscleap/Coswara-Data/tree/master/technical_validation .
10.1038/s41597-023-02273-1,"Due to the format of most of these datasets being Excel files, there is no code published to read these datasets. The SMP dataset 40 has a custom code to read the pnt files. This can be accessed at https://github.com/slf-dot-ch/snowmicropyn.git . The NIR MAPiR software can be used for post-processing and calibration of NIRbox images. This can be accessed at https://www.mapir.camera/collections/software ."
10.1038/s41597-023-02280-2,"The data analysis methods, software and associated parameters used in this study are described in the section of Methods. All the scripts associated with various steps of data curation are available at the GitHub repository: https://github.com/BioinfoMachineLearning/cryoppp , which includes the instructions about how to download the data."
10.1038/s41597-023-02238-4,The Python code for indices calculation based on gauge records and subsequent gridding is available in the code repository 39 here: https://doi.org/10.5281/zenodo.7492877 .
10.1038/s41597-023-02297-7,"The plots made in this Data Descriptor were generated using IGOR PRO v8 (Wavemetrics, Lake Oswego, Oregon)."
10.1038/s41597-023-02294-w,The MARS3D-MUSTANG model chain used to provide the hindcast is an open-access software: https://mars3d.ifremer.fr . The source codes and parameter files are available on the CurviLoire Hindcast repository 51 .
10.1038/s41597-023-02311-y,The Interactive Data Language (IDL) code and Python and shell scripts used to produce the processed DN buoy drift tracks are archived and available for download at the Arctic Data Center 24 .
10.1038/s41597-023-02301-0,"Annotated macros used to analyze the screen are available on GitHub: https://github.com/Mathieu-Maurin/screening_B_cell_polarization . The files are organized as such: - README.md: This file contains brief instructions on how to use the macros. - Macro_1_cell_bead_couple_extraction: The purpose of this macro is to extract bead/cell couples for analysis. It works on composite images reconstructed with the following channels: Channel 1: gamma-Tubulin; Channel 2: Dapi; Channel 3: Lamp1; Channel 4: Fluorescent beads. - Macro_2_polarity_analysis: The purpose of this macro is to determine centrosome and lysosome raw polarity indexes for each bead/cell couple extracted by Macro_1. - Macro_3_normalise_and_concatenate_tables. The purpose of this macro is to normalize data and concatenate tables. It works after Macro 2. Output tables are in the format PlateName-Well_A01_Results. A01 indicates the well coordinate in the plate. It contains all the polarity results from all positions of a given well. The macro concatenates and normalizes data to get a single table for centrosome-polarity or lysosomes-polarity. It computes also median values for each well. Normalization is based on the median value of well B01 of each plate (non-targeting siRNA), which is normalized to 1. - Macro_4_count_cell_beads_and_couple. The purpose of this macro is to control bead, cell, and couple counts. It works after Macro 3 and adds a result table in the final table folder."
10.1038/s41597-023-02303-y,"R code for generating the plots in this paper, for reading in all files and extracting desired information, and for the forward modeling is available from https://github.com/sbroennimann/DOCU-CLIM ."
10.1038/s41597-023-02308-7,"The MATLAB codes to determine these thermohaline parameters from WOD18 (T, S) were published in the two related papers 28 , 29 , and can be obtained at https://doi.org/10.1007/s10872-017-0418-0 . The main program is Thermohaline.m, along with four Matlab functions: validate.m, getgradient.m, ELGCore.m, and Iindex.m (see the Technical Validation Section). The function validate.m is employed to identify if ( T , ρ ) profiles having double gradient structure. The function ‘getgradient.m’ is used to calculate the vertical gradient. The function ELGCore.m is used to get ( h T , G T ) or ( h D , G D ) from T-profile or ρ- profile. The function Iindex.m is used to calculate the i -index ( I ITL ) for the error estimation. Then, the code ‘Thermohaline.m’ generates other thermohaline parameters such as ITL heat content, mixed layer fresh-water content, maximum thermocline gradient, thermocline depth, temperature at thermocline depth, maximum density gradient, pycnocline depth, density at pycnocline depth, barrier layer depth, and compensated layer depth. Since the calculation is local for individual (T, S) profile pair, interested readers may use our MATLAB codes to analyse any (T, S) profiles to get the derived thermohaline data with quality identification (i.e., i -index)."
10.1038/s41597-023-02309-6,"For reproducing the Spilhaus maps in this paper, we packaged an ArcGIS template for topographic maps (Figs. 1a , 2 ), and other maps can be easily generated by importing the grid data. The code of the quasi-Spilhaus square projection, implemented with d3-geo, is stored in the Figshare repository 26 or can be found at https://observablehq.com/d/bb73e74c1685e498 ."
10.1038/s41597-023-02270-4,"The scripts and command lines were uploaded on the github ( https://github.com/fcbayern31/A-pipeline-for-common-genomic-analysis.git ). All softwares, which are in the public domain, were used in accordance with the official instructions. Anything not specified in the method is executed with default parameters."
10.1038/s41597-023-02310-z,"A Jupyter notebook of the EAP model code is available on Github: https://github.com/lisllain/EAP-model . It is shared under a GNU General Public License. Appropriate acknowledgement should be made when the model is used in publications. Matlab code for writing IOP input files for the Hydrolight 4-component user-defined IOP model is also available, as are discretised EAP phase function 2 files for Hydrolight. These will be added to the Github in due course but are available on request in the interim."
10.1038/s41597-023-02314-9,"The calculation of plot-level forest AGB and SOC from the forest inventory data used R codes, which are accessible in Zenodo at https://doi.org/10.5281/zenodo.7809576 25 ."
10.1038/s41597-023-02313-w,"Together with the dataset, we also provide the “data_validation” Matlab script used for technical validation of the data. This script is accompanied by helpful comments and can be used as example for how to access and handle the data (e.g., how to access the data structure, how to loop through participants, scenarios and trials, how to access certain trial info, how to find the instants of touch via IMU or via the platform)."
10.1038/s41597-023-02256-2,No code was written or used for this paper.
10.1038/s41597-023-02321-w,Custom code and data used in the NLP-based prioritisation of the gene set annotations is available in the data repository 39 on Zenodo at https://doi.org/10.5281/zenodo.7980953 (file aop_mapping_nlp.tar.gz).
10.1038/s41597-023-02315-8,Data acquisition was performed with the TIVITA® Suite (version 1.6.0.1 ) . The polygon annotations were created with a software developed in-house which is available on GitHub: https://github.com/MIC-Surgery-Heidelberg . All data analyses and visualizations in this manuscript were performed using Python and the corresponding code is available at https://github.com/IMSY-DKFZ/htc .
10.1038/s41597-023-02306-9,No custom code was used.
10.1038/s41597-023-02319-4,"No custom code was used to generate or process the data described in the manuscript—however, we used the “swindex” package in Stata [Stata/SE 17.0 for Mac (Intel 64-bit)] software to execute the steps detailed in the Methods section."
10.1038/s41597-023-02312-x,The MATLAB scripts used to extract and plots data from the ROS bag (.bag) files are provided on the GitHub page of the Research Center E. Piaggio 35 and are also archived on Zenodo 34 . A description of each script is provided in the README file of the GitHub repository.
10.1038/s41597-023-02290-0,"The Python scripts used to calculate ET P , ET 0 , ET 0 components, and VPD can be found within the repository alongside the data described herein. Accompanying the code is a small subset of GCM data that can be used to test run the script. Python scripts utilize a small subset of libraries in the Python3 base and the xarray (v2022.11.0) library to handle calculations of the gridded datasets. The python code used to generate the dataset 18 described above can be found in the GitHub repository using the following link: https://github.com/nelsbjarke/PET ."
10.1038/s41597-023-02325-6,"All codes for the experimental design, data organization, and technique validation are available at https://github.com/BNUCNL/HAD-fmri . Preprocessing was performed using fMRIPrep version 20.2.1 ( https://fmriprep.org ). Grayordinate-based (CIFTI format) brain activation analysis was performed by combining the Ciftify ( https://github.com/edickie/ciftify ) and HCP pipelines ( https://github.com/Washington-University/HCPpipelines )."
10.1038/s41597-023-02230-y,The source code for the Preston software 32 is available in GitHub at https://github.com/bio-guoda/preston and in Zenodo at https://doi.org/10.5281/zenodo.1410543 . Preston version 4.4 has been assigned the https://doi.org/10.5281/zenodo.7005141 . The MD5 content signature for the zip archive of the source code is hash://md5/5339621a0b95bfbbf450063d76057d2f .
10.1038/s41597-023-02322-9,"DsLMF+ datasets are publicly available at the figshare data repository 33 , and the code for automatically filtering is also published alongside the dataset, archived as “DsLMF.7z”. Furthermore, the annotation tool Labelimg can be accessed and downloaded through the official website link https://github.com/heartexlabs/labelImg , the specific usage can refer to the corresponding README file. The codes used for training and validation of the DsLMF+ datasets in this work adopts DETA, ViT-Adapter-L and YOLOv7 official published open source scripts, and the code of the above three deep learning network for dataset verification can be accessed via the following website link ( https://github.com/jozhang97/deta ), ( https://github.com/czczup/vit-adapter ), and ( https://github.com/wongkinyiu/yolov7 ). Table 9 presents the required site-packages and their corresponding versions for the above three different networks. The software packages can be downloaded according to README files under the corresponding links on different networks, and can be installed with the python package installer (pip). Researchers can complete the label format conversion from YOLO format to COCO format, by visiting the following link ( https://github.com/RapidAI/YOLO2COCO ), the link provides the label format conversion code and the README file that can be used as a reference."
10.1038/s41597-023-02234-8,"MATLAB scripts that implement normalization of incidence angle effect, vegetation removal, soil moisture inversion, and mapping are available at ( https://figshare.com/articles/online_resource/Example_of_sm_inversion_zip/21376089 ). Further questions can be directed towards Chunmei Wang (wangcm@ aircas.ac.cn)"
10.1038/s41597-023-02331-8,No custom code was used.
10.1038/s41597-023-02274-0,The GAMIT/GLOBK software we used to process the GNSS data is available at http://geoweb.mit.edu/gg/ . The scripts we used to do the SNR analysis are available at https://github.com/ericlindsey/gnss-snr-skyplot .
10.1038/s41597-023-02324-7,The code used to generate the data descriptor is a minor modification of the function available in the github link ( https://github.com/feog/DashVolcano ).
10.1038/s41597-023-02336-3,The Authors declare that no custom code was used.
10.1038/s41597-023-02323-8,Code can be found in a public repository at: https://doi.org/10.5281/zenodo.7659599 34 . This includes code to replicate the figures shown here as well as code to merge spatial data from GADM level 1 with the spatial data provided for eight exceptional countries as presented in Table 4 to produce a comprehensive shapefile with geometry for each region in DOSE.
10.1038/s41597-023-02318-5,All processing developed within this study is wrapped in a python environment and is available at https://gitlab.com/mosaic12/orthomosaics . For calculating image footprint locations we used the CameraTransform python package 37 . Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.
10.1038/s41597-023-02328-3,"The https://github.com/ferranlarroyaub/Beepath-Schools repository holds the Python code and scripts to process the input data 40 and to replicate the statistical analysis and the figures. The 3.8 Python version is used to build the code with the main libraries: networkx and osmnx to plot the trajectories on OpenStreet maps. Pandas and numpy to process, clean, and analyze the data in Data-Frame format and perform the basic statistic calculations. Scipy for more advanced calculations such as fitting models to the empirical data and matplotlib for plotting purposes. The Python code is built in different Jupyter notebook files which contain a detailed description of the study and the code documentation."
10.1038/s41597-023-02332-7,All the data processing and data visualization were conducted using R (version 4.2.2) 38 . The source code is available on GitHub ( https://github.com/Chagcarlos/NaneSoil_Figs ). The Code contains detailed comments for the development of Fig. 1 related to the texture triangle. The map presented in Fig. 2 contains the soil sampling locations and was made using QGIS software version 3.16 ( https://qgis.org/en/site/forusers/download.html ) under GNU General Public License (CC BY-SA 3.0).
10.1038/s41597-023-02335-4,"All the code used to generate the processed datasets, as well as the resulting R package are available openly on GitHub ( https://github.com/Syksy/curatedPCaData ). The DOI-linked copy of the package’s GitHub repository is available via Zenodo 93 ."
10.1038/s41597-023-02114-1,No custom code was used to administer the experimental paradigm or collect data. The data stored in the repository are raw data and has not been processed with custom code. Some sample custom scripts to load and visualize data are saved with the data in the Open Science Framework repository ( https://doi.org/10.17605/OSF.IO/9S3U6 ).
10.1038/s41597-023-02338-1,"No custom code was used or generated, otherwise the details on the open-access platforms used are listed in Table 1 ."
10.1038/s41597-023-02348-z,"All custom R and python scripts for quality control, data integration, figures and analysis are available on our GitHub repository ( https://github.com/erbon7/sc_pbmc )."
10.1038/s41597-023-02351-4,"There was no code used in the generation of the data in this work, an only Microsoft Excel is employed to process all the data."
10.1038/s41597-023-02350-5,"We followed the developers’ instructions for the bioinformatics tools used in this study. The software and code used are publicly accessible, with the version and parameters used specified in the Methods section. No custom code was used during the compilation of the dataset."
10.1038/s41597-023-02305-w,"The WorldPop-RF code, used to produce these high-resolution gridded population datasets, is publicly and freely available via: https://github.com/wpgp/popRF ."
10.1038/s41597-023-02334-5,"Code for downloading Landsat images, processing samples, training models, and producing maps will be available from the Figshare repository 29 ."
10.1038/s41597-023-02329-2,The code used for analysis in this study is publicly available at https://github.com/LiqiaoHuang/Household-carbon-footprint-quantification.git .
10.1038/s41597-023-02353-2,"The European Commission is the owner of the ETER source code under contract no. EAC 2021-0170. While the ETER infrastructure is open for public usage, the source code itself is not public."
10.1038/s41597-023-02349-y,"The meta-learning code is freely available and accessible at https://github.com/juannat7/metaflux . The repository contains notebooks that are customizable to one’s needs beyond the scope of this work. Further questions, feedback, or comments can be directed to the corresponding author."
10.1038/s41597-023-02356-z,The code is available at Github ( https://github.com/th2ch-g/Canu-Contig-Overlap-Merge ) for merging overlapped contig ends. Other software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in Methods.
10.1038/s41597-023-02342-5,The R codes used to generate datasets in TICCom were shared on Github 36 ( https://github.com/yunjinxie/TICCom-dataset ) with the identifier ( https://doi.org/10.5281/zenodo.8060109 ). All software tools used in this study are freely available.
10.1038/s41597-023-02211-1,Code for data pre-processing and post-sampling is developed in R and available at https://github.com/vincnardelli/fpca and https://zenodo.org/record/7261389 .
10.1038/s41597-023-02358-x,Examples of the code that we used to produce the datasets presented in this paper (mainly for establishing REPM and HEPM) are provided in GitHub ( https://github.com/CEEGDataset/CEEG-Dataset.git ).
10.1038/s41597-023-02360-3,"The following software and versions were used for quality control and data processing: (1) RNA-seq data processing: read mapping was performed with the STAR alignment tool (2.5.3a) 31 ; quantification of RNA-seq data was performed using Kallisto (0.44.0) 32 ; identification of single-copy orthologous genes was performed using OrthoMCL 47 . (2) Lipidomics data processing: the raw data were processed using the Progenesis QI software (version 2.2, Nonlinear dynamics) for peak detection and alignment; The metaX software 34 was further used to process peak intensity data. Associated codes have been submitted in GitHub ( https://github.com/JiamanZhang/Lab_cross_15_vertebrates_adiposes_papre_code )."
10.1038/s41597-023-02362-1,All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of the software have been described in the Methods section.
10.1038/s41597-023-02365-y,Computational pipelines in this study are available at the public repository ( https://github.com/mario2437/BrainTumor_ScientificData/blob/main/Code_Availability ).
10.1038/s41597-023-02354-1,"The scripts used to extract information from figures, tables, and text are mainly based on open-source codes and models including ResNet 64 , table extractor 65 , and Simple Transformers . The in-house scripts for data extraction and analysis are publicly released at the GitHub repository ( https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-CMA2022 and https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022 ), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for the use of the database published with this article."
10.1038/s41597-023-02330-9,"The code used for technical validation as well as prior AFID studies can be found on the GitHub repository page ( https://github.com/afids ), including the validator tool ( https://github.com/afids/afids-validator )."
10.1038/s41597-023-02366-x,Custom code written for data generation mainly consists of scripts for pre- and postprocessing steps linking together the software mentioned below. These scripts are executed through a Merlin workflow. All these scripts are publicly available in a GitHub repository: https://github.com/Supervitux/COSMO_on_Merlin 58 . GECKO-A is available at their website http://geckoa.lisa.u-pec.fr/ . COSMO conf 4.3 and COSMO therm 2021 and their licenses were purchased from Dassault Systemes ( https://www.3ds.com/ ). We provide our custom COSMO conf jobtemplate (( COSMOConfProtocol.xml in the repository. Merlin version 1.7.5 is freely available from https://merlin.readthedocs.io/en/latest/index.html# .
10.1038/s41597-023-02357-y,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-023-02337-2,"The R packages used in this study are freely available and detailed in the methods section. The R codes utilized for the analysis, along with the necessary input data, are publicly available at https://doi.org/10.5281/zenodo.7950777 . The code demonstrates how to implement the climate downscaling technique and compute the means and sums of climate variables for various periods on the netcdf files to replicate all the findings mentioned in the manuscript."
10.1038/s41597-023-02364-z,"All associated code is available on our Github repository and deposited on Zenodo 15 , including Jupyter notebooks to transform data, scripts to run the FDP and the triple store."
10.1038/s41597-023-02359-w,"The data analysis methods, software, and associated parameters used in the present study were described in the section of Methods. If no detailed parameters were described for the software used in this study, default parameters were employed. No custom scripts were generated in this work."
10.1038/s41597-023-02371-0,No custom code was used in this study. We used Google Forms to create the questionnaire and manage the data. We used Microsoft Excel to tabulate and distribute the data.
10.1038/s41597-023-02352-3,"The library data can be accessed in text or binary format. Whereas the library format is described and straightforward for the text files, loading the binary files is best achieved using the MATLAB command “dataBIN = fread(read,[65341,10],‘float’);” where the [65341,10] sets the proper data format (table format). For the ease of access and download in various format database is available in the compressed format at NSSL webpage 17 ."
10.1038/s41597-023-02368-9,No custom scripts or code were used.
10.1038/s41597-023-02302-z,"An R script was developed to de-duplicate the data, validate the georeference locations, and to produce Tables 2 , 3 . This code ( RData_check_MAYV_compendium.txt ) is included in the data repository as a text file."
10.1038/s41597-023-02341-6,The scripts that facilitate re-use of the data can be found in the GitHub repository https://github.com/Rvs94/MyPredict . These scripts were developed and written in Python 3.9. All required software packages are open-source and available online.
10.1038/s41597-023-02347-0,No custom code was written.
10.1038/s41597-023-02373-y,The codes used to analyze the data in this study were available online ( https://figshare.com/articles/dataset/Code/22121171 ) 24 .
10.1038/s41597-023-02344-3,No custom code has been used during the generation and processing of this dataset.
10.1038/s41597-023-02380-z,No custom programming or coding was used.
10.1038/s41597-023-02018-0,"Three database scripts for Microsoft SQL Server are freely available under an MIT license and provided in GitHub at https://github.com/hubmapconsortium/hra-vccf . The first file, HRA-VCCF_CreateTables.sql, creates three database tables to store the Vessel, CellTypeBiomarker, and VesselCTB data. The second file, HRA-VCCF_UpdateDerivedFields.sql, updates the derived fields in the Vessel table. This should be run if vessels are added or deleted or changes are made to the raw data columns. The third file, HRA-VCCF_GenerateExportTables.sql, applies the rules in the CellTypeBiomarker table to generate the VesselCTB data. It then outputs the data in ASCT + B table format and returns several aggregate summary tables."
10.1038/s41597-023-02378-7,The customizable annotation tool used for the dataset labelling is available from https://github.com/AlexandraBodzas/Supporting-Material-for-Blood-Cells-Dataset.git .
10.1038/s41597-023-02232-w,"While custom code was utilized to generate this dataset from source data, this code specifically incorporates analysis of personal data collected by various governmental programs which cannot be shared. No custom code is necessary to utilize the published collated datasets."
10.1038/s41597-023-02258-0,"All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at https://github.com/FAIRplus/imi-data-catalogue . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog. The DATS model is available on Github at https://github.com/datatagsuite/schema ."
10.1038/s41597-023-02372-z,"The python code is available for transparency and use through Dryad (Data Citation 1 66 ); however, due to the large number of data files needed for input and the need to account for variable file formats there are many custom aspects to the code that are included for specific datasets making the code complicated. The coding environment dependencies and versions are included in a .yml file."
10.1038/s41597-023-02387-6,"The software “Elastotron 2000” V.6 was used to analyze the resonance spectra. Details are given in section 2 in a publication by Kaindl et al . 43 . Because of the age of the instrument this software is not accessible, however any standard data analysis tool (Excel, Matlab, etc), or manual inspection, can be used to replicate its functionality."
10.1038/s41597-023-02384-9,"All data, analysis code, and research materials are available at https://doi.org/10.17605/OSF.IO/KPSBW 19 . Data has been deposited at https://doi.org/10.17605/OSF.IO/KPSBW 19 ."
10.1038/s41597-023-02386-7,"Preprocessing of MRI data was performed with freely available neuroimaging tools (DICOM to NIfTI 36 conversion and defacing 37 ). The manually labeled ROIs were generated with the free software “FSL” 38 . In a final step, the correctness of the BIDS structure of our data was checked using the BIDS validator 39 . Source code and associated web link can be found in Table 3 ."
10.1038/s41597-023-02379-6,No costum code was used for the method.
10.1038/s41597-023-02345-2,No custom code was used to generate or process these data.
10.1038/s41597-023-02300-1,"The method is fully described in the manuscript, a rudimentary version of the code can be found at https://github.com/ceejbanks/noclsa ."
10.1038/s41597-023-02296-8,All code and methods documentation for this study can be found at the Zenodo repository 26 . It can also be accessed through this link to our GitHub repository: https://github.com/open-phytoliths/FAIR-assessment-data-paper-documentation .
10.1038/s41597-023-02383-w,"The R code, saved RF models, and training datasets to reproduce the results of this study are available at https://doi.org/10.6084/m9.figshare.21774812.v2 305 ."
10.1038/s41597-023-02326-5,"Data was collected using MATLAB code and custom hardware. Design files and code are available open-access, with manufacture and usage instructions published in a HardwareX article 51 . The collected data, subject demographic information, and figure generation code are published in a Physionet repository 52 ."
10.1038/s41597-023-02376-9,"The source code for this study is publicly available through the following GitHub repository: https://github.com/BenediktMester/FLODIS 23 . It includes the geocoding algorithm to extract subnational information out of the IDMC database, as well as the main scripts to match the IDMC/EM-DAT database with the GFD and to produce the two final FLODIS datasets. We also provide code for the preprocessing of input datasets, the validation procedure, and the development of the figures."
10.1038/s41597-023-02327-4,We provide the code that we used to run all baseline experiments and all data in our GitHub repository ( https://github.com/DeepMicroscopy/MIDOGpp ).
10.1038/s41597-023-02363-0,No custom code was generated for this work.
10.1038/s41597-023-02388-5,The specific scripts have been deposited in figshare. All data analyses were performed according to the manual and protocols of the published bioinformatic tools. The version and parameters of software have been described in Methods section.
10.1038/s41597-023-02385-8,Codes can be accessed at https://github.com/svish91/UMCES_IFA_2021_Ndep .
10.1038/s41597-023-02389-4,Python code for generating parcellations for the NIF-Ontology is publicly available via GitHub: https://github.com/tgbugs/pyontutils/tree/master/nifstd/nifstd_tools/parcellation . Archives of release are available via Zenodo 77 .
10.1038/s41597-023-02369-8,The code used for converting the data format files and building the SQL-based format are publicly available in an online repository 98 .
10.1038/s41597-023-02340-7,"Our extended cleaning pipeline nccidxclean is publicly available and can be accessed on GitLab: https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the return_cols parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage eda is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. Please note that pertinent links related to data and code availability, referenced in Sections 5 and 6, are provided at their initial mention within the main body of the text ."
10.1038/s41597-023-02399-2,"No custom code was created in the generation or processing of data sets. R version 3.6.0, Notepad version 7.7.1, and Microsoft Excel 2016 were used to process each submitted data set and to build the eight previously described data tables. SQL Server Management Studio 18.12.1 was used to establish primary and foreign key relationships between these data tables to create the CCTD."
10.1038/s41597-023-02370-1,No specific code has been developed for this study.
10.1038/s41597-023-02398-3,"COVID AMP data are available via an application programming interface (API) and are licensed under the Creative Commons Attribution CC BY Standard at: https://api.covidamp.org/docs . We provide a public, interactive web interface for visual exploration of the dataset at: https://covidamp.org/ . Within the site, data is available at: https://covidamp.org/data?type=policy . This page allows for download of the full dataset or filtered subsets of the data. Additionally, documentation of the methods, including a data dictionary and glossary, are available at: https://covidamp.org/about/doc . In addition to this manuscript, a static version of the COVID AMP dataset itself can be cited directly as Zenodo https://doi.org/10.5281/zenodo.8087600 . All policies and directives coded within the dataset have been reviewed and technically validated. We hope that the dataset will support research efforts aimed at improving pandemic response strategies and inform future outbreak policy analysis."
10.1038/s41597-023-02395-6,All codes used for this study are available on GitHub ( https://github.com/AliciaHellens/bZIP11_ATAC-seq ).
10.1038/s41597-023-02400-y,"We processed the brain MRI data using open-source software packages: MRIcroGL (v20), PyDeface (v2.0.2) 11 , MRIQC (v22.0.6) 9 , FSL FLIRT (v6.0) 12 , 13 , and FreeSurfer v7.1.0 14 . No custom code was utilized."
10.1038/s41597-023-02382-x,"The global floodplain alteration dataset was derived entirely through ArcGIS 10.5 and ENVI 5.1 geospatial analysis platforms. To assist in reuse and application of the dataset, we developed additional Python codes aggregated as three web-based tools: Floodplain Mapping Tool: https://colab.research.google.com/drive/1xQlARZXKPexmDInYV-EMoJ-HZxmFL-eW?usp=sharing . Land Use Change Tool: https://colab.research.google.com/drive/1vmIaUCkL66CoTv4rNRIWpJXYXp4TlAKd?usp=sharing . Human Alteration Tool: https://colab.research.google.com/drive/1r2zNJNpd3aWSuDV2Kc792qSEjvDbFtBy?usp=share_link . See Usage Notes section for details."
10.1038/s41597-023-02403-9,"In this study, software tools used according to the description mentioned in the materials and method section. No custom code was used."
10.1038/s41597-023-02404-8,"As mentioned above, the correct position of the annotations was verified by drawing the corresponding bounding boxes on the images using Detectron2. The Python script used for this is in the file bbox_placement_test.py. The input annotation file for this run is a COCO JSON one. This was also generated from the tab-delimited annotation file using a Python script provided in TSV_to_COCO.py. Both script files are available in the Figshare repository 16 ."
10.1038/s41597-023-02394-7,"The versions and parameters of bioinformatic tools used in this study have been described in the Method section. If no parameter is provided, the default is used. No custom code was used."
10.1038/s41597-023-02396-5,"Code for collecting, formatting, and processing the data is available at https://github.com/EmotionCognitionLab/HRV-ER-dataset_release and https://github.com/EmotionCognitionLab/emWave_HRV . Information about the code dependencies and package requirements are available in the same Github repository."
10.1038/s41597-023-02406-6,The code for the SetFit is available here: https://doi.org/10.6084/m9.figshare.23254856.v2 17 .
10.1038/s41597-023-02414-6,This research did not use or generate any coding to present the data described in the manuscript.
10.1038/s41597-023-02411-9,"Data processing, validation and plotting were performed using Excel and Jupyter notebooks 54 in a Python 3 environment 55 . No custom code has been used."
10.1038/s41597-023-02397-4,"No specific code was used in this study. All commands and pipelines used in the data processing were performed according to manuals and protocols of corresponding bioinformatics software, with parameters described in the Methods section. If no detailed parameters were mentioned for a software, default parameters were used."
10.1038/s41597-023-02402-w,"The sofware versions, settings and parameters used are described below. (1) BWA: version 0.7.12-r1039, default parameters; (2) SAMtools/BCFtools: version 0.1.19, default parameters; (3) Trinity: trinityrnaseq-2.9.0, default parameters; (4) RSEM: default parameters; (5) HiSAT2: default parameters; (6) GATK: version 4.0.3.0, default parameters; (7) HiC-Pro: version 2.10.0, default parameters; (8) CANU: version 1.7, parameters: batOptions = -dg 3 -db 3 -dr 1 -ca 500 -cp 50; (9) FALCON: version 0.2.2, default parameters; (10) Pilon: version 1.2.3, parameters: --mindepth 4 --threads 6 --tracks --changes --fix bases --verbose; (11) Juicer: version 0.0.2, default parameters; (12) 3D-DNA: version 180922, default parameters; (13) ALLHiC: version 0.9.8, default parameters; (14) RepeatModeler: RepeatModeler-open-1.0.11, default parameters; (15) RepeatMasker: version 2.1, default parameters; (16) TEclass: librf_lvq_pak-3.1, default parameters; (17) Tandem Repeat Finder (TRF ): version 4.07, default parameters; (18) LTR_Finder: version 1.05, default parameters; (19) LTRharvest: version 1.5.10, default parameters; (20) LTR_retriever: version 2.9.0, default parameters; (21) PASA: version 2.0.2, default parameters; (22) MAKER2: (23) SNAP: version 2006-07-28, default parameters; (24) GENEMARK: version 3.9, parameter used: -f gf3; (25) AUGUSTUS: version 2.5.5, parameters: --species = arabidopsis; (26) StringTie: version 2.2.1, default parameters; (27) Trimmomatic: version 0.32, default parameters; (28) GATK: version 4.0.3.0, default parameters."
10.1038/s41597-023-02392-9,"The Matlab code developed for data loading, data parsing, segmentation and preprocessing required for the use of both datasets (provided via Figshare) alongside with two Matlab Live Scripts that demonstrate the data handling process for each dataset are provided in our Github repository (NeuMa Raw Dataset, https://github.com/NeuroMkt/NeuMa_Dataset_Processing/tree/main/NeuMa_Raw_Usage_Code ; NeuMa Pre-processed Dataset, https://github.com/NeuroMkt/NeuMa_Dataset_Processing/tree/main/NeuMa_PreProcessed_Usage_Code )."
10.1038/s41597-023-02420-8,"The codes and pipelines used in data processing were all executed according to the manual and protocols of the corresponding bioinformatics software. The specific versions of software have been described in Methods. However, the following perl script was used to trim the gaps of the aligned sequences. #! usr/bin/perl -w use strict; sub usage{ print STDERR «USAGE; usage: $0 <phy.file> <cut off> USAGE USAGE exit; } my $file = shift; open IN,”$file” or die $!; open OUT,” >./$file.trim.phy” or die $!; my ($num_species,$len); my (@name,@seq,@gap_ratio,@match_ratio); my %new_seq; my @u_site; my $i = 0; my $resultfile = “./$file.trim.phy”; while (<IN>){ chomp; if (/^\s+(\d+)\s+(\d+)/){ $num_species = $1; $len = $2; next; } my @temp = split; $name[$i] = $temp[0]; @{$seq[$i]} = (split //,$temp[1]); $i++; } print “$num_species\n”; close IN; for (my $j = 0;$j <$len;$j++){ my ($gap,$match) = (0,0); for (my $i = 0;$i < $num_species;$i++){ if ($seq[$i][$j] eq “-“){ $gap++; }else { $match++; } } $gap_ratio[$j] = $gap/$num_species*100; $match_ratio[$j] = $match/$num_species*100; } for (my $i = 0;$i < $num_species;$i++){ for (my $j = 0;$j < $len;$j++){ if ($gap_ratio[$j] >  = $ARGV[0]){ next; }else { $new_seq{$i}. = $seq[$i][$j]; } } print OUT “$name[$i]\t$new_seq{$i}\n”; } my $len_temp = length($new_seq{0}); print “$len_temp\n”; ‘sed -i ‘1i $num_species $len_temp’ $resultfile‘;"
10.1038/s41597-023-02409-3,"Preprocessed data for DIPS-Plus 10 as well as its associated source code and instructions for data processing and reproducibility can be found on Zenodo and GitHub , respectively. The GitHub instructions illustrate how users can install the Python programming language 46 and build an Anaconda virtual environment 47 containing the software dependencies required to preprocess and analyze DIPS-Plus 10 using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus 10 from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code 36 can also be found on GitHub , along with a corresponding DOI ."
10.1038/s41597-023-02407-5,The HR-Kidney dataset is freely available for download at the Image Data Resource under accession number idr0147: https://doi.org/10.17867/10000188 .
10.1038/s41597-023-02427-1,"The execution of all software and pipelines in this study strictly followed the manuals and protocols of the published bioinformatic tools. The versions of the software employed have been specified in the Methods section. If no parameter is provided, the default is used. No custom code was employed."
10.1038/s41597-023-02355-0,Computational tools to process data and plot figures shown in the paper are available on https://github.com/labsyspharm/lincs_proteomics_data_descriptor and https://github.com/datarail/msda .
10.1038/s41597-023-02418-2,Data analysis procedures have been described in detail in the Methods section. Code for the statistical analysis performed in R is available as a supplement file.
10.1038/s41597-023-02415-5,"The code and associated libraries used to create Level 1, Level 2, and Level 3 processed files are based in Python with the following dependencies: Python > = 3.6; netCDF4 > = 1.3.0, NumPy > = 1.13.0, Scipy > = 1.1.0, Pandas > = 0.20, XArray > = 0.11; PVLib > = 0.8.1. Files uploaded to ADC have the following versions: Level 1 are v1.5 (1/8/2020) and Levels 2 and 3 v4.1 (2/1/2023). Code is archived on GitHub, https://github.com/MOSAiC-flux/data-processing ."
10.1038/s41597-023-02393-8,"All code can be found at: https://github.com/matfran/EUSEDcollab.git . We include the R language code to perform the quality control procedure on each time series entry to produce the JSON time series evaluation files for each record. Additionally, a Python language Jupyter notebook is included to demonstrate simple operations that can be undertaken using the database, such as reading and filtering the database, calculating metadata statistics and importing specific time series for analysis."
10.1038/s41597-023-02401-x,No custom code was used in generating the dataset.
10.1038/s41597-023-02429-z,"Here we list the details of the software used for data analysis. Pychopper ( https://github.com/epi2me-labs/pychopper ), version 2, was used to identify, orient and trim FL Nanopore cDNA reads. LoRDEC, version 1.4.1, was used for correcting errors of long-read RNA-seq data based on short-read RNA-seq data. Pinfish ( https://github.com/nanoporetech/pinfish ), version 0.1.0, which is a collection of tools helping to make sense of long-read RNA-seq data. Flair ( https://github.com/BrooksLabUCSC/flair ), version 1.5.0, was used for isoform definition with long-read RNA-seq data. Cuffcompare, version 2.2.1, was used to identify novel isoform based on gene annotation information. Samtools, version 1.9, was used to extract sequence according to the coordinates of novel isoforms. ORFfinder, version 0.4.3, was used to predict ORFs based on nucleotide sequences. AlphaFold Multimer (an extension of AlphaFold2, version 2.2.0) was used to predict the 3D structures of novel isoforms. Then, we also use some in-house scripts to filter and prepare the input and output files, which have been deposited in github ( https://github.com/ZhangNestor/magic )."
10.1038/s41597-023-02374-x,"All code for loading and normalization of the dataset is available in GitHub ( https://github.com/glescki/dicom_image_parser ). For parsing the data, the PyDicom library is recommended, and the loading of the labels can be performed with a parser available in GitHub. Each DICOM file should be loaded separately and then joined within a data structure. For normalization, it is recommended that the spacing in the z-axis of all slices be changed to 1."
10.1038/s41597-023-02432-4,"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare ( https://doi.org/10.6084/m9.figshare.22363012 ). The code used for technical validation can be accessed from ( https://github.com/XLR8-07/FracAtlas ). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results."
10.1038/s41597-023-02426-2,The R scripts and the package versions used for the eQTL analysis and the creation of the QTL viewer RData object are available at https://github.com/TheJacksonLaboratory/CSNA/tree/master/analysis/eQTL_viewer/striatum .
10.1038/s41597-023-02424-4,"The GRAPE dataset can be downloaded at the Figshare as mentioned above 36 . The codes of drawing annotations on ROI images are saved as the python file “draw.py”. The DL models applied in “Technical Validation” are not the as a part of the GRAPE dataset. We uploaded these models at the Figshare as two separated parts, “Baseline model validation for VF estimation” and “Baseline model validation for VF progression prediction”, that correspond to the 2 chapters. The parameters tuning is detailed in the article."
10.1038/s41597-023-02425-3,"Other than air quality data stamped with time and location, we also provide a compilation of land use GIS layers that are used in our and NYCCAS’ LUR models for convenient reproduction of the results in our Github repository ( https://github.com/MIT-Senseable-City-Lab/OSCS/tree/main ). These GIS layers are published by NYC and New York State governments and processed by the authors for modeling, with 2021 as the base year. The audience is encouraged to explore the repository, regarding the details about how we design, build, calibrate, and make use of the CS platform. Python code is available for automatic land use feature extraction, LUR training, and performance evaluation."
10.1038/s41597-023-02405-7,We used the Python programming language for the data processing described above. Volume segmentation was implemented using the pydub libary. The label assignment algorithm is summarized in Fig. 3 . The code is available as part of our dataset in Zenodo: https://doi.org/10.5281/zenodo.5786859 .
10.1038/s41597-023-02343-4,No custom code was used. Software tools used for processing are mentioned in the Methods and Technical Validation sections.
10.1038/s41597-023-02339-0,"The CReSIS toolbox used to process the MCoRDS RES data is available at https://gitlab.com/openpolarradar/opr , and the main documentation can be found at https://gitlab.com/openpolarradar/opr/-/wikis/home ."
10.1038/s41597-023-02416-4,"Code for solving the Eikonal equation and the forward problem of electrocardiography using the boundary element method as used for the atrial simulations is openly available (Stenroos et al . 56 , Schuler et al . 57 ). The electrophysiology of the ventricular-torso model was simulated using the proprietary CARPentry-Pro software (NumeriCor, Graz, Austria). Similar simulations can also be carried out with the publicly available openCARP simulation framework 40 , 41 . Python code for synthesizing single beat P waves and QRST complexes to a 10 s time series using multi-variate normal distributions for amplitude scaling and interval selection is publicly available 58 ."
10.1038/s41597-023-02436-0,"There is no custom computer code used to analyze the data generated from this study; all analyses were conducted using Microsoft Excel, Version 16.59."
10.1038/s41597-023-02441-3,This study didn’t use any customized code.
10.1038/s41597-023-02433-3,"All code, data and tools are openly available at https://github.com/OpenBioLink/ThoughtSource , a snapshot of the GitHub repository is archived at https://doi.org/10.5281/zenodo.8199390 41 , and a snapshot of dataset contents is archived at https://doi.org/10.5281/zenodo.8199538 42 . Our code and data are licensed under an MIT license, while data adapted from existing datasets are available under the licenses of their respective sources."
10.1038/s41597-023-02419-1,"The commented R code for the preprocessing of image data is available at Figshare 10 . In addition, for reproducibility issues, two TDI images are provided. One of them requires a manual selection of the beginning and end points of the cycle."
10.1038/s41597-023-02417-3,"The pipeline is based on the use of Rhinoceros3D 35 and the Cockroach plugin which is available at https://ibois-epfl.github.io/Cockroach-documentation/ 36 . A tutorial for the implementation of the pipeline is freely available online 37 . In this work, we used Rhinoceros3D Version 7 SR29. The collision analysis algorithm is available at: https://github.com/ibois-epfl/collide 42 . The Python script used for the analysis of the collision data is available at: https://doi.org/10.5281/zenodo.7093710 43 ."
10.1038/s41597-023-02423-5,"The custom software and codes used to process the published data set of the EP 37 , the UP 38 , the BP 39 , and CAMP 40 are publicly available on zenodo.org."
10.1038/s41597-023-02431-5,"All software utilized in this study is publicly available, and their respective parameters are detailed in the Methods section. In cases, where no parameters were specified, default values, as recommended by the software developers, were employed. The scripts utilized in this study are accessible at https://github.com/lrslab/Zebrafish-Multisequencing ."
10.1038/s41597-023-02447-x,Custom scripts were not used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-023-02434-2,No specific code or script was used in this work. All commands used in the processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-023-02438-y,"There is no custom code associated with this data descriptor. For data(pre)processing and obtaining data derivatives, we used existing R packages. This included cld for English language checks, quanteda 19 and stringr ( https://stringr.tidyverse.org/ ) for text metadata (number of characters, tokens, punctuation), the stm package 15 for constructing topic models, and the factoextra package ( https://rpkgs.datanovia.com/factoextra/ ) for the determination of the number of clusters for obtaining the higher-order psychological constructs."
10.1038/s41597-023-02435-1,"The MATLAB 2022 (MathWorks, Inc., Natick, MA, USA) codes developed for getting the results presented in this paper are available at figshare 27 , Harvard Dataverse 28 and GitHub ( https://github.com/cmtzmiwa/On-Reliability-of-Annotations-in-Contextual-Emotion-Imagery )."
10.1038/s41597-023-02456-w,No specific script was used in this work. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software.
10.1038/s41597-023-02459-7,No custom code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-023-02449-9,No custom code was used in the processing of this dataset.
10.1038/s41597-023-02446-y,No specific code was used to produce and analyse the presented data.
10.1038/s41597-023-02408-4,The code for calculating the electronic excitation energies and statistical analysis of the dataset is open-source and available at the ORNL-GitHub repository https://github.com/ORNL/Analysis-of-Large-Scale-Molecular-Datasets-with-Python .
10.1038/s41597-023-02377-8,No custom codes or algorithms were used to generate or process the data presented in this manuscript.
10.1038/s41597-023-02391-w,All data processing procedures described in this paper were performed using Python 3.7. The code repository and more detailed usage instructions can be found at https://github.com/HPI-CH/fatigue-dual-task-data . The main functionalities of the code are as follows: • Segment the IMU recordings into walking sessions and fatigue exercise • Calculate spatio-temporal gait parameters from the IMU signals • Summarize gait parameters and other study-related information
10.1038/s41597-023-02452-0,Not applicable in this work.
10.1038/s41597-023-02451-1,"The code for loading the data set 5 , testing, and utility functions is available in a Github repository, which can be found at the following https://github.com/Lukykl1/dataset_pd_vsb . The repository includes code for loading the data in npy format and the metadata in csv format, as well as utility functions for processing and analyzing the data. The code is documented and includes comments to explain each step of the process. To run the Python scripts, you will need to have the necessary libraries installed, including NumPy, pandas, Matplotlib, and any other required dependencies. You can install these libraries using a package manager such as pip or conda. We hope that this code will be useful to researchers and data scientists who are working with these data and looking for an efficient and flexible way to load and process them."
10.1038/s41597-023-02465-9,"We employed ORCA 4.2.1 to run ab initio MD simulations and perform calculation accuracy comparisons between PM3 and HF approaches 40 as well as DFTB + 22.2 for DFTB approach 51 . We used the Amber20 sander to run REMD simulations and perform calculation accuracy comparisons on MM. We also employed Amber20 pmemd.cuda for conventional MD simulations 54 , 55 . We used mdtraj 1.9.7 and MSMBuilder 3.8.0 for trajectory analysis and anchor selection 56 , 57 . We applied pytorch 1.13 and torch-geometric 2.0.4 for the training of VisNet. The time course and distribution analysis were drawn by seaborn 0.11.2. The free energy surfaces were generated via MATLAB R2019a."
10.1038/s41597-023-02457-9,No custom code was used.
10.1038/s41597-023-02466-8,All the codes with comments that were used to generate the CEFIO dataset are available in Figshare 37 .
10.1038/s41597-023-02439-x,"To ensure reproducibility, and recreate the results for other regions and/or years the example code for creating and accessing all the spatial and temporal datasets is available in Zenodo at ( https://doi.org/10.5281/zenodo.7761963 ) 24 ."
10.1038/s41597-023-02454-y,The MATLAB scripts are available for loading and analysing data under the MIT License at https://github.com/honggi82/Scientific_Data_2023 .
10.1038/s41597-023-02469-5,Python was used for all data processing described in this paper. The Python code used to generate all figures in this paper is available on the Augmented Health Lab’s Github: https://github.com/Augmented-Health-Lab/Diatrend .
10.1038/s41597-023-02453-z,Parameters for all commands used to assemble the genome and construct the pangenome are available in figshare 43 .
10.1038/s41597-023-02471-x,"All codes for the experimental design, data organization, and technique validation are available at https://github.com/BNUCNL/NOD-fmri . Preprocessing was performed using fMRIPrep version 20.2.1 ( https://fmriprep.org ). Grayordinate-based (CIFTI format) brain activation analysis was performed by combining the Ciftify ( https://github.com/edickie/ciftify ) and HCP pipelines ( https://github.com/Washington-University/HCPpipelines )."
10.1038/s41597-023-02458-8,"Code for presenting task stimuli and naturalistic stimuli, along with code to preprocess EEG and fMRI imaging data, is available on GitHub ( https://github.com/NathanKlineInstitute/NATVIEW_EEGFMRI ). Additionally, the videos used for naturalistic stimuli will also be made available through the GitHub repository."
10.1038/s41597-023-02437-z,All code used for encoding model fitting is publicly available and can be found on github and zenodo 25 . The code used for filtering the audio for each story to correct for frequency response and phase errors induced by the headphones using calibration data provided by Sensimetrics and custom Python code. This code can be found at https://github.com/alexhuth/sensimetrics_filter .
10.1038/s41597-023-02463-x,"The code associated with this manuscript consists of a “main.ipynb” Jupyter notebook, the source code of FAIRshare, and the source code of the FAIRshare documentation. The “main.ipynb” Jupyter notebook contains the code used to analyze the findings from the review and to conduct other analyses presented in this manuscript (e.g., generate Fig. 1 ). This notebook is available in a GitHub repository called “Code” (also maintained in the FAIR-BioRS GitHub organization). The dataset associated with this notebook was made FAIR according to the FAIR-BioRS guidelines using FAIRshare v2.1.0 75 , and shared under the permissible MIT license. The latest version associated with this manuscript (v3.0.0) is archived on Zenodo 76 . The source code for FAIRshare is hosted on GitHub ( https://github.com/fairdataihub/FAIRshare ). The current version of FAIRshare (v2.1.0) discussed in this manuscript was made FAIR using FAIRshare itself, and shared on Zenodo 75 under the permissible MIT license. The source code for the FAIRshare documentation is maintained on GitHub as well ( https://github.com/fairdataihub/FAIRshare-Docs ) and the current version (5.0.0) was shared under the permissible MIT license on Zenodo 77 ."
10.1038/s41597-023-02455-x,"ALS visualisations were calculated with Relief Visualization Toolbox (version 2.2.1), available at https://github.com/EarthObservation/RVT_py . The code for creating the satellite data records from Sentinel-1 and Sentinel-2 is available at https://github.com/EarthObservation/Sentinel-S1-S2-ML-patches-workflow ."
10.1038/s41597-023-02478-4,The code utilized for the Monte Carlo simulations in this study is provided in the Supplementary Codes 1 and 2.
10.1038/s41597-023-02422-6,MISSING
10.1038/s41597-023-02428-0,"We provide the gait analysis software used to obtain the parameters provided in the register. The Python codes, which are detailed in the Usage Notes section, can be found in the folder named “Gait_analysis_w_Python”. To generate the calibrated signals from the IMU raw measurements, we use a calibration function similar to the one provided in the readLogFileArduinoCalib code. For a correct functioning, the distribution of folders must remain as organized in Zenodo 22 or the file paths have to be updated to their new locations."
10.1038/s41597-023-02481-9,The genome and transcriptome analyses were performed following the manuals and protocols of the cited bioinformatic software. No new codes were written for this study.
10.1038/s41597-023-02461-z,Data used for the computation of SMR for AD and PD at Italian provincial level are available from ISTAT (see the paragraph Data Source). We implemented the procedure described in the Methods section. Data processing was performed in R 4.2.2 11 and the used algorithms is available on Zenodo 9 .
10.1038/s41597-023-02476-6,"Chasing reproducibility, the code used in this study is openly accessible. The code is organized in two files: 1. SValidation.md , containing the code for reading the raw data[?], data transforming, data cleaning and structural validation actions, and; 2. CValidation.md , containing the code related to content validation procedures, which follow the actions taken on the former code file. These files are available on GitHub at https://github.com/rb1970/UrbanFiresData ."
10.1038/s41597-023-02464-w,"The code written to build this database is available on the GitHub repository associated with this paper ( https://github.com/lindsaykatz/hansard-proj ). All scripts were created using R software 21 . The core packages used to develop these scripts are: the XML package 15 , the xml2 package 16 , the tidyverse R packages 22 , the AustralianPoliticians package 9 , and the ausPH package. XML and xml2 were used for parsing the XML documents, AustralianPoliticians and ausPH were used for cleaning up and filling in MP details in the datasets, and tidyverse packages were used in all steps, for tidy wrangling of data."
10.1038/s41597-023-02482-8,All code used in this experiment was written in Python3 and could be publicly accessed at https://github.com/cmb-chula/CancerCellVision-CCA . The code is based on PyTorch 41 and MMDetection 32 .
10.1038/s41597-023-02475-7,"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto 30 ( https://pyeto.readthedocs.io/en/latest/ )."
10.1038/s41597-023-02462-y,The source code of the directory is not publicly released. The re3data subject ontology and several Jupyter notebooks with examples for using the re3data API can be found at: https://github.com/re3data .
10.1038/s41597-023-02490-8,The information for all bioinformatics tools used in this study is listed in Table 4 . All code/scripts used for the genome assembly and analyses can be accessed on GitHub ( https://github.com/zpqu/KS_WGS_scripts ).
10.1038/s41597-023-02444-0,The output files of technical validation 51 and the codes for de-novo reference-based guided assembly 52 is available on figshare.
10.1038/s41597-023-02460-0,The code repository of the presented few-shot methods can be accessed via https://github.com/wllfore/MedFMC_fewshot_baseline . No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-023-02483-7,"Software to generate novel objects is available at https://artbreeder.com . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the mouse-tracking extension (all available at 47 ). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at 47 )."
10.1038/s41597-023-02484-6,"Python scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on GitHub: https://github.com/mbkiss/2DeteCTcodes . They make use of the ASTRA toolbox, which is openly available on ( www.astra-toolbox.com www.astra-toolbox.com ) or accessible as a conda package ( conda install -c astra-toolbox astra-toolbox ). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state very involved and version-dependent. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow for angular sub-sampling the projections and the reference reconstructions were computed with all projections as mentioned in the subsection “Reconstruction production” above."
10.1038/s41597-023-02474-8,The data set is available from Zenodo 7 . R scripts used to pre-process the data is available from the same Zenodo repository 7 . The GEE code for the extraction of snow cover faction is available from Zenodo 65 .
10.1038/s41597-023-02467-7,"The code used for checking, cleaning, and analyzing the data is available in the open GitHub repository “ https://github.com/Plant-Functional-Trait-Course/PFTC_4_Svalbard ”, of which a versioned copy is available at Zenodo 67 ."
10.1038/s41597-023-02333-6,"All single-cell RNA-Seq analyses were performed using FastQC ( http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ), Cell Ranger (download from 10x genomics) and Seurat ( https://satijalab.org/seurat/ )."
10.1038/s41597-023-02445-z,"As described above, all the code of the experiments in available free and open-source. The OpenViBE scenarios are shared with the database 32 , while the free and open-source OpenViBE BCI platform itself can be downloaded there: ( http://openvibe.inria.fr/ ). To access and download the free and open source OpenViBE BCI platform, go to: http://openvibe.inria.fr/ . The source code for all versions of OpenViBE is available at http://openvibe.inria.fr/pub/src/ , for each version you will find a README with build instructions in the archive you have downloaded. You can also find the sources at https://gitlab.inria.fr/openvibe/meta . To install the corresponding versions, which will allow you to reuse the scenarios of the experiments corresponding to this manuscript, you need to download older versions of OpenViBE. OpenViBE 2.1.0 (for dataset A) is available at http://openvibe.inria.fr/pub/bin/win32/ and OpenViBE 2.2.0 (for dataset B) is available at http://openvibe.inria.fr/pub/bin/win64/ . You will then need to open all the scenarios provided in zenodo 32 and you will be able to run them."
10.1038/s41597-023-02486-4,An input file for the CP2K calculations can be found in the Supplementary Information. Further code is not required to reproduce the data presented in this article.
10.1038/s41597-023-02450-2,"All software and scripts were executed according to user manual, and default parameters were applied if not mentioned in the Methods described above."
10.1038/s41597-023-02492-6,"The surface classification and attitude correction of the radiation data set was performed in the IDL software (version 8.8.3). Multiple linear regression and quality checks in addition to the MCARaTS simulations were coded in R (version 4.0.4) using the libraries: zoo (version 1.8.9) and gridExtra (version 2.3). The MCARaTS simulations were prepared using R (version 4.0.4) in combination with the libraries readr (version 1.4), jpeg (version 0.1.9), png (version 0.1.7), and stringr (version 1.4). The simulations were performed by MCARaTS (version 0.1). Code for classification of the surface type fractions and connection, quality check and MLR of the radiation data sets are publicly available 48 ."
10.1038/s41597-023-02440-4,"No specific codes or scripts were used in this study. All software used is in the public domain, with parameters clearly described in the Methods section."
10.1038/s41597-023-02487-3,"All code used to run data statistics, baseline models, and evaluation to analyze the aci-bench corpus is freely available at https://github.com/wyim/aci-bench ."
10.1038/s41597-023-02442-2,All code to generate the SugarBind RDF resources and the generated RDF data files are available from the GitHub repository as a v1.0.2 release at https://github.com/glycoinfo/GlycanBind/releases/tag/v1.0.2 . The entire Github repository for the v1.0.2 release is archived on Zenodo 12 : https://zenodo.org/record/8072786 .
10.1038/s41597-023-02497-1,No custom code has been used during the generation and processing of this dataset.
10.1038/s41597-023-02470-y,"We use the ZENODO service to keep a persistent archive of FyDev code, with https://doi.org/10.5281/zenodo.8098117 33 ."
10.1038/s41597-023-02505-4,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02473-9,The code 189 to generate the carbon-water flux datasets is available at figshare ( https://doi.org/10.6084/m9.figshare.21510183.v2 ).
10.1038/s41597-023-02494-4,The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ( https://doi.org/10.5281/zenodo.8108927 ) 29 .
10.1038/s41597-023-02500-9,"The full dataset 19 and the code used in this paper are freely available at Zenodo: https://doi.org/10.5281/zenodo.7940337 . All data, analyses, and visualizations presented in this paper were prepared in R 4.2.2 under Ubuntu 18.04.5 LTS (64-bit) using the following R packages: data.table (≥1.14.8), dplyr (≥1.1.2), forcats (≥1.0.0), ggforce (≥0.4.1), ggplot2 (≥3.4.2), ggpointgrid (≥1.2.0), ggridges (≥0.5.4), magrittr (≥2.0.3), Momocs (≥1.4.0), outlineR (≥0.1.0), raster (≥3.6–20), readr (≥2.1.4), remotes (≥2.4.2), rgeos (≥0.6–2), rworldmap (≥1.3–6), sp (≥1.6-0)."
10.1038/s41597-023-02468-6,"All statistical analyses were conducted with the software packages of “R” Ver. 4.1.1 (R Development Core Team, 2019) 63 and “EZR” 64 . All statistical significances were recognized by P < 0.05 (*** P < 0.001, ** P < 0.01, * P < 0.05, n.s.: P ≥ 0.05)."
10.1038/s41597-023-02507-2,The source code demonstrating the work with the dataset is available at https://doi.org/10.5281/zenodo.7818443 60 .
10.1038/s41597-023-02506-3,"All bioinformatic tools and softwares for data analysis in this study were used according to the manuals, and the version and code/parameters of software have been introduced in Methods section. No custom code was used."
10.1038/s41597-023-02499-z,The software and codes for processing the collected data and for plotting the figures are conducted in Python 3.7 and included in the dataset at https://doi.org/10.11888/Terre.tpdc.300406 or https://cstr.cn/18406.11.Terre.tpdc.300406 . And they are also available on GitHub: https://github.com/thimpeng/RFI-Suppressed_SMOS_L-band_multi-angular_TB_Refinement .
10.1038/s41597-023-02430-6,No custom code was used in this research article.
10.1038/s41597-023-02510-7,"Our dataset was created by using open-source tools that were published with separate digital object identifiers (doi) minted for each of the repositories. These tools are indexed on Zenodo 25 , 26 , 27 . We have made available a PyTorch 73 and PyTorch Lightning 74 API published to PyPI for accessing our dataset and performing various analyses. Additionally, Our API is accessible in the form of a GitHub repository - https://github.com/Kaszanas/SC2_Datasets , which is available on Zenodo with a separate doi. All of the instructions for accessing the data and specific field documentation are published there 31 . The code used for technical validation experiments is available for preview in a GitHub repository: https://github.com/Kaszanas/SC2EGSet_article_experiments . In the process of preparing this article, PyTorch Lightning has changed its name into Lightning. We have decided to use the old form of the name, following the citation template provided by the Lightning project on GitHub 74 . GitHub Links to the tooling used in the dataset preparation: • https://github.com/Kaszanas/SC2InfoExtractorGo , • https://github.com/Kaszanas/SC2DatasetPreparator , • https://github.com/Kaszanas/SC2MapLocaleExtractor , The official dataset API is available at the following repository: https://github.com/Kaszanas/SC2_Datasets . Additional tooling for potential anonymization tasks with data from private collections is available at: https://github.com/Kaszanas/SC2AnonServerPy ."
10.1038/s41597-023-02521-4,The present study did not use custom scripts to generate the dataset. The parameters and versions of all the bioinformatics tools used for the analysis are described in the Methods section. The code used to run each of the tools is available in the Figshare repository 71 .
10.1038/s41597-023-02509-0,"No custom software codes were developed as part of this research. All bioinformatics tools and pipelines were executed following the manual and protocols provided by the respective software developers. The versions of the software used, along with their corresponding parameters, have been thoroughly described in the Methods section."
10.1038/s41597-023-02516-1,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatic software. There were no custom scripts or code utilized in this study.
10.1038/s41597-023-02514-3,"The ontology is available in both OBO and OWL format from the GitLab repository ( https://gitlab.com/fortunalab/ontoavida ) and can be found at https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo and https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl . OntoAvida OBO and OWL files are also available from the OBO Foundry ( http://www.obofoundry.org/ontology/ontoavida.html ). All files are available under the Creative Commons Attribution 4.0 International License ( http://creativecommons.org/licenses/by/4.0/ ) which allows for the copying, redistribution and adaption of the ontology for any purpose. pyLODE ( https://github.com/rdflib/pyLODE ) was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates ( https://gitlab.com/fortunalab/pyLODE ) to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig. 4 ). The pyLODE file of OntoAvida is available at https://owl.fortunalab.org/ontoavida ."
10.1038/s41597-023-02518-z,"Every step from obtaining Sentinel data to processing images into impervious surface temporal data was done in Google Earth Engine. All steps, including image processing, sample migration, and image classification have been made publicly available in the GEE code snippet ( https://code.earthengine.google.com/74d0845d708a01fde1484c30ca73cc72 ). The collected training and validation samples have been archived in the same code snippet."
10.1038/s41597-023-02522-3,No specific code or script was used in this work. Commands used for data processing were all executed according to the manuals and protocols of the corresponding software.
10.1038/s41597-023-02488-2,"All the employed software as well as their versions and parameters were described in the method section. If no parameters were specified, default settings were employed. Data visualization plots were generated using R v4.1.2 ( https://cran.r-project.org/ , R development core team) and https://bioinformatics.psb.ugent.be/webtools/Venn/ ."
10.1038/s41597-023-02515-2,"The code for extracting names, longitudes and latitudes of reservoirs, ponds, etc. from Baidu web map ( http://map.baidu.com ) can be obtained from https://github.com/lidc54/webMap . The annual Landsat image composition is implemented on the platform of Google Earth Engine. One of the main tools to visualize the dataset is to use the software ArcGIS Desktop see https://desktop.arcgis.com/en/arcmap/latest/get-started/installation-guide/installing-on-your-computer.htm . By using ArcGIS 10.8.2 platform, we distinguished and vectorized the reservoirs’ boundaries based on manual interpretation on Landsat 8 images."
10.1038/s41597-023-02519-y,"The TIHM dataset is available in the corresponding Zenodo repository 10 and consists of five separate tables (Activity, Sleep, Physiology, Labels, and Demographics). For further information on the data records, please refer to the README file. The code for the experiments presented in the manuscript is available on the Github repository ( https://github.com/PBarnaghi/TIHM-Dataset ). The libraries and their versions and dependencies that are used in the code are also provided as a separate configuration file in JSON/YAML format."
10.1038/s41597-023-02503-6,The code implementation was done using Python. Source codes that were used to develop and analyze the data are publicly available in the GitHub repository ( https://github.com/Judy0718/EWELD ).
10.1038/s41597-023-02525-0,"The EEG pipeline code is available at GitHub under the CC-BY 4.0 license, and it is co-deposited in Zenodo, and referenced with a unique DOI 30 . The pipeline was created and tested in Matlab 2020b (The Mathworks, Inc.) on Ubuntu 18.04.5 LTS with the Signal Processing and Statistical and Machine Learning Toolboxes installed. EEGLab (v2022.0) 24 with the plugins bids-matlab-tools (v6.1), bva-io (v1.7), firfilt, (v2.4), cleanLine (v2.0), ICLabel (v1.3), clean_rawdata (v2.6) and dipfilt (v4.3) were installed and used for preprocessing. FieldTrip (revision ee916f5e5) 25 was used for source reconstruction and EEG feature extraction, and the Brain Connectivity Toolbox (version 03 2019) 45 was used for network analysis."
10.1038/s41597-023-02526-z,Codes used in MATLAB 2020a and RStudio to deal with the dataset and plot the figures are available at available at National Tibetan Plateau/Third Pole Environment Data Center 55 .
10.1038/s41597-023-02528-x,"The BCCAQ code used to downscale the CMIP6 GCMs can be found at the Pacific Climate Impacts Consortium (PCIC, https://pacificclimate.org/resources/software-library ) page and on the R Package Documentation ( https://rdrr.io/cran/ClimDown/ )."
10.1038/s41597-023-02517-0,"For this study, no custom code was generated."
10.1038/s41597-023-02477-5,"The following program versions have been used in this study: pymatgen 2022.11.7, custodian 2023.3.10, atomate 1.0.3, LOBSTER 4.1.0, and VASP 5.4.4 for VASP and LOBSTER computations using the workflow. For data validation and processing, we have used pymatgen 2023.6.23 and LobsterPy 0.2.9. All the scripts used in this study, from starting the workflow, generating data records, reproducing technical validation plots, and ML model evaluations, can be accessed here: https://github.com/naik-aakash/lobster-database-paper-analysis-scripts ( https://doi.org/10.5281/zenodo.8172527 )."
10.1038/s41597-023-02531-2,The core Python-based BASE data-processing pipeline code is available under a modified BSD license at https://github.com/AMF-FLX/AMF-BASE-QAQC . The R-based code for generating the article’s figures is available at https://doi.org/10.5281/zenodo.8250754 .
10.1038/s41597-023-02493-5,The command script for MirDeep2 and downstream analysis code written by R are available at the GitHub repository https://github.com/Andelyu/hiPSCs_exosomal_miRNA_project .
10.1038/s41597-023-02443-1,"Script merge_monomers.py, used to create the Psi4 input files that were used for SAPT0 energy calculations, is available as part of the figshare repository containing the Splinter dataset 30 ."
10.1038/s41597-023-02524-1,Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git or https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.
10.1038/s41597-023-02495-3,"Scripts, notebooks and modules to generate metadata in several formats following FAIR principles for marine robotic data is available on GitHub ( https://github.com/CorradoMotta/FAIR-Data-in-Marine-Robotics ), under the GNU General Public License v3.0. The dedicated GitHub page 30 of the project supports the understanding and usage of the codes."
10.1038/s41597-023-02530-3,"The data used to clean the raw data is openly available at https://doi.org/10.17605/OSF.IO/WPEH6 16 . We processed the raw JSON files created by the game using jq and PostgreSQL. We then used R 31 to post-process those files, including replacing the (already hashed) player IDs with new IDs to prevent reidentification of players by FuturLab Ltd."
10.1038/s41597-023-02520-5,No custom code was used during the compilation of the dataset.
10.1038/s41597-023-02412-8,Matlab code was used to generate regularization and statistics. RawGraphs was used to realize Fig. 2 (see Mauri et al . 52 ).
10.1038/s41597-023-02472-w,"The script for the pre- and post-data imputation processing was developed in R 4.1.2, and it is available at Figshare (folder ‘AfroBaT imputation procedure 43 )."
10.1038/s41597-023-02489-1,The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at https://doi.org/10.5281/zenodo.7652533 or https://github.com/fzhu2e/paper-pseudoPAGES2k .
10.1038/s41597-023-02533-0,The R code used to identify cell subclusters and profile cell type-specific chromatin accessible regions of the axolotl brain are available online ( https://doi.org/10.6084/m9.figshare.22548400.v7 ) 30 .
10.1038/s41597-023-02536-x,Source code of this project can be accessed at CLSoilMaps Github repository located at https://github.com/diegodinamarca/CLSoilMaps.git .
10.1038/s41597-023-02502-7,"The software used to generate the dataset is openly available either through their respective repositories linked under “Methods”, or for custom scripts, in the code repository of the project: https://bitbucket.org/genomicepidemiology/twiw_utilities/ as well as Supplementary files 1 and 2."
10.1038/s41597-023-02546-9,"No special code was used for this study. All software mentioned in methods could be found in the community. If no detail parameters were mentioned for the software, default parameters were used as suggested by the developer."
10.1038/s41597-023-02541-0,"Sample codes for estimating gridded emissions from hourly FRE and regional OBB emission coefficients, named as “Code for emission estimation”, are available on the online platform figshare alongside the CHOBE datasets 38 ."
10.1038/s41597-023-02534-z,"The DrugMechDB project website is at https://sulab.github.io/DrugMechDB/ . The code to reproduce results, along with curation guidelines, is available in DrugMechDB GitHub repository at https://github.com/SuLab/DrugMechDB/tree/2.0.1 . All relevant files are hosted at https://doi.org/10.5281/zenodo.8139357 23 . Additionally, contributions of curated mechanistic paths can be done by pull request to the file submission.yaml at SuLab/DrugMechDB/blob/main/SubmissionGuide.md ."
10.1038/s41597-023-02375-w,"The code used to process the files, along with some example scripts are available from a physionet repository 37 ."
10.1038/s41597-023-02535-y,"The HiTIC-NCP dataset generation codes are available on GitHub ( https://github.com/CSLixiang/HiTIC-NCP.git ), and operational under Python 3.8 or JavaScript. In the GitHub repository, we uploaded three code scripts, i.e., “Data preprocessing code.py”, “HiTIC-NCP Code.py” and “Figures code.py”. Additionally, the data samples were uploaded to the “Data Samples” folder."
10.1038/s41597-023-02537-w,"The executables and instructions for d2ome+ software are available on GitHub, https://github.com/rgsadygov/d2ome/releases/tag/v1.05 . The retention time alignment software and its instructions are available on GitHub, https://github.com/rgsadygov/ChromatographicAlignment/releases/tag/v1.0.0 ."
10.1038/s41597-023-02552-x,"All of the programs we use in this study for analysis are based on open data sets. The version information of the open program and its official website: 1. FastQC (v0.11.9) https://github.com/s-andrews/FastQC . 2. CellRanger (v3.1,0) https://github.com/10XGenomics/cellranger . 3. Seurat (v3.1.4) https://satijalab.org/seurat . 4. ClusterProfiler (v4.6.2) https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html . 5. Monocle2 (v2.4.0) http://cole-trapnell-lab.github.io/monocle-release ."
10.1038/s41597-023-02410-w,"The ChoCo dataset and Knowledge Graph, together with the ontological ecosystem and code, are publicly available from several repositories (c.f. Table 2 ). As detailed in Methods, ChoCo is currently released in 2 modalities: • As a JAMS dataset, where audio and score annotations are distinguished by the type attribute in their Sandbox ; and temporal/metrical information is expressed in seconds (for audio) and measure:beat (for scores) ; • As a Knowledge Graph, based on our JAMS ontology to model music annotations, and on the Chord and Roman ontologies to semantically describe chords; Table 2 also provides links to a live SPARQL endpoint. We have implemented a number of actions to ensure that these outputs are in compliance with the FAIR Guiding Principles for scientific data management and stewardship 21 . A GitHub repository hosts data, code, and instructions ( https://github.com/smashub/choco ), to fully reproduce the corpus creation from the original collections. To improve reproducibility, the repository also provides a Docker image for the project (platform agnostic). To improve data consistency, both the latest versions of ChoCo (JAMS file and RDF triples) are available on Zenodo, in synchronisation with GitHub releases. Via GitHub and Zenodo, the ChoCo project has a unique and persistent identifier and is registered in a searchable source. Additionally, via our integration framework, ChoCo contains fine-grained provenance descriptions that allow to keep track of the original source of each harmonic annotation – both in terms of annotators (the person who contributed the harmonic analysis) and data curator (the maintainer of the original collection). Finally, to comply with the original collections, all data and code in ChoCo is released under the Creative Commons Attribution 4.0 licence ( CC-BY 4.0 ), with the exception of the JAAH, CASD, and Mozart Piano Sonata subsets – which follow the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 international licence ( CC-BY-NC-SA 4.0 ). This required an in-depth analysis of the licensing policies of the integrated collections (see Table 8 ). Indeed, for 7 collections, we could not find any specific licensing information from related scientific articles, technical reports, online resources, repositories, dataset metadata, and so forth. For these cases, the authors of these collections were contacted and confirmed whether the use of the CC-BY 4.0 licence – on our derivative integration work – was compatible with their original releasing strategies."
10.1038/s41597-023-02544-x,"The code used for generating building point clouds is available at https://github.com/kdmayer/PointER . The repository includes a detailed description of software and python packages used, as well as their versions."
10.1038/s41597-023-02532-1,The R code used to prepare the survey and produce the tables and figures presented here is available at https://github.com/michaelboissonneault/quantmigsurveycode-eurostatdata .
10.1038/s41597-023-02508-1,All procedures and workflows employed in data processing adhered to the guidelines and protocols outlined in the respective bioinformatics software manuals. This study did not involve the development of any specialized code.
10.1038/s41597-023-02539-8,"The native resolution images were provided in tile format (either 16,384 by 16,384 pixels or 15,181 by 16,384 pixels in size) by Maxar Technologies. Creating the HD tiles from the native resolution tiles was accomplished with the use of Maxar Technologies’ proprietary HD technology. Analyzing the HD image tiles and labeling individual solar panel objects was completed using QGIS version 3.20.0-Odense. QGIS is a free, open-source software and supports object labeling through various methods. Labeling for this dataset was completed using the “Add Polygon Feature,” “Multipart to Single Parts,” and “Bounding Boxes” tools, where the output of each component was the input of the next. The final GeoJSON file for each tile was obtained by exporting the output of the “Bounding Boxes” tool. Converting the image tiles and associated label GeoJSON files to image chips and associated label text files was completed using open-source code provided with the YOLTv4 architecture 26 . Documentation for additional processing of the image chips and labels, as well as for training a YOLTv4 model using a dataset structured this way, can be found in the GitHub repository referenced in the “Background & Summary” section."
10.1038/s41597-023-02543-y,No custom algorithms have been developed in this study. Reference code related to the main analyses performed is available at https://github.com/mauricallari/MAPT
10.1038/s41597-023-02527-y,"CZI and MRXS files can be opened with ZEN (blue edition; RRID:SCR_013672) or Pannoramic Viewer (3DHISTECH Ltd; RRID:SCR_014424) respectively, giving access to image manipulation, measurements, exports and more. CZI/MRXS were exported to LZW-compressed TIFF, a highly flexible and platform-independent format supported by and compatible with a wide range of image processing applications and software. Nutil 44 (RRID: SCR_017183) was used to export TIFF files to PNG, in addition to the processing of these files. The QuickNII 45 (RRID:SCR_016854) and VisuAlign (RRID:SCR_017978) software was used to register images to the reference atlas, using the PNG files as input. Atlas registration software as well as the WHS rat brain atlas v4 35 , 36 , 37 , 38 (RRID:SCR_017124) reference atlas volume and delineations are shared on NITRC ( www.nitrc.org ). The EBRAINS LocaliZoom image viewer software (RRID:SCR_023481) is developed and hosted by the Neural Systems Laboratory at the University of Oslo, Norway."
10.1038/s41597-023-02538-9,The code for creating the benchmark from raw data (including the change masks and relevance masks) and for evaluating the SR outcome is available at https://codeocean.com/capsule/8131193/tree/v2 . The code is documented and accompanied with usage examples.
10.1038/s41597-023-02545-w,No custom code has been used to process the data. The search engine used to match MS/MS spectra to peptide sequences was Mascot Daemon 2.6.1 version (Matrix Science).
10.1038/s41597-023-02554-9,"The VIDIMU dataset 26 ( https://doi.org/10.5281/zenodo.7681316 ) was built using the free tools BodyTrack (v0.8) and OpenSim (v4.4). The VIDIMU-TOOLS code contains the Jupyter notebooks and Python scripts used for data conversion, data synchronization and checking the contents of the dataset to ensure its integrity. A first release of the VIDIMU-TOOLS project is accessible in Zenodo 36 ( https://doi.org/10.5281/zenodo.7693096 ) and the latest version of the code can be found in GitHub ( https://github.com/twyncoder/vidimu-tools )."
10.1038/s41597-023-02547-8,"The basic algorithm code developed for peak detection is available at the general-purpose open repository figshare 26 . The aim of the algorithm is to achieve a tool able to provide an overview of the richness of the dataset content and its potentiality. Otherwise, to provide a robust and detailed quantitative analysis of the different vocalizations included in the dataset is beyond the purposes of the present data descriptor."
10.1038/s41597-023-02448-w,Relevant code and detailed instructions associated with this manuscript can be found in the figshare repository ( https://doi.org/10.6084/m9.figshare.24116484 ) and GitHub repository ( https://github.com/enliten/ENLITEN-Grid-Econ-Data ).
10.1038/s41597-023-02511-6,"ChemDataExtractor 2.2 is available at https://github.com/CambridgeMolecularEngineering/chemdataextractor2 , and the automatically generated dependency parser, and the files used to specify the knowledge representation have been made open source and are available on Figshare 9 ."
10.1038/s41597-023-02556-7,"FastQC (v0.10.1) was used to check the quality of the raw data. No other code or software was used, as the original sequences were submitted."
10.1038/s41597-023-02551-y,The MATLAB code for parsing the JSON file and processing the coordinates can be found at https://doi.org/10.5281/zenodo.8185369 .
10.1038/s41597-023-02559-4,"Version 1.0 of the water-depth-estimation code used for calculating FwDET is available under GPLv3 licensing at https://github.com/csiro-hydroinformatics/water-depth-estimation . The repository also contains a Jupyter notebook (notebooks/example_water_depth.ipynb), which is useful for exploring the water depth outputs."
10.1038/s41597-023-02560-x,The profiles are available in version 1.0 on the FHIR collaboration and publishing platform Simplifier: https://simplifier.net/medizininformatik-initiative-modul-mikrobiologie and in the Figshare repository https://doi.org/10.6084/m9.figshare.22799042 82 .
10.1038/s41597-023-02562-9,"The following open access software were used for quality control and data analysis as described in the main text: CASAVA, version 1.8.2, was used to convert raw image files into sequenced reads. HISAT2, version 2.0.5, was used to map reads to the reference genome. ( http://daehwankimlab.github.io/hisat2/ ). StringTie, version 1.3.1, was used to assemble transcripts and calculate the numbers of transcripts mapped to the transcripts of database. ( https://www.cnblogs.com/raisok/p/11046403.html ). Cuffcompare, version 2.2.1, was used to align transcripts to annotated database. CNCI; CPC2, version 3.2.0; Pfam scan, version 1.3, were used to predict coding potential. EdgeR R package, version 3.12.1, was used to perform differential expression analysis of two conditions/groups. ClusterProfiler, version 4.6.2, was used to perform GO and KEGG pathway analysis. ( http://www.bioconductor.org/packages/release/bioc/html/clusterProfiler.html )."
10.1038/s41597-023-02513-4,All software used in the analysis have been described in the paper and are freely available. Custom code for the analysis and the web application is available on GitHub at https://github.com/CMD-Oxford/targetage-pipeline and https://github.com/CMD-Oxford/TargetAgeApp .
10.1038/s41597-023-02573-6,"The classification of maize for each province in this study was performed on the local computer. The codes used is written in Python, Fortran, and Julia which are available from https://github.com/Pengqy97/TWDTW_codes ."
10.1038/s41597-023-02561-w,"The bioinformatics analysis software used in this study was analyzed using the standard parameters provided by the software developers. If manually adjusted parameters were used, the software version and method used are described in the Methods."
10.1038/s41597-023-02577-2,The authors declare no custom code has been used.
10.1038/s41597-023-02569-2,"All our simulations are made using version 0.14.1 of the SDV library ( https://sdv.dev ). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table 1 ) and a secondary table with transactions (see Table 2 ). A demonstration by the SDV developers is available online ( https://sdv.dev/SDV/user_guides/relational/hma1 ; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library ( https://scikit-learn.org ) and version 3.3.3 of the LightGBM library ( https://lightgbm.readthedocs.io/en/stable )."
10.1038/s41597-023-02571-8,"All pipeline and software used in this study were performed to data analysis according to the manuals and protocols. The parameters and the version of the software are described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used."
10.1038/s41597-023-02485-5,"Code and workflow for running the historical simulation is available at https://github.com/IMMM-SFA/jones-etal_2023_scidata . Model versions, input datasets, and parameters are outlined within."
10.1038/s41597-023-02550-z,The R code used in the analysis of the data is available in GitHub ( https://github.com/GaoYueWorkspace/ICP-related-lncRNAs/ ) or figshare 80 .
10.1038/s41597-023-02568-3,All the codes used to clean the raw datasets have been uploaded to publicly available on the OSF data repository 34 ( https://doi.org/10.17605/OSF.IO/D465N ). The data can be analyzed using software such as R or MATLAB.
10.1038/s41597-023-02579-0,"The code written and used for the data collection is available in the repositories specified in the references. Furthermore, we provide an Excel sheet to aid users to automatically create SPSS syntax to add new variables to the dataset. Programs used, including versions, are outlined in the repository descriptions."
10.1038/s41597-023-02512-5,The source codes used in this work are accessible on GitHub ( https://github.com/GuangyuGuoer/scMF-across-AD-and-stroke ).
10.1038/s41597-023-02578-1,Urbanity Python package source code is hosted under an open source MIT license ( https://github.com/winstonyym/urbanity ). Urbanity dashboard is generated with Dash version 2.7.1. with open source code ( https://github.com/winstonyym/urbdash ).
10.1038/s41597-023-02575-4,The code used to develop this work can be accessed through the following link: - Code for data cleaning and analysis is provided as part of the replication package. It is available at: https://zenodo.org/badge/latestdoi/93561048
10.1038/s41597-023-02581-6,"During development of the ICEHG DS, the generic WFDB (WaweForm Software Package) software package ( https://www.physionet.org/content/wfdb/ ), and WFDB for MATLAB and Octave ( https://www.physionet.org/content/wfdb-matlab/ ), of the PhysioNet’s open-source repository were used."
10.1038/s41597-023-02564-7,"The dataset can be used without any further code. All the code used for the calibration, simulated sequences generation and technical validation are publicly available as repositories at https://github.com/Endomapper . The instructions for installing and using them are available in Synapse https://www.synapse.org/#!Synapse:syn52137895 ."
10.1038/s41597-023-02570-9,"Custom code was used to process the data. For the GPR Data we used MATLAB version: 9.13. 0 (R2022b) to run the codes. The root image processing and soil sensor data is run with Python 3.10.10. Processing codes for the roots images can be found in the Supporting Material for Bauer et al . at https://doi.org/10.34731/pbn7-8g89 . The soil water content data measured with the FDR device was processed using R version 4.0.2. The custom codes can not be made publicly accessable due to copyright issues, but are available upon request, by contacting the corresponding or senior author."
10.1038/s41597-023-02580-7,We stored the CDM data using open-source codes of OHDSI for conforming to the database structure of OMOP CDM ( https://github.com/OHDSI/CommonDataModel ).
10.1038/s41597-023-02540-1,"The NIS-E lements V iewer software (v. 4.11.0, www.microscope.healthcare.nikon.com ) was used to automatically equalize the videos (A uto S cale LUTs tool) and extract individual frames (see ALFI dataset), that were converted to the PNG format using the F iji 22 ( https://fiji.sc ) distribution of I mage J 23 ( https://imagej.nih.gov/ij ). The ilastik tool (v. 1.3.3, www.ilastik.org ) was used to produce initial semantic segmentations of interphase nuclei and mitotic cells for sequences MI01-MI08 (see ALFI annotations for Task 1 ) using the P ixel C lassification workflow. The workflow performed the following steps: (1) load the selected input image sequence; (2) define three user-driven classes: mitosis , interphase , background ; (3) define pixel filters, based on color, intensity, and texture, as image features at different scales; (4) learn examples for each class interactively by user-defined brushstrokes with classes colors on one or more selected input images; (5) classify image pixels for the whole sequence using output filters, and a Random Forest classifier 26 ; (6) correct pixel classification interactively repeating steps 4-5); (7) save the output. M atlab (v. R2020a, www.mathworks.com ) was used to (1) refine the initial masks (see ALFI annotations for Task 1 ) using the V ideo L abeler App; (2) extract the bounding boxes from the masks (see ALFI annotations for Task 1 ) using the regionprops function; (3) associate the bounding boxes of the same object in adjacent frames based on motion, estimated by a Kalman filter (see ALFI annotations for Task 1 and Task 2 ); and (4) produce the use examples (see Usage Notes). The bounding box annotations have been produced using the makesense.ai online tool 25 ( https://github.com/SkalskiP/make-sense )."
10.1038/s41597-023-02601-5,"The data analyses were performed according to the manuals and protocols by the developers of corresponding bioinformatics tools and all software, and codes used in this work are publicly available, with corresponding versions indicated in Methods."
10.1038/s41597-023-02583-4,"All code is hosted freely and open-source on a GitHub repository, accessible via https://github.com/bidsconvertr/bidsconvertr . The documentation is hosted on the GitHub page: https://bidsconvertr.github.io ."
10.1038/s41597-023-02596-z,A collection of Java scripts for the analysis of our datasets have been made available on figshare with no usage restrictions 29 .
10.1038/s41597-023-02498-0,No custom code has been used in the generation of the dataset.
10.1038/s41597-023-02595-0,"All software used in this study is open access and parameters were clearly described in the Methods section. If no detailed parameters of the software are mentioned, the default parameters suggested by the developer were applied. A comprehensive list of software used in this study is provided in this section as well. SMRT Link (v9.0) was used to trim reads of Iso-seq data: https://www.pacb.com/support/software-downloads IsoSeq3 (v3.4.0) was used to cluster isoforms of trimmed reads for Iso-seq data: https://github.com/PacificBiosciences/IsoSeq cDNA_Cupcake (v29.0.0) was used to collapse redundant isoforms: https://github.com/Magdoll/cDNA_Cupcake PacBio_pbIsoCollapse was developed by this study to further remove redundant isoforms: https://github.com/lhuang3s/PacBio_pbIsoCollapse GMAP (v2018-07-04) was used to align the full-length isoforms of Iso-seq to the Sus scrofa reference genome: https://bioconda.github.io/recipes/gmap/README.html SQANTI3 (v5.0) was used to compare the Iso-seq assembly transcriptome with the Ensembl transcriptome: https://github.com/ConesaLab/SQANTI3 vcftools (v0.1.16) was used to calculate the density of SNP: https://vcftools.sourceforge.net ORFfinder (v0.4.3) was used to translate RNA sequences to protein sequences: https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/ORFfinder pfam_scan (v1.6) was used to search domain against a library of Pfam HMMs: https://bioconda.github.io/recipes/pfam_scan/README.html CPC2 and CPAT (v3.0.4) were used to evaluate the coding potential of novel isoforms: http://cpc2.gao-lab.org ; https://cpat.readthedocs.io/en/latest diamond (v2.0.15.153) was used to align the novel isoforms’ sequence to NR and UniProt: https://github.com/bbuchfink/diamond cutadapt (v3.0) was used to trim reads of paired-end RNA-seq and single-end small RNA-seq data: https://cutadapt.readthedocs.io/en/stable STAR (v2.7.10a) was used to map paired-end sequence reads to the Sus scrofa reference genome: https://github.com/alexdobin/STAR SAMtools (v1.6) was used to sort and build an index for short-reads align BAM files: https://github.com/samtools/samtools featureCounts (v2.0.1) was used to calculate the gene counts: https://subread.sourceforge.net/featureCounts.html RseQC (v5.0.1) was used to evaluate the sequence quality: https://rseqc.sourceforge.net salmon (v1.9.0) was used to calculate the gene expression and transcript expression: https://combine-lab.github.io/salmon SUPPA2 (v2.3) was used to analyze alternative splicing and differential alternative splicing: https://github.com/comprna/SUPPA seqkit (v2.1.0) was used to calculate the GC content of small RNA-seq data: https://github.com/shenwei356/seqkit bowtie (v1.3.1) was used to align miRNA reads to pig genome: https://bowtie-bio.sourceforge.net/index.shtml miRDeep2 (v0.1.3) was used to predict potential miRNA: https://github.com/rajewsky-lab/mirdeep2 scatterplot3d (v0.3-42) was used to plot the 3D PCA map: https://cran.r-project.org/web/packages/scatterplot3d/index.html Other maps were plotted by ggplot2 (v3.4.0): https://ggplot2.tidyverse.org Custom scripts to handle genomic annotations: https://github.com/sunyumail93/Bed12Processing The pipeline code for Iso-seq: https://github.com/zhipengliu92/PipIsoseq The pipeline code for RNA-seq: https://github.com/sunyumail93/PipeRNAseq The pipeline code for small RNA-seq: https://github.com/sunyumail93/PipeSmRNAseq"
10.1038/s41597-023-02588-z,All code used in this study is freely available at Figshare https://doi.org/10.6084/m9.figshare.23761401 32 and GitHub https://github.com/clydeandforth/RNA_seq_EAB .
10.1038/s41597-023-02558-5,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-023-02553-w,All of the products of this manuscript as well as the parameterized code pipelines are free and openly available. Please see “Usage Notes.”
10.1038/s41597-023-02574-5,No custom code was used to generate or process the data described in this article.
10.1038/s41597-023-02542-z,"The code used to cluster the time series is not publicly available because, in the absence of input data, it can not be executed. However, special attention has been paid to provide a detailed description of the clustering approach for transparency in this article. In addition, the source code used in R 29 to perform the data analysis is provided with the ELMAS dataset. All scripts have been tested working as of 19/03/2023 on a machine running Windows 10, and using R version 4.1.0 (2021-05-18). The required packages to run the scripts are detailed in the code, and the purpose of each script is defined in its header."
10.1038/s41597-023-02587-0,The codes used to generate the dataset are available at https://zenodo.org/record/7843730#.ZEIxAC0Rq3I 81 .
10.1038/s41597-023-02572-7,"Codes for the raw text processing, model training, and statistics generation are available on our project GitHub ( https://github.com/MIT-SUL-Team/Twitter-Sentiment-Geographical-Index )."
10.1038/s41597-023-02600-6,"The code used to collect, store, filter, and synchronize the data is not published, as it can only be used with the raw data recorded on the specific prototype vehicles, which contains partly proprietary data. As large portions of the code deal with engineering challenges, such as translating data between formats used in different programming languages, ensuring compatibility between software versions, and performing operations in our custom log data base, we do not expect it to be interesting to the readers or useful for any other applications. Instead we directly explain the relevant processing and filtering steps in the respective sections above. As mentioned above, no specific code is necessary to load or interpret the ZTBus dataset 9 . However, for convenience, the sample Matlab code provided allows to load some parts of the data and recreate most of the figures shown in this manuscript. The code has been developed with Matlab version 9.12 (R2022a) and does not require any specialized toolboxes. It is distributed under the GNU General Public License version 3 (GPLv3) alongside the ZTBus dataset 9 ."
10.1038/s41597-023-02584-3,The GEE codes used to generate the cropland and cotton maps are available to the public at https://zenodo.org/record/7856467 64 .
10.1038/s41597-023-02605-1,The computer code and instructions be obtained through a public repository at https://github.com/khuonghtran/SSMM . Examples of input data for the code can be obtained from https://openprairie.sdstate.edu/global_land_surface_season_data/5/ .
10.1038/s41597-023-02555-8,"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at https://doi.org/10.57745/QEZBNA 24 ."
10.1038/s41597-023-02586-1,"The pollution detection algorithm described in Beck et al . 80 to identify and flag periods of primary polluted data is available on Zenodo ( https://doi.org/10.5281/zenodo.5761101 ). High Resolution ToF-AMS Analysis guide from J. L. Jimenez research group’s wiki (CIRES, University of Colorado at Boulder, USA): https://cires1.colorado.edu/jimenez-group/wiki/index.php/High_Resolution_ToF-AMS_Analysis_Guide (last accessed: 03/03/2022)."
10.1038/s41597-023-02611-3,The R code used in the analysis of the data is available on figshare ( https://doi.org/10.6084/m9.figshare.22292479 ).
10.1038/s41597-023-02529-w,"The algorithms and formulas used in this study have been previously provided. This research used three programmatic free and open-source platforms: (1) R Statistical Software and Programming Language; (2) Quantum GIS (QGIS) software; and (3) Python software. A range of R Packages for geospatial big data analytics used in this research are presented in Bivand 33 . QGIS is used for data exploration purposes because of its features of viewing, editing, and analysing geospatial data 34 . Python is the development programmatic environment for the MUSE model 35 . The MUSE-RASA model has been built from the integration of the R-based geospatial RASA model with a Python-based MUSE model to end with the MUSE-RASA model. The R code used to create the shape files in the RASA model is available upon request with proper justification from the corresponding author. The MUSE model is an open source code available in Giarola, et al . 36 . Due to sponsorship agreements, the authors are not allowed to make the RASA code publicly available."
10.1038/s41597-023-02609-x,All analytical code used for processing and technical validation is available on the Benayoun Laboratory GitHub repository ( https://github.com/BenayounLaboratory/Killifish_RNASeq_2023 ). The provided R code was run and tested on R v4.3.0.
10.1038/s41597-023-02592-3,The R code used in this study is provided at Figshare and Github ( https://github.com/Lee-June-Sung/Co-expression_network_using_DGCA.git ).
10.1038/s41597-023-02607-z,The inorganic nitrogen deposition database1.0 describes the data is available at Figshare data record ( https://doi.org/10.6084/m9.figshare.24057120.v3 ) 20 .
10.1038/s41597-023-02599-w,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] Spatial macrolocalisation model ; [2] Cement production types and capacity estimation models ; [3] Sentinel-2 cement assets deployment models .
10.1038/s41597-023-02590-5,"Code to perform PCA, generate heat maps and volcano plots, and carry out the transcriptome differential expression analysis is publicly available under repository https://github.com/alanlorenzetti/protDynContGenExp_v2 ."
10.1038/s41597-023-02594-1,"The versions, settings and options of software tools used in this work are described below: (1) Hifiasm: v0.12, default parameters; (2) CEGMA: v2.5, default parameters; (3) BUSCO: v5, default parameters; (4) HiC-Pro: v2.10.0, default parameters; (5) BWA: 0.7.10-r789, default parameters; (6) LACHESIS, parameters: CLUSTER_MIN_RE_SITES = 78 CLUSTER_MAX_LINK_DENSITY = 2 ORDER_MIN_N_RES_IN_TRUNK = 15 ORDER_MIN_N_RES_IN_SHREDS = 15; (7) Circlize: 0.4.10, default parameters; (8) Diamond v0.9.29.130, default parameters; (9) MCScanX, default parameters; (10) JCVI: v0.9.13, default parameters; (11) VGSC: v2.0, default parameters; (12) RepeatModeler2: v2.0.1, default parameters; (13) RECON: v1.0.8, default parameters; (14) RepeatScout: v1.0.6, default parameters; (15) LTR_retriever: v2.8, default parameters; (16) LTRharvest: v1.5.9, default parameters; (17) LTR_FINDER: v1.1, default parameters; (18) RepeatMasker: v4.1.0, default parameters; (19) MISA: v2.1, default parameters; (20) TRF: v409, parameters:1 1 2 80 5 200 2000 –d -h; (21) Augustus: v2.4, default parameters; (22) SNAP: v2006-07-28, default parameters; (23) GeMoMa: v1.7, default parameters; (24) Hisat: v2.0.4, default parameters; (25) Stringtie: v1.2.3, default parameters; (26) GeneMarkS-T: v5.1, default parameters; (27) Trinity: v2.11, default parameters; (28) PASA: v2.0.2, default parameters; (29) EVM: v1.1.1, default parameters; (30) EggNOG-mapper: v2, default parameters; (31) tRNAscan-SE: v1.3.1, default parameters; (32) Barrnap v0.9, default parameters; (33) Infenal v1.1, default parameters; (34) GenBlastA: v1.0.4, default parameters; (35) GeneWise: v2.4.1, default parameters; (36) InterProScan: v5.34-73.0, default parameters; (37) Subphaser: v1.2, parameters: -q 90; (38) SyRi: v1.5, default parameters; (39) Plotsr: v1.0.0, default parameters; No customized code was developed by the authors."
10.1038/s41597-023-02602-4,The R code scripts and dataset are available on ‘Figshare’ for reproducibility 54 . The author’s GitHub online repository will be continuously updated to ensure sustainable usage of these codes( https://github.com/Gi-Seop/STK ). Tested system All codes were tested in the following system environment (Table 4 ).
10.1038/s41597-023-02604-2,The different available software and the versions used to acquire and process data presented in the dataset are summarized in Table 7 .
10.1038/s41597-023-02603-3,"The repository of preprocessing and quality assessment codes for FLS learning curve study, expert-novice, and neuro-modulation studies have been made public and can be assessed at Figshare 34 ."
10.1038/s41597-023-02624-y,"All software used in this study species are in the public domain, except for those explicitly described in the text and methods. No custom scripts or code was used during the curation and validation of the dataset in this study. Also the following software tools were used: tRNA sequences of cytoplasmic were downloaded from GtRNAdb 32 , 33 . tRNA sequences of mitochondrial were predicted with tRNAscan-SE 34 , 35 software. Principal Component Analysis (PCA), Correlation Analysis, Pie plots, Venn plots, and radar plots were performed in the online analysis tool Bioinformatics ( www.bioinformatics.com ). The seed sequence motif was done using the BioLadder online tool ( www.bioladder.cn/ )."
10.1038/s41597-023-02585-2,"The file code is in the figshare repository 23 as “paper_dataset.zip”. They are mainly two folders. The folder “First_step” has the file “Train_Patch_level_model.py”. This file contains the necessary statements to perform patch-level classification for regions of interest and non-interest. In the folder “Second_step,” three files are located. The file “WSI_prediction_MIL” contains the main functionalities for training an algorithm based on Multiple Instance Learning (MIL). This script calls several functions stored in the “DataGenerator” and “utils_1.” files. The file “DataGenerator” holds the necessary functions for loading data in the form of bags required for a Multiple Instance Learning algorithm. In the “utils_1” folder, all the necessary functions for extracting metrics, calculating loss, and more can be accessed. These functions are utilized in the “WSI_prediction_MIL” file."
10.1038/s41597-023-02610-4,This work did not utilise a custom script. Data processing was carried out using the protocols and manuals of the relevant bioinformatics software.
10.1038/s41597-023-02593-2,The parameters of the software were default. No specific script was used in this work.
10.1038/s41597-023-02597-y,The processing of gridded data is carried out in ArcGis10.6. The dataset for dietary MeHg exposure in the GBA at 1 km × 1 km scale is available in the open-access online dataset Zenodo 48 .
10.1038/s41597-023-02606-0,"Prisma Flow Diagram ( http://www.prisma-statement.org/documents/PRISMA_2020_flow_diagram_new_SRs_v2.docx ) was used to generate Fig. 1 . The statistical software R (version 4.1.2, 2021-11-01-“Bird Hippie” Copyright © 2021. The R Foundation for Statistical Computing) was used to produce Figs. 2 – 5 . R scripts used to produce figures are openly available on the figshare repository."
10.1038/s41597-023-02622-0,"All software used, with versions and non-default parameters, is described precisely and referenced in the method section to ensure easy access and reproducibility. For further transparency, the complete set of codes employed throughout the bioinformatics workflow have been uploaded to a GitHub repository at https://github.com/Meora-Rajeev/Biofloc-Metagenomics 50 ."
10.1038/s41597-023-02567-4,"Data and MATLAB scripts are available in figshare 46 . Further, CSV files of the MATLAB variables and Python scripts to read the MAT files directly are also available in figshare 46"
10.1038/s41597-023-02589-y,"The codes for how we generate the trajectory dataset based on visual embedded traffic camera records, evaluate the vehicle Re-ID and trajectory recovery metrics, and report statistical characteristics are available in our GitHub repository 49 . There are also tips for installing Python requirements."
10.1038/s41597-023-02621-1,The code described in the usage notes is available on GitHub ( https://github.com/cedricgilon/iridia-af ). It includes utils tool and example code to start using IRIDIA-AF database.
10.1038/s41597-023-02640-y,"Each CSV file has a large amount of data. Programming languages (such as Python) for data visualization and manipulation are recommended for data exploration and analysis. The data preprocessing codes are written in Python on Jupyter Notebook, which can be found at “ https://github.com/weijiaze/Scientific-data/blob/master/imputefile.ipynb . The code runs on a Windows computer using Python 3. The Python 3 codes to reproduce the examples provided in this article can be found at https://github.com/weijiaze/Scientific-data ."
10.1038/s41597-023-02630-0,"Python scripts for exploratory data analysis and dataset comparison, as well as supplementary data, are publicly available at https://github.com/piashiba/HIBASkinLesionsDataset ."
10.1038/s41597-023-02631-z,"All software used for data processing were executed following the manual of the bioinformatic software cited above, and all commands used to assemble the genome are available in figshare 68 . If no detailed parameters are described for the software, the default parameters were used. Additionally, R codes used for figure construction are also available in figshare 68 ."
10.1038/s41597-023-02608-y,The code and detailed documentation on how to use it to reproduce the results presented in this study is publicly available at https://github.com/esla/trypanosome_parasite_detection under the permissive Berkeley Software Distribution (BSD) 3-Clause license.
10.1038/s41597-023-02618-w,"Publicly available source codes and manuals for H08 and mizuRoute were downloaded from http://h08.nies.go.jp and https://github.com/ESCOMP/mizuRoute . Model parameters detail for each basin is available in the supplement information. Data processing and plotting were performed using MATLAB and Generic Mapping Tool 6 (GMT 6), whereas QGIS 3.22 was utilized for geospatial analysis. The MATLAB codes used for data processing can be accessed through the GitHub directory ( https://github.com/DIPESHSINGHCHUPHAL/Streamflow-India )."
10.1038/s41597-023-02612-2,"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on https://renkulab.io/gitlab/mltox/adore . The repository contains code on how to load the data, prepare it for modeling, e.g ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder scripts for random forests ( 14_analysis_regression_rf.py and 34_analysis_regression_test_rf.py )."
10.1038/s41597-023-02614-0,"Notes about software discussed in the paper 1. Note that DANDI utilizes the OMERO-Zarr, a software package for efficient storage and retrieval of large microscopy datasets. 2. Note that DataLad datasets are standard git/git-annex repositories, and these tools may be used directly in cases where the DataLad tool is not desired or available. 3. While Jupyter alone is not optimal for use with electrophysiology data, it can be used with other Python libraries such as MNE-Python to load, preprocess, and plot example EEG data in a Jupyter notebook through vscode. 4. For NWB explorer, see http://nwbexplorer.opensourcebrain.org/ for more information. 5. For BioImageSuite/Viewer, see https://bioimagesuiteweb.github.io/webapp/viewer.html for more information. 6. For itk/vtk viewer, see https://kitware.github.io/itk-vtk-viewer/docs/ for more information."
10.1038/s41597-023-02613-1,Code to create Figs. 2 – 4 can be found here: https://github.com/HOPE-UIB-BIO/Global-Food-plants-and-LAPD-HI . Note: Small adjustments for aesthetics were made in an external illustration software.
10.1038/s41597-023-02582-5,The algorithm for the calculation of energy dissipation rates was written in Python. It was developed for the described turbulence records and is freely available on Zenodo 34 .
10.1038/s41597-023-02591-4,MISSING
10.1038/s41597-023-02617-x,The code is available at the repository https://gitlab.ebi.ac.uk/literature-services/public-projects/europepmc-corpus/ and also on Figshare 40 . The scripts include cleaning and formatting the annotations from Hypothes.is platform and generates the dataset in IOB format for input to deep learning algorithms.
10.1038/s41597-023-02623-z,The used scripts to implement our adaptive classification scheme is available in the following link: https://github.com/AminNaboureh/Adaptive_LC_classification.git .
10.1038/s41597-023-02632-y,Not applicable.
10.1038/s41597-023-02620-2,The code for the application and previously mentioned models is shared on GitHub ( https://github.com/hpi-dhc/sonar ).
10.1038/s41597-023-02316-7,"All code for data download and reformatting can be found in the appropriate USGS repository 50 . The nwmTools R package is available on GitHub and the dataset is documented and published via HydroShare 51 . All the data are currently open, and publicly available at this URL: https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/ ."
10.1038/s41597-023-02565-6,"All codes used to process the data sets are publicly available. Here, we list the repositories used for different processing steps following the order of the flow chart (Fig. 2 ). Preprocessing and elevation retrieval (Interactive Data Language, IDL): • Converting POSPac MMS 8 output to separate GNSS and INS files: https://gitlab.awi.de/als-seaice/sea-convert • Retrieving ellipsoidal elevation point clouds from the ALS data: https://gitlab.awi.de/als-seaice/als_level1b_seaice Processing gridded data (python3): • Gridding of ALS point cloud data to regular grid including all subroutines presented in Fig. 2b,c : https://github.com/awi-als-toolbox/awi-als-toolbox 40 • Ice drift correction: https://gitlab.awi.de/floenavi-crs/icedrift • Retrieve position of Polarstern: https://gitlab.awi.de/floenavi-crs/floenavi General processing scripts (bash, python3): • Batch processing scripts and config files with flight-specific parameters https://gitlab.awi.de/als-seaice/mosaic-als-proc"
10.1038/s41597-023-02629-7,"The CE-NBI image data set can be used without any other code for technical and clinical image classification and assessment purposes. However, the algorithms already developed on this data set are available ( https://github.com/NazilaEsmaeili/CE-NBI-Classification ) to compare the performance of the newly developed approaches with the existing methods. The provided codes are available for public access only for research purposes."
10.1038/s41597-023-02615-z,"The code we used to produce thermal gradients with the BOA is available in R on Github thanks to Benjamin Galuardi (available on https://github.com/galuardi/boaR , accessed on May 19th, 2022). The code to produce the backward FSLEs belongs to Ismael Hernández-Carrasco, was extensively described in Hernández-Carrasco et al . 34 , and is available upon request. An example jupyter notebook for front visualization is available on a github repository ( https://github.com/FlorianeSudre/NOMAD_notebooks )."
10.1038/s41597-023-02643-9,"No code is used in this study. Figures were produced using R statistical software 38 and packages 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 ."
10.1038/s41597-023-02638-6,The code used to replicate our index calculations and the creation of all graphics is available at the Harvard Dataverse: https://doi.org/10.7910/DVN/NFSXTR 16 . We provide replication code in R and in STATA for the ease of the user; the files produce identical calculations. We used the R code for group data collection and updates.
10.1038/s41597-023-02566-5,ESRI ArcGIS 10.6™ and later versions were used to process the data and produce the output datasets. No custom code has been used.
10.1038/s41597-023-02549-6,The new Köppen-Geiger classifications have been produced using Python version 3.10. The code can be accessed at https://github.com/hylken/Koppen-Geiger_maps and is licensed under the GNU General Public License v3.0.
10.1038/s41597-023-02651-9,No custom code was developed since the extraction of travel time was done through the Google Maps internal Directions API ( https://developers.google.com/maps/documentation/directions/overview ).
10.1038/s41597-023-02656-4,"The scripts for processing and reading the LESO datasets are accessible on Github ( https://github.com/soonyenju/LESO ) under the MIT license. The tools and libraries, including Python v3.9, Numpy v1.20.3, Xarray v0.19.0, Pandas v1.3.3, Deep Forest v2021.2.1 (DF21), scigeo v0.0.13, and sciml v0.0.5, were used to build the LESO framework for generating datasets of surface O 3 concentrations. The validation of LESO datasets was processed using scitbx v0.0.42 and scikit-learn v0.24.2."
10.1038/s41597-023-02625-x,The code used in this research is available at Figshare repository 39 .
10.1038/s41597-023-02650-w,All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse ( https://doi.org/10.7303/syn50614194 ). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.
10.1038/s41597-023-02619-9,No code was used to generate or process the data presented in this manuscript.
10.1038/s41597-023-02658-2,All software and pipelines were executed according to the manual and protocol of published tools. No custom code was generated for these analyses.
10.1038/s41597-023-02634-w,The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at https://doi.org/10.57760/sciencedb.10422/ .
10.1038/s41597-023-02639-5,"The code that was used to preprocess the raw recorded data such as procedures to fix tag positions, remove range outliers, range offset compensation, removing measurements with A6 in environment2 and creating cleaned-up data set is published as a separate GitHub repository 27 and archived with Zenodo to provide permanent access to a usable instance of code. All code is available under the terms of Apache-2.0 License."
10.1038/s41597-023-02642-w,"A Python code example is provided in the ‘Code’ folder. This example helps the reader to understand how to open hyperspectral images, to extract spectra, or to display the results obtained after a Principal Component Analysis (PCA) applied on the first image and a PCA on all average spectra where one average spectrum is obtained per leaf/image."
10.1038/s41597-023-02648-4,"The ET 0 data are derived using approximately 1,000 lines of code written in bash scripts and NCAR Command  Language (NCL). While not written as a portable library, access to the code is not restricted, and it is available for download from USGS Sciencebase ( https://doi.org/10.5066/P9IIQMV1 )."
10.1038/s41597-023-02635-9,R code and associated shapefiles for biogeographic determinations can be found at https://github.com/seanmkeogh/SHELD_biogeo .
10.1038/s41597-023-02637-7,The computing codes of the GCAM model along with parameter settings are available at https://github.com/JGCRI/gcam-core . The interface software of the PLUS model can be obtained from https://github.com/HPSCIL/Patch-generating_Land_Use_Simulation_Model .
10.1038/s41597-023-02654-6,"SM2RAIN algorithm code is available in python, R, and Matlab on GitHub ( https://github.com/IRPIhydrology/sm2rain )."
10.1038/s41597-023-02647-5,The ASPEN software package and a description of its functionality are available at https://www.eol.ucar.edu/content/aspen .
10.1038/s41597-023-02667-1,"All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatics software. The settings and parameters of these softwares are described below. (1) BWA v0.7.10-r789: aln, default parameters; (2) LACHESIS: CLUSTER_MIN_RE_SITES = 547; CLUSTER_MAX_LINK_DENSITY = 2; CLUSTER_NONINFORMATIVE_RATIO = 2; ORDER_MIN_N_RES_IN_TRUN = 1094; ORDER_MIN_N_RES_IN_SHREDS = 1076; (3) BUSCO v3.0: --evalue 1e-03 (E-value cutoff for BLAST searches), -sp human; (4) LTR_FINDER: default parameters; (5) RepeatScout: default parameters; (6) PASTEClassifier: default parameters; (7) RepeatMasker: -nolow -no_is -norna -engine wublast; (8) Genscan: default parameters; (9) Augustus v2.4: default parameters; (10) GlimmerHMM v3.0.4: default parameters; (11) GeneID v1.4: default parameters; (12) SNAP: version 2006-07-28, default parameters; (13) GeMoMa v1.3.1: default parameters; (14) Hisat v2.0.4: --max-intronlen 20000, --min-intronlen 20; (15) Stringtie v1.2.3: default parameters; (16) TransDecoder v2.0: default parameters; (17) GeneMark-ST v5.1: default parameters; (18) PASA v2.0.2: -align_tools gmap, -maxIntronLen 20000; (19) EVM v1.1.1: default parameters; (20) BLAST V2.2.31: -evalue 1e-5; (21) tRNAscan-SE: default parameters."
10.1038/s41597-023-02626-w,"We provide a full website with vignettes (including a complete and a basic workflow) that demonstrates the functionality of our script with the input data ( https://jbdorey.github.io/BeeBDC/index.html ). Our “basic workflow” is for users that simply wish to download our flagged but unfiltered dataset and apply (i) manual filters based on our filtering columns or further filter to only include certain (ii) countries, (iii) date ranges, or (iv) uncertainty levels. Secondly, we provide these annotated R- scripts run from start to finish on our GitHub ( https://github.com/jbdorey/BeeBDC/tree/main/inst ). Our scripts, related files, and downloaded instructions can be found on our GitHub page ( https://github.com/jbdorey/BeeBDC )."
10.1038/s41597-023-02627-9,Analysis code is deposited on Zenodo 30 and on GitHub at https://github.com/yochannah/subjective-data-models-analysis . Code is shared under a permissive MIT licence.
10.1038/s41597-023-02636-8,"For data processing and generation of data products, we relied on open-source software packages. Specific software versions, when relevant, have been duly specified. The R code used to retrieve data from NCBI Nucleotide, GBIF, EID2, and NHM datasets is available at figshare 92 . Codes for cross-referencing Nucleotide and PubMed, as well as meta-network analysis, are housed in the same repository. GBIF and NCBI Nucleotide data are within the *.RData file, which can be loaded into R using the ‘load’ statement."
10.1038/s41597-023-02649-3,No custom code was generated for this work.
10.1038/s41597-023-02523-2,"No custom code is being published related to the creation of this dataset, however any fields that involve calculations have been documented in the metadata to allow users of the dataset to replicate and evaluate the calculations which were made to produce the dataset."
10.1038/s41597-023-02661-7,Codes for benchmarking scaffolding approaches and constructing a non-redundant non-coding gene annotation are freely available at GitHub ( https://github.com/jkimlab/NCMD_study ). The other programs used in this study were executed following their manuals. The version and used parameters of programs are specified in the Methods section.
10.1038/s41597-023-02668-0,This work did not utilize a custom script. Data processing was carried out using the protocols and manuals of the relevant bioinformatics software.
10.1038/s41597-023-02686-y,A compiled protocol and supplementary files and information are deposited as ‘report.zip’ at Zenodo 22 . The index.html links to the analysis report and compiled R markdown file. The raw code and source annotations are deposited as ‘analysis.zip’ in the same repository.
10.1038/s41597-023-02669-z,The code used to create and analyze the datasets is openly accessible via https://github.com/MCM-Fischer/VSDFullBodyBoneModels and versioned at Zenondo: https://doi.org/10.5281/zenodo.8316730 53 .
10.1038/s41597-023-02688-w,No custom code was utilized in this study. Data processing was performed by relevant pipelines and software according to the manual and protocols and the version as well as useful parameters have been described in the Methods section. The default parameters as developers suggested were used in those pipelines and software of which parameters were not specifically mentioned in this work.
10.1038/s41597-023-02677-z,"All software used in this work is in the public domain and the parameters are clearly described in the Methods section. Where no detailed parameters have been mentioned for a type of software, default parameters were used as suggested by the developer."
10.1038/s41597-023-02665-3,The code utilized for maize mapping is open-source and will be published on GitHub ( https://github.com/lixingang/ChinaMaizeCls ) and along with the maize maps 45 ( https://doi.org/10.6084/m9.figshare.22689751.v17 ).
10.1038/s41597-023-02652-8,"The FAIR EVA source code developed in Python is fully open access ( https://github.com/IFCA-Advanced-Computing/FAIR_eva ), a running instance can be found in fair.csic.es and in full detail at DIGITAL.CSIC 54 [ https://doi.org/10.20350/digitalCSIC/14559 ]."
10.1038/s41597-023-02664-4,The scanning sequence for HCP D/A is available on request from the Connectome Coordination Facility ( https://nda.nih.gov/ccf/ ). The data processing used the openly accessible HCP pipeline version 4.3.0 ( https://github.com/Washington-University/HCPpipelines/releases/tag/v4.3.0 ) and Freesurfer version 6.0 ( https://surfer.nmr.mgh.harvard.edu/pub/dist/freesurfer/ ) run on the connectome workbench version 1.4.2 ( https://www.humanconnectome.org/software/get-connectome-workbench ).
10.1038/s41597-023-02666-2,"The dataset and the raw data are hosted in Zenodo https://doi.org/10.5281/zenodo.8342596 under the CC0 license 36 . All the code for reproducing the experimental protocol, the building and preprocessing of the dataset, and the use of the baseline model are available in the repository https://github.com/soundclim/anuraset under the MIT license. We open the Python code to fast development of new deep learning models and experiments in Pytorch."
10.1038/s41597-023-02646-6,"The audio classifier and the algorithm for extracting breathing features are available in a public repository ( https://github.com/smarty4covid/smarty4covid.git ). Furthermore, the repository includes the weights of the CNNs used by the classifier and a script for generating triples from the available data for the purpose of customizing the smarty4covid OWL knowledge base."
10.1038/s41597-023-02675-1,No novel code was used in the construction of the OIMHS dataset.
10.1038/s41597-023-02674-2,All code used to analyse and visualise this data set has been deposited in the GitHub repository https://github.com/SKBuddenborg/Smansoni_rnaseq and is archived at Zenodo ( https://doi.org/10.5281/zenodo.7849283 ) 38 .
10.1038/s41597-023-02672-4,"The source codes used for data generation and initial image processing for melt-pool detection and geometrical analysis are also available along with the data. These codes can serve as a useful tool for future integration. The descriptions of the inputs and outputs of the used image processing function are summarized in Tables 5 , 6 , respectively."
10.1038/s41597-023-02653-7,"The code used for preprocessing and segmenting figures is publicly available on GitHub: https://github.com/lamps-lab/Patent-figure-segmentor and https://github.com/GoFigure-LANL/figure-segmentation . Similarly, the software used for extracting semantic information, including object names and viewpoints from patent captions, is publicly available at https://github.com/lamps-lab/Visual-Descriptor ."
10.1038/s41597-023-02676-0,"Refer to the README file accessible at the GitHub repository ( https://github.com/fbranda/west-nile ) for further instructions on how to use the dataset, import it either in R or Python, and carry out some exploratory analysis. The same link also hosts the dynamic version of WNVDB and all source codes to reproduce the results reported in this Data Descriptor."
10.1038/s41597-023-02693-z,The source code is included as part of the dataset 68 . All code is implemented using MATLAB (MathWorks Inc.)
10.1038/s41597-023-02699-7,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-023-02644-8,No custom code was used in the development of this dataset.
10.1038/s41597-023-02660-8,"The validation code and models are released in the Github repository https://github.com/GrainSpace/GrainSet . We conducted benchmark analysis using PyTorch and OpenCV, and all parameters used in our implementation are reflected in the repository."
10.1038/s41597-023-02662-6,The dataset is freely available as described in Data Records. The custom code to generate or process these data can be found in the following GitHub repository: https://github.com/micschaefer/does-utils . The rights to the source code of the validation model belong to Saarstahl AG and unfortunately cannot be published.
10.1038/s41597-023-02697-9,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS https://epos-eu.github.io/epos-open-source/ . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor ( https://github.com/epos-eu/SHAPEness-Metadata-Editor ) and the specifications for EPOS-DCAT-AP extension ( https://github.com/epos-eu/EPOS-DCAT-AP )."
10.1038/s41597-023-02673-3,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab ( https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset ). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment."
10.1038/s41597-023-02698-8,The code used to collect and store the individual consumption data is available at https://github.com/feelab-info/eGaugeDataAcquisition . The code was developed using Python3.6 and deployed on a Linux machine (Raspbian see http://www.raspbian.org/ )). The Python3 code to reproduce the examples presented in this paper is available on the dataset repository at https://doi.org/10.17605/OSF.IO/K3G8N 34 .
10.1038/s41597-023-02690-2,"The energy calculations with the 76 different post-SCF functionals were performed using the SCM software package 29 . The GFN2-xTB energies were computed using the XTB version 6.3.3 software package 30 , 31 , 32 . The G4MP2 energies were obtained from a previous paper by Kim et al . 16 , 26 . The workflow of the calculations and collection of data are build using the Python3.7.10 and BASH scripts. The atomistic simulation environment (ASE) 33 was used to create the database file in SQLite3 format. The csv files were created using pandas. The plots were generated using the matplotlib library. All scripts are available on GitHub under the MIT license agreement ( https://github.com/chemsurajit/largeDFTdata )."
10.1038/s41597-023-02683-1,"The repository of brain tumor recurrence prediction data can be found on our GitHub ( https://github.com/siolmsstate/brain_mri ). Pydicom version 2.3.0 and SimpleITK version 2.1.0 have been used in data preprocessing. The baseline model framework is generated using TensorFlow version 2.8.0. We release sample codes for users to get started with raw data, guiding through loading the data and all preprocessing steps. Fundamental data visualization is also available."
10.1038/s41597-023-02694-y,The code used to build the data described in the GRIDCERF package can be accessed on GitHub here: https://github.com/IMMM-SFA/vernon-etal_2023_scidata . They are in a Jupyter Notebook format to allow for interactive visualization and description throughout. The CERF modeling software is also available at https://github.com/IMMM-SFA/cerf and is detailed in Vernon et al . 12 .
10.1038/s41597-023-02717-8,The custom R code used to convert data retrieved from Scopus to the appropriate format for visualisation in CitNetExplorer is available from GitHub ( https://github.com/MichaelBoireau/Scopus2CitNet ). The custom PYTHON script used for plotting the scientometric aspects of the included literature is also available from GitHub ( https://github.com/LSLeClercq/ABCal ).
10.1038/s41597-023-02679-x,"The code for computing joint angles and moments is provided alongside the data in FigShare 28 . The function uses the batch OpenSim processing tools 36 to run the inverse kinematics and inverse dynamics through MATLAB and generate a folder containing “ ik . mot ” and “ inverse_dynamics.sto ” files for the joint angles and moments from the inverse kinematics and inverse dynamics, respectively. The code is accompanied by a README file describing these details and its usage."
10.1038/s41597-023-02714-x,No in-house code or scripts were used in this study. Commands and pipelines used for data processing were executed using their corresponding default parameters.
10.1038/s41597-023-02712-z,No custom code was made during the collection and validation of this dataset.
10.1038/s41597-023-02702-1,The code for data validation is open-source and available at the GitHub repository https://github.com/LiqiaoHuang/Recipe_dataset.git .
10.1038/s41597-023-02681-3,"The code to reproduce results, documentation, and tutorials are available in the GitHub repositories: metadata_gen 31 1.0.0 – a tool for generating a property graph metadata, csvrelgen 30 1.0.0 – a tool for creating complex relationships between concepts, mizgra 27 1.0.0 – a main tool for generating a property graph serialization, esx_verify 35 1.0.1 – a tool for checking semantics correctness. All software is provided with open-source licenses and has a Docker version. A sample usage contains: 1. In the first step, users should use ESX files. 2. In the second step, users can optionally check syntactical correctness via xmllint or other XML Schema validators. 3. In the third step, users can optionally check semantic correctness via esx_verify. 4. In the fourth step, users may use csvrelgen to generate relationships. 5. In the fifth step, users can use metadata_gen to generate metadata (in XML) for the dataset. 6. In the sixth step, users may prepare RDF resources, e.g. via the SPARQL endpoint. 7. In the seventh step, users can use ESX (see item 1), CSV (see item 4), XML metadata (see item 5), and RDF (see item 6). 8. In the last step, users can load the GraphML data to the graph database, e.g. Neo4j 37 ."
10.1038/s41597-023-02710-1,The code used to assess significance of the interactions was deposited under the following link: https://github.com/MaximeCarlier12/Interactome_proteins_ecoli . The MIT license is available at: https://github.com/MaximeCarlier12/Interactome_proteins_ecoli/blob/master/LICENSE .
10.1038/s41597-023-02715-w,"Raw sequencing data were analyzed using publicly available bioinformatics softwares. We used common data analysis software packages and no custom code was created. Software tools used are as follows: FastQC: v0.11.7, https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Bowtie: v1.2.2, https://bowtie-bio.sourceforge.net/index.shtml GtRNAdb: http://gtrnadb.ucsc.edu/ tRNAscan-SE: v2.0, http://lowelab.ucsc.edu/tRNAscan-SE/ Cutadapt: v1.17, https://github.com/marcelm/cutadapt/ edgeR: v3.24.3, https://bioconductor.org/packages/release/bioc/html/edgeR.html R software: v3.5.1, https://www.r-project.org/ OmicStudio tools ( https://www.omicstudio.cn/tool ) was used for prediction of target genes. GraphPad Prism 9 (GraphPad Software Inc., USA) was used for statistical analyses and data visualization."
10.1038/s41597-023-02684-0,"The proprietary software of Artec Leo and Autoscan Inspec was used for processing the scans. For the analysis of the dimensions of instruments, a Python script was written using the Trimesh, Numpy and Pandas library. Another Python script is provided for rescaling and smoothing the original models into a multifold of models, using deformations and affine transformations. All code is available in the data repository 30 ."
10.1038/s41597-023-02633-x,"The code to format and process data was developed in R computing environment and is freely available in the Zenodo repository 22 . This is a static copy of the data peer-reviewed in 2023, which is a release from the dynamic Github repository https://github.com/Tania-Maxwell/MarSOC-Dataset . When using data from this dataset please cite this publication, along with the original sources. Both dataset and code are available under a Creative Commons License (CC-BY)."
10.1038/s41597-023-02700-3,"R-code to analyse the data is made available on the website of the University of Antwerp and via the Zenodo repository 38 ( https://doi.org/10.5281/zenodo.10049811 ) under the creative Commons license option (CC-BY). We added the open license to our dataset page (CC-BY). This license enables reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. We encourage other researchers who use our dataset to share their code with us. In that way, we can group all analyses that are performed on this MOSA dataset together."
10.1038/s41597-023-02692-0,"No custom code was used nor is required to generate or process the datasets provided. All shape files were generated and can be opened with the opensource software QGIS or a similar GIS software platform, such as ArcGIS."
10.1038/s41597-023-02557-6,"This is an image data set and annotations have been provided in a widely used format for computer vision data sets–the COCO Camera Traps JSON format. Since the annotations have already been included in a standardized format, no additional code has been supplied."
10.1038/s41597-023-02708-9,All the relevant codes to produce the global dataset of future climates are publicly available at the GitHub repository ( https://github.com/mit-jp/SD-BC ). The pattern-change kernels” (PCKs) for all the CMIP6 models are created using Grid Analysis and Display System (GrADS). The SD-BC procedure is carried out using FORTRAN 90 programming language.
10.1038/s41597-023-02695-x,The Python codes for generating and processing data and be accessed through GitHub ( https://github.com/Xiongkovsky/glass_vis_lstm_code ).
10.1038/s41597-023-02713-y,"The original scripts to clean, process, and match the original 1940 Census and mortality records were written in the R programming language. They are available at https://github.com/caseybreen/censocdevgithub.com/caseybreen/censocdev . The code to reproduce all figures and tables in this paper is available from the Open Science Framework 43 ."
10.1038/s41597-023-02725-8,All software and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and code/parameters of the software have been detailed and described in Methods. No custom code was used during the compilation of the dataset.
10.1038/s41597-023-02727-6,"All bioinformatic tools utilized in this study, along with their respective parameters, are clearly described in “Methods” section. In instances where specific parameters for the software were not specified, default parameters were employed as recommended by the developer."
10.1038/s41597-023-02732-9,"The code used to create Can-SWaP (i.e., generating the watersheds, populating the attributes with RiverATLAS, extracting statistics) is available on Github at https://github.com/FNRobinne/Can-SWaP.git , and as an archive on Figshare 87 . It will require the user to download the necessary HydroSHEDS products. Running the code requires RTools installed on the user’s machine. The final versions of these datasets were built in QGis 3.28.3-Firenze, R v4.2.2, RStudio 2022.12.0 Build 353, and ArcGIS 10.x for some data conversion from ESRI proprietary formats into OGC formats."
10.1038/s41597-023-02730-x,"All software and pipelines were executed following the manuals and protocols provided by the published bioinformatic tools. The version and parameters of software have been described in Methods. If no detail parameters were mentioned for a software, default parameters were used as recommended by developer."
10.1038/s41597-023-02687-x,The code and models used to perform the experiments are available online at the following link: https://github.com/simula/cellular .
10.1038/s41597-023-02728-5,Code used to produce the graphs is available in the dataverseNL (DANS) repository alongside the static version of the dataset through https://doi.org/10.34894/Z43J6I . Figures and analysis were done using Rstudio version 2023.03.0 742 and R version 4.2.2 712 .
10.1038/s41597-023-02722-x,No custom code was used in this project.
10.1038/s41597-023-02735-6,"All R codes (R3.5.3, https://www.r-project.org ) for creating provincial and gridded population datasets for China are stored in public repository Figshare 30 . Explanations are internalized in the script to help users with implementation."
10.1038/s41597-023-02682-2,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project 15 and its repositories available at https://github.com/LegoSorter ."
10.1038/s41597-023-02709-8,The analysis results associated with this paper is available on Github ( https://github.com/saminagul12345/breast_cancer ) and the R code used in the analysis of the data is available on Github ( https://github.com/saminagul12345/breast_cancer ).
10.1038/s41597-023-02723-w,"The custom MATLAB code used to calculate hand anatomical angles is freely available on Zenodo 37 . Its use requires a prior calibration procedure of the data glove, which was developed in previous work 26 ."
10.1038/s41597-023-02645-7,Computational tools have been developed for processing BSMN data. A list of these tools and their repositories is available on the Synapse site at https://bsmn.synapse.org/Explore/Tools and is included in Table 7 .
10.1038/s41597-023-02734-7,"The codes, which are used to generate the 3D representative models and the related results of this manuscript, are released and publicly available at https://github.com/Chall513032/CoalSegmentation ."
10.1038/s41597-023-02721-y,"Code and data are available free of charge. The code is provided in our Github repository https://github.com/aimat-lab/3DSC . For reproducibility, the SHA of the final commit for this publication is 2471dd51a298a854cb4f365ebd39e72c7cbf3634. The data is available on figshare 30 ."
10.1038/s41597-023-02739-2,No custom code was used.
10.1038/s41597-023-02703-0,The script code for toxins cDNA analysis can be accessed as supplementary materials to the article “The mining of toxin-like polypeptides from EST database by single residue distribution analysis” 15 . No special constants were used. Identity search engine 12 is available at https://github.com/levitsky/identipy .
10.1038/s41597-023-02705-y,"The project framework that has been implemented for evaluating our corpus is stored at the corpus repository 32 in the LUPAN_code.zip file: https://doi.org/10.57745/XIVJ65 . It includes all the scripts used for constructing the corpus and the code of the preliminary experiments presented in Technical validation. The code has the following structure: 1. Corpus construction: • pdf2text.py – text extraction from the documents in the PDF format • segment_construction.py – segment construction from the annotated documents in the txt format 2. Preliminary experiments: • data_loader.py - load segments from the txt format, split the data into 80%/20% for learning/test • segment_classification.py – 4-label classification by the CamemBERT model To reconstruct the experiments: (a) Download and place the PDF documents to the Corpus_PDF folder. The links to the original documents are provided in the Text extraction section above. (b) Extract text from the documents using our pdf2text.py module. The resulting files will be saved to the Corpus_txt directory. (c) Manually pre-process the documents as it is decribed in Methods (annotation details) and Data records (data formats). To skip this part download Corpus_Manual_Annotation_Consolidated_Version.zip from the corpus repository, extract it to the same directory with the code. (d) Construct label segments from manually annotated documents using segment_construction.py module. To skip this part download Corpus_Extracted_Segments_Consolidated_Version.zip from the corpus repository, extract it to the same directory with the code. (e) Prepare label segments for learning by using data_loader.py . (f) Use segment_classification.py for learning and validation. For detailed examples please refer to the README file provided with the code."
10.1038/s41597-023-02742-7,The code used to check species names can be found in the R package ‘plantlist’ version 0.7.2.
10.1038/s41597-023-02711-0,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02756-1,The code used to calculate the DEDI dataset is available via GitHub ( https://github.com/XiaZhang1113/Daily-drought-index–DEDI ) under the MIT license. The scripts are written with the open-source Python language version 3.8.6 ( https://www.python.org/ ) and the Climate Data Operators (CDO) version 1.9.10 ( https://code.mpimet.mpg.de/ ). Any updates will be published on GitHub.
10.1038/s41597-023-02628-8,"All code used to generate and manipulate the dataset, as well as code used in the Technical Validation is available at the GitHub repository 44 . Further details and documentation regarding code usage are included therein."
10.1038/s41597-023-02548-7,All code necessary to reproduce our Level 2 datasets are written in the open source R Statistical Software 34 version 4.2.2 and are publicly available in our ESS-DIVE repository accessible at https://doi.org/10.15485/1960313 10 in ec1_processingscripts_v1.zip . All scripts follow a standardized structure outlined in template.R and are named in the following format: [Sample Type]_[Analyte].R that correspond to their respective dataset name.
10.1038/s41597-023-02743-6,"The authors provide an open-source Python package ( https://github.com/Stocastico/xapi_analysis ) that simplifies the data analysis of datasets like the ones described in this manuscript. The library requires Python ≥3.9, Pandas and Seaborn, and it has been created using the nbdev v2 environment. The code for the applications used in English Literacy and STEM pilots will not be released as open source since the authors are planning to further develop and exploit them. Instead, two videos ( English Literacy video , STEM video ) are shared so that the reader can see how the applications were and how the students could interact with them. The code used for PBIS has been released under the EUPL license and the repository ( https://gitlab.com/aretewp5/PBISAR-App ) is available on Gitlab. And finally, the LXD pilot has been released with an MIT license and the repository ( https://github.com/WEKIT-ECS/MIRAGE-XR/ ) is available on GitHub."
10.1038/s41597-023-02738-3,"The preprocessing of the kinematic data (removal of unused columns in.csv files (see Table S1 , checking for corrupt files, removal of sensitive data, naming and sorting of files into folders) as well as the normalization of the kinematic data was performed using custom Python 46 code, and can be found in the folder called Code within the CeTI-Age-Kinematic-Hand 42 database. The custom Python scripts 46 , 47 , 56 used for the descriptive analyses and ML-based movement classifications reported in the Technical Validation section can also be found in the Code folder of CeTI-Age-Kinematic-Hand 42 ."
10.1038/s41597-023-02707-w,"All software used in this work is in the public domain and their parameters are described in the Methods section. If a software did not mention parameters, the default parameters suggested by the developer were used."
10.1038/s41597-023-02718-7,"You can access the car-following data and apply baseline algorithms with trained models, including the parameters of traditional models like IDM and GHR, for each dataset. These datasets and models are publicly available on GitHub at https://github.com/HKUST-DRIVE-AI-LAB/FollowNet ."
10.1038/s41597-023-02750-7,"The R code used to preprocess the metabolomics data was based on R 4.0.1 and the R package maplet 27 . The code is available at https://github.com/krumsieklab/prostate-cancer-edrn . In the same script, we also provide an example to illustrate how to use the maplet R package to load the data and run differential analyses based on the available clinical parameters. This can serve as a template for users to build their own analysis pipelines."
10.1038/s41597-023-02755-2,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at https://nanohub.org/tools/vaspingestor 22 , 23 . This code is also available at https://github.com/katnykiel/vasp_ingestor ."
10.1038/s41597-023-02655-5,"If no detailed parameters were mentioned, all software and tools in this study were used with their default parameters. No specific code or script was used in the study."
10.1038/s41597-023-02741-8,"The code to replicate the experiment is available online. The frontend of the experiment was developed using Next.js and Chakra UI. Next.js is a React framework that enables server-side rendering, automatic code splitting, and other useful features for building web applications. Chakra UI is a component library that provides a set of customizable and accessible UI components to build user interfaces quickly and easily. The UI layout is described in the file ui.json and rendered on the app through the function renderJSON . The backend was implemented using Node.js and Express, with a PostgreSQL database used to store data. The frontend and backend communicate with each other through a RESTful API, providing a secure and efficient way for data to be transmitted between the two. Researchers interested in replicating the experiment can access the code and customize it to their needs, allowing for greater flexibility and control over the experimental setup. More information can be found in the Github repository ( https://github.com/mu-sse/adaptiveUIs-project/tree/main/app-mixing-machine ). The code for conducting technical validation has been developed using Python and is accessible through the GitHub repository provided in this document. To make use of the IBM API data quality for AI, a free sign-up process is necessary, which can be initiated at the following URL: https://www.ibm.com/account/reg/us-en/signup?formid=urx-50307 . This sign-up process will provide access to the required API keys, as detailed in the accompanying documentation. For additional information and access to the code, please refer to the GitHub repository available online ( https://gist.github.com/aicarrera/e6f99ea7f857de4c949afd2dfe1ff9be ). Code illustrating dataset usage in the context of sequential recommendation (next-step prediction) through Markov chains has been developed using Python, building upon prior research 21 . More information is available online in the Github repository ( https://github.com/mu-sse/adaptiveUIs-project/tree/main/HMI-sequence-recommender )."
10.1038/s41597-023-02746-3,"Our code is fully available in Martín-Forés et al . (2023); https://doi.org/10.6084/m9.figshare.23513478 27 . The code can also be accessed on github, https://github.com/MartinFores/AFA ."
10.1038/s41597-023-02720-z,Python 3.8 was used for conversion of the MODIS products from HDF files to raster and all data handling and processing was thereafter done in R version 4.0.0. All data processing and modelling procedures are available as R scripts on a public Github repository: https://github.com/evabendix/AntAir-ICE . Using this code it is possible to download new available MODIS LST and IST scenes and apply the model to continue the near-surface air temperature dataset.
10.1038/s41597-023-02659-1,"The dataset is stored on Zenodo ( https://doi.org/10.5281/zenodo.8250217 ) 28 and curated on Github ( https://github.com/cldf-datasets/gata ). The current release of the repository is Version 1.0.0 and was peer-reviewed in 2023. All data is available under a CC-BY 4.0 license. All scripts that have been used during the pre-processing of the data are made available within the repository. Specifically, Python-scripts were used after the manual annotation of the data for the standardization of all annotations, as well as for the aggregation of the individual spreadsheets. The script that was used for the conversion into CLDF is also part of this repository."
10.1038/s41597-023-02748-1,The R and Python scripts written to generate Fig. 5 and Figure S2 are available in GitHub 23 .
10.1038/s41597-023-02740-9,No custom code has been used during the generation of these datasets.
10.1038/s41597-023-02678-y,No custom code has been used.
10.1038/s41597-023-02754-3,This work does not require any code.
10.1038/s41597-023-02736-5,"The code is fully operational under R 4.0.3. Variable standardization and statistical processing were performed using dplyr , caret , mgcv , randomForest , MLmetrics , Kendall , trend , SpatialEco and iml packages. The full code and sample dataset are publicly available through the figshare repository ( https://doi.org/10.6084/m9.figshare.23528232 ) 72 ."
10.1038/s41597-023-02724-9,"The R script and functions written by the authors used in the analysis are available at the following GitHub repository: https://github.com/balsedie/trilomorph . Analyses have been computed with R version 4.1.3 113 with specific functions (available at Github) and the package geomorph version 4.0.1 57 , 58 ."
10.1038/s41597-023-02696-w,All code for processing the raw MCD19A2 HDF-EOS files as well as reconstructing the missing data is available on GitHub: https://github.com/lu-liang-geo/AOD-reconstruction or Figshare 55 . The code is all provided in Python using open-source libraries for reproducibility.
10.1038/s41597-023-02761-4,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02770-3,No specific script was used in this work. The codes and pipelines used for data processing were all executed according to the manual and protocols of the corresponding bioinformatics softwares (detail parameters see Table S2 ).
10.1038/s41597-023-02598-x,The code for all analyses and figures of this study has been deposited at https://github.com/dznetubingen/rimod-ftd-dataset .
10.1038/s41597-023-02764-1,The GitHub page of the Research Center E. Piaggio 22 contains the MATLAB scripts that were used to extract and visualize data from the ROS bag (.bag) files. This GitHub repository contains also the README files describing these scripts. The same files can also be accessed on Zenodo 23 .
10.1038/s41597-023-02776-x,The specific codes for analyses of RNA-seq data are available at https://github.com/matevzl533/Noccaea_praecox_transcriptome .
10.1038/s41597-023-02765-0,No custom scripts or code were used in this study.
10.1038/s41597-023-02777-w,The construction of the gridded dataset PISCOt v1.2 was performed using the R (v3.6.3) and Python (v3.8.5) programming languages. The entire code used is freely available at figshare 137 and GitHub ( https://github.com/adrHuerta/PISCOt_v1-2 ) under the GNU General Public License v3.0.
10.1038/s41597-023-02763-2,"The GitHub page of the Research Center E. Piaggio 37 and Zenodo 36 host the MATLAB code attached to the data presented in this paper. This can be used to extract and visualize data from ROS bag files. In particular, the GitHub repository contains a README file describing each of the scripts individually."
10.1038/s41597-023-02747-2,"A Python source code is provided to help future dataset users. We provide an API for accessing the data and some examples of the type of studies that can be done by using this dataset. The API ( phyafb_api.py file) is composed of the following functions: • read_hr_file : This function is designed to read an HR file for a specific game and player. It returns a list containing the HR values recorded. If there is no file available for the given game and player (indicating that the player did not participate in that particular game), the function returns an empty list. • read_playing_file : The purpose of this function is to read a PLAYING file for a specific game and player. It returns a list consisting of binary values (1 or 0). A value of 1 signifies that the player was on the court at that particular moment, while a value of 0 indicates that the player was on the bench. If there is no file available for the given game and player, the function returns an empty list. • read_game_file : This function facilitates access to specific fields within the GAME files. Along with the game and player parameters, the field name needs to be specified as an additional parameter. The function then returns a list containing the values of the specified field. If there is no file available for the given game and player, the function returns an empty list. We also provide a Python script phyafb_demo.py to obtain the figures shown in the previous section."
10.1038/s41597-023-02778-9,"The source code for our data pipeline is publicly available on GitHub at https://github.com/PDBe-KB/process-complex-data . This repository contains a Python package that aggregates data for macromolecular complexes from the PDBe graph database, assigns unique identifiers, and generates human-readable names. Additionally, we have created a demo repository that allows developers to test and benchmark the complex identifier-generating process. This repository is available on GitHub at https://github.com/PDBe-KB/pdbe-complex-analysis-demo ."
10.1038/s41597-023-02762-3,We provide the raw data files obtained during the data collection structured by a user identifier. We did not implement any specialized code to pre-process the data.
10.1038/s41597-023-02781-0,All code used in this paper can be downloaded on GitHub at https://github.com/zexuwu/hald .
10.1038/s41597-023-02767-y,See usage notes for information on code availability to compute or process the available data.
10.1038/s41597-023-02760-5,The code used to produce the AHRA V5 MO dates is archived at Zenodo 48 . Additional code used to complete the analysis and produce figures in this paper is available at Zenodo 49 .
10.1038/s41597-023-02766-z,"The code that accompanies this article is publicly available in the following GitHub repository: https://github.com/BartsMRIPhysics/Speech_MRI_2D_UNet (software licence: Apache version 2.0). The repository contains already trained versions of a state-of-the-art speech MR image segmentation method 47 that are ready to use immediately. These versions were trained using the datasets described in this article. The repository also contains instructions and Python code to train and evaluate new versions of the method using the datasets described in this article. The code is designed to allow users to choose several important training parameters such as the training and validation dataset split, the number of epochs of training, the learning rate and the mini-batch size. In addition, the code is designed to be compatible with any dataset as long as it is organised and named in a specific way. The repository contains Python code to check that the datasets are not corrupted and are organised and named in the specific way required by the segmentation method, as well as Python code to perform the image pre-processing required by the method, namely normalising the images and saving the normalised images as MAT files."
10.1038/s41597-023-02779-8,The R scripts for reproducing the main figures are available through the GitHub repository at https://github.com/kaipengl/batcheffectsdataset .
10.1038/s41597-023-02749-0,"Python 3 was used for data processing and analysis. The simple code demonstration suggested in this work can be found in the GitHub repository stated in the Data Record section. The code demonstrates the methodology for extraction and subsequent processing of the thermal images. Few images were selected from the dataset for illustration purposes. The code uses two specific GitHub packages: Flirextracter 23 and Labelme 24 . Flirextractor is an efficient Python package for extracting temperature data from thermal images and converting it into an array, then saving it as a CSV file for further access. The link to the GitHub package describing the usage of Flirextractor in detail is as follows: https://github.com/aloisklink/flirextractor . Labelme is a graphical image annotation tool written in Python. It allows users to demarcate the desired area of any shape with simple mouse clicks. Detailed instructions for Labelme can be found at the following link: https://github.com/wkentaro/labelme . The GitHub code repository exhibits A meticulously structured hierarchy designed to enhance navigation and understandability. It incorporates three primary directories: ‘data’, ‘notebook’, and ‘src’. The ‘data’ directory segregates original, converted, and processed files, whereas the ‘notebook’ directory encompasses Jupyter notebooks detailing file conversion, data visualization, and data analysis procedures. The ‘src’ directory hosts a Python script dedicated to image conversion. Furthermore, the ‘data’ folder manifests a systematic organization structured to efficiently manage different stages of data processing. It consists of three primary subdirectories: ‘original’, ‘convert’, and ‘labelme’. The ‘original’ and ‘processed’ subdirectories mirror each other, containing dated folders representing different acquisition dates, with each date folder further divided into ‘view_1’, ‘view_2’, and ‘view_3’ subdirectories. These subdirectories contain the respective images captured from each view named “smap-YYYY-MM-DDTHH-MM-SS.MS.jpeg”. During the data processing phase, corresponding ‘json’ files for each image are generated in the ‘labelme’ directory. In addition, for each image, a dedicated directory is established within the ‘convert’ subdirectory, encapsulating original, processed, and labeled images and a text file enumerating the detected labels. Detailed file structure tree plots, included within the respective README files, furnish comprehensive visual representations of the file and folder organization, thus facilitating an understanding of the hierarchical structure and interrelationships among different components of the codebase. Moreover, an example involving a representative image has been conducted to illustrate the efficacy of the employed image processing techniques; the results highlight the segmentation, further processing, and final presentation of the image augmented with enhanced colors and informative labels. Please contact the corresponding author directly if you have any particular requirements."
10.1038/s41597-023-02576-3,"The Python codes to collect, process, and plot the dataset as well as the supplementary files for this study are publicly available through the GitHub repository ( https://github.com/axin1301/Satellite-imagery-dataset ). Detailed instruction for the running environment, file structure, and codes is available in the repository."
10.1038/s41597-023-02789-6,"The R script used to generate the integrated dataset, conduct validation dataset analysis, and produce relevant plots can be found at: https://github.com/Junaid13913/PostnatalMouseHypothalamus_SingleCellTranscriptome . Our final datasets with UMAP embeddings are available at the figshare repository: ( https://doi.org/10.6084/m9.figshare.21981251.v1 ) 23 ."
10.1038/s41597-023-02680-4,The DHSVM source code is available at https://github.com/pnnl/DHSVM-PNNL Source codes that are used to develop and analyze the data are available at https://github.com/hydro-yan/NG-IDF The BCQC SNOTEL data are available at https://climate.pnnl.gov/?category=Hydrology .
10.1038/s41597-023-02752-5,The code is available on https://github.com/kingjr/meg-masc/ .
10.1038/s41597-023-02726-7,Not applicable.
10.1038/s41597-023-02704-z,"The Matlab code GaitPrint.m, template scripts (MATLAB, Python, R), and all associated functions used for post-processing of all raw data are available as part of the database on figshare."
10.1038/s41597-023-02753-4,"The code used for the whole procedure, is written in the R programming language, and is available on GitHub ( https://github.com/angeludias/PPP_emissions )."
10.1038/s41597-023-02757-0,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the Integration folder ( https://gitub.u-bordeaux.fr/erias/oregano ).
10.1038/s41597-023-02794-9,"The six data tables can be directly downloaded from Figshare 21 and imported into any spreadsheet, database or statistical software. For instance, in R, the files can be simply uploaded as follows: art <- read.csv(""postdiv.arthropods.csv"") rep <- read.csv(""postdiv.reptiles.csv"") pla <- read.csv(""postdiv.plants.csv"") par <- read.csv(""postdiv.parasites.csv"") plot <- read.csv(""postdiv.plots.csv"") tax <- read.csv(""postdiv.taxonomy.csv"") We provide a series of examples of code in the R language to extract basic information from the database in the file “postdiv.examples.R”."
10.1038/s41597-023-02791-y,"No custom code was used for this study. All data analyses were conducted using published bioinformatics software with default settings, unless otherwise specified."
10.1038/s41597-023-02786-9,"The code implementation is done in the R programming language version 4.1.0 and MATLAB R2018a. The custom code used for data processing, technical validation, visualization is available on the github page ( https://github.com/Industrialpark/SEMLab_HFUT-Building-Electricpowerloaddata )."
10.1038/s41597-023-02701-2,"The code we used to generate the morphed faces is publicly available, including Action Unit(AU) extractor ( https://github.com/TadasBaltrusaitis/OpenFace ), StyleGAN2 encoder ( https://github.com/omertov/encoder4editing ), StyleGAN2 generator ( https://github.com/rosinality/stylegan2-pytorch ), SAM ( https://github.com/yuval-alaluf/SAM )."
10.1038/s41597-023-02809-5,The source code for this dataset is publicly available through the Figshare repository 25 . The data may also be visualized with an interactive web visualization tool: https://acclimation.statgen.org/
10.1038/s41597-023-02706-x,"The code used to generate the interactive visualization of the attacks shown in Fig. 2 is provided in the convert.py ( https://github.com/ricardomourarpm/Gulf_of_Guinea_Piracy ). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colab ( https://colab.research.google.com/ ). The Anaconda Distribution ( https://jupyter.org/install.html ) is usually a good choice since it includes Python, the Jupyter Notebook, and other commonly used scientific computing and data science packages."
10.1038/s41597-023-02798-5,"We used open-source tools to ensure transparency and reproducibility of our research, including R (4.3.0), Python 3.6.7, and Google Earth Engine. Time series tools for training data collection are available on GitHub ( https://github.com/parevalo/measures_collector ) as is the repository for filtering training data ( https://github.com/ma-friedl/GlanceFiltering ). Custom continental definitions can be found at this repository: https://measures-glance.github.io/glance-grids/params . Continuous Change Detection and Classification (CCDC) tools and applications can be found on Google Earth Engine ( https://glance.earthengine.app/view/fulltstools ) and python ( https://github.com/repository-preservation/lcmap-pyccd )."
10.1038/s41597-023-02807-7,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-023-02803-x,No custom code was written to analyze the survey data described in this article.
10.1038/s41597-023-02804-w,"Custom scripts were developed in R and python to process, manage, and clean the data. These scripts are available publicly at the repository https://github.com/AmosRAncell/PLUSWIND . Additionally, users may contact the corresponding author with questions about these scripts or about our source data."
10.1038/s41597-023-02782-z,"The software versions, settings and parameters used are described below: 1. GenomeScope v2.0; p = 2, k = 21 2. HiFiasm v0.15.4-r343; ran on Galaxy with default parameters, with the exception of purging level = 0 (none). 3. QUAST v5.0.2; python quast.py [Assembly file name] 4. BUSCO v5.4.7; busco -i [Assembly file name] -l vertebrata_odb10 -m genome 5. Meryl v1.3; (meryldb generation) Meryl was run on all four raw read files separately to generate a meryl database for that sequencing run, and then the four meryl databases were merged using the “union-sum” function, to make a meryl database for all the reads. The k value was 21 for all runs. 6. Merqury v1.3; ran on Galaxy with following parameters; Evaluation mode: Default mode, k-mer counts database: fScoJap1.meryldb.meryldb, Number of assemblies: One assembly (“Two assemblies” for running on c1 & c2 simultaneously), Genome assembly: [Assembly file name] 7. purge_dups v1.2.5; ran on Galaxy using workflow “VGP purge assembly with purge_dups pipeline”; Hifiasm Primary assembly: fScoJap1_c1.fasta, Hifiasm alternate assembly: [fScoJap1_c2.fasta] 8. salsa v2.3; ran on Galaxy with parameters; Initial assembly file: p1.fastq, Bed alignment: Aligned bed format files of Hi-C data (fScoJap1_S_2476_8_R1_001.fasta, fScoJap1_S_2476_8_R2_001.fasta) 9. gEVAL v2.2.0; 10. RepeatMasker v4.1.5; ran with following parameters; Repeat library source: Dfam 3.7, Species: zebra fish; Search engine: RMBlast v2.14.0 + ; Sensitive search option. 11. tidk v0.2.1; tidk find -c Scombriformes -f [GCF_027409825.1_fScoJap1.pri_genomic.fna] -w 10000 12. primrose v1.3.0; primrose [fScoJap1_HiFi.bam fScoJap1_5mC-HiFi.bam] 13. pbmm2 v1.10.0; pbmm2 index [GCF_027409825.1_fScoJap1.pri_genomic.fna] fScoJap1_5mC-HiFi.bam fScoJap_5mC-HiFi.mmi; pbmm2 align [fScoJap1_5mC-HiFi.mmi fScoJap_5mC-HiFi.bam] [fScoJap1_5mC-HiFi_aligned_sorted.bam]–sort 14. pb-CpG-tools v1.1.0; python aligned_bam_to_cpg_scores.py -b [fScoJap_5mC_HiFi_aligned_sorted.bam] -f [GCF_027409825.1_fScoJap1.pri_genomic.fna] -o cpg_regions -p model -d /pileup_calling_model/ 15. EMBOSS v6.5.7.0; newcpgreport -window 100 -shift 1 -minlen 200 -minoe 0.6 -minpc 50. [GCF_027409825.1_fScoJap1.pri_genomic.fna] 16. TBtools-II v1.113; ran in GUI through Graphics > Comparative Genomics > One Step MCScanX option with following parameters; Input Genome Sequence File (.fa) of Species One: GCF_027409825.1_fScoJap1.pri_genomic.fna, Input Gene Structure Annotation File (.gff/.gtf3) of Species One: GCF_027409825.1_fScoJap1.pri_genomic.gff, Input Genome Sequence File (.fa) of Species Two: GCF_027409825.1_fScoJap1.pri_genomic.fna, Input Gene Structure Annotation File (.gff/.gtf3) of Species Two: GCF_027409825.1_fScoJap1.pri_genomic.gff, CPU for BlastP: 2, E-value: 1e-10, Num of BlastHits: 5 17. BUSCO v4.1.4; ran on RefSeq annotation “GCF_027409825.1-RS_2023_01” with following parameters; Lineage: actinopterygii_odb10, Mode: Protein No custom scripts or code was used in validation of the dataset."
10.1038/s41597-023-02769-w,"FigShare R code used to generate Figs. 3 , 4 is available at https://doi.org/10.6084/m9.figshare.23292389 along with the source file. FigShare R code to generate heatmap of any given wheat gene IDs (using TraesIDs) using the normalized count data file is available at https://doi.org/10.6084/m9.figshare.23292389 . The code is also available on GitHub repository as: https://github.com/plantbiologyqau/R-code-for-gene-expression-heatmap/tree/main"
10.1038/s41597-023-02759-y,The code that is used to generate the Canada and Atlantic bubble weighted average index of public health mitigation is available through the GitHub repository.
10.1038/s41597-023-02817-5,The codes that were used to generate the CCPU dataset are available in Figshare 30 .
10.1038/s41597-023-02800-0,"No custom programming or coding was used. Instead, the analysis utilized bash commands and the corresponding scripts stored within the GitHub repository accessible at: https://github.com/LandiMi2/GenomeAssemblyTMEB117 ."
10.1038/s41597-023-02802-y,"All the models were trained using Python 3.9 and Tensorflow 2.10. Models were trained locally on a 3090 GPU with 24 GB of memory. A GitHub repository which contains the Jupyter notebook for training is available at https://github.com/mattfishman/Drosophila-Heart-OCM . To run the code, first download the data and clone the repository. There is a requirements.txt file within the repository that contains all the dependencies. Next, use the utilities to generate a pickle file by changing the path inside create_pickle.py to point to the parent directory that contains all the flies. Insert the pickle file path into the training notebook and run all cells begin training. Our trained model has been provided as a starting place for training, but users may opt to train without this initialization. This notebook will output the model as a.h5 file. The path to the trained model can then be inserted into processing_utils.py to use this model to predict the segmentation on new images. Additional details can be found in the README file within the repository for how to run additional code to produce cardiac parameters."
10.1038/s41597-023-02806-8,This dataset 28 only comprised raw data. No codes were generated for the creation of the repository. Data processing and generation of BIDS files were done with freely available software.
10.1038/s41597-023-02808-6,The Python code for ETL pipelines is available on the open-access GitLab at https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git .
10.1038/s41597-023-02811-x,"All bioinformatic software and pipeline used in this study were implemented according to the protocols provided by the software developers. The versions and parameters for each software can be found in the Methods section. Unless otherwise stated, default parameters were employed."
10.1038/s41597-023-02805-9,QDM approach in this study is carried out using the R-packages of the Multivariate Bias Correction of Climate Model Outputs (MBC) project and it is available through the following Github link: https://github.com/cran/MBC . The UNet downscaling approach is carried out using the python-packages of the tensorflow2 and it is available through the following Github link: https://github.com/tensorflow/tensorflow . All code used in this study can be available through the following Github link: https://github.com/LinHai-debug/CLIMEA-BCUD-code .
10.1038/s41597-023-02823-7,This study did not employ a custom script; data processing was conducted following the protocols and manuals of the relevant bioinformatics software mentioned in Methods section.
10.1038/s41597-023-02790-z,"The data analysis methods, software and associated parameters used in present study are described in the section of Methods. If no detail parameters were described for software used in this study, default parameters were employed. No custom scripts were generated in this work."
10.1038/s41597-023-02785-w,The provided datasets were established without the use of any custom code.
10.1038/s41597-023-02801-z,"Accompanying scripts are made available at https://github.com/kueddelmaier/POLIANNA . The repository contains scripts to split the raw policy text retrieved from EUR-Lex into articles, to process new data labeled with Inception, and to generate summary statistics."
10.1038/s41597-023-02793-w,"Matlab and Python scripts are provided in the codes directory of dataset for the users to replicate some of the figures: • FMCW_Radar_process.m This script is used to load the raw signals recorded by the AWR2243 radar. Then it is used to visualise the first and second FFT through distance dimension and angle dimension, respectively. Lastly, by reading the human location’s phase variation, we can get human-related signals, including lip motion. This step can be transferred to PYTHON and other coding methods which supports reading binary files. • UWB_radar_process.m This script is used to load the raw signals recorded by the Xethru X4M03 radar and process the data to STFT spectrums. This step can be transferred to PYTHON and other coding methods which supports reading binary files. • plot_InParallel.m This script provides a template to plot the spectrums that are shown on paper. First, the preprocessing data is needed to be downloaded. • uwb_cutting.py; mmWave_cutting.py; kinect_cutting.py; laser_cutting.py; This script can be utilised to cut the different data sequence in NPY format with given Kinect timestamp, and convert BVH to CSV files. In advance of this step, the radar signals should be processed to spectrums with provided raw data in DAT. This step can be directly used with the radar scripts we provide."
10.1038/s41597-023-02819-3,"The custom Matlab code used for experiment control and data collection is specific to our setup, as described above in section “The refractometer setup”. It was running in Matlab version 2015b, 64 bit. It is stored publically in GitHub repository 18 ."
10.1038/s41597-023-02813-9,"Two R scripts (“preprocessing.R” and “lmeModelling.R”), resulting from the step-by-step coding for our data preprocessing and technical validation, respectively, are released in the repository of OSF 32 . Also released is the source code file (mergeChineseInfo.java) of a Java program for integrating lexical property information of CLD for the words in HKC by means of word matching, on the premise of a standardised format (word-based and UTF-8 comma-delimited data format)."
10.1038/s41597-023-02768-x,MISSING
10.1038/s41597-023-02821-9,"All commands and pipelines used were performed according to the manuals or protocols of the tools used in this study. The software and tools used are publicly accessible, with the version and parameters specified in the Methods section. If no detailed parameters were mentioned, default parameters were used. No custom code was used in this study."
10.1038/s41597-023-02838-0,"All commands and pipelines used in the data processing were all executed according to the manuals and protocols of the corresponding bioinformatics software. If no detailed parameters were provided, default parameters were used. The version of the software has been specified in the Methods section. No custom programming or coding was used."
10.1038/s41597-023-02797-6,The codes used to produce figures in this manuscript (Fig. 2b–d ) are available in R programming language on the main GitHub repository: https://github.com/catarinasiopa/PolLimCrop.git .
10.1038/s41597-023-02716-9,"The source code used for the dataset generation is open source ( https://github.com/CrySyS/CAN-Dataset-Generator ), which allows others to extend or modify the dataset. Fabrication attack generation requires a few easily accessible hardware components, while the masquerade attacks can be generated on any general-purpose computer."
10.1038/s41597-023-02815-7,"Code generated by our analysis, including the R scripts for data preparation and composite index analysis, is available on GitHub at https://github.com/shentroy/Code_city_ETI ."
10.1038/s41597-023-02826-4,"No custom software was used to process the data described in this paper. The open-source software used to conduct this study was Python version 3.7.6. The packages and libraries used included Numpy (V 1.18.1), Pandas (V 1.3.0), Matplotlib (V 3.1.3), and Scipy (V 1.4.1). Specific functions for the statistical analysis including the T-test, F-test, and Kolmogorov-Smirnov functions were conducted using the Scipy stats module."
10.1038/s41597-023-02745-4,Code and associated data files are available in the figshare data repository 63 .
10.1038/s41597-023-02822-8,"The Python code for dataset generation, validation, and visualization is available at https://github.com/FQMei/HG-Land-ET.git ."
10.1038/s41597-023-02814-8,"The code for collection PMC-Patients dataset and benchmark, as well as the code for reproducing the baseline models implemented in this paper is available at https://github.com/zhao-zy15/PMC-Patients . There is also a leaderboard of PMC-Patients benchmarks available at https://pmc-patients.github.io/ . For those who are interested in improving ReCDS performances, please refer to https://github.com/pmc-patients/pmc-patients for evaluation code and submission guidelines."
10.1038/s41597-023-02792-x,"The replication code for the two demonstrations of our data is publicly available on OSF 42 , and can be used under a CC-BY license."
10.1038/s41597-023-02833-5,The script used in this study was deposited in GitHub ( https://github.com/Myung-ShinKim/scRNA-seq_pipeline ).
10.1038/s41597-023-02812-w,All data cleaning and preparation was performed in R (version 4.2.1). The R code used to generate the dataset is available on OSF as a markdown notebook.
10.1038/s41597-023-02783-y,No custom code was used.
10.1038/s41597-023-02799-4,"Code usage and dataset availability: The generation of this data did not involve the use of any code. Additionally, no code is necessary to access or analyze this dataset. However, to reproduce the semantic segmentation results, the source code are available at https://github.com/BinaLab/RescueNet-A-High-Resolution-Post-Disaster-UAV-Dataset-for-Semantic-Segmentation/tree/main ."
10.1038/s41597-023-02737-4,"The data described in this manuscript was generated using some custom code located in CodeAvailability.zip of the Zenodo repository T1DiabetesGranada: a longitudinal multi-modal dataset of type 1 diabetes mellitus 27 . The code is provided as Jupyter Notebooks created with Python v. 3.8. The code was used to conduct the tasks described in sections Data preparation and Data Records, such as data curation and transformation, and variables extraction."
10.1038/s41597-023-02830-8,The data gap is addressed using the R language and QGIS. The R code can be obtained from the open-access online dataset Figshare 41 .
10.1038/s41597-023-02663-5,The code for 2D and 3D skeleton pose data generation and the use-case code evaluating STS speed is available here: https://github.com/ale152/SitToStandPD .
10.1038/s41597-023-02842-4,"Code, documentation, and tutorials for this project are available on the MarFERReT repository 299 : https://github.com/armbrustlab/marferret . This repository has also been archived for the v.1.1 release 310 and the archived code is available on Zenodo here: https://zenodo.org/records/10278540 . Information on the software versions and parameters used in this publication are included in the MarFERReT containerized build on the repository and in the archived code."
10.1038/s41597-023-02849-x,All bioinformatic tools used in this study followed the corresponding manuals and protocols. The versions and code/parameters of software are described in the Methods. Default parameters were employed if no detailed parameters were mentioned for the software used in this study.
10.1038/s41597-023-02837-1,No custom code was used.
10.1038/s41597-023-02839-z,No custom code was generated for this work.
10.1038/s41597-023-02843-3,All software used in this study was run according to the official instructions. The version and parameters of the software and the other custom codes used were described in Methods. Anything not specified in Methods was run with default parameters.
10.1038/s41597-023-02840-6,"Code for simulating additional new IMUs for this dataset can be found in the code folder of the dataset and is based on the virtual IMUs included for each task 45 . A plotting script is included for visualizing the angle, moment, and power data from each of the tasks across participants. Finally, a function is included which groups the individual tasks by the groups used in the above analyses. Example scripts are included which demonstrate how to use each function."
10.1038/s41597-023-02816-6,The code for parsing YouTube and Reddit networks is available under an MIT license at https://github.com/ChillsTV/AffectiveStimuliScraper .
10.1038/s41597-023-02845-1,"No custom code was used in this paper, and all of the programs used were publicly available."
10.1038/s41597-023-02852-2,The following software used in the study was run with default parameters if not specifically stated in the Methods paragraph and no custom code was used for the purposes of this study. FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc (version 0.11.9). fastp: https://github.com/OpenGene/fastp (version 0.20.0). hisat2: https://daehwankimlab.github.io/hisat2/download (version 2.2.1). FeatureCounts: https://sourceforge.net/projects/subread (version 2.0.4). Differential analysis: R package DESeq 2 1.38.3. clusterProfile: R package clusterProfiler v4.6.2. Veen: https://hiplot.cn/basic/venn .
10.1038/s41597-023-02758-z,"All code used to create the dataset from original sources, validate the dataset, as well as generate the figures are available at OSF repository ( https://osf.io/mrghc )."
10.1038/s41597-023-02657-3,"To ensure that the dataset can be easily reproduced and expanded upon in the future, we have made all the Python and R code used to generate and validate the resource available on a code repository ( https://github.com/JiaxinWang123/ScientificData_Labeled_Hardwood_Images ). StoManager1’s source code and an online demonstration are available on GitHub ( https://github.com/JiaxinWang123/StoManager1 ), along with a user-friendly Windows application on Zenodo 13 ."
10.1038/s41597-023-02848-y,"The source code is publicly available in GitHub, with a direct URL: https://github.com/marcensea/diptera-wips.git ."
10.1038/s41597-023-02834-4,The Matlab code for calculating the average ears are available at: https://www.github.com/willowfly/babyEar4k/ .
10.1038/s41597-023-02858-w,No code was used in the creation of these data. Titles should avoid the use of acronyms and abbreviations where possible. Colons and parentheses are not permitted.
10.1038/s41597-023-02818-4,"The technical details of the reconstruction algorithm used to generate phase images are described in detail in the Methods section, providing the necessary information for reproducibility of the phase reconstruction of the presented dataset."
10.1038/s41597-023-02857-x,"Matlab code used to create the gridded product for each mooring 12 , 13 is available at https://github.com/csiro-oceandata-code/IMOSMooringGridding/ . The SOM code used to create the SOM-filled dataset is available from https://github.com/ilarinieminen/SOM-Toolboxand https://github.com/ilarinieminen/SOM-Toolbox or http://www.cis.hut.fi/projects/somtoolbox/http://www.cis.hut.fi/projects/somtoolbox/ . Matlab function griddata using natural neighbor interpolation method was used to create the time, distance and depth gridded data product."
10.1038/s41597-023-02820-w,"MATLAB code for recording behavioral data is publically available as an app on Figshare 11 . Interface allows researchers to customize sampling intervals, determine the number of animals analyzed simultaneously and synchronize both lateral and top video cameras for analysis. The output has a first column denoting the frame sample was taken and the following columns the behavior of each animal at the corresponding time point. The commercially available ANY-maze Video Tracking System software can be downloaded at www.anymaze.com . Example codes for (i) an app that shows synchronized acceleration vectors an video recordings 24 and (ii) visualizing simultaneously Behavioral (eg. Shakes, Reproductive) and Accelerometer data (eg. y-axis vector) 25 , are provided on Figshare."
10.1038/s41597-023-02751-6,"The scripts of the modelling approach described in this paper was made available at recherche.data.gouv.fr 57 . Our code is available within the GPASOIL_scripts.tgz file along the tgz file that contain the GPASOIL-v1.0 and GPASOIL-v1.1 soil P pools. GPASOIL_scripts.tgz contains the whole file tree and associated scripts that were used to generate GPASOIL-v1_output.tgz. The directory GPASOIL/ contains few sub-directories: - GrabData_and_PrepDriver/ that contains all scripts (bash and python) used to generate the input of the soil P dynamic model. These input correspond to the different drivers described in the current manuscript. The input files generated (in netcdf format) are within the directory output_prepDriver/ and are here provided. The procedures to download the original dataset (if this dataset is available on the web) used to generate the input files and information are given for each driver in specific README.txt files. - main/ contains the soil P dynamic model. Different version are provided: (model = v0), (model = v1.0) and (model = v1.1). The 2nd one (i.e. (model = v1.0)) was used to generate GPASOILv1.0 while the latter (i.e. (model = v1.1)) was used to generate GPASOILv1.1 - evaluation/ contains the script to compare the model output to regional databases. The databases are not provided. General information about the procedure to get the regional databases are provided in a specific README.txt file. - output_main/ is the directory that receives the different soil P maps simulated. A container generated with Singularity 3 was also provided and allows users to run the different scripts on a server or local computer without issue of incompatibility about Ubuntu distribution or Python packages. Modeling and analysis were performed in using Python (Python Software Foundation. Python Language Reference, version 3.6.10., available at http://www.python.org , last access: January 2020)."
10.1038/s41597-023-02810-y,"The transformation of RGB frames into estimated monocular depth frames was conducted using models from the works cited in 20 , 39 . These models are available at ( https://github.com/isl-org/MiDaS ). In a similar manner, the generation of segmentation masks from RGB frames was accomplished using the SAM model 21 , accessible from ( https://github.com/facebookresearch/segment-anything ). Additionally, the VOT algorithms employed in this study were sourced from the official codes provided by the respective authors, as detailed in Table 10 . For ease of access and utilization, all relevant codes, finetuned models, and predicted bounding boxes of the visual object trackers have been collated on our project’s Figshare page. These resources can be accessed via the following links: ( https://doi.org/10.6084/m9.figshare.24590268.v2 ) 38 , and ( https://github.com/HamadYA/D-PTUAC )."
10.1038/s41597-023-02828-2,The complete repository can be found at: https://github.com/rodaguayo/PatagoniaMet 116 .
10.1038/s41597-023-02856-y,Code to estimate SPEI can be downloaded from: https://github.com/sbegueria/SPEI .
10.1038/s41597-023-02854-0,"We provided participants with a template repository on GitHub ( https://github.com/cuamc-dop-ids/hptbi-hackathon ) that included a skeleton for working with either R or Python. The R package, the participant template repository, and administrator repository have been uploaded to Zenodo 14 ( https://doi.org/10.5281/zenodo.8400499 ) as well."
10.1038/s41597-023-02847-z,"The code for cropping, generating dataset labels is publicly available: https://github.com/Aizu0/CAS-Landslide-Dataset-production-code ."
10.1038/s41597-023-02831-7,Scripts used with R programming language and additional related files are provided at: https://doi.org/10.57745/HV33V1 .
10.1038/s41597-023-02825-5,Code for sequence data processing and for reproducing the graphs of this paper is available through our figshare repository 26 ( https://figshare.com/s/b2962b2174747c6bc869 ).
10.1038/s41597-023-02899-1,"The R-code and ArcGIS toolboxes allowing to reproduce the standardization procedure, the computation of focal and distance statistics, and the technical validation are openly available on the SWECO25 GitHub repository https://github.com/NKulling/SWECO25 ."
10.1038/s41597-023-02784-x,Programming R code is openly available together with the database from Figshare 167 .
10.1038/s41597-023-02876-8,"The dataset described in this paper is available at figshare 31 under the Creative Commons Attribution 4.0 license ( http://creativecommons.org/licenses/by/4.0 ). This license permits the use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original authors and the source. The dataset was generated by IMSR-Tool, an open-source Java program that is accessible via zenodo 32 . The latest release of IMSR-Tool (version 1.06) includes a runnable desktop application and a user manual that are publicly available at https://github.com/thumit/IMSRtool/releases/tag/1.06 . IMSR-Tool is licensed under the GNU General Public License version 3 or later (GNU-GPL3, http://www.gnu.org/licenses ), which allows users to freely download, use, distribute, and modify the tool and its source code, given that the modified tool and source-code must be released to the public under the same GNU-GPL3 license."
10.1038/s41597-023-02871-z,"The necessary scripts for this study are available in the GitHub repository ( https://github.com/cmctec/ARCADE ). This repository contains scripts to convert COCO annotation files into 2D segmentation masks and YOLOv8 labels format (and back), contrast enhancement scripts, scripts for evaluating vessel and stenosis segmentation models, and code for training and post-processing binary segmentation results."
10.1038/s41597-023-02873-x,"The TreeML-SM script “TreeML-SM.py”, transformation script “point_transformation.py” for location transformation from project coordinate system to global coordinate system, and pre-trained point cloud segmentation model are published in the GitHub repository ( https://github.com/hadi-yazdi/TreeML-Data ). Please refer to the Readme file in the Github repository for further information."
10.1038/s41597-023-02866-w,All the code to generate FISHGLOB_data from the raw datasets can be accessed here: https://github.com/AquaAuma/FishGlob_data and archived on Zenodo for download https://zenodo.org/records/10218308 .
10.1038/s41597-023-02851-3,"Default parameters were employed if no detailed parameters were mentioned below. (1) Trimmomatic v0.36: phred33, LEADING:3, TRAILING:3, SLIDINGWINDOW:4:15, MINLEN:36 (2) Jellyfish v2.3.0: −C −m 21 (3) GenomeScope v2: k-mer length 21, ploidy 2 (4) Population genomic analyses: All bash command lines and scripts are available at the GitHub repository: https://github.com/CWJeongLab/Ellobium , which includes detailed parameters used for population genomic analyses."
10.1038/s41597-023-02841-5,"Provided code can be found in the same repository as the dataset 30 , named dataset_processData.m . This code is written and tested in Matlab R2020a. No additional toolbox is required to run this code. At the top of the code, there are options to set folder name and path with variable folderName . Select participants and sessions of interest for processing (cut data from whole sessions into trials) with variables Participants and Sessions . A pdf version of the code is also included ( dataset_processData.pdf )."
10.1038/s41597-023-02879-5,No custom code is required to access and use the ARTINA spectra database.
10.1038/s41597-023-02864-y,"The code for creating the annotations and demonstrating interactions with the data is available as Release v2.0.0 29 , and is also available at https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/releases/tag/v2.0.0 . The Colaboratory usage notebook is available here: https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/tree/main/usage_notebooks . In order to run the notebook, users must set up a GCP project by following the instructions here https://learn.canceridc.dev/introduction/getting-started-with-gcp . The notebooks used to query, run the analysis, and create the DICOM Segmentation objects and Structured Reports are listed here: 1. NSCLC-Radiomics analysis: https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nsclc_nnunet_and_bpr_infer.ipynb . 2. NLST analysis: https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nlst_nnunet_and_bpr_infer.ipynb . The queries that that were used to perform the filtering of the relevant series are here: 3. NSCLC-Radiomics: https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NSCLC_Radiomics_query.txt . 4. NLST: https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NLST_query.txt ."
10.1038/s41597-023-02829-1,No custom code was used in this work. The R packages that were used to analyze the RNA-seq data are given in the methods section.
10.1038/s41597-023-02874-w,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02836-2,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-023-02900-x,No custom scripts or codes were used in the management and verification of the data sets in this study. All software and pipelines used for data processing were executed according to the manuals and protocols of the bioinformatics software cited above. The specific parameters were described if the default parameters were not applied for data analysis.
10.1038/s41597-023-02893-7,"No specific code was used in this study. The standard bioinformatic tools were used for data analysis. Furthermore, the parameter setting of the bioinformatics tools was performed in accordance with the manual and protocols and described in the Methods Section."
10.1038/s41597-023-02892-8,The code used in our analysis is available on GitHub ( https://github.com/zddzxxsmile/Chaos-game-representation-of-EEG-microstate ).
10.1038/s41597-023-02863-z,The software used for bias adjustment and statistical downscaling is ISIMIP3BASD v2.5 29 .
10.1038/s41597-023-02859-9,"Code used to generate the optimized coordinates for OSCILAT is not retained or distributed with the data in the data warehouse because of the way that Census firewalls its restricted data. The approval process for projects within the FSRDC is designed to minimize researchers’ access to only the information needed for their specific project. Because it retains only the bare minimum of information necessary to link census responses to standardized location information, access to OSCILAT is substantially easier to obtain than access to all of the MAFX and census responses used to generate OSCILAT. Because the code to generate OSCILAT responses includes information about those data sets it would only be accessible to individuals who had gained approval for all of the census responses, MAFXs, and the ACF. Rather than impose this burden on OSCILAT users, the code is retained only within the FSRDC project space."
10.1038/s41597-023-02733-8,"The R code for the quality control of the observational data and their assimilation ModE-RA ensemble can be found together with the entire ModE-RA dataset and observation feedback archive at the World Data Center for Climate (WDCC) at Deutsches Klimarechenzentrum in Hamburg, Germany ( https://doi.org/10.26050/WDCC/ModE-RA_s14203-18501 )."
10.1038/s41597-023-02884-8,No custom code was used in the analysis of the data in this manuscript.
10.1038/s41597-023-02869-7,"The source codes of data collection, processing and analysis are stored at: ( https://github.com/xuanyshi/Finer-Grained-Clinical-Trial-Results )."
10.1038/s41597-023-02878-6,"All software used in this paper have been described in the Methods section with the respective version number. For this study, no custom code was generated."
10.1038/s41597-023-02903-8,List of commands run and scripts used are available on GitHub under: https://github.com/TheaFrances/E.scolopes-V2.2-BRAKER2-gene-annotation 54 .
10.1038/s41597-023-02889-3,There was no custom code used in this study.
10.1038/s41597-024-02910-3,"RepeatModeler: -database weevil_rdb -threads 100 -LTRStruct RepeatMasker: -e ncbi -pa 80 -norna -lib -a -xsmall -gff Hisat2: -p 40 --dta --phred33 -q -S --summary-file Samtools: view -@ 40 -uhS & sort -@ 40 minimap2: -ax splice -t 40 -uf -C5 rnammer: -S euk -m tsu,ssu,lsu -multi -gff tRNAscan-SE -E -o weevil_trna -f weevil_secondary -m weevil_stat -H Diamond: blastp -p 40 --query --db --evalue 1e-6 --max-hsps 1 -k 1 --outfmt 6 Interproscan: -appl pfam -dp -f TSV -goterms -iprlookup -pa -t p -cpu 90"
10.1038/s41597-023-02888-4,"All code covering data pre-processing, quality control, and figures included in this manuscript are deposited and available in our Zenodo repository 20 . We include additional interactive notebooks which give users examples of how to perform differential gene expression analysis using DESeq2, discover gene modules using weighed gene co-expression analysis (WGCNA), and find enriched terms in databases."
10.1038/s41597-023-02827-3,We hereby declare that no custom code was employed in the production or analysis of the data presented in this work.
10.1038/s41597-023-02796-7,No particular code was created for this dataset.
10.1038/s41597-023-02824-6,Data analysis was carried using readily available (open source) R code and packages. Code used in data extrapolation and interpolation is provided in the Supplementary Information. Example code for effort mapping and to explore fishing capacity (R-shiny) is provided in https://github.com/Global-Fishing-Effort/RousseauEtAl2023 .
10.1038/s41597-023-02867-9,"Data visualization was conducted using R (version 3.5.1). The Rscript “BiocharDS_V1.0. R” includes codes to generate the location of the sites (Fig. 2 ) and check the distribution of key information in the dataset (Figs 3 , 4 ). All the code and data used are available."
10.1038/s41597-023-02901-w,No custom code was used for this study. All data analyses were conducted using published bioinformatics software with default settings unless otherwise specified.
10.1038/s41597-023-02907-4,No specific code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-023-02883-9,"The R code used to process AIS data was made available by previous Zenodo repositories 51 , 69 , while the QGIS model is deposited in the figshare public repository 46 along with the released dataset. The R shiny web app is free of use using default login credentials (user: guest, password: guest23) and the related code will be released once it is finalised to be directly used in the field. All this could facilitate the adoption of the proposed integrated mapping approach and its tailoring in other contexts/case studies and decision-making frameworks."
10.1038/s41597-023-02902-9,None of the code or equations are new. All methods are from previously published research. The code used to validate the data fields are standard Excel functions.
10.1038/s41597-023-02835-3,"No code was used to generate this dataset. Figure 5 was plotted by using Ocean Data View software (odv.awi.de), while the others have been generated using MATLAB ©."
10.1038/s41597-023-02890-w,All the source codes used for producing this dataset are openly available from the following link: https://github.com/urbanbigdatacentre/access_uk_open . The versions of the relevant software used are stated for each of the key elements in the main body of the paper.
10.1038/s41597-023-02875-9,Code used in this paper is available here: https://github.com/wsag/WBM/tree/main/utilities . This GitHub wsag/WBM repository is licensed under the “GNU General Public License v3.0” and is one of the open-source public software access and use licenses. File names: 1. networkTools - Executable. Usage: » networkTools -v JOB_PARAMETERS.init 2. networkTools_manual.init - processing parameters *.init file template. Use corresponding options for the endorheic delineation of a given river network. Input/Output options and parameters are described in it.
10.1038/s41597-023-02846-0,The code files used to process the dataset provided by GoiEner are publicly available on GitHub ( https://github.com/DeustoTech/GoiEner-dataset ) and are licensed under the GPL-3.0. The repository includes a comprehensive README.md file with detailed information and instructions on how to run the code. The code is written in R version 4.2.2 and is compatible with both Windows and Linux operating systems.
10.1038/s41597-023-02904-7,The R package REcoTox 43 version 0.4.1 for processing US EPA ECOTOX Knowledgebase files is available on GitHub ( https://github.com/tsufz/REcoTox/releases/tag/0.4.1 ) under AGPL-3.0 license. The REcoTox and graphics processing scripts are available on Zenodo 44 under AGPL-3.0 license.
10.1038/s41597-023-02731-w,The R code to convert the raw measurements into the final database and to create all tables and figures used in this publication can be found at GitHub ( https://github.com/Peter-T-Ruehr/InsectBiteForceDatabase ) and Zenodo 49 .
10.1038/s41597-023-02744-5,The SOSpine dataset was prepared as described in the methods sections with minimal custom processing. Open-source software used for computer vision training and validation can be found on Github ( https://github.com/AlexeyAB/darknet ). Python code used to generate the figures and perform the technical validation can be found within the FigShare project as described above within the SOSpine.zip file. Python code and necessary dataset files can also be found on GitHub ( https://github.com/alanbalugu/SOSpine ).
10.1038/s41597-023-02891-9,"All data visualizations in this paper were created using Python and its publicly available libraries, including Pandas, NumPy, Matplotlib, Seaborn, and Missingno. A coding guide has been compiled in a Jupyter notebook and is provided along with the dataset in the Figshare repository."
10.1038/s41597-023-02855-z,The code to reproduce the dataset ESPO-G6-R2 dataset and the figures from this paper are available in the release ESPO-G6-R2 v1.0.0 ( https://github.com/Ouranosinc/ESPO-G/releases/tag/ESPO-G6-R2v1.0.0 ) of the ESPO-G GitHub repository ( https://github.com/Ouranosinc/ESPO-G ). The code works with xclim version 0.41.0 and xscen version 0.5.13.
10.1038/s41597-023-02729-4,No specific script was used in this work. The codes and pipelines used in data processing were all executed according to the manual and protocols of the corresponding bioinformatics software. The specific versions of software have been described in Methods.
10.1038/s41597-023-02616-y,Bioclimatic variables were calculated using SPLASH 19 . SPLASH code is available from bitbucket ( https://bitbucket.org/labprentice/splash/src/master/ ).
10.1038/s41597-023-02689-9,"The R code used to process trait data is included with the Pelagic Species Trait Database files on Borealis 13 . Raw data inputs for the code are the data collection tables, with all other tables containing the processed, output data. Users are recommended to download the entire dataset, as the file structure is integrated into the code for data inputs."
10.1038/s41597-023-02850-4,The code for analysing the RNA-sequencing data from mapping to genome and transcriptome to variant calling was combined into a Snakemake 66 pipeline and is available on GitHub: https://github.com/SchreiberM/BARN .
10.1038/s41597-023-02780-1,"The raw aerial imagery and annotation tools used in this study are publicly accessible 36 , 45 . The source code developed by the authors to process the imagery and develop the tank inventory dataset are available on GitHub ( https://github.com/celinerobi/ast-data-pipeline )."
10.1038/s41597-024-02912-1,Genome annotation: (1) RepeatMasker: parameters: -e ncbi -a -nolow -no_is -norna. (2) TE-class: parameters: all parameters were set as default. (3) Braker2: parameters: all parameters were set as default. (4) PASA: --ALIGNERS blat. (5) EvidenceModeler: parameters: all parameters were set as default. Genome assembly: (1) NextDenovo: parameters: all parameters were set as default. Gene family identifcation and phylogenetic analysis: (1) RAxML: parameters: -f a -m PROTGAMMAAUTO. (2) MCMCTREE: parameters: all parameters were set as default. Other analysis modules that were not mentioned parameters were used default parameters. The other custom codes used in this analysis were mentioned in methods sections.
10.1038/s41597-023-02887-5,"There is no customized code in generation or processing of datasets. For setting up and training the Deep Learning Models, the publicly available codes in Python language from TensorFlow 9 and Keras 10 libraries were used. The trend error mitigation and all the figure plots in the paper were implemented using the existing routines/functions in MATLAB software."
10.1038/s41597-023-02882-w,pydar is available at Zenodo 19 ( https://zenodo.org/record/8120858 ).
10.1038/s41597-023-02894-6,Genome assembly: 1. jellyfish: parameters: -m 19 -s 100M 2. GenomeScope: all parameters were set as default 3. wtdbg2: parameters: -x sq -g 1G -K 2000 –edge-min 4 -p 19 -S 4 -L 5000 –tidy-reads 8000 4. wtpoa-cns: all parameters were set as default 5. 3D-DNA: all parameters were set as default 6. Juicebox: all parameters were set as default 7. BUSCO: parameters: -l busco_downloads/lineages/aves_odb10 Genome annotation: 1. RepeatMasker: parameters: -poly -xsmall -engine ncbi -no_is 2. Repeatmodele: parameters: -engine ncbi 3. EDTA: parameters: –species others –step all –anno 1 -t 30 –rmout RepeatMasker.out 4. Trinity: parameters: –seqType fq –SS_lib_type RF –normalize_reads 5. Maker: all parameters were set as default 6. GeMoMa: GeMoMaPipeline parameters: GeMoMa.p = 20 GeMoMa.c = 0.3 AnnotationFinalizer.r = NO; AnnotationFinalizer parameters: u = YES; 7. eggNOG-mapper: all parameters were set as default Whole genome alignment: 1. BWA: all parameters were set as default The parameters not mentioned analysis modules in our study were used as default parameters.
10.1038/s41597-024-02916-x,We used curatedMetagenomicData as our sole data source to access data records of interest ( https://doi.org/doi:10.18129/B9.bioc.curatedMetagenomicData ). R versions 4.3 and later can be used to use this package. All procedures in this package are done by only 4 functions: (1) curatedMetagenomicData(): used to access the data (2) mergeData(): used to merge lists of the same data type (e.g. relative abundance data) across studies (3) returnSamples(): used to return samples across studies sampleMetadata(): used to return sample metadata The custom R code is available in Figshare 26 .
10.1038/s41597-024-02921-0,"All data harmonization, modeling, and validation procedures for the LTS-US dataset 54 were scripted in the R Statistical Environment 71 , using the tidyverse 72 , lubridate 73 , data.table 74 , sf 75 , keras 76 , tensorflow 77 , caret 78 , CAST 79 , yaml 80 , reticulate 81 , xgboost 82 , nnet 47 , viridis 83 , trend 84 , multiROC 85 , ggpubr 86 , fastshap 87 , maps 88 , ggtext 89 , and ggforce 90 packages. To enhance reproducibility, all scripts are designed to work within a single pipeline that uses the targets package 91 . The targets pipeline is divided into four main components: “1_aggregate”, “2_train”, “3_predict”, and “4_qc”. Each component corresponds to one of the steps presented above and can be customized by users to fit their specific needs. The associated pipeline setup and user guide can be found on the Environmental Data Initiative 54 , where the “README_targets.pdf” file details directory architecture and how to execute the pipeline. When downloading the “scripts.zip” folder to access the targets pipeline, future users should be aware that empty files within the directory are necessary for running the pipeline, as those folders will become populated each time the pipeline is run. To ensure reproducibility across operating platforms, all scripts for the pipeline can be executed within a container. Running the pipeline within the container allows users to execute the entire pipeline without the need to make small, yet important, edits to the code, or to configure their own operating environment to conform to the pipeline’s requirements. For example, recent versions of the sf package default to using the s2 spherical geometry engine instead of the Graphic Environment Operating System (GEOS), which assumes planar coordinates. End users on a system with one version of the sf library might need to adjust the code to use the correct geometry engine, whereas users with another version might be able to run the pipeline without any adjustments. The container crystallizes a known-working set of libraries, both at the system level (e.g., GEOS, GDAL, PROJ) and at the R level (e.g., sf), so that anybody can run the code without reconfiguring their own environment. This also provides future proofing by ensuring that the inevitable changes to other libraries over time do not lead to errors. To help end users, who are less familiar with running containerized code, a tutorial for installing and executing the pipeline within the container is located in the Environmental Data Initiative repository as a compressed entity (see “README_container.pdf”) 54 . The EDI repository also contains both a rendered (“lake_trophic_status_docker_image.tar.gz”; ~3.5 GB) and unrendered (“lts_container.zip”; ~4.0 KB) docker image. While the document “README_container.pdf” details information for running both the rendered and unrendered images, future users can choose either format depending on their familiarity with rendering Docker images and their capacity to download larger Docker images."
10.1038/s41597-024-02925-w,Image analysis is performed using the open-source software Fiji-ImageJ (1.53c) and MATLAB (R2021a). Shape characterisation of particles and calculation of shape descriptor parameters including surface roughness and sphericity were performed utilising the SHAPE code by Angelidakis et al . 34 publicly available from the link below: https://github.com/vsangelidakis/SHAPE .
10.1038/s41597-024-02915-y,"The R codes used for Differential analysis, identification of risk genes and risk pathways, calculation of cross talk between risk pathways, individual drug screening, optimization of drug combination are available in https://github.com/ouqiyjl/oDrugCP/tree/main ."
10.1038/s41597-024-02918-9,We released and shared the code for our data synthesis( https://github.com/CV-Altrai2023/RailFOD23 ).
10.1038/s41597-024-02923-y,The code used to produce the presented human thermal bioclimate dataset is publicly available at the Zenodo repository 88 . It is free to re-use and modify with attribution under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.
10.1038/s41597-024-02935-8,"The data analysis methods, software and associated parameters used in this study are described in the section of Methods. The code used to derive all datasets from the raw microdata can be downloaded at https://doi.org/10.3886/E192045V2 ."
10.1038/s41597-023-02877-7,All relevant code used in these analyses can be found at https://github.com/mvee18/benchmarkingpaper and in the figshare repository 105 . The README in either repository provides additional useful information for the usage and description of files.
10.1038/s41597-023-02897-3,The code used to generate the databases in this work can be found at https://github.com/Dingyun-Huang/chemdataextractorTADF . The repository contains ChemDataExtractor v2.1 which has been modified for text-mining TADF properties; iPython notebooks that demonstrate an example data-extraction pipeline and data-cleaning and post-processing are also provided.
10.1038/s41597-023-02691-1,"The GHSL project produces free and open tools such as the GHS-DUG 31 tool ( https://ghsl.jrc.ec.europa.eu/tools.php ) that outputs the settlement classification, by applying the Degree of Urbanisation to input population and built-up area grids. One of the outputs of this tool is the shapefile urban centres delineations that are the spatial units used in this study."
10.1038/s41597-024-02933-w,Oracle-MNIST are freely available online at GitHub ( https://github.com/wm-bupt/oracle-mnist ). Tutorials for loading the dataset and code for training and testing oracle character recognition models are also publicly available without restriction.
10.1038/s41597-024-02936-7,No custom code was used in creating the dataset.
10.1038/s41597-024-02939-4,All versions of third-party software and scripts used in this study are described and referenced accordingly in the “Materials and methods” section for ease of access and reproducibility.
10.1038/s41597-023-02906-5,Code for generating block/block group-level 3-binned income for the projected years under different SSPs can be found at GitHub ( https://github.com/crystalandwan/Income-Reconciliation.git ).
10.1038/s41597-024-02928-7,"All code that has been used during the creation of this dataset is published on Zenodo (v0.2) 51 and curated on GitHub ( https://github.com/pano-tacanan-history/blumpanotacana ). For converting the data to CLDF 7 , we have used the Python tools cldfbench (v1.13.0) 59 using the pylexibank plugin (v3.4.0) 62 . The dataset is linked to Concepticon (v3.1.0) 40 , Glottolog (v4.7) 52 , and CLTS (v2.2.0) 53 , 57 . The code for integrating data with other datasets via SQL is presented in the main README.md. The scripts that were used to create the plots and to compute the coverage and synonymy is part of the ‘analysis’ folder, where another README.md file leads through the replication of all necessary steps. The code for the initial addition of IDS data is added to ‘raw/archive/‘ for documentation. This list was then filtered while finalizing the concept list. All the orthography profiles that are used during the conversion of graphemes are part of ‘etc/orthography’."
10.1038/s41597-024-02914-z,Step-by-step guidance and the source code for machine learning benchmarks can be found on Zenodo 8 .
10.1038/s41597-023-02885-7,"The software and pipelines used in this study were executed following the developers’ instructions, and the versions and parameters of these bioinformatic tools were described in the Methods section. If the parameter is not provided, the default value is used. No custom script or code was used."
10.1038/s41597-023-02880-y,"DuckDB and Python were employed for constructing the mobility networks. The entire network dataset, along with the code utilized for network construction and supplementary tables can be found at: https://doi.org/10.17605/OSF.IO/GWQ6U 50 ."
10.1038/s41597-024-02943-8,All code associated with this project is available as open source. The code is available on GitHub ( https://github.com/rutishauserlab/workingmem-release-NWB ). MATLAB scripts are included in this repository to reproduce all figures shown and to illustrate how to use the data.
10.1038/s41597-023-02898-2,"The dataset described here, which represents the core of the refractiveindex.info database, is available at Figshare 5 . It presently (as of December 2023) contains 3135 data records on 605 materials in the part of the dataset corresponding to linear optical properties ( nk ), and 193 records on 89 materials in the part corresponding to nonlinear optical properties ( n 2 ). The code that underpins the refractiveindex.info database is made accessible under the Creative Commons Zero (CC0) license ( https://creativecommons.org/publicdomain/zero/1.0 ). This license facilitates the unrestricted use, distribution, and modification of the code, making it widely accessible for various applications. The entire codebase, including detailed documentation, is publicly available on the refractiveindex.info-database GitHub project ( https://github.com/polyanskiy/refractiveindex.info-database ). This repository is regularly updated, ensuring it evolves to meet the ongoing needs of both the scientific and engineering sectors. For additional utility, users can explore the refractiveindex.info-scripts project on GitHub ( https://github.com/polyanskiy/refractiveindex.info-scripts ), which offers scripts for deriving optical constants from established models and tools for converting Zemax glass catalogs to the dataset’s YAML format. It is essential to note that the data encapsulated within the refractiveindex.info dataset is meticulously curated from publicly available sources. This includes peer-reviewed journals, authoritative books, and manufacturer datasheets, ensuring that the dataset is not only expansive but also anchored in reliability and veracity. Each data record within the dataset explicitly cites the source, offering users a pathway to delve deeper into the original data and its context. All journal papers from which data are presently used in the refractiveindex.info dataset, excluding those without a DOI identifier, are included in the following reference list 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 , 160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 , 169 , 170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 , 190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 , 211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 , 225 , 226 , 227 , 228 , 229 , 230 , 231 , 232 , 233 , 234 , 235 , 236 , 237 , 238 , 239 , 240 , 241 , 242 , 243 , 244 , 245 , 246 , 247 , 248 , 249 , 250 , 251 , 252 , 253 , 254 , 255 , 256 , 257 , 258 , 259 , 260 , 261 , 262 , 263 , 264 , 265 , 266 , 267 , 268 , 269 , 270 , 271 , 272 , 273 , 274 , 275 , 276 , 277 , 278 , 279 , 280 , 281 , 282 , 283 , 284 , 285 , 286 , 287 , 288 , 289 , 290 , 291 , 292 , 293 , 294 , 295 , 296 , 297 , 298 , 299 , 300 , 301 , 302 , 303 , 304 , 305 , 306 , 307 , 308 , 309 , 310 , 311 , 312 , 313 , 314 , 315 , 316 , 317 , 318 , 319 , 320 , 321 , 322 , 323 , 324 , 325 , 326 , 327 , 328 , 329 , 330 , 331 , 332 , 333 , 334 , 335 , 336 , 337 , 338 , 339 , 340 , 341 , 342 , 343 , 344 , 345 , 346 , 347 , 348 , 349 , 350 , 351 , 352 , 353 , 354 , 355 , 356 , 357 , 358 , 359 , 360 , 361 , 362 , 363 , 364 , 365 , 366 , 367 , 368 , 369 , 370 , 371 , 372 , 373 , 374 , 375 , 376 , 377 , 378 , 379 , 380 , 381 , 382 , 383 , 384 , 385 , 386 , 387 , 388 , 389 , 390 , 391 , 392 , 393 , 394 , 395 , 396 , 397 , 398 , 399 , 400 , 401 , 402 , 403 , 404 , 405 , 406 , 407 , 408 , 409 , 410 , 411 , 412 , 413 , 414 , 415 , 416 , 417 , 418 , 419 , 420 , 421 , 422 , 423 , 424 , 425 , 426 , 427 , 428 , 429 , 430 , 431 , 432 , 433 , 434 , 435 , 436 , 437 , 438 , 439 , 440 , 441 , 442 , 443 , 444 , 445 , 446 , 447 , 448 , 449 , 450 , 451 , 452 , 453 , 454 , 455 , 456 , 457 , 458 , 459 , 460 , 461 , 462 , 463 , 464 , 465 , 466 , 467 , 468 , 469 , 470 , 471 . By integrating a comprehensive data collection, adopting a standard-based data file format, ensuring ongoing updates, and maintaining open access, the refractiveindex.info emerges as an essential tool for researchers, engineers, and students delving into the complex world of optical constants and material properties."
10.1038/s41597-024-02929-6,"The majority of the data analyses were completed using standard bioinformatic tools running on the Linux system. The version and code/parameters of the main software tools are described in the text. Additional scripts used to generate the results and the figures can be found in the github repository: https://github.com/wzuhou/Genome_assembly_annotation . In addition, a diagrammatic pipeline is available on the home page ( https://github.com/wzuhou/Genome_assembly_annotation/blob/main/README.md#pipeline )."
10.1038/s41597-024-02951-8,The data were processed in R (version 4.3.2). The code to reproduce the results of this data description is publicly available at https://git-dmz.thuenen.de/duden/harmyields_public . The code is subject to the MIT license ( https://opensource.org/license/mit/ ) and can be used freely.
10.1038/s41597-024-02927-8,"All code is available on the Poranne Group repository on GitLab: https://gitlab.com/porannegroup/compas , licensed under a CC-BY license. Further details are provided in the repository’s online README.md file."
10.1038/s41597-023-02896-4,"The Python processing routines for the met masts, lidar and loads data are publicly available at https://github.com/NREL/NSO_processing_scripts ."
10.1038/s41597-024-02908-x,In order to do the evaluation we made the Python package ( https://github.com/qbeer/coco-froc-analysis ) accessible. We generated FROC curves with this tool and generally it is possible to use it for binary evaluation for COCO formatted data.
10.1038/s41597-024-02932-x,"The wind wave climate ensemble was produced using WAVEWATCH III v6.07.1 32 release available at https://github.com/NOAA-EMC/WW3 . The wind wave climate ensemble performance was evaluated through the combined use of the Climate Data Operator (CDO) suite 70 and the PCMDI Metrics Package (PMP) Python library, publicly available at https://github.com/PCMDI/pcmdi_metrics.git ."
10.1038/s41597-024-02917-w,"The LS-WPC method is integrated into LS-TOOL, software that functions as a tool for computing crucial topographic attributes, including the slope steepness, slope steepness factor, slope length, slope length factor, and LS-factor, which play a vital role in the assessment and evaluation of soil erosion. LS-TOOL allows users to flexibly compute the topographic factor in specific regions of interest by simply inputting the desired analysis area. The maximum computable area size depends on the physical memory of the computer. The graphical user interface (GUI) of LS-TOOL is shown in Fig. 13 . The areas denoted by the red letters A, B, C, and D are referred to in the text (where details are provided). Area A: Selection of the data type, DEM file, and output file path; Area B: calculation options, including file prefix, models, use of cut-off or not, whether to fill no-data or sink cells, how to fill no-data cells (average or minimum value of the surrounding eight cells), consider channels or not, threshold of the accumulated area, and set the cut-off slope value; Area C: algorithm options, single-flow direction (SFD) or multiple-flow direction (MFD); Area D: selection of which file(s) to save (S: slope steepness; L: slope length, S factor, L factor or ALL). LS-TOOL is available at https://doi.org/10.11888/Terre.tpdc.300613 , or contact zhm@nwsuaf.edu.cn."
10.1038/s41597-024-02911-2,The source code for the image quality assessment by Fu et.al . can be accessed at https://github.com/hzfu/EyeQ . The source code for the baseline model training and testing is available at https://github.com/tianyizheming/ichallenge_baseline .
10.1038/s41597-024-02924-x,"Data and custom code are available on GitHub repository for the automation of Raman predictions, post-processing, and database construction, under a CC BY 4.0 License 51 . The ab initio code CRYSTAL is commercially available, and the TZVP basis sets used (see Supplementary Section S1 of the SI) can be accessed from the CRYSTAL website."
10.1038/s41597-024-02944-7,No custom scripts or code was used in this study. All software and pipelines were executed according to the manuals and protocols of related published bioinformatic tools. Corresponding versions and codes/parameters of software have been described in Methods.
10.1038/s41597-024-02950-9,"The code used for exploratory data analysis, validation and visualization in this study is openly available for access and use. The codebase, which includes Jupyter Notebooks, Python scripts, and relevant libraries, is hosted on a public GitHub repository ( https://github.com/Urban-Analytics/UTM-Hanoi ). The code is distributed under the MIT License, allowing for modification, distribution, and reuse, as long as proper credit is given to the original authors and the license terms are followed."
10.1038/s41597-024-02955-4,The data were extracted from each of public archived and searched to extract data associated with the TC using Matlab codes. The NetCDF files were written using Matlab codes. These codes are available at https://doi.org/10.26188/24515113 59 .
10.1038/s41597-024-02954-5,No custom code was used in the generation or processing of this dataset. Software used in this study includes ArcGIS Pro v3.0 and ArcGIS Survey123 v.3.10.
10.1038/s41597-024-02947-4,"A MATLAB file named as load.m is included in the repository, as shown in Fig. 4 . After extracting the dataset on the local computer, the users must run the load.m file using MATLAB R2020 or later. The file will automatically load the entire dataset into the MATLAB workspace."
10.1038/s41597-024-02922-z,"The code used for preparing the PRIDE data upload, the creation of curated data views on the University of Copenhagen FTP large file storage called ERDA, the workflows for sample raw file processing are available on github.com/RasmussenLab/hela_qc_mnt_data . The software used for processing is provided as a python package in the provided GitHub repository."
10.1038/s41597-024-02946-5,"Our code for generating the dataset is available within the figshare repository within the scripts folder. An overview over the files can be found within Table 4 . For using the laser and ToF C++ code ROS (Kinetic or higher version) needs to be installed. For using the Matlab file (convert.m) a Matlab installation is necessary (i.e. version MATLAB R2022a). Furthermore the code for generating the signals simulated by the speaker can be found within the scripts folder, too. Our code may be used under the following terms. The material may not be used for commercial purposes. When using it appropriate credits need to be made. The Creative Commons CC BY-NC 4.0 applies."
10.1038/s41597-024-02949-2,"All code is provided within the data archive 8 , and separately at https://github.com/GeoscienceAustralia/ptha/tree/master/misc/hunga_tonga_data_paper ."
10.1038/s41597-024-02930-z,"The code for pre-processing the data, as well as the subsequent analyses, is available online 37 . The code was written in R version 4.1.2 and last ran on April 16, 2022."
10.1038/s41597-024-02945-6,The code to download the dataset is publicly available for download on GitHub: https://github.com/grimmlab/MFWD .
10.1038/s41597-023-02421-7,"The source code of the efficacy measurement and harmonizer transformer is publicly available in a GitHub repository at https://github.com/Imaging-AI-for-Health-virtual-lab/harmonizer . The following are the versions of software and Python libraries used to obtain the results presented in this study: - FreeSurfer version 7.1.1. For T 1 -weighted images belonging to ICBM and NKI2 datasets, we used FreeSurfer version 5.3. ABIDEI T 1 -weighted images were already processed using FreeSurfer version 5.1. - fractalbrain toolkit version 1.1 - neuroHarmonize v. 2.1.0 package - eXtreme Gradient Boosting (XGBoost) version 0.90."
10.1038/s41597-023-02886-6,The data is provided as Excel and CSV spreadsheets that can be used without code for manipulation. Sample Python scripts are available on GitHub to ease analysis and demonstrate technical validation procedures ( https://github.com/we3lab/wwtp-energy-tariffs ).
10.1038/s41597-024-02926-9,"The raw data can be utilized to generate fluorescence data (‘flouAreshape’) and digitized data (‘floudigreshape’) using the provided code (‘Digitze_Data.m’) available on figshare ( https://doi.org/10.6084/m9.figshare.23911368 ). The data processing involved in Technical Validations entails the classification of randomly generated graphs by verifying group isomorphism and obtaining the true solution for the MIS problem. This task is accomplished using custom Python code, also provided. The MIS solver is included in the ‘MISSolver’ class, saved in the Python script ‘MISSolver.py’. This solver is based on the github code ( https://github.com/GiggleLiu/MISAlgorithms.jl ) and its corresponding reference 27 . The ‘GraphTable’ class, found in Data Records, is stored in the Python script ‘GraphTable.py’. Instances of ‘GraphTable’ are saved using the Python pickle module. The ‘GraphLinkedList’ class and a ‘GraphNode’ class are preserved in the Python script ‘GraphLinkedList.py’. The code for saving the experimented solution to the ‘GraphTable’ instances is implemented in the Python Jupyter notebook ‘Save_Graphtable.pynb’. An example code for reading the ‘.pkl’ instance is provided in ‘Open_Graphtable_Example.pynb’."
10.1038/s41597-023-02905-6,"The raw data in “xlsx” or “HDF5” format was transformed into dictionaries containing the relevant data in matrices stored in a “pickle” format. For the physiological data pre-processing, we rely on the “biosppy” library 60 , which contains modules for filtering the EDA and PPG signals, peak extraction and EDA decomposition into the EDR and EDL components. For the statistical tests analysis, we used the “SciPy” library ( https://github.com/scipy/scipy ). For further information regarding the raw or transformed data, code incompatibilities, or others, we welcome the reader to contact our corresponding author. The processing was done in Python 3.7.4, and the required code is available in the “5_Scripts” folder on Zenodo ( https://zenodo.org/record/8136135 ) so it can be easily replicated."
10.1038/s41597-024-02963-4,The codes for the described algorithm will be available in Figshare 23 and at https://github.com/Giacomo-Roncoroni/LPR_CE4 .
10.1038/s41597-024-02968-z,"Matching algorithms were developed in R 43 , while manual review and verification occurred using MS Excel (Microsoft, Redmond, USA). Figures were also created in R. All code can be found on GitHub, under ~/resources/code: https://github.com/iressef-egh/senegal-cfl"
10.1038/s41597-024-02913-0,The fully reproducible codes are publicly available at GitHub ( https://github.com/lulingliu/GlobPOP ).
10.1038/s41597-024-02985-y,No custom code was generated or applied for analysis of the genomic data presented. All software tools used for the analyses and the applied parameter settings are detailed in Table 7 .
10.1038/s41597-024-02966-1,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatic softwares. There were no custom scripts or code utilized in this study.
10.1038/s41597-024-02977-y,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-024-02990-1,"The bioinformatic analyses were performed using the manuals and protocols by the software developers. If manually adjusted parameters were used, the software version and method used are described in the Methods."
10.1038/s41597-024-02961-6,Data filtering and statistical analysis was performed using the open access software jamovi 8 . No custom code was used for the curation or validation of the dataset.
10.1038/s41597-024-02909-w,"nextDenovo: input_type = raw, read_type = ont, read_cutoff = 1k, seed_cutoff = 34747, sort_options = -m 20 g -t 14, minimap2_options_raw = -t 14, pa_correction = 8, correction_options = -p 14, minimap2_options_cns = -t 14, minimap2_options_map = -t 14, nextgraph_options = -a 1 NextPolish: sgs_options = -max_depth 100 -bwa, lgs_options = -min_read_len 1k -max_depth 100, lgs_minimap2_options = -x map-ont TEsorter: -db rexdb-plant Repeatmasker: -pa 14 -s -xsmall Blastp: E-value ≤ 1e-5 Swiss-Prot: E-value ≤ 1e−5 Nr: E-value ≤ 1e−5 Orthofinder: -S diamond -M msa -T fasttree trimAl: -gt 0.6 -cons 60 RAxML: raxmlHPC-PTHREADS -m PROTGAMMAJTT -f a -p 123 -x 123 -# 100 Wgdi: pvalue = 0.05 Other commands and pipelines used in data processing were executed using their corresponding default parameters."
10.1038/s41597-024-02940-x,No custom script was used in this work. Software that was used to analyse data was listed in methods in detail and commands were used based on the manuals.
10.1038/s41597-024-02937-6,All software and pipeline were executed following the instructions and protocols provided by the respective bioinformatic tools’ publications. All the scripts for the genome assemble we have deposited in a public repository https://doi.org/10.6084/m9.figshare.24570898.v1 42 . The software versions and corresponding code/parameters used are comprehensively outlined in the Methods section.
10.1038/s41597-024-02952-7,Code to read and process the RAFT synthetic TC dataset can be found at https://doi.org/10.5281/zenodo.7976242 .
10.1038/s41597-024-02920-1,All the events were automatically labeled by the Matlab programs. All Matlab codes are available on GitHub 49 https://github.com/yellow07200/ESD_labeling_tool .
10.1038/s41597-023-02787-8,All the MATLAB scripts used in the Technical Validation section for analysis and figure generation are available in the ‘code.zip’ 40 . The EEGLAB toolbox was used for analysis in MATLAB scripts. The channel_location.locs and channel_location.ced files contain the location information of channels for our EEG data and can be easily imported into EEGLAB. These scripts and files are stored in the ‘code.zip’ folder and shared with the dataset.
10.1038/s41597-024-02948-3,The fire indices have been generated using the open source GEFF modelling system v4.1( https://github.com/ecmwf-projects/geff ). The code to reproduce the results of this manuscript is openly available on a public repository: https://github.com/fdg10371/Jupyter_notebooks .
10.1038/s41597-024-02970-5,The image deconvolutions and associated image registrations were completed using the proprietary software SoftWoRx 7.0.0 following the microscope.
10.1038/s41597-024-02953-6,"Data processing was carried out in Python (v3.9) and all code developed for its pre-processing, transformation and analysis is user-friendly, documented, and freely available via our Github repository 44 ."
10.1038/s41597-024-02962-5,All code used in this study is freely available in https://github.com/chiras/database-curation . The developed global and country-level datasets are also provided in the same repository as well as in Zenodo. A web interface with a list of sequences that were kept or removed during curation is available at https://its2curation.molecular.eco .
10.1038/s41597-024-02965-2,"All software used in this work is publicly available, with versions and parameters clearly described in Methods. If no detailed parameters were mentioned for a software, the default parameters suggested by the developer were used. No custom code was used during this study for the curation and/or validation of the datasets."
10.1038/s41597-024-02979-w,We archived all code to generate our merged dataset on Zenodo 39 (available at https://doi.org/10.5281/zenodo.6803199 ) to facilitate updating these spatial layers with new versions of LANDFIRE NVC or additional years of CDL.
10.1038/s41597-024-02971-4,All emission calculations involved in the method chapter are completed in Excel.
10.1038/s41597-024-02972-3,"Example codes are publicly available in a Github repository at: https://github.com/Biophotonics-Tyndall/PUB-DataDescriptorCode.git . The repository contains one MATLAB script (DRS_cal_proc_example.m) demonstrating DRS data processing and systematic artifact correction, one Python function (calibDRS.py) for SNV transformation, and three MATLAB functions to namely calculate area-under-curve (AUC_DRS_fnc.m), splice two spectra via spline interpolation (DRS_splice_fnc.m) and calculate SNV transformation (snv_DRS_fnc.m) for DRS data."
10.1038/s41597-023-02788-7,"The data preparation codes and the specific codes to generate the fused datasets presented in this study are on the fusionData github repository ( https://github.com/ummel/fusionData ). The generalized codes for the fusion, analysis, and validation of the datasets are available on the fusionModel github repository ( https://github.com/ummel/fusionModel )."
10.1038/s41597-024-02973-2,"The steps from quality control to variant calling and refinement are presented below. 1 . FASTQC (v0.11.5): code for quality control for high throughput sequence data fastqc -t 8 /my_sample_R1. fastq.gz fastqc -t 8 /my_sample_R2. fastq.gz 2 . MulitQC (v1.8): Consolidate all the samples using “multiqc.” 3 . BWA-mem (0.7.17); code for mapping raw reads RGID = “ID_my_sample”, RGSM = “ID” bwa mem -t 8 -k 32 -M -R @RG\\tID: ${RGID}\\tLB:${RGSM}\\tPL:ILLUMINA\\tSM:${RGSM}${REF} ${input}/${RGID}.R1.fastq.gz ${input}/${RGID}.R2.fastq.gz | samtools view -bS - > ${my_sample}.bam 4 . Samtools (v1.8): code for sorting and indexing bam files samtools sort ${my_sample}. bam > ${my_sample}.sorted.bam samtools index ${my_sample}. sorted.bam -@ 8 5 . Picard (v2.18.2) : code for marking duplicate reads : java -Xmx8G -jar ${picard}/picard.jar MarkDuplicates I = ${my_sample}.sorted.bam o = ${my_sample}_dedup.bam M = ${my_sample}_dedup.metrics.txt TMP_DIR = ${KNOWNVAR}/tmp MAX_FILE_HANDLES_FOR_READ_ENDS_MAP = 4000 OPTICAL_DUPLICATE_PIXEL_DISTANCE = 2500 CREATE_INDEX = true VALIDATION_STRINGENCY = LENIENT # To calculate the total number of clean reads, mapped and unmapped reads samtools flagstat ${my_sample}_dedup.bam > ${my_sample}_dedup.flagstat.txt 6 . GATK (v3.8-1-0-gf15c1c3ef): codes for Base Quality Score Recalibration (BQSR) steps # BQSR applies machine learning and builds a mode of covariation (true variation and artifacts) based on the input data and set of known variants as training resources and truth sets . java -Xmx80G -jar ${GATK} -T BaseRecalibrator -R ${REF} -I ${my-sample}_dedup.bam -knownSites ${KNOWNVAR} -o ${my_sample}_recal_table #Apply the recalibration to your sequence data java -Xmx80G -jar ${GATK} -T PrintReads -R ${REF} -I ${my_sample}_dedup.bam -BQSR ${my_sample}_drecal_table -o ${my_sample}_recal.bam 7 . GATK (v3.8-1-0-gf15c1c3ef):Codes for variant calling in GVCF mode by HaplotypeCaller java -Xmx80G -jar ${GATK} -T HaplotypeCaller -R ${REF} -I ${my_sample}_recal.bam --genotyping_mode DISCOVERY --emitRefConfidence GVCF --variant_index_type LINEAR --variant_index_parameter 128000 -stand_call_conf 30 -o ${my_sample}_g.vcf.gz 8 . GATK (v3.8-1-0-gf15c1c3ef): Joint genotyping for all individual VCF samples # Use either --variant or -V options java -d64 -Xmx48g -jar ${GenomeAnalysisTK.jar} -T GenotypeGVCFs -R ${REF} --variant my_sample_g.vcf.gz --variant my_sample1_g.vcf.gz --variant my_sample2_g.vcf.gz --dbsnp ${KNOWNVAR} -o allsample_joint.vcf.gz 9 . GATK (v3.8-1-0-gf15c1c3ef): Code for VQSR steps java -d64 -Xmx48g -jar ${GenomeAnalysisTK.jar} -T VariantRecalibrator -R ${REF} -input ${allsample_joint}. vcf.gz -resource: dbSNP, known = false, training = true, truth = true, prior = 15.0${TRUEVAR} -resource: dbSNP, known = true, training = false, truth = false, prior = 2.0${KNOWNVAR} -an DP -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR -mode SNP -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 -recalFile ${allsample_joint)_recalibrate_SNP.recal -tranchesFile ${allsample_joint}_recalibrate_SNP.tranches -rscriptFile ${allsample_joint}_recalibrate_SNP_plots.R #Apply the SNP recalibration model to the variant call sets using ApplyRecalibration GATK walker . java -d64 -Xmx48g -Djava.io.tmpdir = $ {allsample_joint.vcf}/javatempdir -jar ${GenomeAnalysisTK.jar} -T ApplyRecalibration -R ${REF} -input ${allsample_joint). vcf.gz --ts_filter_level 99.0 -mode SNP -tranchesFile ${allsample_joint}_recalibrate_SNP.tranches -recalFile ${allsample_joint}_recalibrate_SNP.recal -o ${allsample_joint}_snp_VQSR_ApplyRecal_filtered.vcf.gz #Post-processing to remove variants failing the GATK filtering parameters and restricting the alleles into biallelic markers only . java -d64 -Xmx48g -jar ${GenomeAnalysisTK.jar} -R ${REF} -T SelectVariants --variant ${allsample_joint} _snp_VQSR_ApplyRecal_filtered.vcf.gz -o ${final_filtered}. vcf.gz -selectType SNP -env -ef -restrictAllelesTo BIALLELIC"
10.1038/s41597-024-02934-9,"Images were processed using AutoMorph software, which is described in Hsiang et al . 2 and freely available on GitHub at www.github.com/HullLab/AutoMorph . The classifier software used to tag images with taxonomic identifications can be found on GitHub at https://github.com/GregDMeyer/classifier ."
10.1038/s41597-024-02969-y,MISSING
10.1038/s41597-024-02978-x,"The dataset 12 acquisition pipeline was developed using Python programming language, and the data is provided in a CSV format, which makes it compatible with a variety of data analysis tools and software packages. The pipeline utilized Python libraries, including matplotlib 3.6.0, pandas 1.5.3, NumPy 1.24, and SciPy 1.0 for data aggregation, statistical analysis (e.g., bias checks using chi-squared test), and visualization. Jupyter Notebook 5.0 served as the interface for the pipeline. To assist the user community in both the collection and analysis of the data, the code and tools developed for this dataset are available through GitHub. Users can access the repository, which contains the Python scripts, at https://github.com/hky365/Global-Data-Sharing-Initiative-.git . This can help users to reproduce the analyses, adapt the code for their specific needs, and collaborate with other researchers."
10.1038/s41597-024-02957-2,"The code used for the experiment is publicly available at platform and cloud-based service for software development and version control GitHub. https://github.com/DECEIVER-dot/BITalino-Toolbox . We developed the code for these applications in MaxMSP. All required packages are listed in the requirements.txt file. This repository contains three applications developed in MaxMSP that enable direct communication with an IRCAM R-IoT module embedded in a BITalino board. The applications provide various functionalities, including biosignal data recording, Bluetooth connectivity, and interactive annotation while viewing video recordings of experiments. Please note that the software is designed to work with the MaxMSP programming environment and requires external libraries for specific functionalities. The first application allows seamless communication via a USB connection with the IRCAM R-IoT module on the BITalino board. It lets the user record biosignal data directly into a CSV file format with two different sample rates. The recorded data is saved in its raw form without normalization or interpolation. Please be aware that the current software version does not include data normalization or interpolation. The second application builds upon the functionality of the first application but adds Bluetooth connectivity as an alternative communication method. With this application, the user can connect the IRCAM R-IoT module on the BITalino board and the computer using Bluetooth. It provides the same data recording capabilities as the USB version, but motion data is not recorded with this application version. Experiment Annotation with Video Recording (Mira and iOS) The third application is designed for interactive annotation while viewing video recordings of experiments. Developed in MaxMSP, it requires the installation of external libraries for the Mira interface, which provides enhanced interaction capabilities. Additionally, this application relies on an iOS device to run the software effectively. Using this application, researchers or experimenters can annotate the video recordings in real-time, allowing for precise and synchronized annotation of events or observations. We implemented code in Python using the BioSPPy library to extract relevant information from biosignals. BioSPPy is a Python library for biosignal processing providing a set of algorithms for processing and analyzing physiological signals, such as electrocardiography, electrodermal activity, and electromyography, to name a few. BioSPPy simplifies extracting relevant information from biosignals and enables researchers and developers to focus on their analysis tasks. BioSPPy offers various modules and functionalities, including signal processing and feature Extraction such as HRV, EDA, and EMG analysis. BioSPPy is an open-source library and can be easily installed using Python package managers like pip or conda. Its modular design and user-friendly API make it accessible to beginners and experienced biosignal processing researchers. For more information, documentation, and code examples, please visit the official BioSPPy GitHub repository at ( https://github.com/PIA-Group/BioSPPy ). RStudio ( https://posit.co/download/rstudio-desktop/ ) code was developed for conducting tests and interclass correlation analysis. The provided code allows users to perform various statistical tests and calculate interclass correlation coefficients using R programming language. Please note that the file naming and location should be adjusted according to the user’s computer file structure. We include the required external libraries for running the tests."
10.1038/s41597-024-02964-3,"The processing steps to build the three datasets composing the CoDB were carried out in R, utilizing the libraries tidyerse 24 , haven 25 , labelled 26 , and tibble 27 . All the code is available on the GitHub repository of this project: https://github.com/JuanGaleano/CORESIDENCE ."
10.1038/s41597-024-02983-0,"The source code used to generate the datasets is available at https://github.com/MasterAI-EAM/self-cleaning . Other codes used are available at https://github.com/MasterAI-EAM/SciCrawler , https://github.com/MasterAI-EAM/TextMaster and https://github.com/CambridgeMolecularEngineering/chemdataextractor2 ."
10.1038/s41597-024-02984-z,"The custom code for importing dataset into variables in Matlab environment (Mathworks, USA) and Python programming language is available at github repository 28 . The code used for processing DICOM images was based on the cornerstone3D 29 , dicomParser 30 and Nanodicom 31 libraries. The code used for image annotation was based on markerjs2 32 library."
10.1038/s41597-024-02987-w,The scripts used to anonymise and arrange the data according to BIDS format are available at https://github.com/natmegsweden/NatMEG-PD_data . The scripts for the MRI warping procedure are available from https://github.com/mcvinding/warpimg .
10.1038/s41597-024-02989-8,"No custom code was used in this study. All bioinformatics tools, commands and pipelines used in data processing were executed following the manual and protocols provided by the respective software developers. The versions of the software used, along with their corresponding parameters, have been thoroughly described in the Methods section."
10.1038/s41597-024-02975-0,All code is available from https://github.com/casus/UMOD under MIT open source licence.
10.1038/s41597-024-02986-x,"MatLab code and instructions in a README text file for cross-taxonomical analysis can be downloaded from FigShare ( https://doi.org/10.6084/m9.figshare.13241972.v18 ; folder: ‘Derived life history traits across ectotherms’ 39 ) to calculate (i) generation time, survivorship curve, age at sexual maturity, progressive growth, retrogressive growth, mean sexual reproduction, degree of iteroparity, net reproductive rate, and mature life expectancy (Table 1 ), as well as population growth rate and demographic resilience (damping ratio), and (ii) run an elasticity analysis for each species listed in DEBBIES for a predefined, experienced feeding level. For users interested in exploring the dynamics of single species, MatLab code and instructions in a README text file can be downloaded from FigShare ( https://doi.org/10.6084/m9.figshare.13241972.v18 ; folder: ‘Derived life history traits single ecotherm’ 39 ) to calculate (i) generation time, survivorship curve, age at sexual maturity, progressive growth, retrogressive growth, mean sexual reproduction, degree of iteroparity, net reproductive rate, and mature life expectancy (Table 1 ), as well as population growth rate and demographic resilience (damping ratio), and (ii) run an elasticity analysis for a single species for a range of experienced feeding levels defined by the user."
10.1038/s41597-024-02974-1,"All the software used to process the data set presented here is publicly available and distributed by their developers. All versions have been specified in the main text, along with the options used when departing from defaults. Custom scripts used in intermediate or summarizing steps are available at https://gitlab.com/malaspina-public/picoplankton-vertical-profiles . Code for bin decontamination step can be found at https://github.com/felipehcoutinho/QueroBins ."
10.1038/s41597-024-02931-y,All code is available on GitHub without restrictions ( https://github.com/DVS-Lab/srndna-datapaper ). A snapshot of the repository has been placed on Zenodo for permanent archiving ( https://zenodo.org/doi/10.5281/zenodo.10456520 ). This repository includes a README.md file that describes how the dataset was generated. This repository also includes stimulus presentation scripts and the sourcedata generated by those scripts.
10.1038/s41597-024-02960-7,All CNNEE code for enhancer prediction and eRNA identification is publicly available at https://github.com/WangYF33/CNNEE .
10.1038/s41597-024-02938-5,All R codes used in this paper have been uploaded to the GitHub ( https://github.com/zhaoxueheng/chemical-treatment ) database.
10.1038/s41597-024-03002-y,"Cellranger version 7.1.0, which was used to generate count matrices, is available as docker container 39 . All supplementary materials 25 , 26 generated using R code, contain a script (command.R) providing all the information needed to reconstruct the supplementary materials."
10.1038/s41597-024-02991-0,The genome and transcriptome analyses were performed following the manuals and protocols of the cited bioinformatic sofware. No new codes were written for this study.
10.1038/s41597-024-03007-7,The software and code used are publicly accessible. No custom programming or coding was used.
10.1038/s41597-024-02997-8,"The software used in this study is in the public domain, with parameters clearly described in Methods. Where detailed parameters were not provided for the software, default parameters were used instead, as suggested by the developers. No custom script or code was used."
10.1038/s41597-024-02976-z,"The code used for data pre-processing has been deposited on https://github.com/gitHBDX/mirblood-code . The following software versions were used: unitas v1.7.7, SeqMap v1.0.13, SPORTS v1.1, Bowtie v1.3, SCANPY v1.8.2, Python v3.10.6, Plotly v5.10.0, Plotly Express v0.4.1, SciPy v1.9.1, Seaborn v0.12.2, and UpSetPlot v0.8.0."
10.1038/s41597-024-03001-z,No custom code was generated as part of this publication or required for use of the data.
10.1038/s41597-024-03012-w,"A public GitHub repository has been set up to help users process the data, which will be updated based on user feedback or input as well as when changes occur to the dataset. This repository can be found at https://github.com/hart-davis/ArcTiCA ."
10.1038/s41597-024-03010-y,The versions of software used were included in the methods section. No specific script was utilized for this work. All commands and pipelines employed in data processing were executed in accordance with the manual and protocols of the corresponding bioinformatic software.
10.1038/s41597-024-02999-6,"No special codes or scripts were used in this work, and Data processing was carried out based on the protocols and manuals of the corresponding bioinformatics software."
10.1038/s41597-024-02995-w,"The variant calling procedure was conducted in accordance with the standard bioinformatic workflow recommended by GATK, and all the steps were performed in the CentOS system. The detailed codes and parameters used in this study are provided as follows: (1) Quality control for the raw reads: Software: FASTP v0.21, FastQC v0.12.1 Code for trimming: fastp -I ${read1} -I ${read2} -o ${read1.qc} -O ${read2.qc} -q 5 -g -u 50 -n 15 -l 150 --min_trim_length 10 --overlap_diff_limit 1 --overlap_diff_percent_limit 10 Code for quality control : Fastqc -f fastq -t 6 -o ${sample.qc} ${read1.qc} ${read2.qc} (2) Genome alignment: Software: BWA-mem v0.7.17 Code: bwa mem -t 10 -M -R “@RG\tID:${individual}\tLB:${individual}\tPL:illumina\tSM:${individual}” /genome.index/bwa.index/gga7.${read1.qc} ${read2.qc} > ${individual}.sam (3) Sorting, files converting, and indexing: Software: SAMtools v1.12 Code for sorting and file converting: samtools sort -m 10 G -S ${individual}.sam -o ${individual}.sorted.bam -@ 10 Code for bam file indexing: samtools index -b ${individual}.sorted.bam ${individual}.sorted.bai (4) Statistics for sequencing depth: Software: Mosdepth v0.2.9 Code: mosdepth -t 6 ${individual.depth} ${individual.bam} (5) Removing the duplicates Software: Picard v2.26 Code: java -Xms100g -Xmx200g -jar picard.jar MarkDuplicates INPUT = ${individual}.sorted.bam OUTPUT = ${individual}.rmdup.bam M = ${individual}.metrices.txt REMOVE_DUPLICATES = true CREATE_INDEX = true (6) Variants calling Software: GATK v4.2.2 Code for the GVCF model generated using HaplotypeCaller: gatk --java-options “-Xmx60g -Xms20g” HaplotypeCaller --native-pair-hmm-threads 80 -R Gallus_gallus.bGalGal1.mat.broiler.GRCg7b.dna_sm.toplevel.fa -I ${individual}.rmdup.bam -ERC GVCF -O ${individual}.g.vcf Code for combining GVCF files: gatk --java-options “-Xmx80g -Xms60g” CombineGVCFs -R Gallus_gallus.bGalGal1.mat.broiler.GRCg7b.dna_sm.toplevel.fa --variant individual1.g.vcf --variant individual2.g.vcf --variant individual3.g.vcf -O merge.g.vcf Code for variant joint calling: gatk --java-options “-Xmx80g -Xms60g” GenotypeGVCFs -R Gallus_gallus.bGalGal1.mat.broiler.GRCg7b.dna_sm.toplevel.fa -V merge.g.vcf -O merge.vcf (7) Variants extraction and hard filtration: Software: GATK v4.2.2.0 Code for SNP extraction: gatk --java-options “-Xmx80g -Xms60g” SelectVariants -V merge.vcf -O raw.snp.vcf --select-type-to-include SNP Code for InDel extraction: gatk --java-options “-Xmx80g -Xms60g” SelectVariants -V merge.vcf -O raw.indel.vcf --select-type-to-include INDEL --max-indel-size 50 Code for filtration: gatk --java-options “-Xmx80g -Xms60g” VariantFiltration -R Gallus_gallus.bGalGal1.mat.broiler.GRCg7b.dna_sm.toplevel.fa -V raw.snp.vcf -filter-expression QD < 2.0 || FS > 40.0 || SOR > 3.0 || MQ < 40.0 || ReadPosRankSum < −8.0” --filter-name filter -O filtered.snp.vcf (8) Further filtration: Software: VCFtools v0.1.13 Code: vcftools --vcf filtered.snp.vcf --max-alleles 2 --min-alleles 2 --min-meanDP 3 --remove-filtered-all –recode --out qc.snp (9) Genotypes imputation: Software: Beagle v5.1 Code: java -Xmx100g -Xms50g -jar beagle.jar gt = qc.snp.vcf impute = true nthreads = 20 out = qc.phased.snp"
10.1038/s41597-023-02895-5,The code to process the DTW records is archived at the OSF repository: https://osf.io/swdg9
10.1038/s41597-024-03018-4,Codes to reproduce the results of this work are available on the Figshare 43 and the GitHub project page ( https://github.com/shirotak/CellLine_HRD_DrugRes ).
10.1038/s41597-024-02981-2,Code for converting DICOM images with segmentation masks to standard DICOM segmentation objects is available on GitHub: https://github.com/lassoan/LabelmapToDICOMSeg .
10.1038/s41597-024-03006-8,No custom code was used to develop the data processing methods described in this article.
10.1038/s41597-024-03011-x,All code used in this project is publicly available. All relevant software and references are listed in Methods and Technical vation.
10.1038/s41597-024-03003-x,"All scripts used in the generation of classifiers and dataset, as well as source code for the web app hosted at syringae.org are available on GitHub at https://github.com/cwf30/SYRINGAE 35 . To aid in reproducibility, a conda environment YAML file (SYRINGAE_env.yml) and a readme file outlining scripts used (README.txt) are also provided."
10.1038/s41597-024-02988-9,"No custom software code was written for this research. All bioinformatics tools and pipelines were executed as per the manual and protocols provided by their respective software developers. The software versions used, along with their corresponding parameters, have been thoroughly described in the Methods section."
10.1038/s41597-024-03013-9,"Qualified researchers can access data from the current study (see the Participants section above for details). Scripts, supporting documents, and other information necessary to implement all aspects of data organization, preparation, and analysis using open-source software packages. Except for special instructions in the paper, all tools always use default parameters. No custom code was instructed."
10.1038/s41597-024-02959-0,All code is publicly available on our GitHub repository: https://github.com/brainlife/ezbids .
10.1038/s41597-023-02872-y,There is no in-house code used for ML model. We used OnionNet 47 http://github.com/zhenglz/onionnet/ ML model to train on PLAS-20k dataset.
10.1038/s41597-024-03031-7,"The pipeline used for processing raw data generated within the Reich lab is available in the ‘Workflow Description Language’ (WDL) here: https://github.com/DReichLab/adna-workflow , and includes individual python scripts for components of the pipeline."
10.1038/s41597-024-03005-9,"The code associated with this work is available on GitHub ( https://github.com/clissa/fluocells-scientific-data ). The repository contains utils to: • perform data operations ( dataOps/: i ) converting raw TIFF images into PNG with metadata, ii) recreating expected data folders structure, iii) convert VIA annotation to binary masks, iv) encode binary masks into various annotation formats and types, v) preprocess yellow masks from previous FNC version 24 ) • implement deep learning modelling strategies ( fluocells/models/: contains network blocks to implement c-ResUnet architecture 6 ; compute_metrics.py, evaluate.py and training.py: contain utils to implement model training and evaluation) • explore, analyze and evaluate models interactively ( notebooks/: contains jupyter notebooks with examples of how to deal with standard stages of data analysis, namely i) exploratory data analysis, ii) implementation of model architecture and training pipeline, and iii) experiments"
10.1038/s41597-024-03017-5,RF was run with scikit-learn ( https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html ) under Python 3.7. The pre-processing and harmonized code of Landsat-7/8 and Sentinle-2 60 has been uploaded to Figshare.
10.1038/s41597-024-03014-8,All data processing commands and pipelines are executed according to instructions and guidelines provided by relevant bioinformatics software. No custom scripts or code were used in this study.
10.1038/s41597-024-03009-5,"The programs and software used to generate all the results were Python and ESRI ArcMap 10.7. All DL models (FCN 8S, SegNet, UNet, RefineNet, LinkNet, Attention UNet, and HRNet) as well as the dataset are publicly available through the figshare repository ( https://doi.org/10.6084/m9.figshare.24305557 ) 32 ."
10.1038/s41597-024-03022-8,"The codes for measurements, descriptive statistics and quantitative validation are available in an OSF repository 19"
10.1038/s41597-024-03024-6,"All software used in this work is in the public domain, with parameters being clearly described in Methods. If no detail parameters were mentioned for a software, default parameters were used as suggested by developer."
10.1038/s41597-024-03020-w,"The R code used for summaries of the Data Records and for creating the plots in Fig. 4 , as well as the R code for the analyses described in the Usage Notes are available for download from figshare 54 . We did not use any other custom code during data collection."
10.1038/s41597-024-03008-6,"The MATLAB code we used in this work to obtain the FSP deformation axes was developed by Andrenacci et al . 55 for use in Lavecchia et al . 1 and is available from the ZENODO repository ( https://zenodo.org/record/5910783 ) 55 , together with a complete guide to use."
10.1038/s41597-023-02832-6,"R code for calculating aggregated intactness scores for a focal region (e.g., ecoregion or country) and/or taxonomic group can be downloaded with the bii4africa dataset on Figshare 35 ; see Data Records section."
10.1038/s41597-023-02865-x,"All data preparation, processing and analysis, and the generation of plots and data tables were carried out using MATLAB. The code used to process buoy data from displacements to the wave parameter time series data presented here is provided with the data packages on the SEED portal in html format and can be viewed as they appear in MATLAB by anyone without access to that proprietary software."
10.1038/s41597-024-03049-x,There is no custom code was used during this study. All software and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and code/parameters of software have been detailed and described in Methods.
10.1038/s41597-024-03043-3,All the data analysis procedures were done following published manuals or public protocols of the software described in the Methods. Parameters for each software were detailed. Codes used to run gene annotation pipelines were deposited in GitHub with the link: https://github.com/Xu-lab-Evolution/Waterlily_aphid_genome_project .
10.1038/s41597-024-03048-y,There were no custom scripts or code utilized in this study.
10.1038/s41597-024-03050-4,All versions of third-party software and scripts used in this study are described and referenced accordingly in the Methods section.
10.1038/s41597-024-02994-x,"The GEE code for PV power stations classification based on Sentinel-2 imagery and DEM data is available at https://github.com/MrSuperNiu/PV_ScientificData_Classification_Code . The code is written in JavaScript, including all the mentioned steps in this paper, including feature calculation, random forest training, etc."
10.1038/s41597-024-03023-7,Custom codes for processing the data and the figures can be obtained for free from Figshare 42 ( https://doi.org/10.6084/m9.figshare.23641092 ). The data processing and technical validations were conducted in MATLAB R2015b. A “README.txt” file was used for a brief description of the code in the code repository.
10.1038/s41597-024-03042-4,"Python scripts for training data sampling, earth observation and census input feature collection, random forest model fine tuning and mode prediction on Google Earth Engine are available at https://github.com/QiMengEnv/CONUS_Longitudinal_LCZ . All data processing and visualizations are done in Python 3.9. The post-classification processing is done in JavaScript on Google Earth Engine Code Editor and is also available with the same URL."
10.1038/s41597-024-03034-4,No custom code has been used in the generation of this work.
10.1038/s41597-024-02996-9,The code used to create figures for this study can be accessed at the following repository: https://doi.org/10.5061/dryad.tx95x6b3h 30 . Figures and analyses were produced using R v4.1.3.
10.1038/s41597-024-03053-1,Most of the weather data used in this study were downloaded using a Python script. Only weather data obtained from agricultural weather stations were manually downloaded. All datasets were processed and analyzed using SAS. The Python and SAS codes are available at https://github.com/YuanChihSu/Crop_Disaster_Dataset . A full list of weather station codes and altitudes is also provided.
10.1038/s41597-023-02881-x,"Please refer to the Supplementary Information , Section 2, in order to access the taxonomy maps made for Step 1 of the data harmonization process. Access to these taxonomy maps is also provided through our website here: https://www.coronanet-project.org/external_data_harmonization.html . Meanwhile, we make the code and data for replicating Steps 2 to 3 available in the following folders in the CoronaNet public git repository: https://github.com/CoronaNetDataScience/corona_tscs/tree/master/RCode/collaboration https://github.com/CoronaNetDataScience/corona_tscs/tree/master/data/collaboration ."
10.1038/s41597-024-03045-1,"SfM photogrammetry was performed using Pix4D Mapper (Pix4D SA, Lausanne, Switzerland, v.4.8.3) software, following the instructions provided in the user manual, which can be found at https://support.pix4d.com/hc/en-us/sections/360003718992-Manual . The processing templates for each UAV sensor are included in the repository, in a dedicated top-level folder named “Pix4D templates”."
10.1038/s41597-024-03051-3,The source code for AquaCrop-OSPy v6.1—the crop model upon which ACEA is based—is freely available via github.com/aquacropos/aquacrop. The source code and most inputs for ACEA (version 2.0) are available via Zenodo 77 . Note that some input datasets are not included but can be directly obtained from the original sources instead. You can find brief instructions and references to input datasets in the readme file.
10.1038/s41597-024-03052-2,The in-house R and Python scripts for GC-MS and heat map analysis are available in the figshare repository ( https://doi.org/10.6084/m9.figshare.23522490.v6 ).
10.1038/s41597-024-03044-2,The code is publicly available at https://github.com/QiongWuChina/CUSCN30Python .
10.1038/s41597-024-03037-1,Our workflow includes scripts that were written and tested using R version 4.1.1. The scripts can be accessed from the data product 8 which includes an appropriate licence (CC0 1.0 Universal) license permitting reuse without restrictions.
10.1038/s41597-024-03030-8,"The custom-written code used for data acquisition and analysis in this paper can be downloaded from figshare 60 . The provided files contain the necessary scripts and functions for data acquisition and signal processing. • readme.pdf with instructions about loading the dataset, running the code, and code execution. • SciDataEMG contains: - Code ( SciDataEMG.m ) - The .mat files in the SciData dataset ( SubGroup1.mat , SubGroup2.mat , SubGroup3.mat ) contain summarized or processed data, which can be loaded into Matlab for further analysis and visualization. To facilitate data management and analysis, the data from all thirty participants were consolidated into a summarized format using Matlab. The raw sEMG time and signal data for each subgroup of participants were saved in a .mat file (e.g., SciData/RawEMGData/MatlabData/SubGroup1.mat ) for computational efficiency since the dataset of 30 subjects is too large. These files are commonly used for efficient processing and analysis using Matlab functions and tools. - Results ( P_9_M_9 sEMG signal.png , P_9_M_9 pre-processed sEMG signal.png which are plotted results for representative subject example from the code (Subject 09)."
10.1038/s41597-024-03028-2,"Codes used for extracting the 8-day and monthly VIIRS data, along with the outlier removal and gap-filling scripts, are available through the GitHub repository: https://github.com/beingdeepshah/MODIS_VIIRS_GWR_Codes ."
10.1038/s41597-024-03015-7,The source code used to generate the scientific content of the dataset is available on Github ( https://github.com/HMU-BioServer/ExMdb ).
10.1038/s41597-024-03027-3,"The dataset AndroDex is publicly available and can be accessed via the following links: Binaries of all files along with the code to convert images of any size are available at 18 : https://doi.org/10.6084/m9.figshare.23931477.v1 , whereas images converted are available at 17 https://doi.org/10.6084/m9.figshare.23931204.v1 . All the files are password protected to make sure that none of the files were deleted by server and the password is androdex . The AVPass obfuscation technique that applied is available at ( https://github.com/sslab-gatech/avpass ) whereas Obfuscapk technique is available at ( https://github.com/ClaudiuGeorgiu/Obfuscapk )."
10.1038/s41597-024-03038-0,"All data processing was performed in R software (Version 3.6.3). The BIO-MATE R software is freely available ( https://github.com/KimBaldry/BIOMATE-Rpackage ). The semi-automated workflow and accompanying processing data used to construct the data product, along with the code used to create the data descriptor is freely accessible via Git Hub ( https://github.com/KimBaldry/BIO-MATE )."
10.1038/s41597-024-03029-1,"All code is available in the GitHub repository ( https://github.com/rutishauserlab/bmovie-release-NWB-BIDS ). The python code includes scripts to read and plot the data from the NWB files and perform the analyses presented in this data descriptor. The code relies heavily on open-source Python packages such as numpy 61 , scipy 62 , pynwb 18 , mne-python 63 , nilearn 48 , and pycortex 64 . The movie annotation files are also provided in the GitHub repository under ‘assets/annotations’ folder. The scripts related to the estimation of tSNR and ISC were adapted from the code provided in the GitHub repository associated with the budapest-fmri-data study 49 , 50 (see: https://github.com/mvdoc/budapest-fmri-data )."
10.1038/s41597-024-02998-7,The programs used to generate all the results were Python (3.10) JavaScript and ArcGIS (10.8). Analysis scripts used in this study will be available at https://doi.org/10.6084/m9.figshare.23993517.v4 44 .
10.1038/s41597-024-03033-5,"Code used for constructing the low-carbon policy intensity is written in Python 3.10.8 and Stata 15, and has been uploaded to figshare 26 ."
10.1038/s41597-024-03039-z,The code base is made available in 34 as noted above.
10.1038/s41597-024-03046-0,"We did not use any custom code in this study. The versions and parameters of the bioinformatic tools used in this study were described in the Methods section. If a parameter was used with other than its default value, this was stated above as well."
10.1038/s41597-024-03016-6,No specific script was used in this work. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic softwares.
10.1038/s41597-024-03032-6,"Different tools have been employed for data analysis, and the following sections describe their versions, settings, and parameters: • FastQC (v0.11.3) with default parameters; • cutadapt (v1.18) with the following parameters: -m 20, -q 20; • TB-profiler (v4.2.0) with the following parameter: --min_depth 50; • freebayes (v1.3.5) with the following parameters: -p 1, --min-coverage 5, -q 20; • snippy (v3.1) with the following parameters: snippy-vcf_filter, --minfrac 0.1, --minqual 20; • bcftools (v1.12) with default parameters; • JolyTree (v1.1b.191021ac) with default parameters; • iTol online software."
10.1038/s41597-024-03019-3,MISSING
10.1038/s41597-024-03026-4,The generated dataset (SkinPiX) 58 version 1.1 is available in open source ( https://doi.org/10.57745/7FHQOY ) and the KNIME workflows used to process the data are provided there.
10.1038/s41597-024-03047-z,"Since the newly developed SPEI-GD dataset has a high temporal and spatial resolution (the amount of data in the intermediate process is about 2 000 GB), we ran codes on MATLAB and R programming Language, utilizing parallel computing tools and chunked computation algorithms to solve the problem of limited computer memory and long computation time. The code files are available at https://github.com/XuebangLiu/SPEI-GD ."
10.1038/s41597-024-02980-3,"The code used for checking, cleaning, and analysing the data are available in the open GitHub repository ( https://github.com/Plant-Functional-Trait-Course/pftc3_punaproject_pftc5 ), of which a versioned copy is available at Zenodo 71 . There is also a link to the code from the published dataset 71 ."
10.1038/s41597-024-03056-y,The following software’s and tools were used in this manuscript. No custom code was utilized during the analysis of the study. fastx toolkit (v0.0.14) https://github.com/agordon/fastx_toolkit segemehl (v0.2.0–418) http://legacy.bioinf.uni-leipzig.de/Software/segemehl STAR (v2.7.4a) https://github.com/alexdobin/STAR miRbase database https://www.mirbase.org miRanda (v3.3a) ( http://www.microrna.org/microrna/home.do ) FEELnc pipeline (v.0.2.1) https://github.com/tderrien/FEELnc CPC2 (v0.1) http://cpc2.gao-lab.org/ DESeq 2 (v.1.38.3) https://bioconductor.org/packages/release/bioc/html/DESeq2.html EdgeR (v.3.40.2) https://bioconductor.org/packages/release/bioc/html/edgeR.html featureCounts (v2.0.1) https://github.com/ShiLab-Bioinformatics/subread psych package (corr.test() for Pearson correlation coefficient) DAVID https://david.ncifcrf.gov/
10.1038/s41597-024-03057-x,No custom code was used during this study for the curation and/or validation of the dataset.
10.1038/s41597-024-03054-0,"The code that accompanies this data descriptor is publicly available in the GitHub repository https://github.com/8ernabemoreno/Isfjorden-shallows_longterm-seawater-temp-lux . It contains Python code that might be useful for basic level users to create CF-NetCDF (.nc) files from .csv, and (ii) minimally process long-term data (e.g., annual, and monthly means)."
10.1038/s41597-024-02967-0,"The dataset was generated collaboratively by a team of researchers, working in the R or STATA platforms depending on their institutional and disciplinary preference. The final data generation code in R also draws upon a range of raw quantitative and qualitative datasets all with different owners, many of which contain confidential information that can be used to identify the names, demographics, opinions, and locations of individual respondents. Hence, not all code can be made publicly available. However, we have made all non-sensitive data generation code public through a public GitHub repository ( https://github.com/mpoudyal/n4sdatagen ), with clear explanation of those not in the public domain and how they fit within the data generation process in the README file within the repository. More importantly, we want to make clear that no custom code is necessary to utilise the collated dataset, and that the Data Description document on the data archive contains detailed written descriptions of all data generation processes including detailed description of variables developed in the process. Finally, parts of the codes containing sensitive information are available upon request to authors and after permission from the owners of the original datasets."
10.1038/s41597-024-03004-w,The code implementation was done in Matlab (2017a). The functions for far-red SIF and VIs estimation used in this study are available at https://github.com/wugh16/SIF_VI_process_functions.git . The far-red SIF and VIs data is available at https://doi.org/10.3334/ORNLDAAC/2136 53 .
10.1038/s41597-024-02958-1,All code for data preparation and analysis are publicly available on GitHub ( https://github.com/BHFDSC/CCU037_01 ).
10.1038/s41597-024-03082-w,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-024-03075-9,The code used to generate the data supporting the figure in the manuscript is directly available together with the dataset at: https://doi.org/10.25829/idiv.3547-rtgq13 .
10.1038/s41597-024-03055-z,"Code used to produce the LULC forecasts is available on our EnviDat repository ( https://doi.org/10.16904/envidat.458 ) 25 . The script is based on input data which are not included as they are not freely available, therefore it will not run as it is. These input data and their sources are listed in Supplementary Table 1 ; access to them can be requested to their owners. Alternatively, the code can be modified to run on other data."
10.1038/s41597-024-03064-y,All the code developed for this work is available under an open-source MIT license. It can be found at https://github.com/casus/deepdedust .
10.1038/s41597-024-03070-0,MISSING
10.1038/s41597-024-03066-w,"The software versions, settings and parameters used are described below. No custom code was used during this study for the curation and/or validation of the dataset. Hifiasm v0.16.0: -t 32 Nextpolish v1.2.4: rerun = 4, gs_options = -max_depth 100 -bwa fastp v0.21.0: -n 0 -f 5 -F 5 -t 5 -T 5 -q 20 KMC v3.2.1: -k27 -t64 -m512 -ci1 -cs1000000 GenomeScope v1: 27 150 output HiC-Pro v2.8.1: -c config-hicpro.txt -i -o bowtie2 v2.3.2: -end-to-end --very-sensitive -L 30 LACHESIS v1: CLUSTER_MIN_RE_SITES = 100, CLUSTER_MAX_LINK_DENSITY = 2.5, CLUSTER NONINFORMATIVE RATIO = 1.4, ORDER MIN N RES IN TRUNK = 60, ORDER MIN N RES IN SHREDS = 60 Juicebox v 1.11.08: default BUSCO v4.0.5: -l mammalia_odb10 -o result -m genome -c 10 -f GMATA v2.2: default TRF v 4.07b: 2 7 7 80 10 50 500 -f -d -m MITE-Hunter: -n 20 -P 0.2 -c 3 RepeatModeler v1.0.11: -engine wublast RepeatMasker v1.331: nolow -no_is -gff -norna -engine abblast -lib lib GeMoMa v1.6.1: default STAR v2.7.3a: default Stringtie v1.3.4d: default PASA v2.3.3: -c Assembly.config -C -R -g genome.fasta -T -u trans.fasta -t trans.clean.fasta -f fl.acc --CPU 32 --ALIGNERS gmap Augustus v3.3.1: default EVidenceModeler v1.1.1: --segmentSize 1000000 --overlapSize 100000 InterProScan v 5.32: default Blastp v2.7.1: -e 1e-5 tRNAscan-SE v2.0: --thread 16 -E -I Infernal v1.1.2: default RNAmmer v1.2: -S euk -m lsu,ssu,tsu -gff CEGMA v 2–2.5: --genome genome.fasta --vrt --mam Bwa v0.7.15: mem -t 32 samtools v2.17: depth Bcftools v1.8.0: default"
10.1038/s41597-024-03080-y,"The ERA5 datasets can be accessed through the ECMWF website https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset . The RRTMG code can be downloaded at http://rtweb.aer.com/rrtm_frame.html . The datasets contain the multi-year averaged monthly mean spectral kernel of TOA, surface and atmosphere for surface temperature, air temperature, surface albedo and water vapor (LW and SW), as well as program scripts exemplifying their use, are available at Huang and Huang 34 ."
10.1038/s41597-024-03078-6,The algorithm for developing the nonparametric quantile mapping function has been explained in detail in the original study 32 . The algorithm is developed in MATLAB and source code is provided together with the data set on DaRUS.
10.1038/s41597-024-03061-1,"The code utilised in producing this dataset was originally a series of individual scripts in R and, for submitting jobs, to the HPC, in Bash . We have compiled these scripts, including job submission scripts, into a single ordered R notebook to ease comprehension and replicability 66 . All packages indicated in the notebook utilised the most recent version available on November 1, 2021. The code notebook is available at the following Github repository release: https://github.com/jjniev01/areal_sandbox ."
10.1038/s41597-024-03063-z,"The MATLAB code that is associated with the data provided in this paper is hosted on the GitHub pages of the Research Center E. Piaggio ( https://github.com/mpollayil/Code-for-Habitat-Data-Analysis ) and on 23 . Data from ROS bag files may be extracted and visualized with this. The GitHub repository, in particular, has a README file that details every script separately."
10.1038/s41597-024-03000-0,"Scripts for segmentation merging and visualization, statistics calculation and fan-shape correction are publicly available at https://gitlab.com/nct_tso_public/diome . All the scripts are written in Python 3.11 and are public under the MIT license."
10.1038/s41597-024-03087-5,All genome assembly code and the corresponding pipeline description are available at https://github.com/zhengchangsulab/A-genome-assebmly-and-annotation-pipeline .
10.1038/s41597-024-03085-7,No code was used in this study.
10.1038/s41597-024-03079-5,"Clarification of workflow to create our dataset is available as Supplementary Figure S1 . Name and definition of traits are presented in Table S1 . Additionally, our dataset is provided at FigShare 34 ."
10.1038/s41597-024-03062-0,"The Github repository ( https://github.com/simula/european-cloud-cover.git ) contains code to create the data set, load the data and to run the technical validation described in the previous section."
10.1038/s41597-024-03058-w,"The open-access dMRI data processing software used in this study can be accessed on GitHub. The repositories contain the processing pipeline and scripts used for harmonizing and processing the dMRI data, as well as for performing whole brain tractography and white matter parcellation. Researchers and clinicians interested in utilizing these tools for their own research or clinical applications can easily download and customize the software to fit their specific needs. Additionally, the GitHub repositories include detailed documentation on how to use the software, as well as examples of how to run the scripts on sample data. The repositories also provide information on the dependencies required to run the software, ensuring that researchers have access to all the necessary tools to use the software effectively. By making the dMRI data processing software openly accessible on GitHub, we hope to encourage further research and clinical applications of the software and facilitate collaboration across the scientific community. Please refer to the following GitHub links for each of the specific dMRI data processing software: a) Convolutional neural network dMRI brain segmentation b) https://github.com/pnlbwh/CNN-Diffusion-MRIBrain-Segmentation c) DMRI data harmonization: https://github.com/pnlbwh/dMRIharmonization , in this study we used the multi-shell version of this script: https://github.com/pnlbwh/multi-shell-dMRIharmonization d) UKF two tensor whole brain tractography: https://github.com/pnlbwh/ukftractography e) White Matter Analysis: https://github.com/SlicerDMRI/whitematteranalysis f) SlicerDMRI: http://dmri.slicer.org"
10.1038/s41597-024-03065-x,"For the purpose of acquiring physiological data, the BerryMed patient monitor was employed. And the data export function was developed based on the C/C++ source code provided by the manufacturer. As there was no license from BerryMed to make their source code public, only the code we modified was provided. All physiological data is stored in header-less CSV format. To fully grasp the structure of the data file and integrate it into a specific project, please refer to the introduction provided in the Usage Notes section. Before analysis, the raw data underwent upsampling, filtering, and cropping procedures. For data collation and analysis, Python libraries like Pandas (Version 1.5.3), Scipy (Version 1.8.0), NeuroKit2 (Version 0.2.3), and Statsmodels (Version 0.14.0) were employed. Furthermore, Matplotlib (Version 3.7.0), Seaborn (Version 0.12.2) and NeuroKit2 were used for data visualization. To record the timestamp required to achieve the desired altitude during the experiment, the researcher employed a script program. The datetime library was utilized to acquire the timestamp and calculate the time periods. The source code is available on Github ( https://github.com/oca-john/Harespod ) and Gitee ( https://gitee.com/oca-john/Harespod )."
10.1038/s41597-024-03077-7,"All software used for data processing was implemented following the manual provided by the bioinformatic software cited in the method section. When specific parameters for the software were not detailed, the default settings were utilized."
10.1038/s41597-024-03084-8,No custom code was made during the collection and validation of this dataset.
10.1038/s41597-024-03072-y,The datasets are available in the form of XLSX files. No specific code was used to construct the datasets.
10.1038/s41597-024-03076-8,Pre-processing scripts for each of the omics datasets are available at the Github repository ( https://github.com/EpiRNAsLab/Multiomics ).
10.1038/s41597-024-03059-9,All replication code is available at https://github.com/jhsuss/wealth-inequality .
10.1038/s41597-024-03089-3,"Data processing was performed in Python 3.10, and data used for the computation of HMEAs at city level are available can be accessed at Github repository located at https://github.com/Olivia-2012/HMEAs_DataSet . We implemented the procedure described in the Methods section."
10.1038/s41597-024-03060-2,"We have used open tools available on GitHub, and we have also uploaded our own tools and code used in this manuscript. In particular, •     For defining the iPAs, we have used the open tool “pyClusterROI” ( https://ccraddock.github.io/cluster_roi/ ). •     For obtaining the transcriptomic expression matrices, we have used abagen ( https://abagen.readthedocs.io/en/stable/ ). •     All the analyses and code involved in the methods and results of this manuscript have been uploaded to https://github.com/compneurobilbao/bha2 ."
10.1038/s41597-024-03067-9,"The turbine dataset was generated by aggregating the SCADA data obtained from the entire wind farm. It consists of five wind turbines, all of them of the same model and manufacturer: Fuhrländer FL2500 2.5 MW. To facilitate the manipulation and pre-processing of the data, we have developed functions in the programming languages R and MATLAB to serve as an interface. These functions efficiently transform the raw data into a structured table format. In this format, each variable corresponds to a column, while each entry represents a five-minute interval of data recorded in the rows. The database and the code are freely available at 25 and at the GitHub page https://github.com/alecuba16/fuhrlander ."
10.1038/s41597-024-03073-x,"The code for the different GANs and the diffusion model is openly shared by the creators of the generative models, see below. We therefore share our modifications to make the code work for 5-channel images, instead of 3-channel images, the used segmentation code, and some additional help scripts. https://github.com/muhamadusman/Assist/ Generative models Progressive growing GAN, https://github.com/tkarras/progressive_growing_of_gans StyleGAN 1, https://github.com/NVlabs/stylegan StyleGAN 2, https://github.com/NVlabs/stylegan2 StyleGAN 3, https://github.com/NVlabs/stylegan3 Diffusion model, https://github.com/openai/guided-diffusion Segmentation models U-Net, https://github.com/MIC-DKFZ/nnUNet Swin Transformer, https://github.com/open-mmlab/mmsegmentation ."
10.1038/s41597-024-03086-6,"We provide the PVBM toolbox code used to extract the microvascular biomarkers within the RDR repository as a notebook file 22 . The PVBM toolbox code is also available via https://pvbm.readthedocs.io/en/latest/ . For reproducibility and convenience in case any user wants to customize the extraction, all the .py files and a Readme file are available."
10.1038/s41597-024-03036-2,"The machine annotator code is written in Python 3.9, using libraries for pandas, NumPy, multiprocessing, tqdm, transformers, SentencePiece, neologdn, etc. and can be operated on Google Colab or a local integrated development environment. The annotation guidelines 26 , deposited dataset 27 and machine annotator 28 are available at figshare. The manually reviewed and machine-annotated datasets used for technical validation are also available at figshare 29 ."
10.1038/s41597-024-03021-9,The image pre-processing code used to build the dataset can be found at the following link: https://cbica.github.io/CaPTk/preprocessing_brats.html .
10.1038/s41597-024-02942-9,The code implementation was done in R 4.1.2 using R studio. The scripts to perform data visualization are available in 37 .
10.1038/s41597-024-03074-w,"The equations used to calculate RH, VPD, and WBGT max are available in R on GitHub ( https://github.com/emilylynnwilliams/CHC-CMIP6_SourceCode ). The CHC-CMIP6 dataset was processed using code written in the Interactive Data Language and Python."
10.1038/s41597-024-03083-9,"The scripts utilized to parse literature and extract events are home-written codes which are publicly available at GitHub repository https://github.com/bionlp-hzau/Cancer-Alterome . The underlying python3 libraries used in this project are all open-source: E-direct ( https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect ), OGER++ ( https://github.com/OntoGene/OGER ), PhenoTagger ( https://github.com/ncbi-nlp/PhenoTagger ), PubTator ( https://www.ncbi.nlm.nih.gov/research/pubtator/ ), AGAC-based model (AGAC-NER: https://github.com/YaoXinZhi/BERT-CRF-for-BioNLP-OST2019-AGAC-Task1 and AGAC-RE: https://github.com/YaoXinZhi/BERT-for-BioNLP-OST2019-AGAC-Task2 ), Pytorch ( http://www.pytorch.org ) and sci-kit-learn ( http://scikit-learn.ory ). More details on the guidelines of code usage are given in Supplementary 4 ."
10.1038/s41597-024-03090-w,The original training code of the IIS baseline algorithm and the trained weights and biases are publicly available at: https://github.com/DIAGNijmegen/SPIDER-Baseline-IIS . The nnU-Net baseline algorithm from Isensee et al . can be found here: https://github.com/MIC-DKFZ/nnUNet . Both trained algorithms can also be used on the grand challenge platform: 1. https://grand-challenge.org/algorithms/spider-baseline-iis/ 2. https://grand-challenge.org/algorithms/spider-baseline-nnu-net The source code used to compute the evaluation metrics on the test set are published at: https://github.com/DIAGNijmegen/SPIDER-Evaluation .
10.1038/s41597-024-03068-8,"The code files used to extract the dataset are available in the following Github repository: https://github.com/nsamlani/Code_Digitization_CETESB . The forms have changed in content and in layout over the years. Therefore, we include the bounding box coordinates for the text and the checkboxes every year (grouped text files named “bounding_boxes_text.txt” and “bounding_boxes_checkboxes.txt” for text and checkboxes, respectively). This information needs to be included in the corresponding codes to allow the reproduction of the outputs presented in this dataset."
10.1038/s41597-023-02844-2,"The programs used to generate the dataset were ENVI (5.3), ESRI ArcGIS (10.6) and Pytorch deep learning framework. All used codes to generate the dataset are available in the following GitHub ( https://github.com/Zhiyu-Xu/Fine-grained-urban-blue-green-gray-landscape-dataset-for-36-Chinese-cities )."
10.1038/s41597-024-03095-5,The code used to curate the EAGLE-I data for this article is available within the Figshare data repository: https://doi.org/10.6084/m9.figshare.24237376 22 .
10.1038/s41597-024-03100-x,Tweets were extracted from Twitter using publicly available scripts and the ‘Twitter API v2 for academic research’. No other code was used in the development of the database.
10.1038/s41597-024-03107-4,"The data analyses were performed according to the manuals and protocols by the developers of corresponding bioinformatics tools and all software, and codes used in this work are publicly available, with corresponding versions indicated in Methods."
10.1038/s41597-024-03097-3,No specific code was used in this study. All analytical processes were executed according to the manuals and protocols of the corresponding bioinformatic tools.
10.1038/s41597-023-02868-8,"The data presented in this study were produced as follows, and the respective codes are available in the additional Figshare repository 62 https://doi.org/10.6084/m9.figshare.c.6787140.v6 . All the statistics were derived from the NFI, SoEF or CBM input data and processed as described in the text. The input data and the processing steps performed to obtain the harmonised reference statistics are provided in the Excel files “Biomass Calculations (Excel)” and “Increment Calculations (Excel)”. The forest area map was obtained using the script “Forest_Map_code.R” (R version 4.0.2, 2020-06-22) to adjust the Copernicus 2018 Forest type map using the Copernicus 2018 Tree Cover map and match the reference statistics of forest area for 2020. The FAWS area map was obtained from the forest area map and six maps of restrictions to wood availability using the script “FAWS_Map_code.R” to map the FNAWS areas. The FNAWS area map was then used to mask the forest area map and obtain the FAWS area map using the raster calculator function of the QGIS software v. 3.22. The map of forest biomass density was obtained from the ESA CCI Biomass map for 2020 (v4) using the script “Biomass_Map_code.R” and the raster calculator function of the QGIS software to apply the bias-adjustment approach and match the reference biomass statistics for 2020 (as described in the text). The map of biomass available for wood supply was obtained by masking the bias-adjusted biomass map with the FAWS area map using the raster calculator function of the QGIS software."
10.1038/s41597-024-03098-2,"The R code used to perform batch searching, matching, and checking of species names from the original species names using the R package ‘ rfishbase ’ 43 and to clean coordinates using the R package ‘ CoordinateCleaner ’ 37 (“SchiSOFT_check_clean.R”) is available at https://doi.org/10.6084/m9.figshare.24638538.v1 ."
10.1038/s41597-024-03091-9,All code developed for this study is openly available as DeltaDTM.jl 41 at https://zenodo.org/doi/10.5281/zenodo.10051451 under the GNU General Public License v3.0. The code is written in the Julia programming language 42 .
10.1038/s41597-024-03081-x,Code for analysis and validation is available at https://github.com/YoonjungAhn/HISTPLUS .
10.1038/s41597-024-03071-z,Code is available on Google Colab at: https://colab.research.google.com/drive/1lgyPhYVXr4MffWnN7m-5Bo3Lt5f4f49D?usp=sharing .
10.1038/s41597-024-03106-5,No custom code was used. We share raw data.
10.1038/s41597-024-03113-6,No specific code or script were used in this study.
10.1038/s41597-024-03108-3,All software and pipelines were executed according to the manuals and protocols of published bioinformatics tools. The software version and code/parameters are described in the Methods section.
10.1038/s41597-024-03096-4,"No specific code was developed in this work. All the data were analyzed following the manuals suggested by the developers of the bioinformatic tools, which have been described in the Methods section."
10.1038/s41597-024-03104-7,MCSdb is free available at Figshare 43 and https://cellknowledge.com.cn/mcsdb/ to all users. The source code for the MCSdb website has been uploaded to GitHub: https://github.com/ZhangCellab/MCSdb .
10.1038/s41597-024-03102-9,"All calculations were done with the help of ESRI ArcGIS 10.7, and no computer code is used to generate the data in the manuscript."
10.1038/s41597-024-03103-8,"As described in the Data Records section, our database contains the R scripts that we used to extract and format data for each lake. Additionally, the database contains example scripts for organizing and visualizing the data."
10.1038/s41597-024-03093-7,No custom code was generated.
10.1038/s41597-024-03111-8,The process and script files can be downloaded at http://bioinfo.ihb.ac.cn/software/FishSNP or https://github.com/hliihbcas/FishSNP .
10.1038/s41597-024-03110-9,"The data calculation is mainly done by Matlab 2014, and the code can be found in Supplementary Information."
10.1038/s41597-024-03092-8,The code used to process Envisat ASAR data is written in IDL. The code for processing remote sensing data and producing continuous AGB map is written in Google Earth Engine using JavaScript. All the code is available at: https://github.com/uoedwq/ForestAGB .
10.1038/s41597-024-03114-5,"The software required for data processing and analysis and image generation in this study are accessible, the software versions as follows: 1. Cell Ranger (version 6.1.2, https://support.10xgenomics.com/single-cell-gene-expression/software/overview/welcome ). 2. OmicStudio ( https://www.omicstudio.cn/cell ). 3. Monocle 2 (version 2.22.0, http://cole-trapnell-lab.github.io/monocle-release/docs ). 4. CellPhoneDB (version 3.1.0, https://www.cellphonedb.org )."
10.1038/s41597-024-03127-0,All codes used for analysis can be found on GitHub at https://github.com/pangbo2019/BRCA_CCS .
10.1038/s41597-024-03129-y,AIT has developed the web scraping script for data collection of parts of EUPRO; it can be retrieved from: https://figshare.com/articles/software/cordis_wrapper_rb/24711738 .
10.1038/s41597-024-03123-4,"FastQC (version 0.11.3, https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ) was adopted to check the quality of raw FASTQ sequencing files. Metabolite profiling was analysed with ProteoWizard package ( http://proteowizard.sourceforge.net ), XCMS Online software ( https://xcmsonline.scripps.edu/ ), SIMCA 13.0 (Umetrics AB, Umea, Sweden) software, MultiQuant software ( https://sciex.com/products/software/multiquant-software ) and MetaboAnalyst plotform ( https://www.metaboanalyst.ca ), respectively."
10.1038/s41597-024-03117-2,"The dataset and required code to generate the dataset are publicly available on Zenodo 26 , Kaggle ( https://www.kaggle.com/datasets/ipateam/nuinsseg ), and GitHub ( https://github.com/masih4/NuInsSeg ), respectively."
10.1038/s41597-024-03112-7,There is no custom code produced during the collection and validation of this dataset.
10.1038/s41597-024-03101-w,No custom code was used to generate the described databases.
10.1038/s41597-024-03118-1,Script files were created using the R statistical programming and are available with the data provided and on Github ( https://github.com/achemura/cropsuitafriq/ ).
10.1038/s41597-024-03120-7,All code used to process and standardise the data are included in the OpenDengue Github repository 49 .
10.1038/s41597-024-03126-1,"The genome assembly pipeline, codes and the documentation are available at https://github.com/zhengchangsulab/A-genome-assebmly-and-annotation-pipeline ."
10.1038/s41597-024-03119-0,The analysis results of this research were performed using the BMKCloud platform ( www.biocloud.net ). We followed all published bioinformatics tool manuals and protocols for the execution of all software and pipelines. The software version and code/parameters have been described in the Methods section. There was no custom code used during this study for the curation and/or validation of the dataset.
10.1038/s41597-024-03109-2,"Pre- and post-processing code was written in R, and it is publicly available at https://github.com/MiRoVaGo/hyades ."
10.1038/s41597-024-03132-3,The Engagnition software is available to the public through its official repository ( https://github.com/dailyminiii/Engagnition ). This repository mainly includes the code for analyzing data distribution and technical validation.
10.1038/s41597-024-03025-5,"A git repository is publicly available at https://github.com/ReparkHjc/SDCD , in this repository several python scripts for visualisation, benchmarking and data pre-processing are available."
10.1038/s41597-024-03133-2,"The codes used in data generation and processing are in two formats, JavaScript used in GEE and Python. The codes are available in GitHub at: ( https://github.com/Zhangshaoy/SFAC.git ). Each repository includes a guide for the use of the codes. An online visualization map using the GEE experimental app is also provided: ( https://zsy11600.users.earthengine.app/view/sfac )."
10.1038/s41597-024-03105-6,No custom code has been used.
10.1038/s41597-024-03130-5,The current study did not involve any codes or scripts. Variable codes in this dataset are as follow. The initial of each head of the variable name indicates variable category. Each number alongside from this initial represents specific item in this variable category. Item information can be found in the coding book at the Science Data Bank repository that we provided. D = socioDemographic characteristics; P = family Parenting data; E = family socioEconomic status; L = Living events data; H = Health-related data; C = Center for epidemiologic depression scale data; S = Zung’s Self-reporting depression scale data; SP = Positive Suicide ideation data that measured by Positive and Negative Suicide Ideation scale - Chinese version; SN = Negative Suicide ideation data that measured by Positive and Negative Suicide Ideation scale - Chinese version.
10.1038/s41597-024-03139-w,"Datasets were harmonized, summarized, and visualized using custom scripts written in R (version 4.0). These scripts are publicly available on GitHub ( https://github.com/logan-berner/arctic_plant_biomass_synthesis_scripts ) and archived on Zenodo 76 ."
10.1038/s41597-024-03134-1,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-024-03138-x,"The code supporting the analyses is accessible in the Zenodo through Gitlab repository 11 , under the GNU Affero General Public License v3.0."
10.1038/s41597-024-03116-3,The author reports there are no custom code used.
10.1038/s41597-024-03155-w,"Figures 2 , 3 were generated using the data provided in the data set using the open source library Open3D 27 . To facilitate the reproduction of these figures, a copy of the raw data is included in the repository’s corresponding directory. Data set preparation was done with a combination of libraries, including Open3D 27 , numpy 28 and bpy. Blender version 3.3.0 was used to label the 234 triangle meshes. Further on, various functions were programmed to interact with the data set. These include functions for scaling labels up and down, cropping triangle meshes by labels, converting vertex labels to triangle or edge labels and calculating surface, volume and cluster centroids of the cropped out meshes like the labelled regions. Additionally, based on the technology, category, and subcategory, the components can be filtered. All the software used in the study are open source available at https://github.com/bensch98/eec-analysis ."
10.1038/s41597-024-03157-8,The data analyses were performed all software and parameters were mentioned in Methods. The core code is available at https://github.com/YuXuanLiua/pig-nosed-turtle/ .
10.1038/s41597-024-03143-0,"The time-series Landsat analysis and wetland remote sensing mapping were programed on the Google Earth Engine platform with JavaScript, and the corresponding codes can be freely visited on the Zenodo platform 56 and GitHub ( https://github.com/zhangxiaoradi/CodeRepository/blob/main/GWL_FCS30D_Code )."
10.1038/s41597-024-03150-1,No specific script was utilized in this study. The codes and pipelines used for genome sequencing data analysis were performed following the instructions of corresponding bioinformatics tools. The version and parameters of the software have been included in the Methods section.
10.1038/s41597-024-03135-0,The codes used for the data-processing in this study are provided in the Zenodo repository publication 24 .
10.1038/s41597-024-03152-z,All the scripts for the analysis of the loom files up to the visualizations are available on Github ( https://github.com/MariaTsayo/mulTI_Metatlas/ ) and under the DOI of Zenodo ( https://doi.org/10.5281/zenodo.10469288 ) 60 with a detailed description of each one.
10.1038/s41597-024-03161-y,"The software R (R-4.3.0, RStudio 2023.03.1 + 446) 66 was used for processing the data. The R script can be found within the data 45 on location ‘/3. Processing/R workspace/ Hooyberg_SurveyCoastalVisits_2022-08-31.R’)."
10.1038/s41597-024-03154-x,"The source code is tracked in the Git versioning system and can be publicly accessed from the repository at https://github.com/novakge/project-parsers and https://github.com/novakge/project-indicators without registration. It is licensed under the terms GNU General Public License v3.0. A runnable (reproducible) code capsule can be found at Code Ocean . The code is tested against MATLAB R2020a or later releases with the Global Optimization Toolbox. A developer manual, including examples, is located in the repository’s Readme file ."
10.1038/s41597-024-03168-5,"All the analyses were conducted within the R environment (R version 4.2.1). We utilized raster and terra packages 21 , 22 to create and structure raster datasets, while ncdf4 package 23 was employed to handle NetCDF files. Additionally, data pre-processing and wrangling tasks were performed using the tidyverse package 24 . The code for downloading and generating the raster files can be found in the “bottomT.R” script. Unfortunately, we are unable to share the code pertaining to the TP estimates as the authors of this paper are not authorized to distribute the raw MEDITS data. Distribution and use of the MEDITs dataset is regulated by EU Regulation 2017/1004 25 ."
10.1038/s41597-024-03163-w,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatic software. This study does not involve custom scripts or code.
10.1038/s41597-024-03146-x,The codes used to analyze the data in this study are available in the GitHub repository at the following URL: ( https://github.com/XiaoningHong/MouseBrain_ScientificData ).
10.1038/s41597-024-03142-1,"The versions of the software employed in this study have been specified in the Methods section. The default parameter was used, if no parameter was provided. No custom code was used in this study for the curation and/or validation of the datasets."
10.1038/s41597-024-03145-y,We hosted our codebase on the github repository: https://github.com/csuet/bronchoscopy_nsd . The code can be used to extract the segmentation and classification labels. It can also be used to train baseline models for single-task learning or multi-task learning. Please follow the instructions in the README.md file for further processing.
10.1038/s41597-024-03173-8,"All software used in this study are in the public domain, with parameters described in Methods and this section. If no detailed parameters were mentioned for the software, default parameters were used according to the software introduction."
10.1038/s41597-024-03164-9,"All publicly available code can be accessed from the SICdb GitHub Code Repository ( https://github.com/nrodemund/sicdb ). However, due to the partial use of the code to appropriately remove sensitive patient information in accordance with HIPAA regulations, not all codes are fully publicly accessible. Furthermore, the GDPR restricts the sharing of certain code components to ensure the highest level of anonymization."
10.1038/s41597-024-03147-w,The Multi-sensor Matchup System 114 is available from https://github.com/bcdev/MMS 127 . SLSTR pre-processing code to regrid Vis/NIR channels to match infrared bands 43 . Code used to validate level 3 and 4 products is available 128
10.1038/s41597-024-03149-8,The guidelines for data retrieval and utilization have been uploaded to GitHub 48 . The specific contents comprise: 1. Input data introduction.ipynb : A brief introduction and data demonstration about the input data for the traffic assignment process in the dataset. 2. A guide for TransCAD users.md : It is a guide for users who want to view and modify the dataset in the Graphical User Interface (GUI) of TransCAD. 3. AequilibraE_assignmnet.py : A Python code file for users who want to get access to the traffic assignment results by using the AqeuilibraE.
10.1038/s41597-024-03160-z,There is no custom code for this project.
10.1038/s41597-024-03137-y,"The custom code “DataExtraction.py” is available 26 from the same repository, ( https://doi.org/10.7910/DVN/SG9TMD ). The code consists of functions to generate HR from BVP and ECG as well as functions to create images and videos of seat pressure distribution heatmaps. It also includes functions for extracting zipped files of the dataset. The code is tested and run with Python 3.11 45 ."
10.1038/s41597-024-03124-3,"Reported multi-omics data processing and analysis software pmartR and small molecule identification and annotation software LIQUID have been made publicly available and are openly accessible to the global scientific community. These software packages can be formally cited from their corresponding Zenodo citations 253 , 254 . All GitHub source code repositories, used to verify and corroborate data source code collections, have been assigned corresponding globally unique and persistent DOI at Zenodo under a CC BY 4.0 licence. Potential users should consult corresponding GitHub landing pages for any additional licences, references, and disclaimers provided."
10.1038/s41597-024-03122-5,All codes used in the image registration between WSI and smartphone images described in the manuscript were written in Python 3 and are available through our GitHub repository ( https://github.com/p024eb/PLISM-registration ). We have provided all the necessary libraries and python scripts that allow the tracing of our results.
10.1038/s41597-024-03151-0,No specific code was used to produce the data described in this manuscript.
10.1038/s41597-024-03158-7,The data files and the python scripts used for model training are available online through GitHub repository: https://github.com/Dsayddd/RoadSurface .
10.1038/s41597-024-03159-6,"The code for the detection of potential outliers and the extraction of land cover and meteorological data was developed using Python 3.9.7. The corresponding Jupyter Notebooks are available at the figshare repository 15 . The outlier detection code uses the Globe-LFMC-2.0 15 file as input. When executing the land cover and meteorological data extraction code, it is essential to have downloaded the required input data first."
10.1038/s41597-024-03128-z,The published data set contains raw anonymized data. All anonymization steps can be found in Section Data Anonymization . The SAM mapping can be found in Questionnaire Emotion and Mood . No additional code was used to generate the data set.
10.1038/s41597-024-03186-3,All codes for data standardization are available at the HydroShare data respository 11 .
10.1038/s41597-024-03148-9,"The statistical analysis and modeling employed for the database validation was performed with Pandora & IsoMemo Applications. Source code for spatiotemporal model (TimeR) Data Search and Spatiotemporal Modelling 23.12.0.2software – Time R Model is available online ( https://pandoraapp.earth/app/iso-memo-app ), and also available for download at GitHub ( https://rdrr.io/github/Pandora-IsoMemo/iso-app/ )."
10.1038/s41597-024-03162-x,"All statistical analyses were performed in R (v.4.1.3). The BIOMASS R-package is an open source library available from the CRAN R repository. The BIOMASS vignettes and individual function helps contain detailed notes on usage to derive plot level AGB estimates with uncertainty estimates through error propagation using Monte Carlo method. The codes associated with the error propagation from plot-scale AGB to LiDAR AGB maps, generating additional metadata and binned covariance matrices as described in Methods section are available on GitHub ( https://github.com/surajreddyr/LIDAR_AGB/tree/main )."
10.1038/s41597-024-03172-9,"The Python code utilised in this study is available on Kaggle, via the link: https://www.kaggle.com/code/ariffazlanalymann/ml-varanus-morphology-sex-prediction ."
10.1038/s41597-024-03179-2,Data processing was performed using the ESRI Arcpy Python Library. Spatial analysis and map production was done using ArcGIS Pro software. Plots were generated with Matplotlib Python Library. Code written for maps processing and detections postprocessing is available at GitHub repository: https://github.com/Szubbi/WallToWallMapingBuildingsPoland .
10.1038/s41597-024-03189-0,All commands and pipelines were executed following the manuals and protocols of the corresponding bioinformatic software. The versions and parameters of the software have been detailed in the Methods section.
10.1038/s41597-024-03178-3,All data processed with publicly available bioinformatics tools or pipelines followed the analysis guidelines provided by those tools. No custom code was used during this study for the curation and/or validation of the dataset.
10.1038/s41597-024-03144-z,"Software is available on GitHub and can be accessed via the following link. https://github.com/dailyminiii/MultiSenseBadminton . This comprehensive software package includes examples for reading and parsing HDF5 files, performing data preprocessing by extracting and filtering, and displaying the results. Additionally, it offers functionality for training a deep-learning model using the preprocessed data, generating a T-SNE plot based on the preprocessed data, and creating a visualization video based on the raw data presented in Fig. 13 ."
10.1038/s41597-024-03069-7,"The software used for processing the data is described in the methods. A custom Python code 39 , manual curation, and MetAtlas 40 were used for analysis of LC-MS data. Metagenomic analyses used the DOE-JGI Metagenome Annotation Pipeline (v.5.0) 28 ."
10.1038/s41597-024-03136-z,No custom codes were used in this study. All bioinformatics tools and software applications were executed in accordance with their respective manuals and protocols. The specific software versions and the parameters used are detailed in the methods section.
10.1038/s41597-024-03195-2,No custom code was generated for the purposes of our paper.
10.1038/s41597-024-03198-z,Multiresolution segmentation and assigned class algorithm are executed in the eCognition Developer 9.0. River network superposition and spatial statistical analysis are completed in ArcGIS 10.5. The calculation of NDVI time series is completed in Google Earth Engine ( https://code.earthengine.google.com/d6f93ac6e504ec71de8607e746ebb84e ).
10.1038/s41597-024-03176-5,No custom code has been used.
10.1038/s41597-024-03180-9,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository https://github.com/cxqwindy/CO2_reduction_electrocatalysts_db . The underlying machine-learning libraries used in this project are all open-source: rxn4chemistry( rxn4chemistry ), ChemDataExtractor ( chemdataextractor.org ) 37 , gensim ( radimrehurek.com ) 48 , PyMuPDF( PyMuPDF ), Pytorch ( www.pytorch.org ) and scikit-learn ( scikit-learn.org ) 49 ."
10.1038/s41597-024-03184-5,"All code for the generation of the dataset was written in Matlab R2022 and can be found at https://github.com/aebruano/HEMStoEC . Daily information is received by the data acquisition system in a zipped file, which should be placed in the same directory (denoted as root directory) of the function files. A sample can be found in 2023_06_11_00_00_00.zip. The README and the VARS files provide information about the format of the files enclosed in the zip file. Matlab data is extracted from the unzipped file using the Matlab function extract_quadro_10.m. The command extract_quadro_10(‘2023_06_11_00_00_00’ ) creates a Matlab data file 2023_06_11_00_00_00.mat inside the 2023_06_11_00_00_0 0 directory. Gaps are identified and data is interpolated using the function Validate_Quadro_4.m. A data file 2023_06_11_00_00_00_cor.mat is created, again inside the 2023_06_11_00_00_0 0 directory, upon the command Validate_Quadro_4(‘2023_06_11_00_00_00’,‘2023_06_11_23_23_59’) .Data with a common time basis is achieved using the Matlab function convert_quadro_10_cor.m. Using the command convert_quadro_10_cor(‘2023_06_11_00_00_00’,‘2023_06_11_23_23_59’,”, minutes(15),hours(1)), the data file 2023_06_11_00_00_00 to 2023_06_11_23_23_59 excl pst 15 min est 1 hr_cor.mat is created, this time in the root directory. A matlab file, Factor.mat , needs to be placed in the root directory."
10.1038/s41597-024-03188-1,"The core codes and associated files we used for mapping rapeseed flowering phenology and spatial distribution are available at https://github.com/liuwenbinwhu/China-annual-rapeseed-maps30 . Moreover, MATLAB R2022a, ArcGIS Pro, and Origin 2017 were used for data pre-processing, spatial analysis, and figure production."
10.1038/s41597-024-03197-0,Our matlab scripts are also accessible on GitHub ( https://github.com/Germann-lab/dTOR-985-Connectome.git ). The files necessary for using our connectome in the Lead-DBS software package are accessible from Lead-DBS ( https://www.lead-dbs.org/ ).
10.1038/s41597-024-03194-3,No specific programs or codes were used in this study.
10.1038/s41597-024-03187-2,The Besca 38 toolkit was used to process the data. The corresponding Jupyter notebooks are available from the Zenodo together with the data. The processing workflow for the 10X data is available from record 8399409 42 standard_workflow_besca2.ipynb or standard_workflow_besca2.html and the processing workflow for the Smart-seq2 data is available from record 8399458 44 standard_workflow_besca2.ipynb or standard_workflow_besca2.html. The Jupyter notebook to integrate both datasets is available from record 8399475 45 integrate_10x_smartseq2.ipynb or integrate_10x_smartseq2.html.
10.1038/s41597-024-03177-4,No custom code has been used. Data processing has been described through the Methods section.
10.1038/s41597-024-03141-2,All codes used in the development and validation of the ONFIRE Dataset 32 are freely available on Zenodo 79 : https://zenodo.org/records/10512198 .
10.1038/s41597-024-03200-8,"Behavioral data is recommended to be opened using MATLAB 2022b. According to the provided data instructions, you can directly process the data to obtain reaction times and accuracy for each participant. However, data from participants with extremely low accuracy should not be included in the effective statistical range. For fNIRS data, it is advisable to open it using MATLAB 2022b. Design a band-pass filter to filter the raw data before proceeding with further analysis. When processing EEG data, it is necessary to preprocess the data for each participant using the MATLAB plugin eeglab. Specific instructions can be found in the official eeglab user manual. We also provide reference data processing scripts, which can be obtained on GitHub. No custom code was used during this study."
10.1038/s41597-024-03167-6,No custom code was used to generate or process the data.
10.1038/s41597-024-03153-y,"All experiments have been modelled and programmed using Python and open datasets. In the interest of full reproducibility, the code and datasets to reproduce the experiments are included in a .zip file available at Figshare 29 and in the public GitHub repository 30 . The results have been obtained using Python 3.10.9 and the packages used and their corresponding versions are: keras 2.11.0, keras_tuner 1.3.0, matplotlib 3.7.1, numpy 1.23.5, pandas 2.0.3, pytest 7.4.0, scikit_learn 1.2.2, scipy 1.10.1, seaborn 0.12.2, statsmodels 0.13.5, tensorflow 2.11.0 , and openpyxl 3.11.1 . The instructions to run the scripts for the demos or the experiments are available in the README.md file included both in the .zip file at Figshare 29 and in the GitHub repository 30 ."
10.1038/s41597-024-03204-4,"No specific code was developed in this work. The parameters of bioinformatics tools and all software used for data processing were described in the Methods section and Supplementary table 1 . If no detailed parameters are mentioned, the default parameters were used."
10.1038/s41597-024-03088-4,"The code used for the extraction, translation, pre-processing, and protection of the vessel identification is available in a GitHub repository ( https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23 ). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colaboratory ( https://colab.research.google.com/ ). The Anaconda Distribution ( https://jupyter.org/install.html ) is an excellent choice for scientific computing and data science as it includes Python, Jupyter Notebook, and other commonly used packages. One must use synthetically generated or protected data to run the codes since the original dataset cannot be shared for privacy and confidentiality reasons. Moreover, some values/variable vectors in files 4_1_k_identifiers_analysis.py and 5_Anonimization.py are not real ( https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23 ), as they should be withheld from the general public to ensure data protection and privacy."
10.1038/s41597-024-03199-y,No custom code was generated for this work.
10.1038/s41597-024-03166-7,No custom code was used to process the data.
10.1038/s41597-024-03221-3,All R code that was used in the HeatCRD is provided.
10.1038/s41597-024-03205-3,Software and their versions used for RNA-seq analysis were described in Methods. No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-024-03191-6,Code for Radiomics feature extraction and GLM model is included in the Zenodo reposity of the dataset 16 .
10.1038/s41597-024-03174-7,"In order to eliminate possible mistakes, in the data format or any data fields, data were converted to all three data formats using a traceable computer code, which is openly accessible through the https://github.com/pozsgaig/BALA_database GitHub repository."
10.1038/s41597-024-03171-w,The PheKnowLator ecosystem coding resources are described in detail in Supplementary Table 3 by ecosystem component. The PKT-KG algorithm is publicly available through GitHub ( https://github.com/callahantiff/PheKnowLator ) and PyPI ( https://pypi.org/project/pkt-kg ). The SPARQL Endpoint deployment code and documentation are also available through GitHub: https://github.com/callahantiff/PheKnowLator/tree/master/builds/deploy/triple-store#readme . A list of the computational resources used to evaluate the PheKnowLator ecosystem is provided in Supplementary Table 4 . The code used to scrape the GitHub API is available from Zenodo 93 . The survey of open-source KG construction tools is also available on Zenodo 91 . The v2.1.0 PheKnowLator code is available on GitHub ( https://github.com/callahantiff/PheKnowLator/releases/tag/v2.1.0 ) and from Zenodo ( https://zenodo.org/record/4685943 ) 105 .
10.1038/s41597-024-03196-1,The source code of the document-level information extraction pipeline is available at https://github.com/GGNoWayBack/cathodedataextractor .
10.1038/s41597-024-03165-8,"The associated GitHub repository 142 ( https://github.com/andrew-guerin/water_quality_interpolations ) contains code used to perform the final interpolations, scripts for reprojection and resampling of rasters, copies of the interpolated data layers, copies of the calcium and pH databases used for the interpolations (excluding proprietary data from third parties, which cannot be publicly shared under existing data agreements), and Shiny app scripts for interactive maps which show the distribution of data points, with summary data (where these can be shared). Please note that, as a result of the large number of data points included, the Shiny maps may take a few moments to load."
10.1038/s41597-024-03215-1,No code was developed for implementing a software.
10.1038/s41597-024-03182-7,The code used to generate the results in this paper is available at https://github.com/MikhailKulyabin/OCTDL .
10.1038/s41597-024-03212-4,"The collected SMILES datasets can be accessed in the corresponding supplementary information files 7 , 30 and Zenodo 38 . The Python codes of the SMILES to BigSMILES conversion algorithm, BigSMILES to SMILES conversion algorithm, and examples of the analysis conducted in this study, including technical validations, are available at GitHub: ( https://github.com/CDAL-SChoi/BigSMILES_homopolymer )."
10.1038/s41597-024-03222-2,"Readers can access the tutorials and code of our original and preprocessed datasets on Github ( https://github.com/zwqzwq0/MPDB ). Two folders called preprocessing and classification can be found, which contain MATLAB code for preprocessing and python code for classification."
10.1038/s41597-024-03208-0,"The dataset can be efficiently managed, visualized and preprocessed using four Jupyter notebooks. These notebooks are accessible for download at https://github.com/sathanasoulias/Plegma-Dataset To ensure the proper functioning of these notebooks, it is necessary to have Python version 3 along with the Pandas, Plotly and Numpy libraries installed. Moreover, the primary Javascript functions used in the data collection process (Z-wave service and DataBroker service) are located in the data_collection folder giving more details about the implementation of such a system."
10.1038/s41597-024-03193-4,"We provide all code for mask creation using JSON annotations and phase extraction using CSV files, as well as the training IDs for four-fold validation and usage instructions in the GitHub repository of the paper ( https://github.com/Negin-Ghamsarian/Cataract-1K )."
10.1038/s41597-024-03226-y,"All software with respective versions and parameters used to generate the resources presented here (i.e., transcriptome assembly, pre- and post-assembly processing stages, and transcriptome annotation) are listed in the Methods section. Software programs without associated parameters were used with the default settings."
10.1038/s41597-024-03202-6,The data can be found at https://datadryad.org/stash/dataset/doi:10.5061/dryad.g1jwstqw7 44 . All code is freely available at https://github.com/heplersa/USDMdata .
10.1038/s41597-024-03213-3,"The dataset and related files, and the R software scripts used to produce the figures are available on the figshare repository 31 ."
10.1038/s41597-024-03239-7,The code used in this study is deposited to figshare ( https://doi.org/10.6084/m9.figshare.23671647.v7 ) 18 .
10.1038/s41597-024-03241-z,No custom code was used to generate or process the data described in the manuscript.
10.1038/s41597-024-03209-z,"The published softwares used in this work were cited in the Methods section. If no detailed parameters were mentioned for the software, default parameters were applied."
10.1038/s41597-024-03227-x,All quality control analyses were performed using FastQC ( http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ). Bulk RNAseq analysis was performed using Salmon ( https://combine-lab.github.io/salmon/ ) and DESeq 2 ( https://github.com/thelovelab/DESeq2 ). Single-cell and single-nucleus analyses were performed using Cell Ranger (downloaded from 10x genomics) and Seurat ( https://satijalab.org/seurat/ ). All code generated for the analyses presented here is available on the McKey Lab GitHub at https://github.com/McKeyLab/RODatasets .
10.1038/s41597-024-03201-7,"The Java script for the GEE platform generating the row dataset, and the Matlab scripts generating the definitive data set are deposited in https://doi.org/10.6084/m9.figshare.24794295.v2 , and are freely available."
10.1038/s41597-024-03236-w,The code used to create the final product (different machine learning models) is available at https://github.com/APJ1812/INCOIS_pCO2 . The study uses general machine learning codes available in Python.
10.1038/s41597-024-03140-3,"Analysis in this study was performed with the following open-access programs. plink1.9 v1.90b6.4: https://www.cog-genomics.org/plink/1.9/ plink2 v2.0, alpha software for processing genetic data and performing GWAS: https://www.cog-genomics.org/plink/2.0/ TOPMed genotype imputation on GRC38: https://imputation.biodatacatalyst.nhlbi.nih.gov/ R package liftOver: https://www.bioconductor.org/help/workflows/liftOver/ R package TwoSampleMR: https://mrcieu.github.io/TwoSampleMR/news/index.html R package Coloc: https://github.com/chr1swallace/coloc PheWeb v1.1.19, a web server for browsing phenome-wide associations: https://github.com/statgen/pheweb In addition, we uploaded the R code for conducting Mendelian randomization and colocalization (as the filenames ‘mr.R’ and ‘coloc.R’, resepectiviely) to Zenodo: ( https://doi.org/10.5281/zenodo.10011473 )."
10.1038/s41597-024-03234-y,"The EEG preprocessing, ERP analysis code and code used for classification is avaliable at https://github.com/yiyuchen-lab/DeceptionGame ."
10.1038/s41597-024-03240-0,"The data and predictive models presented in this study are publicly available: • Dataset . You can download the images from the Zenodo repository 30 https://zenodo.org/record/8333888 . After downloading, place the train and val files from the Deep Learning Datasets folder into the data folder of the CWLD semantic segmentation model. • CWLD semantic segmentation model . Code scripts and project instructions on how to use this dataset to train segmentation models are available for download in the Zenodo repository 41 , and weight files for trained models are also provided for readers to try out the models without training. Visit https://zenodo.org/records/10911443 . The requirements.txt file provides the libraries needed to run the project, and the README.md file describes in detail the deployment process and functionality of each module, as well as the role of the various toolkits in utils. In addition, the model and the corresponding code for executing the model are available on the GitHub platform at https://github.com/huangleinxidimejd/CWLD_Model ."
10.1038/s41597-024-03220-4,"The following are the commands for data processing. The analysis is deployed on CentOS 7 platform. All software versions have been specified in the Methods section. The reference genome version we used is GDDH13_v1.1, detailed annotation and gene prediction information can be found here ( https://www.rosaceae.org/species/malus/malus_x_domestica/genome_GDDH13_v1.1 ). 1. Quality control $ fastp -i sample_raw_1.fq.gz -o sample_clean_1.fq.gz -I sample_raw_2.fq.gz -O sample_clean_2.fq.gz -r --length_required 60 -f 12 2. Read mapping $ hisat2 --dta --summary-file sample.summary.txt --new-summary --min-introlen 20 --max-introlen 5000 reference.genome -1 sample_clean_1.fq.gz -2 sample_clean_2.fq.gz -S sample.sam 3. Convert and sort $ samtools sort sample.bam sample.sam 4. Normalize $ stringtie -G reference.gff3 -e -B -o sample.gtf -A sample.tab sample.bam"
10.1038/s41597-024-03232-0,"All software employed for data processing was executed following the guidelines of the bioinformatic software cited above. If no detailed parameters are mentioned, the default parameters were used."
10.1038/s41597-024-03243-x,There is no specific custom code used to generate the data/figures presented in this work.
10.1038/s41597-024-03244-w,PCRaster GLOBal Water Balance model: version 2.0 is a grid-based global hydrology and water resources model developed at Utrecht University and freely available at https://globalhydrology.nl/research/models/pcr-globwb-2-0/ .
10.1038/s41597-024-03214-2,The script used to process the ICESat-2 data and extract ICESat-2 crossover points from it is available at the following link: https://github.com/snowhydro/icesat-cross-point .
10.1038/s41597-024-03235-x,We provide R code enabling the calculation of selected variables. This code is available in the repository 29 under the name TGH total scores code.R. Its proper operation requires the use of the R environment at least version 4.3.1 and the tidyverse package 36 .
10.1038/s41597-024-03192-5,No custom code was used to process the data described in this paper.
10.1038/s41597-024-03251-x,No custom code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-024-03237-9,No custom code was generated for this work.
10.1038/s41597-024-03228-w,No custom code was generated in this work.
10.1038/s41597-024-03216-0,No code is available to reproduce the data. The R package mcomDb is publically available on GitHub ( https://github.com/NRM-MOC/mcomDb ) and includes a function for shifting the dataset from a long format to a wide format.
10.1038/s41597-024-03252-w,"Code to load, reconstruct, and visualize fastMRI prostate data, as well as code to run training, validation, and testing for the deep-learning-based classification and reconstruction, may be found at https://github.com/cai2r/fastMRI_prostate . The fastMRI repository (which also contains code for the VarNet) may be found at https://github.com/facebookresearch/fastMRI ."
10.1038/s41597-024-03238-8,No custom code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-024-03224-0,"The code used to produce the bias-corrected NESM3 simulations is publicly available on the GitHub repository: https://github.com/Mengzhuo-Zhang/BC_scripts_NESM3 (last access: 21 January 2024). The code consists of an NCL (version 6.6.2, https://www.ncl.ucar.edu ) script to compute non-linear trends of the variables and a few CDO (version 1.7.0, https://code.mpimet.mpg.de/projects/cdo ) scripts to regrid data and correct NESM3 bias."
10.1038/s41597-024-03233-z,No custom code was used during this study for the curation and/or validation of the dataset.
10.1038/s41597-024-03253-9,No novel code used in the construction of RAS dataset 22 .
10.1038/s41597-024-03223-1,"The programs used to generate all the results were Python 3.7, MATLAB (R2018b), and ArcGIS (10.4). The code and scripts used for training, testing, and predicting the NTL data are available in the open GitHub repository “ https://github.com/xian1234/NTLSTM ”, and the code for calibrating and validating the data is available at “ https://www.mathworks.com/matlabcentral/fileexchange/119308-modest ”."
10.1038/s41597-024-03249-5,The python code that was used to train the model to generate the dataset descripted above can be found at Figshare 38 . Further questions can be directed towards Jianlong Feng (fjl181988@tust.edu.cn).
10.1038/s41597-024-03246-8,All software and pipelines were executed in strict accordance with the manuals and protocols provided by the published bioinformatic tools. No custom programming or coding was used.
10.1038/s41597-024-03248-6,"Parameters of software tools involved in the methods are described below:1) Fastp: version 0.23.2, default parameters;2) Kmerfreq_AR: version 2.0.4, parameters: (k-mer size of 17);3) Jellyfish: version 2.2.6, parameters: (count -m 17 -s 10 G -t 10 -C);4) GenomeScope: version 2.0, parameters: (k-mer size of 17, read length of 100, maximum k-mer coverage of 1000);5) NextDenovo: version 2.4.0, parameters: (read_cutoff = 3k, seed_cutoff = 27k, blocksize = 5 g);6) wtdbg 2.huge: version 1.2.8, parameters: (wtdbg-1.2.8 -k 0 -p 21 -S 2, wtdbg-cns -c 0 -k 13, kbm-1.2.8 -k 0 -p 19 -S 2 -O 0, wtdbg-cns -k 11 -c 3);7) SMARTdenovo: version 1.0.0, parameters: (-c 3 and -k 11);8) NextPolish: version 1.01, default parameters;9) Bowtie2: version 2.3.2, parameters: (-end-to-end,–very-sensitive –L 30);10) Juicer: version 2.0, default parameters;11) HiC-Pro: version 2.7.8, default parameters;12) LACHESIS: latest version, parameters: (CLUSTER MIN RE SITES = 100; CLUSTER MAX LINK DENSITY = 2.5; CLUSTER NONINFORMATIVE RATIO = 1.4; ORDER MIN N RES IN TRUNK = 60; ORDER MIN N RES IN SHREDS = 60);13) BUSCO: version 5.4.6, parameters: (embryophyta_odb10);14) EDTA: version 1.7.0, parameters: (sudo docker run -it -v $PWD:/in -w /in oushujun/edta:1.9.5);15) LTRharvest: lastest version, parameters: (-minlenltr 100 -maxlenltr 7000 -mintsd 4 -maxtsd 6 -motif TGCA -motifmis 1 -similar 85 -vic 10 -seed 20 -seqids yes);16) LTR_FINDER_parallel: version 1.2, default parameters;17) LTR_retriever: version 2.9.0, default parameters;18) Generic Repeat Finder: version 1.7.0, default parameters;19) TIR-Learner: version 1.7.0, default parameters;20) HelitronScanner: version 1. 0, default parameters;21) RepeatModeler: version 2.0.2a, default parameters;22) RepeatMasker: version 4.1.1, parameters: (-pa 30 -lib –no_is -poly -html -gff -dir masker);23) TEsorter: version 1.1.4, default parameters;24) Augustus: version 3.4.0, default parameters;25) GeMoMa: version 1.6.1, default parameters;26) TransDecoder: version 5.5.0, parameters: (-G universal, -m 100);27) EVidenceModeler: version 1.1.1, default parameters;28) TransposonPSI: version 1.0.0, default parameters;29) BLASTP: version 2.10.1, parameters: (-outfmt 6, -evalue 1e-15);30) eggNOG: version 5.0, default parameters;31) Blast2GO: version 1.44, default parameters;32) PlantTFDB: version 5.0, default parameters;33) CAFE: version 4.2.1, default parameters;34) Rfam library: version 11.0, default parameters;35) tRNAscan-SE: version 2.0, default parameters;36) Minigraph2: version 2.25 (r1173), parameters: (-ax asm5);37) D-GENIES: version 1.5.0, default parameters;"
10.1038/s41597-024-03094-6,The standard workflow used for processing and generating the Feature-Based Molecular Networking can be found in the GNPS documentation . The scripts used to resolve the taxonomy of the species in the collection are available in this repository: https://github.com/luigiquiros/metadata_preparation . The workflow for ISDB annotation and taxonomical re-weighting is available here: https://taxonomicallyinformedannotation.github.io/tima-r/index.html . The script for cleaning and consolidating the annotations is available here: https://github.com/luigiquiros/inventa . The scripts for the generation of the interactive figures for this Data Descriptor are available in this repository: https://github.com/luigiquiros/Celastraceae-Set-publication-examples . The scripts used to generate the interactive TMAP are available in this repository: https://github.com/mandelbrot-project/pf_1600_datanote/releases/tag/v0.1 .
10.1038/s41597-024-03247-7,All data processing and testing described in Methods and Technical Validation sections were conducted using MATLAB version R2021a. Main codes used to construct CROPGRIDS are distributed in the “CODES.zip” folder (Table 3 ) along with CROPGRIDS dataset available for public download from the figshare repository 25 at https://doi.org/10.6084/m9.figshare.22491997 .
10.1038/s41597-024-03242-y,The MRI dataset can be found in https://openneuro.org/datasets/ds003037/versions/2.1.0 28 . Please download the latest available version as there may be updates. Clinical and cognitive data are available in Zenodo https://doi.org/10.5281/zenodo.10409461 29 . No custom code was used in this work.
10.1038/s41597-024-03259-3,"The code that is described at Usage Notes , along with all functions that are called by this code, can be accessed through Zenodo without restrictions ( https://doi.org/10.5281/zenodo.10370064 ) 16 . The GPL license enables re-users to run, study, share, and modify the software in any medium or format. Other researchers who use this dataset and code are encouraged to also share their code. The code will also be available within the Auditory Modeling Toolbox (AMT) version 1.6 as the function exp_heeringa2024 27 . All code was written in MATLAB and has been tested in version R2023b."
10.1038/s41597-024-03263-7,"The software used for labelling the pavement damage object bounding boxes is LabelImg , provided by https://github.com/HumanSignal/labelImg . In fairness to test the usefulness of the dataset, the framework used for training and testing object detection algorithms in the technical validation is MMDetection , provided by https://github.com/open-mmlab/mmdetection ."
10.1038/s41597-024-03170-x,"The MRI quality metrics used in this paper are output by the MRQy software, which is available from GitHub ( https://github.com/ccipd/MRQy ). For DICOM to file to image conversion is the ITK-SNAP tool. Use DICOM Anonymize software to anonymize the sensitive information in the original DICOM header file, and then rename the case."
10.1038/s41597-024-03255-7,No custom scripts or code were used in this study.
10.1038/s41597-024-03270-8,"In this study, no custom scripts or command lines were utilized. All software employed for data processing and analysis are publicly available. The specific versions and parameters of each software are detailed in the Methods section. If no specific parameters were mentioned for a particular software, default parameters were used. The software was applied following the manuals and protocols provided by the respective bioinformatic tools."
10.1038/s41597-024-03156-9,Codes used in processing the data are included in the raw instrument folders.
10.1038/s41597-024-03260-w,All software and pipelines were executed according to the manual and protocol of published tools. No custom code was generated for these analyses.
10.1038/s41597-024-03190-7,"The entire workflow is available in the RASMI GitHub repository ( https://github.com/TomerFishman/MaterialIntensityEstimator ): the methods for creating the MI value ranges are implemented in Python 3 code, and the random forest classification of structure types is implemented in the Orange machine learning and data mining suite 58 . The GitHub repository also stores the resulting data and supporting figures. Released versions of the data are archived in Zenodo. The Zenodo version used for this data descriptor is v20230905 59 ."
10.1038/s41597-024-03218-y,"All associated code for downloading, loading, and preprocessing this dataset may be found at: https://github.com/indzhykulianlab/hcat-data ."
10.1038/s41597-024-03275-3,"The codes used in this study are available from Figshare 46 or the School of cities GitHub ( https://github.com/schoolofcities/commercial-boundaries ). In addition, modified metropolitan areas, a saved Pytorch state file and commercial boundary result data can also be obtained from those pages. Readers can easily replicate the identified commercial boundary using the given codes and datasets."
10.1038/s41597-024-03250-y,The IRIS code is publicly available ( https://github.com/njsparks/iris ) and a release of the version described here has been archived 33 .
10.1038/s41597-024-03230-2,Sample codes for developing the HiMIC-Monthly dataset are available from Zenodo 68 at https://doi.org/10.5281/zenodo.8352539 .
10.1038/s41597-024-03268-2,"The code used for conducting spectral analysis, and generating topographic maps of the EEG data was uploaded to the OpenNeuro dataset along with other data 16 ."
10.1038/s41597-024-03262-8,All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software. No specific code has been developed for this study.
10.1038/s41597-024-03206-2,No custom code was used.
10.1038/s41597-024-03267-3,"Code for generating derivative tables from the Brain Quantifier outputs, converting model slopes to nifti files, and automate the generation of scan screenshots was implemented in python. Modeling and statistics were done in R. All scripts are available in the dataset 17 “code” folder. T1 and T2 maps as well as volumetric results were obtained using research applications for which access was granted by research collaboration agreements between the authors."
10.1038/s41597-024-03269-1,"(1) SMRTLink: version10.1, default parameters. (2) ccs: version 6.2.0, main parameters: --min-rq 0.9 --min-passes 3 -j 6 --min-length 200. (3) Lima: version 2.1.0, main parameters: --isoseq --num-threads 6. (4) Isoseq3: version 3.4.0, main parameters: refine --require-polya, cluster --num-threads 6 --verbose --use-qvs. (5) CD-HIT: version 4.6.1, main parameters: -c 0.99 -M 0 (cd-hit-est). (6) BUSCO: version 3.0.2, main parameters: -m tran -c 4 -f. (7) BLAST: version 2.2.31, main parameters: -outfmt 5 (Alternative splicing). (8) IsoSeq_AS_de_novo: version 1.0, default parameters. (9) MISA: version 1.0, default parameters. (10) TransDecoder: version5.0.0, main parameters: -m 50 -G universal -S. (11) CPAT: version 1.2.2, main parameters: -cutoff 0.38. (12) CPC2: version 0.1, default parameters. (13) CNCI: version 2, default parameters. (14) PfamScan: version 1.60, main parameters: -translate orf. (15) iTAK: version 1.7a, default parameters. (16) diamond: version 2.0.15, -k 100 -e-evalue 1e-5 -f 5. (17) InterProScan: version 5.34–73.0, main parameters: -appl Pfam -goterms -iprlookup -pa -f xml -dp -t p. (18) Hmmscan: version 3.3.2, main parameters: --noali --cut_nc --acc -notextw."
10.1038/s41597-024-03276-2,No specific script was used in this work. The codes and pipelines used in data processing were all executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-024-03254-8,We supply the raw data in.csv files and have not used any ad-hoc code to process them.
10.1038/s41597-024-03274-4,"The analyses were conducted using the version and parameters as described below: 1) SnapGene, version 6.1, parameters used: interrupted circle, show polymorphism cite locations, unique 6+ cutters 2) Sequencher, version 5.4.6, parameters used: minimum match percentage 85, minimum overlap 20. 3) MEGA, version 11 24 , parameters used: NJ tree with Kimura2 parameter model 33 ."
10.1038/s41597-024-03169-4,The script and the ontologies utilized in this study are openly accessible and can be obtained from the following GitHub repositories:• demo-orowan • PMD Core Ontology (PMDco) • Tensile Test Ontology (TTO) • Precipitate Geometry Ontology (PGO)
10.1038/s41597-024-03210-6,"Software, versions and settings are described under appropriate methods subheadings, above."
10.1038/s41597-024-03273-5,"The programs used to generate the datasets and all the results were ESRI ArcGIS (10.6), Python (3.7 or 3.8) and Google Earth Engine (GEE). The scripts utilized for ChinaSoybean10 described in this paper can be accessed at https://github.com/ZihangLou/ChinaSoybean10 ."
10.1038/s41597-024-03245-9,No code was developed for this work.
10.1038/s41597-024-03266-4,No novel code was used in the construction of the PSFHS dataset.
10.1038/s41597-024-03278-0,The command and pipelines used for data analyses in this study were executed according to corresponding protocols of bioinformatics software. No custom programming or coding was used. The version and parameters have been mentioned in Methods.
10.1038/s41597-024-03294-0,The code used to reproduce these analyses are available on GitHub https://github.com/wlchin/scientific_data_ab1_renca as snakemake 14 workflows.
10.1038/s41597-024-03231-1,"In this study, no custom code was utilized. All software used in this study is open access. A comprehensive list of software used in this study is provided in this section as well. • PatternLab for Proteomics V ( http://patternlabforproteomics.org/ ). • DiagnoMass ( https://www.diagnomass.com/ ). • RawVegetable ( http://patternlabforproteomics.org/rawvegetable/ )."
10.1038/s41597-024-03298-w,Chlorine and lead data were processed in Excel. The test case for the suggested data correction approach was completed in Excel. The code for the suggested data clamping approach was written in R and is available in the Figshare repository.
10.1038/s41597-024-03303-2,The code 27 used to produce the MICA project dataset is available on GitHub at https://github.com/ICPSR/mica-data-descriptor and through Zenodo with the identifier https://doi.org/10.5281/zenodo.8432666 . Data manipulation and pre-processing were performed in Python. Data curation for distribution was performed in SPSS.
10.1038/s41597-024-03271-7,"The codes to process the data and generate the figures, and the details of selected flux sites are available at https://github.com/kunlz/Codes.longterm.SiTHv2.product . The model codes of SiTHv2 are available at https://github.com/kunlz/SiTHv2 ."
10.1038/s41597-024-03115-4,No custom code was used.
10.1038/s41597-024-03293-1,"All commands and pipelines used were performed according to the manuals or protocols of the tools used in this study. The software and tools used are publicly accessible, with the version and parameters specified in the Methods section. If no detailed parameters were mentioned, default parameters were used. No custom code was used in this study."
10.1038/s41597-024-03281-5,Both the code and dataset presented herein are fully accessible on our GitHub repository at https://github.com/kieucq/tcg-binary-dataset . All Python codes follow the standard GNU Open Source Licence.
10.1038/s41597-024-03258-4,"The AR detection algorithm, tARget version 4 70 , is available via the Global Atmospheric Rivers Dataverse ( https://dataverse.ucla.edu/dataverse/ar )."
10.1038/s41597-024-03282-4,The TPRR setup is available through the Weather Research and Forecasting (WRF) model. WRF is provided through a git repository ( https://github.com/wrf-model/WRF/tags ) available at the model’s website ( https://www2.mmm.ucar.edu/wrf/users ). The users can download the source code of WRF model in the git repository anonymously. The Python scripts used in this study for data post processing can be available through the following git repository: https://github.com/PayphoneChoou/TPRR_CODE .
10.1038/s41597-024-03309-w,"No specifc code was used in this study. The data analyses adhered to the manuals and protocols offered by the creators of the corresponding bioinformatics tools, the parameter settings of which were outlined in the methods section."
10.1038/s41597-024-03290-4,"Codes for the dataset pre-processing are written using python, including TIFF read, morphological opening-and-closing operation, TIFF write and mosaic process. The codes are available at: https://github.com/Siyu1993/WaterPreprocessing . Then the image could be visualized in QGIS software (V3.16)."
10.1038/s41597-024-03300-5,"All software with their specific version used for data processing are clearly described in the methods section. If no specific variable or parameters are mentioned for a software, the default parameters were used."
10.1038/s41597-024-03297-x,"All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in the Methods section. Metabolic network reconstructions were carried out using pathway tools 27.0 (April 12, 2023), with annual updates planned."
10.1038/s41597-024-03317-w,"The entire process, including the development of the dataset 35 and the conduction of experiments, was implemented using the Python programming language. The complete code and dataset are hosted on GitHub at: https://github.com/hrouhizadeh/BioWiC ."
10.1038/s41597-024-03289-x,All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in Methods section. No custom code was used.
10.1038/s41597-024-03264-6,All bioinformatic tools were executed following their respective protocols and manuals. The software version used was described in Methods. Below is detailed parameter information about some bioinformatics tools. Genome size estimation jellyfish count -C -m 21 -s 50000000000 -t 32 reads_R*.fq -o reads.jf jellyfish histo -t 32 reads.jf >reads.histo genomescope.R -i reads.histo -o output_dir -k 21 Genome assembly hifiasm -o hass --primary -t 48 --h1 hic_read1.fq.gz --h2 hic_read2.fq.gz \ --ul ont.reads.fq.gz hifi_reads.fastq.gz 2 > asm.log yak count -k31 -b37 -t16 -o pat.yak paternal.fq.gz yak count -k31 -b37 -t16 -o mat.yak maternal.fq.gz hifiasm -o hass -t 48 -1 pat.yak -2 mat.yak /dev/null 2 > asm.trio.log Purge haplotigs minimap2 -t 48 -ax map-hifi hass.p_ctg.fa hifi_reads.fastq.gz --secondary = no | samtools sort -@ 48 -m 1 G -o hifi_read.aln.bam -T tmp.align purge_haplotigs hist -b hifi_read.aln.bam -g hass.p_ctg.fa -t 48 purge_haplotigs cov -i hifi_read.aln.bam.gencov -l 15 -m 68 -h 140 purge_haplotigs purge -g hass.p_ctg.fa -c coverage_stats.csv -t 48 Genome sequences correction yak count -t 48 -k 21 -b 37 -o k21.yak femal.illumina.reads.gz yak count -t 48 -k 31 -b 37 -o k31.yak femal.illumina.reads.gz nextPolish2 -t 48 -o curated.np2.fasta hifi_read.aln.bam curated.fasta k21.yak k32.yak Hi-C data analysis juicer.sh -s DpnII -g hass -z curated.np2.fasta -t 60 -p chrom.sizes Busco analysis busco -m genome -i genome.fasta -l insecta_odb10 -o busco_out --cpu 45 –offline HiFi reads mapping minimap2 -t 48 -ax map-hifi genome.fasta hifi_reads.fastq.gz > hifi_read.aln.sam Transcript assembling hisat2 -p 48 -q -x genome.index -1 $j.1.fq.gz -2 $j.2.fq.gz -S $j.sam samtools view -bS -@ 10 -o $j.bam $j.sam samtools sort -@ 10 -o $j.sorted.bam $j.bam stringtie $j.sorted.bam -p 16 -o $j.gtf ls *.gtf > gtf.list taco_run -p 16 gtf.list Repeat annotation EDTA.pl --genome genome.fa --cds transcript.cds --sensitive 1 --threads 45 --anno 1 --overwrite 1 --species others --force 1 RepeatMasker -lib repeat.lib -pa 48 -html -xsmall -gff genome.fa > repeatmasker.log Gene prediction braker.pl --species = hass I am running a few minutes late; my previous meeting is running over. --genome = genome.fa.mod.MAKER.masked I am running a few minutes late; my previous meeting is running over. --bam rna.aln.bam \ --prot_seq = Arthropoda.10.pep.fa \ --gff3 --threads = 48 --workingdir = braker3_out --min_contig = 10000 --overwrite --addUTR = on Genome annotation emapper.py -i pep.fa -o pep.fa --itype proteins --cpu 32 --excel --evalue 1.0e-5 pfam_scan.pl -fasta pep.fa -dir PfamScan/data/35.0 -outfile pfam_out.tbl -e_seq1.0e-5 -e_dom 1.0e-5 -cpu 8 blastp -query pep.fa -db tremble_invertebrates -evalue 1.0e-5 -num_threads 16 -out blastp.tremble.out -max_target_seqs. 1 -outfmt 6 -subject_besthit
10.1038/s41597-024-03261-9,"We provide a development kit programmed with Python language for this dataset, which contains scripts for visualizing and parsing the dataset. The toolkit is available at the code repository 38 ( https://github.com/ztsrxh/RSRD_dev_toolkit ). The projection.py provides functions for reading calibration parameters, reading disparity and depth maps, projecting points onto images and pixels onto points, as well as their visualization. The read_imu_rtk.py shows an example that parses the motion information and convert them into relative location under ENU coordinate. The data_reader.py implements the Dataloader in PyTorch that provides training samples. The cam_extrinsic.py implements the calculation of camera extrinsic parameter between two time clocks. The extrinsic is presented as the translation and rotation matrices from the current time to the origin. The code has MIT license for unrestricted usage."
10.1038/s41597-024-03299-9,"The source code and instructions to reproduce our results are freely available at https://github.com/BioinfoMachineLearning/cryo2struct . To keep the data files of Cryo2StructData permanent, we published all data to the Harvard Dataverse ( https://dataverse.harvard.edu/dataverse/Cryo2StructData ), an online data management and sharing platform with a permanent Digital Object Identifier number for each dataset. The Cryo2StructData Dataverse comprises the Full Cryo2StructData, referred to as Cryo2StructData: Full Dataset 13 ( https://doi.org/10.7910/DVN/FCDG0W ), along with its associated trained deep transformer model and data splits, referred as Cryo2StructData: Trained Model and Data Splits (Full) 34 ( https://doi.org/10.7910/DVN/SXNYRE ). Similarly, within the Cryo2StructData Dataverse, we find the Small Subsample of the complete Cryo2StructData, denoted as Cryo2StructData: Small Subsample Dataset 31 ( https://doi.org/10.7910/DVN/CGUENL ), accompanied by its respective trained deep transformer model and data splits, recognized as Cryo2StructData: Trained Model and Data Splits (Small Subset) 35 ( https://doi.org/10.7910/DVN/DTV4JF ). Finally, the test dataset has been made available as Cryo2StructData: Test Dataset 30 ( https://doi.org/10.7910/DVN/2GSSC9 ). The metadata of Cryo2StructData is available at https://doi.org/10.7910/DVN/JMN60H ."
10.1038/s41597-024-03313-0,"Fastp: -q 10 -u 50 -y -g -Y 10 -e 20 -l 100 -b 150 -B 150 SOAP: -m 260 -x 440 Jellyfish: -h 100000 Hifiasm: l = 2, n = 3 LACHESIS: CLUSTER_MIN_RE_SITES = 31;CLUSTER_MAX_LINK_DENSITY = 2;ORDER_MIN_N_RES_IN_TRUNK = 15;ORDER_MIN_N_RES_IN_SHREDS = 15 LTRharvest: -minlenltr 100 -maxlenltr 40000 -mintsd 4 -maxtsd 6 -motif TGCA -motifmis 1 -similar 85 -vic 10 -seed 20 -seqids yes LTR_finder: -D 40000 -d 100 -L 9000 -l 50 -p 20 -C -M 0.9 Diamond alignment (Orthofinder): e ≤ 1e −3 MAFFFT: --localpair --maxiterate 1000 Gblocks: -b5 = h PAML: burnin 5000000; sampfreq. 30; nsample 10000000 DIAMOND v. 0.9.29.13: e < 1e −5 , C > 0.5 MCScanX: -m 15 Nucmer program from MUMmer v. 4.0: --maxmatch -c 500 -b 500 -l 100 -t 6 Delta-filter program from MUMmer v. 4.0: -1 -i 90 -l 500 Show-coords program from MUMmer v. 4.0: -THrd"
10.1038/s41597-024-03296-y,Python and ArcGIS are the software used to generate all the results. The code used for the downscaling calculations is available on the Figshare 41 .
10.1038/s41597-024-03286-0,Custom-designed scripts were not used to generate or process any data presented. The publicly available software was used in their default settings unless stated otherwise within the text.
10.1038/s41597-024-03288-y,No custom code was used during this study for the curation and validation of the dataset.
10.1038/s41597-024-03211-5,No custom codes or algorithms were used in the generation or processing of data in this manuscript.
10.1038/s41597-024-03292-2,R programming code used to create all visualizations in the above quality assessment and technical validation is available as R-Markdown files in the same repository as is used to share the measurement data 26 . One Rmd for the yearly Quality Assessment and one for each yearly Technical Validation is provided.
10.1038/s41597-024-03280-6,The DUC specification is available as a JSON schema accessible via the following URL: https://doi.org/10.5281/zenodo.7767323 . This specification is available under a Creative Commons Zero v1.0 Universal license. Additional instructions and software tools are hyper-referenced on this resource to support the adoption and use of DUC.
10.1038/s41597-024-03287-z,The Python and R code used to create the road networks and perform the analyses is provided together with the output data (see section “Data Records”).
10.1038/s41597-024-03279-z,The code for the web-based tool for construction of a DUC/CCE profile ( https://ducejprd.le.ac.uk ) can be found at this public repository https://github.com/Cafe-Variome/DucCCE with read only access.
10.1038/s41597-024-03272-6,No custom code was used.
10.1038/s41597-024-03318-9,The data was processed in SPSS v28. The code (SPSS syntax files) used to generate the processed versions of the datasets from the raw versions are included in the data repository entries for each dataset as described in the Data Records section above. There are no restrictions on access to these files.
10.1038/s41597-024-03340-x,"No custom scripts were utilized in this study. All commands and pipelines for data processing were carried out in compliance with the established protocols of the bioinformatics software, on a local high-performance server (PowerEdge T630, Dell Technologies)."
10.1038/s41597-024-03333-w,"ANPP was estimated using a published method by Smart et al . 11 using the BUGS model as described therein. For amplicon datasets, reads were paired, quality checked and clustered into operational taxonomic units (OTUs) using the DADA2 pipeline using default settings https://benjjneb.github.io/dada2/ . To derive a metric of the ratio of fungi to bacteria, pre-processed metagenomic reads were taxonomically annotated using the Kraken2 software https://github.com/DerrickWood/kraken2 and the PlusPFP reference database containing genomes from all domains of life https://benlangmead.github.io/aws-indexes/k2 ."
10.1038/s41597-024-03283-3,The code used to process the Sentinel-2 data with FORCE version 3.7.10 can be obtained via data_UMR https://doi.org/10.17192/FDR/166 .
10.1038/s41597-024-03334-9,"The study utilized publicly available software with clear methodological descriptions of their parameters. In cases where no specific parameters were provided, we opted to use the default parameters as suggested by the software developer."
10.1038/s41597-024-03285-1,"All the software programs used in this article ( de novo transcriptome assembly, pre- and post-assembly steps and transcriptome annotation) are listed with the version in the Methods paragraph. In case of no details on parameters the programs were used with the default settings."
10.1038/s41597-024-03301-4,"The Python script used to extract the sound samples collected from the InsectSound1000 dataset from the raw, long-term recordings is available online on the InsetSound1000 GitHub page ( https://github.com/Jelt0/InsectSound1000Tools ). The script can easily be modified for extraction, e.g., samples of different lengths, different sample rates or even different thresholds for activity detection. Furthermore, the accommodating GitHub repository contains a Jupyter Notebook to split the InsectSound1000 dataset into training, validation and test subsets while ensuring that recording dates do not overlap in the different subsets."
10.1038/s41597-024-03336-7,"All data processing including quality controls and figure generation was done using the language and environment for statistical computing R version 4.2.1 33 and the packages data.table 34 , ggplot2 35 , and dplyr 36 ."
10.1038/s41597-024-03328-7,"The generation of the GWP is conducted using multiple processing steps including data acquisition and preparation, classification, interpolation, as well as enhancement of overestimated pixels. Global MODIS data was downloaded and stored in computing environments of DLR’s Earth Observation Center. Further processing tasks have been performed in internal CPU and GPU clusters available at DLR’s Earth Observation Center using DLR proprietary software along with specialized Python (v3.8, https://www.python.org/downloads/windows/ ) and IDL (v8.0, https://www.nv5geospatialsoftware.com/Products/IDL ) scripts. Due to the utilization of proprietary tools, it is not possible to openly disclose the implemented processing pipeline to the public. The calculation of the global mosaics at different temporal scales which are available for download at the Geoservice of the DLR Earth Observation Center were carried out using GDAL (Geospatial Data Abstraction Library v.3.6, https://gdal.org/index.html ). The corresponding scripts are available at https://download.geoservice.dlr.de/GWP/files/code/ ."
10.1038/s41597-024-03332-x,"No unique codes were used in the compilation or processing of this dataset. When applicable, the software versions and any deviations from default settings are explicitly indicated."
10.1038/s41597-024-03306-z,The code used for processing and evaluating each of the maps in this study is publicly accessible at https://github.com/nasaharvest/crop-mask/blob/master/src/compare_covermaps.py .
10.1038/s41597-024-03337-6,A script to download and resample the data is available on GitHub ( https://github.com/UMEssen/saros-dataset ).
10.1038/s41597-024-03322-z,"In this study, all analyses were conducted following the manuals and tutorials of software and pipeline. The detailed software versions are specified in the methods section. Unless specified otherwise, default or author-recommended parameters were used for software and analysis pipeline. Detailed information about the parameters and custom scripts utilized in this research can be obtained by downloading them from https://github.com/life404/genome-NycAvi.git ."
10.1038/s41597-024-03321-0,"Write data analysis code using Python and install packages such as Nltk, Numpy, and Pandas to assist. The code runs on the local computer. The data annotation in this paper uses the Brat tool (version: Brat-1.3p1), running on a Linux system. The code is mainly used for CS-A corpus generation and the quality analysis of the corpus. The code has been uploaded to the GitHub repository and is accessible using the following link: https://github.com/Xiduoduosci/CS_A_corpus ."
10.1038/s41597-024-03185-4,Not applicable.
10.1038/s41597-024-03331-y,The ontology code of HOIP is available at the NCBO BioPortal ontology repository site ( https://bioportal.bioontology.org/ontologies/HOIP ). The license code was under Creative Commons 4.0. HOIP is also available on the GitHub website at https://github.com/yuki-yamagata/hoip .
10.1038/s41597-024-03311-2,"The software code to run the entire experimental campaign can be downloaded from the codes folder of our open software repository 47 . Each subfolder corresponds to a specific activity of the dataset creation procedure reported in this paper. The ROSPaCe proposed versions are available in a public repository and can be freely downloaded. The repository contains a folder for each of the datasets, and an additional CSV file that enlists the features included."
10.1038/s41597-024-03217-z,Codes used for data annotation are available through GitHub: ( https://github.com/kwahid/Weekly_DWI_Data_Descriptor ).
10.1038/s41597-024-03316-x,The code used for analysis in this study is publicly available at: https://github.com/chenzhiheng970717/SD_code.git .
10.1038/s41597-024-03338-5,The authors declare that no custom code was used in this study.
10.1038/s41597-024-03329-6,All code used to produce the analyses in this paper is available via an open repository: https://github.com/aberke/amazon-study . The repository also includes the survey instruments and custom software used in the data collection process.
10.1038/s41597-024-03327-8,"Python scripts and IPython notebooks can be accessed at Zenodo ( https://doi.org/10.5281/zenodo.10023059 ) 26 . These files also include scripts, which were used to generate the data in Table 1 and to create Figs. 1 , 2 . They are stored along with data files in the pub.zip archive and listed in detail in the Methods section and the README.md file. The coloram.py script is a PyMOL plugin for coloring structures in PyMOL."
10.1038/s41597-024-03310-3,We used Python to write programmes to process and validate the dataset. Here are some of the programmes we used: • Check.py: This programme is used to validate and check the accuracy of the dataset. • ShowWave.py: This file is used to view the waveform of the current when the appliance is started. • ShowWaveFFT.py: This file is used to perform a Fast Fourier Transform on the current and voltage waveforms. • LWIP_NILMF417IGT_MakeFile_CRC_new: This folder contains the embedded programme used in the data acquisition section. • Upper_computer_monitoring_system.mp4: This file is the video of the monitoring programme of the upper computer during the aggregation data collection. These programmes are helpful in efficiently processing and validating the datasets to ensure the correctness and usability of the data. The source code of this programme has been posted on https://github.com/TagEnd/TDHA-Acquisition-System-Submit and the TDHA dataset has been posted both on the Science Databank https://www.scidb.cn/en/detail?dataSetId=876623ff38634ccb8426b07146720914&version=V2 and on our custom platform http://f-lab.ncepu.edu.cn/TDHA .
10.1038/s41597-024-03302-3,"The code is available on GitHub ( https://github.com/Wenxiu0902/Ozone_prediction ) primarily using Python and R languages. It includes data preprocessing, model training, testing, prediction, and visualization sections. Additionally, sample model input data is also provided."
10.1038/s41597-024-03295-z,"All software used for pre-processing, de-identification, and data conversion of the NRRD images are based on publicly available tools. Specifically, the de-facing algorithm is publicly available at https://github.com/ReubenDo/pydeface-niftyreg and based on the publicly available tools SimpleITK 29 and NiftyReg 24 . Data conversion was performed using publicly available software tools: 3D Slicer 25 , dicom3tools 26 , and DCMqi 27 . Moreover, we provide scripts to easily convert the dataset from DICOM format into NIfTI or NRRD format, to ensure that our data can be utilized with a wide range of tools. All the scripts for processing and converting the data are available at https://github.com/ReubenDo/ReMIND/ ."
10.1038/s41597-024-03326-9,"Source code for the components of the DS-PACK assembly can be found in the following repositories. Among these, ELIXIR Luxembourg is the main developer of DAISY, Data Catalog and ADA systems and the Keycloak DAISY synch plugin. • Data Information System DAISY, GNU Affero General Public License (AGPL 3.0), https://github.com/elixir-luxembourg/daisy • Data Catalog, GNU Affero General Public License (AGPL 3.0) https://github.com/FAIRplus/imi-data-catalogue • Keycloak, Apache 2.0 License, https://github.com/keycloak/keycloak • Keycloak - DAISY synch plugin, Apache 2.0 License, https://gitlab.lcsb.uni.lu/keycloak-projects/keycloak-providers • Resource Entitlement Management System (REMS), MIT License, https://github.com/CSCfi/rems • ADA Discovery Analytics (ADA), Apache 2.0 License, https://github.com/ada-discovery The source code to connect DS-PACK components is located within each component. We are working on a containerised demo of DS-PACK execution, which will be made available under the ELIXIR Luxembourg GitHub repository. https://github.com/elixir-luxembourg/"
10.1038/s41597-024-03312-1,We make available functions for users to use our datasets: • Loading c3d files and the hSMAL model with the captured parameters to visualize the mocap data and the fitted results. • Projecting the reconstructed model in image planes with provided camera information. • Quantitative evaluation using the mocap data and silhouette subsets. Further detail about environment settings and code usage can be found in https://github.com/Celiali/PFERD.git .
10.1038/s41597-024-03356-3,The code for stimulus artifact removal and power spectrum analysis is available on G-Node. ( https://doi.org/10.12751/g-node.lzvqb5 ) 55 .
10.1038/s41597-024-03350-9,"In line with the scientific data principles of findability, accessibility, Interoperability, and reusability 27 , the tools used throughout the generation of these data are publicly available. Specifically, we used the FeTS toolkit [FeTS] to perform all pre-processing steps, including co-registration, and skull stripping, which is publicly available at ( https://fets-ai.github.io/Front-End/ ) 17 . The nnU-Net model as used for initial pre-automated segmentation is publicly available at ( https://github.com/ecalabr/nnUNet_models )."
10.1038/s41597-024-03339-4,"The python code to create the DLECZ, classify the settlement types and create built-up area density maps from building vector data in order to validate the results, is available at https://doi.org/10.6084/m9.figshare.c.6839949 77 ."
10.1038/s41597-024-03355-4,Data analysis procedures have been described in detail in the Methods section. No custom code was used during this study for the curation and/or validation of the dataset.
10.1038/s41597-024-03361-6,"All sofware and pipelines used in this study were performed with the parameters described in the Methods section. If no detail parameters were mentioned for the sofware, default parameters were used as suggested by developer."
10.1038/s41597-024-03365-2,All data is provided as human readable ASCII text in CSV format and can be browsed and processed without proprietary code.
10.1038/s41597-024-03369-y,There was no code used for the generation of the datasets.
10.1038/s41597-024-03324-x,The code for the JSON-LD metadata generator and the NSDRA reusability assessment are available online in GitHub ( https://github.com/NSDRA ) and archived on Zenodo with the following DOIs: NSDRA Metadata Generator Web Application: https://doi.org/10.5281/zenodo.10886178 NSDRA Reusability Assessment Web Application: https://doi.org/10.5281/zenodo.10886180 .
10.1038/s41597-024-03325-w,"The presented dataset was processed based on CloudnetPy version 1.39.0. However, to account for the Arctic clouds, modifications such as merging the observational data and the LLS processing needed to be done. The adjusted Cloudnet source code is archived via Zenodo 117 ."
10.1038/s41597-024-03371-4,The Python scripts and Jupyter notebooks supporting the conclusions of this study can be accessed and downloaded via GitHub ( https://github.com/Fraunhofer-ITMP/patent-clinical-candidate-characteristics ). The repository is structured into “Data” and “Notebook” sections. The “Data” section is the exact replica of the Zenodo dump mentioned previously. The “Notebook” section includes all the analyses presented in this study organized based on the sections within the results.
10.1038/s41597-024-03219-x,"The sample selection process used ISOSCELES, a program written in Python 2.7 using the open source packages GDAL, OGR, SciPy, Numpy, Sci-kit Learn, and Pandas. It is available at https://github.com/btswan87/isosceles . Main geospatial data operations and manipulations use open packages, including Python, dask, sqalchemy2, geopandas, pandas, SciPy, Sci-kit Learn, psycopg2-binary, sqlalchemy, postgres, GDAL, OGR, DBeaver, and PostgreSQL. Regularization was performed using ArcPy. Database is a Docker image from CrunchyData with Postgres 14.2 and PostGIS 3.1."
10.1038/s41597-024-03345-6,"To assist users in accessing this dataset, we launch a dedicated repository. The code is available on Github ( https://github.com/carlosalexnflve/lndb_medicalreports2annotation.git ). This repository serves as a central hub for accessing the code and information related to the dataset. It offers users a convenient way to understand the process of matching lung nodules between medical reports and manual image annotation."
10.1038/s41597-024-03354-5,"The code is openly available under Obfuscate.ipynb: https://doi.org/10.6084/m9.figshare.25441735.v1 . We use the following software libraries: Docker 53 , DGGRID 43 , PostgreSQL 54 , PostGIS 55 , PostgREST 56 (for the API). We supply a working prototype through obfuscate.geoinsight.ai."
10.1038/s41597-024-03358-1,"The code associated with this study is available in our Github repository: https://github.com/ngaggion/CheXmask-Database . The repository encompasses Python 3 code for various components, including data preparation, data post-processing, technical validation, and the deep learning models. The data preparation section includes scripts for preprocessing the images. The data post-processing section provides scripts for converting the segmentation masks from run-length encoding to a binary mask format, examples of how to read the model and the necessary code to revert the pre-processing steps for each dataset. The technical validation section includes the code for the individual RCA analysis and the processing of the physician results. Additionally, the repository includes the code for the deep learning models used for image segmentation, including the HybridGNet model architecture, weights, training and inference scripts. The software prerequisites for running the code are outlined in the repository’s README file."
10.1038/s41597-024-03256-6,Records in the dataset do not require additional code for subsequent analysis.
10.1038/s41597-024-03320-1,"The codes used to produce the integrated dataset from SAR, vegetation and soil data have been uploaded to figshare along with it 15 . These contain a number of tools that can be easily adapted and reused to use the BELSAR data for other purposes. To rebuild the integrated dataset from the BELSAR-Campaign data, the contents of the ESA repository must first be downloaded. Then, running the following python command: python integrated_dataset.py path_output path_radardata path_insitu where path_output , path_radardata , and path_insitu are a user-defined path to a directory where output files will be written and the paths to RadarData and Insitu , respectively, will extract zonal statistics from the SAR data and match them to the corresponding in situ vegetation and soil measurements andto generate the integrated dataset. For test purposes, this scripts can be run for one or a subset of SAR acquisitions by adding their indices, from 0 to 319, as arguments to the python command, e.g., python integrated_dataset.py path_output path_radardata path_insitu 319 for the last acquisition and python integrated_dataset.py path_output path_radardata path_insitu 317 319 for the last three acquisitions."
10.1038/s41597-024-03330-z,"The code for training MOrgAna and the SegFormer is publicly available on GitHub: https://github.com/LabTrivedi/MOrgAna and 19 . The data splits for MOrgAna and SegFormer training and evaluation, the configuration files for SegFormer training, the CellProfiler project as well as the workflow for the Technical Validation are publicly available on GitHub and co-deposited on Zenodo 22 ."
10.1038/s41597-024-03323-y,"Matlab computer software used to read the data and produce the figures from the netCDF files, together with a Python script to check the required content of an ATOMIX netCDF file is available from the ATOMIX Shear Probes GitHub repository ( https://github.com/SCOR-ATOMIX/ShearProbes_BenchmarkDescriptor ). The present paper is based on version 1.0, available from Fer et al . 14 , at https://doi.org/10.5281/zenodo.10610150 ."
10.1038/s41597-024-03344-7,"The extraction of collected data, annotation of activities, conversion of collected data into formatted data record files, and analysis of sensor data utilize Python 3.7 and a variety of Python libraries. The codes are available on our lab’s GitHub site ( https://github.com/cdsnlab/ScientificData ). The DOO-RE dataset files and outcomes of data collection are hosted on the public repositories, accessible at figshare ( https://doi.org/10.6084/m9.figshare.24558619 ) 31 and Zenodo ( https://zenodo.org/records/7763477 ), with further inquiries welcome via contacting the corresponding authors. For optimal viewing, it is advised to open DOO-RE’s sensor data files, which are in CSV format, using Excel. JSON format metadata files can be easily viewed or modified using any text editor. Our MQTT-based IoT system (JAVA-based), named Lapras, gathers data from ambient sensors, actuators, and a camera. This system archives the raw data in a MongoDB database and uses Python to preprocess and extract sensor data for the creation of DOO-RE. Information and code for this collected system can also be requested through the lab’s website site or the corresponding author. We are open to assisting and providing information about DOO-RE, with the exception of requests for privacy-sensitive content, such as video recordings."
10.1038/s41597-024-03183-6,"Tables 8 , 9 detail all the software and versions used in this study for transcriptomics and metagenomics, respectively. Unless specific parameter details are provided, the programs were utilised with their default parameters."
10.1038/s41597-024-03372-3,The self-written scripts for sequencing processing and visualization has been uploaded to Github: https://github.com/zzl2516/Leafminer/tree/main .
10.1038/s41597-024-03360-7,No coding has been applied.
10.1038/s41597-024-03375-0,"The article includes a list of software programs, such as de novo transcriptome assembly, pre- and post-assembly procedures, and transcriptome annotation, all of which are specified alongside their respective versions within the Methods section. If specific parameter details are not provided, the programs were used with their default settings."
10.1038/s41597-024-03363-4,"No specific custom codes were developed in this study. Data analyses were conducted following the guidelines and protocols provided by the developers of the respective bioinformatics tools, as detailed in the methods section."
10.1038/s41597-024-03346-5,No custom code was generated for this work.
10.1038/s41597-024-03376-z,No custom code was used for this study. All sofware and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and code/parameters of sofware have been detailed described in Methods.
10.1038/s41597-024-03349-2,"The Metadata Workbook and related content is freely available on Zenodo 41 ( https://zenodo.org/records/10278069 ) and GitHub ( https://github.com/LeaSeep/MetaDataFormat ). The repository contains the macro-embedded Metadata Workbook, the isolated VBA scripts, the macro-embedded Merge Workbook, as well as the converter to turn a Metadatasheet to a SummarizedExperiment Object. The repository includes a pre-commit hook that extracts the associated VBA scripts automatically, facilitating easy evaluation of code changes directly within GitHub."
10.1038/s41597-024-03364-3,The Python codes for generating and processing the daily gap-free NDVI data in China can be accessed through GitHub ( https://github.com/mainearth/Daily-Gap-free-NDVI-Code.git ).
10.1038/s41597-024-03367-0,The code data utilized for computations and analyses within this paper is accessible via the GitHub repository at ( https://github.com/YNAU-WYH/Agricultural-policy-dataset.git ).
10.1038/s41597-024-03348-3,"The scripts for constructing the inputs to the dataset and generating the dataset shapefile, data outputs, and figures are in https://github.com/openmodels/spawning-dataset . This repository also contains the necessary input files, intermediate outputs, and final outputs. Instructions for reproducing the dataset are provided in the README.md file."
10.1038/s41597-024-03319-8,The source codes to generate these datasets from CORDEX climate data can be found at: https://zenodo.org/record/7300024#.ZBbi4XbMI2x 95 .
10.1038/s41597-024-03314-z,The code for doing the computations described in this paper is available at the UNC dataverse sites given in the “Data Records” section above.
10.1038/s41597-024-03343-8,"The entire workflow is available as dataset along with this article, and can also freely be accessed on zenodo ( http://www.zenodo.org ) 29 as well as the PANABC web page ( http://www.panabc.info ), without restrictions. Table X of the dataset provides legends with an overview of the content. No custom code was used during this study for the curation and/or validation of the dataset."
10.1038/s41597-024-03225-z,"All code used for data extraction, processing, visualization, and technical validation is available as SQL queries (Google’s Bigquery syntax) and Jupyter notebooks in the corresponding PhysioNet page and on GitHub. https://github.com/joamats/pulse-ox-dataset 1.    The publicly-available scripts are structured as follows: 1. The folders MIMIC-III, MIMIC-IV, and eICU-CRD contain the SQL queries to fetch the data, alongside auxiliary tables that need to be created first in a user’s BigQuery environment. 2. The source folder contains the Jupyter notebook (1_dataset.ipynb) to create the dataset, which is calling the main SQL scripts needed to create the final CSV file. It also contains the notebooks 2_CONSORT_diagram.ipynb , 3_tableones.ipynb , 4_technical_validation.ipynb, and 5_missingness.ipynb to recreate all the analyses. 2.    The source folder contains the Jupyter notebook (1_dataset.ipynb) to create the dataset, which is calling the main SQL scripts needed to create the final CSV file. It also contains the notebooks 2_CONSORT_diagram.ipynb , 3_tableones.ipynb , 4_technical_validation.ipynb, and 5_missingness.ipynb to recreate all the analyses."
10.1038/s41597-024-03304-1,"Python code running the Weighted Climate Dataset dashboard and scripts for aggregating data are available at https://github.com/CoMoS-SA/climaterepo . The Weighted Climate Dataset leverages Streamlit . We employed R 35 to process the data, exploiting package exactextractr 36 for the weighted aggregations."
10.1038/s41597-024-03379-w,All the tools mentioned in the data analysis used in this study were publicly available and the sources and versions of the analytical programs and codes were indicated in the Materials and Methods. No custom code or pipelines were generated in this manuscript.
10.1038/s41597-024-03277-1,No custom code was generated for this project. All software with parameters are listed in Supplementary information.
10.1038/s41597-024-03388-9,DCTPep can be freely accessed at http://dctpep.cpu-bioinfor.org/ . The data are stored in the Figshare repository available at https://doi.org/10.6084/m9.figshare.25796353.v1 .
10.1038/s41597-024-03307-y,The SPECTRALPACA 27 dataset includes the Python code that facilitates loading of the spectral data. This code and its usage instructions can be found in the dataset archive in the folder named code .
10.1038/s41597-024-03368-z,The code to compile the data into the selected output formats is available in the APD GitHub repository ( https://github.com/traitecoevo/APD ).
10.1038/s41597-024-03385-y,"The creation of the dataset was entirely based on the open-source software platform, 3D Slicer, without the use of custom code."
10.1038/s41597-024-03391-0,"The data calculation was primarily performed by PyCharm Community Edition 2022.1.3, with Python 3.11 runtime environment (hosted on an Intel i5-1135G7 CPU @ 2.4 GHz, 16 GB RAM, and Windows 11 operating system). The custom code includes parameters that are integral to the dataset generation process. Specifically, the minimum stay time (“thre_sail”) is set to 1 hour, the minimum sailing time (“thre_stop”) is set to 1 hour, and the distance to the nearest port(“thre_shold”) is set to 0.2°. Researchers and interested parties can access the code through the provided, subject to the authors’ permission. The detailed codes can be found in Supplementary information."
10.1038/s41597-024-03359-0,"To facilitate the process of downloading and exploring the dataset, we have made available a set of useful Python codes on our GitHub repository at https://github.com/cosmoimd/real-colon-dataset . These scripts facilitate easy access to the data and assist in its analysis, enabling users to reproduce all the plots presented in this paper. Code for the training and testing of the polyp detection models can be found separately at https://github.com/cosmoimd/DeepLearningExamples ."
10.1038/s41597-024-03362-5,No novel code was used in the construction of ROP dataset.
10.1038/s41597-024-03380-3,The source code for database is available in GitHub at https://github.com/NavandarM/PrimBrainLnc.git . The commands for tools used for lncRNA prediction and scripts for its downstream analysis is available at https://github.com/NavandarM/lncRNA-expression-and-regulation-across-different-brain-regions-in-primates.git .
10.1038/s41597-024-03353-6,The relational codes and example mentioned in this study and a brief description (readme.md) have been uploaded in github https://github.com/benchidefeng/fNIRS-experiment-for-automated-driving-scenarios.git Or Please contact the corresponding author with any further queries regarding code availability.
10.1038/s41597-024-03125-2,No custom codes were used in this study. Microsoft Excel was used to tabulate and distribute the data.
10.1038/s41597-024-03392-z,All the code is freely accessible in https://github.com/KarryBramley/ACcoding-Dataset .
10.1038/s41597-024-03398-7,"The code for all modules is openly available on GitHub ( https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing ). All scripts were developed in Python 3.10 51 . Package openpyxl v3.1.2 was utilized to export segmented text in Excel (.xlsx) files, and egi-pynetstation v1.0.1, g3pylib v0.1.1, psychopy v2023.2.3 36 were used to implement the scripts for EGI device control, Tobii eye-tracker control, stimuli presentation respectively. In the data pre-processing scripts, MNE v1.6.0 38 , pybv v0.7.5 52 , pyprep v0.4.3 39 , mne-iclabel v0.5.1 40 were used to implement the pre-processing pipeline, while mne-bids v0.14 42 , 43 was used to organize the data into BIDS 41 , 42 format. The text embeddings were calculated using Hugging Face transformers v4.36.2 50 . For more details about code usage, please refer to the GitHub repository."
10.1038/s41597-024-03394-x,All iPTMs code used to renalyse FragPipe and produce included figures is available at github.com/SdelciLab/iPTMs.
10.1038/s41597-024-03405-x,"There is no custom code was used during this study.The public softwares used in this work, were cited in the Methods section. If no detail parameters were mentioned for a software, default parameters were applied with the guidance."
10.1038/s41597-024-03351-8,"The BCPD++ open-source codes could be accessed at the GitHub repository 36 , and the SOS relaxation algorithm to check and repair the hexahedral meshes were available at the GitHub repository 70 . The repository for the online platform 71 can be accessed at Github repository 72 ."
10.1038/s41597-024-03386-x,"We provide a sample codebase that can be used to import and structure the raw data from the SoccerMon dataset, as a public software repository on GitHub: http://www.github.com/simula/soccermon ."
10.1038/s41597-024-03417-7,Programs used in data processing were executed with the default parameters except where otherwise specified in the Methods. No custom code was used in these analyses.
10.1038/s41597-024-03408-8,No code was used in this study.
10.1038/s41597-024-03399-6,MISSING
10.1038/s41597-024-03413-x,There was no original code in this work.
10.1038/s41597-024-03412-y,"All commands and pipelines employed in data processing were executed in strict accordance with the manuals and protocols of the respective bioinformatics software. In cases where detailed parameters were not explicitly provided, default parameters were applied. The version of each software utilized has been explicitly specified in the Methods section. No custom programming or coding was employed in the analysis."
10.1038/s41597-024-03229-9,No specific script was used in this work. The codes and pipelines used in data processing were all executed according to the manual and protocols of the corresponding bioinformatic tools (Table S1 ). The software versions are described in the Methods section.
10.1038/s41597-024-03378-x,We have documented within the Data Descriptor the Pseudocodes that support the methodology of this study. Codes used for inferencing results along with XGBoost model generated in this study are hosted at Zenodo ( https://doi.org/10.5281/zenodo.11085013 ).
10.1038/s41597-024-03420-y,A custom Python code used to read data is freely available on the dataset (Python Folder). All processing code used by Vicon are available for free on Vicon website ( https://www.vicon.com/ ).
10.1038/s41597-024-03397-8,"The described dataset was manually organised according to BIDS (v1.8.0). DICOM files were converted to NIFTI format using MRIcroGL dcm2niix software (NITRC, v1.0.20220720). Masks and annotations were prepared using 3DSlicer software (v5.2.1). Python (Spyder, v5.4.3) was used to order scans by imaging modality and scanning dates, and for the stripping of unnecessary NIFTI slices in some patients where portions of the skull not related to the Circle of Willis were also imaged. Code used for these purposes is not published as it is not required for generation or processing of the published dataset."
10.1038/s41597-024-03410-0,"The data, the bioinformatics pipeline, and the R-pipeline that performs the technical validation are available in Zenodo 37 ."
10.1038/s41597-024-03384-z,"The code used to prepare data is available on Github, in the ‘prepare_files’ folder and the code to format the folders in ‘model’ > ’data’ > ’ForestNetDataset’: https://github.com/aedebus/Cam-ForestNet . The folders are organised to be ready-to-use with our classification model, Cam-ForestNet, by simply unzipping the relevant ‘my_examples’ folder in ‘model’ > ’data’ > ’ForestNetDataset’. QGIS 3.24.0, Python 3.8./3.5., and the Google Earth Engine Python API were used. Ubuntu 20.04.3 LTS was used to run the code."
10.1038/s41597-024-03181-8,"For all users interested to work themselves with the Orbis data and verify or expand on our results, we distribute the full replication package. The codes which begin with raw Orbis data and arrive at GBDD are available at https://github.com/FAME-GRAPE/GDDB . There are no restrictions on access. The replication codes are accompanied with documentation detailing each step. The codes are distributed as STATA 17 project, do-files and datasets."
10.1038/s41597-024-03335-8,The digitized SJW (in National Institute of Korean History at https://sjw.history.go.kr/main.do ) and ISN (in Kyujanggak Institute for Korean Studies at https://kyudb.snu.ac.kr/search/search.do ) are openly available as image and text (in Korean and Chinese) files. The precipitation data reconstructed in this study is available at https://doi.org/10.5281/zenodo.11076768 .
10.1038/s41597-024-03131-4,The DPCfam pipeline source code is available for download at https://gitlab.com/area7/DPCfam/dpcfam . The repository includes usage notes and example datasets to test the algorithm.
10.1038/s41597-024-03291-3,"Matlab code for analysis of annual cumulative footprint climatology, land cover type, semivariogram, window size, and for plotting are provided at https://github.com/CodesRes/Representativeness-of-flux-tower-in-FLUXNET2015 ."
10.1038/s41597-024-03381-2,"No custom code was employed in the research presented in this publication. UrbanOccupationsOETR _1840s_Ottoman_Bursa_pop_micro_dataset is freely available via UrbanOccupationsOETR public dataset website ( https://urbanoccupations.ku.edu.tr/public-datasets/ ) and Zenodo ( https://zenodo.org/records/11124537 ), and no password is required for download or utilization."
10.1038/s41597-024-03423-9,No specific code was used in this study. The data analyses used standard bioinformatic tools specified in the methods.
10.1038/s41597-024-03442-6,"All pipeline and software used in this study were performed to data analysis according to the manuals and protocols. The parameters and the version of the software are described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used."
10.1038/s41597-024-03438-2,No specific code or script were used in this study.
10.1038/s41597-024-03400-2,The code with the ensemble bias correction method using the quantile mapping approach is available at https://github.com/lizanafj/ensemble-bias-correction .
10.1038/s41597-024-03424-8,All scripts and pipelines used for the genome assembly and gene annotation were performed according to their manuals and protocols of the applied bioinformatics software. No specific code has been developed for this study.
10.1038/s41597-024-03439-1,"No specific codes or scripts were used in this study. All software used is in the public domain, with parameters clearly described in the Methods section."
10.1038/s41597-024-03419-5,"The data in this paper was organized based on Microsoft Excel 2016 and the original data record format (.xlsx) was provided, which will make it easy for the user to import into any of the data analysis software such as Matlab, Python, and SPSS. No custom code was used in this study to organize or validate the dataset."
10.1038/s41597-024-03435-5,No special code was used for analysis of the current dataset. All of the analyses were done with the following open programs: SCOPE-tools ( https://github.com/SingleronBio/SCOPE-tools/blob/0.1.0/scopetools/barcode.py ). STAR v2.6.1b ( https://github.com/alexdobin/STAR ). Seurat v3.1.2 ( https://github.com/satijalab/seurat ).
10.1038/s41597-024-03416-8,"All code for data extraction, database construction, usage, and validation is available at https://github.com/cns-iu/hra-literature . Code is licensed under the MIT License and data under the Creative Commons Attribution 4.0 International License."
10.1038/s41597-024-03411-z,"Code, dataset and some intermediate results are freely available on the following GitHub repository: https://github.com/HUANGZHIHAO1994/GCCMPD-climate-policy-dataset ."
10.1038/s41597-024-03402-0,Custom code was not used in this study.
10.1038/s41597-024-03415-9,All code for calculating percent change and all analyses included in this manuscript can be found at GitHub https://github.com/mag449/AgEvidenceMethodsCode .
10.1038/s41597-024-03418-6,"We hosted the code used for the study in the Zenodo repository 22 ; the scripts provided can be used for image registration, radiomic feature extraction, and robustness analysis."
10.1038/s41597-024-03428-4,The results were generated using Python (version 3.11) and QGIS (version 3.30). Python scripts can be accessed on GitHub at the following repository: https://github.com/ZhangXG-NJU/HQTPDEM .
10.1038/s41597-024-03390-1,"Metadata concerning the stimuli presented during the BOLD fMRI runs are publicly available at https://github.com/individual-brain-charting/public_protocols . They include: (1) the behavioral protocols of the tasks; (2) the demo presentations of the tasks as video-demo presentations; (3) the instructions to the participants; and (4) the scripts to extract paradigm descriptors from log files for the GLM estimation. Regarding the behavioral protocols, Clips task consists in a reproduction of the protocol featuring the original study, only with minor adjustments and most of them concerned with experimental settings; Retinotopy and Raiders tasks were re-written from scratch in Python with no change of the design referring to the original paradigms. Regarding the demo presentations of the Retinotopy tasks, they can also be found on the YouTube channel of the IBC project: https://www.youtube.com/@individualbraincharting6314 . The scripts used for data analysis are publicly available under the Simplified BSD license: https://github.com/individual-brain-charting/public_analysis_code . The API code of the IBC data-fetcher from EBRAINS is available on a public GitHub repository at https://github.com/individual-brain-charting/api . This public repository includes also information about installation and a minimal example usage."
10.1038/s41597-024-03357-2,Scripts using R and python programming languages are provided to produce figures. Additional code and related files are available at figshare repository 39 .
10.1038/s41597-024-03431-9,"All software and pipelines were executed in accordance with the manual and protocols of the published bioinformatics tools, adhering to the specified versions and meticulously documenting the code/parameters used, as elaborated in the Methods section."
10.1038/s41597-024-03421-x,"The open access software and versions mentioned in the main text were utilized for quality control and data analysis as described in the methods. 1. The University of California Santa Cruz (UCSC) Genome Browser was used to annotate mouse genetic information 47 , 48 : http://hgdownload.soe.ucsc.edu/goldenPath /mm10/database/ 2. The R command ‘prcomp’ was utilized for inter-group comparisons of PCA in male and female telencephalons. 3. The Python command ‘seaborn.scatterplot’ was utilized for generating a Volcano plot in males and female telencephalons. 4. The Python command ‘seaborn.clustermap’ was utilized for generating hierarchical clustering and heatmap. 5. The Gene Ontology (GO) gene set from the Molecular Signatures Database was utilized 39 : https://www.gsea-msigdb.org/gsea/msigdb/ . All code and scripts used for data analysis in this study can be found in the following links and articles: http://hgdownload.soe.ucsc.edu/goldenPath /mm10/database/, https://www.r-project.org/help.html , https://www.gsea-msigdb.org/gsea/msigdb/ 39 , 47 , 48 ."
10.1038/s41597-024-03432-8,The scRNA-seq analysis scripts for this study have been uploaded to GitHub ( https://github.com/yingHH/Aaedes_midgut_scRNA-Seq ).
10.1038/s41597-024-03342-9,Data analyses were primarily conducted using standard bioinformatics tools on the Linux operating system. We provide detailed information about the versions and code parameters of the software tools used at https://github.com/WondossenA/WGS_Ethiopian_cattle/blob/main/code_explanation.md .
10.1038/s41597-024-03401-1,All software and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and code/parameters of software have been described in Methods.
10.1038/s41597-024-03393-y,"Software tools used, with versions and any parameters differing from default are described below: 1. FastQC v0.11.8 29 2. MultiQC v1.13 30 3. Trimmomatic v0.39 31 , parameters: SLIDINGWINDOW:4:20 LEADING:5 TRAILING:5 MINLEN:25 4. Trinity v2.8.6 32 , parameters: -seqType fq -min_contig_length 300 -min_kmer_cov 1 -min_glue 2 -KMER_SIZE 25 -SS_lib_type RF -CPU 8 -max_memory 80 G 5. Bowtie2 v2.4.4 68 6. Jellyfish v2.3.0 69 7. Salmon v0.14.1 36 8. Samtools v1.9 70 9. FastaStatistics 34 on Eruropean Galaxy server 71 10. TransRate v1.0.3 33 11. BUSCO v5.4.3 35 , parameters: dataset arthropoda_odb10 (Creation date: 2020-09-10, number of genomes: 90, number of BUSCOs: 1013 12. TransDecoder v5.7.0, parameters: default (open reading frame >100 amino acid) 13. Diamond v2.0.15 41 on European Galaxy server 59 , parameters: -s ggnog_swissprot_2023_03 -query-gencode ‘Standard Code’ -strand ‘both’ -comp-based-stats ‘Default mode, Hauser 2016’ -min-orf 1 -ultra-sensitive -algo ‘Double indexed (0)’ -matrix ‘BLOSUM62’-comp-based-stats ‘1’ -masking ‘Tantan’ -max-target-seqs ‘1’ -evalue ‘1e-05’ -id ‘0’ -query-cover ‘0’ -subject-cover ‘0’ -block-size ‘0.4’ -motif-masking ‘Disabled (0)’ 14. Hmmer v3.3.2 43 15. UniProt : ( http://www.uniprot.org/help/uniprotkb ), March 2023 16. TMHMM v2.0 44 , 45 , 46 17. eggNOG-mapper v2.1.12 47 , 48 , parameters: -m diamond -evalue 0.001 -score 60 -pident 40 -query_cover 20 -subject_cover 20 -itype proteins -tax_scope auto -target_orthologs all -go_evidence all -pfam_realign none -report_orthologs -decorate_gff yes -excel 18. Orthofinder v2.5.4 49 , 50 , 51 , 52 , parameters: -S diamond 19. FigTree v1.4.4 ( http://tree.bio.ed.ac.uk/software/figtree/ )"
10.1038/s41597-024-03389-8,The R code used to perform differential expression analysis is available in figshare 18 .
10.1038/s41597-024-03433-7,"The code of the CLEMD was written in MATLAB. The scripts include four files with all the technical verification. The scripts show histogram plots, daily current patterns, weekly real power patterns, current harmonics bar graphs, measured and designed circuits graph. All the scripts are available at the university’s github repository ( https://github.com/CLEMDatabase/CLEMD )."
10.1038/s41597-024-03403-z,The code used for the data preparation described herein is available on GitHub at https://github.com/innophore/structure-editing . The CavitomiX plugin for Schrodinger’s PyMOL is freely available for download at https://innophore.com/cavitomix/ and https://pymolwiki.org/index.php/CavitOmiX .
10.1038/s41597-024-03434-6,"The code used to generate the cumulative curves from the raw data is available on GitHub: https://github.com/pches/HarDWR This GitHub repository includes: waterRightAnalysis.def - A Singularity definition file used to create the run environment for the scripts described below. wrDataHarmonization.R - This script reads in the raw water rights data and performs the harmonization of the records. wrDataHarmonization_CustomFunctions.R - A supporting file for wrDataHarmonization.R which contains all the required custom written functions for the main script. wrCumulation.R - This script reads in the harmonized water rights data and performs the calculations used to create the cumulative curves. wrCumulation_CustomFunctionsR - A supporting file for wrCumulation.R which contains all the required custom written functions for the main script. checkWMA_tables.py - A script used to validate the calculated cumulative curve values. simplifyWMAs.R - The script which reads in the raw downloads of the various state’s WMAs as spatial data, harmonizes them, and merges them into two spatial layers."
10.1038/s41597-024-03284-2,All the data has been generated using the functionalities of 3D Slicer ( https://www.slicer.org/ ). The surface meshes (.stl) have been generated using a Python implementation based on TorchIO ( https://torchio.readthedocs.io/ ) and PyACVD ( https://github.com/pyvista/pyacvd ). The implementation is freely available at https://github.com/apepe91/AD_NRRD_TO_STL .
10.1038/s41597-024-03443-5,The source code of the Pan-Canadian Chemical Library website is available in the GitHub repository https://github.com/cbedart/PCCL in the “PCCL_website” section. All Python scripts used to generate the Pan-Canadian Chemical Library are to be compiled into a single Python package named the Bespoke Library Toolkit (BLT): https://github.com/cbedart/BespokeLibraryToolkit and are available upon request from MS.
10.1038/s41597-024-03425-7,The present study employed the MATLAB r2020a version (compatible with newer versions) to implement the EIVD algorithm and conducted data fusion. The code (“EIVD_example.m”) has been uploaded along with the data via figshare for public access and download 64 .
10.1038/s41597-024-03441-7,MISSING
10.1038/s41597-024-03448-0,"The MGED-KG project is available at Zenodo 49 https://zenodo.org/records/11315713 . This code is written in Python language, uses Django, Elasticsearch and other open-source libraries, and is compatible with a variety of operating systems, such as Windows, Linux. We provide detailed deployment steps and installation documentation so that users can quickly set up an environment and run code. We also maintain and update the code regularly to ensure its stability and availability."
10.1038/s41597-024-03445-3,Code used to conduct analyses for the corresponding publication analysing these data 2 is available at: https://github.com/Ewelti/EuroAquaticMacroInverts . Code to calculate site and year level biodiversity metrics is available at: https://github.com/Ewelti/EuroAquaticMacroInverts/blob/main/R/Initial_Biodiversity_FuncTrait_and_climate_calcs/TaxaDiversity_calculation.R .
10.1038/s41597-024-03437-3,"If no detailed parameters were mentioned, all software and tools in this study were used with their default parameters. No specific code or script was used in this study."
10.1038/s41597-024-03396-9,"The code used for the concatenation of scans is available as part of the XRH toolbox at https://doi.org/10.5281/zenodo.11148752 34 . Concatenation code description A high-level overview of the custom concatenation code is given below. This can be used as template to reproduce the code in any language the reader is more familiar with. Start 1. Prompt user to select the “BOTTOM” stack and store its title and bit depth. 2. Prompt user to select the “TOP” stack and store its title and bit depth. 3. Set measurements for analysis. 4. Create a dialog box to configure options. 5. Retrieve selected options from the dialog box. 6. If bit depths are different, display error message and exit. 7. If manual XY translation option is selected: a. Set the measurement tool to a point. b. Prompt the user to select a point of alignment in the “btm” stack and measure its coordinates. c. Prompt the user to select a point of alignment in the “top” stack and measure its coordinates. d. Calculate the translation values and convert them to pixel units. e. Translate the “top” stack using the calculated translation values. 8. If automatic slice selection option is selected: a. Prompt user to navigate to fusion point in “btm” stack. b. Create reference image from selected slice. c. Normalize reference image. d. Normalize each slice in “top” stack. e. Subtract reference image from “top” stack. f. Calculate standard deviation for each slice. g. Find slice with minimum standard deviation. 9. If manual slice selection option is selected: a. Prompt user to navigate to fusion point on both “top” and “bottom” volumes and retrieve the slice numbers. 10. Create duplicates of “btm” and “top” stacks by cropping btm volume between “slice 1” up to the “selected slice”, and “top” volume from “selected slice” up to “last slice” 11. Perform intensity calibration by sampling ROIs and fitting a straight line. 12. Apply intensity calibration parameters to “top” stack. 13. Concatenate cropped “btm” and cropped & calibrated “top” stacks into single stack. 14. Perform preview concatenation by creating radial reslice. End"
10.1038/s41597-024-03459-x,"All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. All software used in this work is publicly available, with versions and parameters clearly described in Methods. If no detailed parameters were mentioned for a software, the default parameters suggested by the developer were used. No custom code was used during this study for the curation and/or validation of the datasets."
10.1038/s41597-024-03451-5,The manuscript did not use custom code to generate or process the data described.
10.1038/s41597-024-03370-5,All codes are publicly available in the following GitHub repository https://github.com/Quanlab-Bioimage/301RoseDataSet (software licence: Apache version 2.0). The repository contains already training detection network and semantic segmentation network both of which can be used directly. Running these two networks can obtain the results presented in Figs. 3 – 4 . The repository also contains the python codes for retraining the networks and resetting training parameters. Most of these codes are from YOLOv5 and UNet.
10.1038/s41597-024-03452-4,The code is available on github.com/faisalalnasser13/DustSCAN and is composed of Python Jupyter Notebooks.
10.1038/s41597-024-03395-w,The code to build the main and profile datasets are provided at: https://github.com/GabrielMorin1109/MSTCP-Dataset . All scripts were coded in Python and Bash and the list of dependencies can be found in the files included in the envs folder.
10.1038/s41597-024-03341-w,"All code used to access and process the data involved in the Geo-GCDF v3 is available in the GitHub repository. The repository also contains environment files which define the Python packages and associated versions used, as well as a configuration file that defines specific parameters used. The code is publicly available and free to use with appropriate attribution (see Citing the Dataset in the Usage Notes section)."
10.1038/s41597-024-03475-x,The code and data files that we used to assess interrater reliability (see section Technical Validation) are available at the GESIS Data Archive for the Social Sciences 39 .
10.1038/s41597-024-03465-z,No custom code was used
10.1038/s41597-024-03460-4,Faunal abundance and algal surface cover datasets were obtained by using the online software BIIGLE 2.0 ( Benthic Image Indexing and Graphical Labelling Environment ) 35 .
10.1038/s41597-024-03481-z,"All the bioinformatics tools were used according to the users’ manuals. Otherwise specified in the context, default settings were used during data processing. No specific custom code has been developed in this study."
10.1038/s41597-024-03453-3,"There are no restrictions to use of the data, which are publicly available ( https://doi.org/10.5066/P9J0ZIOF ) 1 as well as the code to produce the data ( https://doi.org/10.5281/zenodo.7879199 ) 39 ."
10.1038/s41597-024-03446-2,"The generation of the GLAMOUR dataset is based on SHAFTS, Google Earth Engine, Google Cloud Storage and their Python interfaces. The snapshot of the source code used in this study has been archived on Zenodo ( https://doi.org/10.5281/zenodo.10608714 ). And the up-to-date streamlined workflow for large-scale building morphology mapping can be accessed via the GBuildingMap function in SHAFTS ( https://github.com/LllC-mmd/SHAFTS )."
10.1038/s41597-024-03382-1,The codes used to analyze the data in this study were available online ( https://doi.org/10.6084/m9.figshare.24998411.v3 ) 22 .
10.1038/s41597-024-03407-9,"Fox Insight was built by several technology partners, each with its own policies on code availability. Routine longitudinal assessments are developed through a web-based application built on Ruby on Rails® software by Mondo Robot and the code base is proprietary 28 . One-time questionnaires are deployed through Qualtrics®; while the survey platform code is proprietary, Qualtrics® provides an open-source application programming interface (API) for data processing. SQL code, developed at LONI, used to collate and process data is proprietary."
10.1038/s41597-024-03466-y,"Regression models to estimate of human capital-weights (Eqs. 1 , 2 and 3 ) were run using the GLIMMIX procedure in SAS 9.4. /* Model 1 */ proc glimmix data = data INITGLM ; title "" Fixed all ""; class sample SEXR ; model norm_sal = nbedu experience|experience SEXR / solution dist = gamma link = log ; weight perwt ; where 0 < norm_sal < 15 and 20 < = agegr < = 64 ; NLOPTIONS TECH = NRRIDG MAXITER = 100 ; run ; /* Model 2 */ proc glimmix data = data INITGLM ; title "" Random intercept ""; class sample SEXR ; model norm_sal = nbedu experience|experience SEXR / solution dist = gamma link = log ; random intercept / subject = sample ; weight perwt ; where 0 < norm_sal < 15 and 20 < = agegr < = 64 ; NLOPTIONS TECH = NRRIDG MAXITER = 100 ; run ; /* Model 3 */ proc glimmix data = data INITGLM ; title "" Random intercept with interaction ""; class sample SEXR ; model norm_sal = nbedu|nbedu_mean experience|experience SEXR / solution dist = gamma link = log ; random intercept / subject = sample ; weight perwt ; where 0 < norm_sal < 15 and 20 < = agegr < = 64 ; NLOPTIONS TECH = NRRIDG MAXITER = 100 ; run ; The code for the calculation of the Human capital weighted population estimates is available on Github ( https://github.com/gmarois/HCWP/blob/main/Code%20for%20HCWP.sas )."
10.1038/s41597-024-03454-2,"A Python3.8 version was used to perform restructuring, validation and visualisation of the data, code of which is available at the data repository folder."
10.1038/s41597-024-03315-y,"The study utilized freely available software to the public, and the parameters are explicitly outlined in the Methods section. In cases where specific parameters were not explicitly stated for the software, default settings recommended by the developers were employed. The study did not utilize custom scripts or code."
10.1038/s41597-024-03464-0,No custom code was used.
10.1038/s41597-024-03455-1,No custom code has been used. All relevant information are described in the Methods section and Usage Notes.
10.1038/s41597-024-03488-6,"The Data Records are published in Figshare: Neuman, Y., Cohen, Y. A dataset of personality utterances. figshare https://doi.org/10.6084/m9.figshare.24971943.v1 . All files are CSV files with a very simple structure. The files are (1) A list of the 100 personas (2) The file with the personality types, utterances, and accompanying data. The link also includes the code for running the SetFit model."
10.1038/s41597-024-03479-7,"Together with data, in the Repository users can find MATLAB and Python codes to start interacting with the datasets and obtain some simple plots. MATLAB code was written in MATLAB® R2022a (The MathWorks, Inc.). Python code was written using version 3.7.16. Required packages to run the provided scripts can be found in appropriate lists within the repository. We do not assure compatibility with older or newer MATLAB/python versions. We tested the execution speed of our scripts using the following hardware: a 13th Gen Intel(R) Core(TM) i7-1360P processor at 2.20 GHz, with 16.0 GB of installed RAM (15.7 GB usable). Specifically, the get_all_data_from_level_h5 function is particularly computationally intensive. On average, extracting data for a neuron (including all conditions and trials) takes 1.42 seconds using MATLAB and 0.18 seconds using Python (For example, extracting the entire MonkeyS_V6A_reach9pos.h5 dataset takes 260 seconds using MATLAB and 33 seconds using Python). Subsequent calculations and plotting, in the example scripts provided, require negligible time. The following examples derived from the MATLAB implementation, but Python functioning is identical. function [spikes, markers, all_strings] = get_all_data_from_level_h5(filename,str) % The function extracts the spikes and the markers from a h5 dataset % INPUT : % filename = a string that identifies the h5 file from which extract data % str = a string that identifies the group from which extract the data . The % function automatically extracts every dataset of the group % OUTPUT : % spikes = contains the extracted spike timing for each dataset % markers = contains the extracted marker timing for each dataset % all_strings = keeps track of the datasets extracted Example script H5_raster The script plots the neural and behavioral (event markers) data contained in an h5 file in the form of a raster plot (Fig. 6 ). Note that the User can change ‘str’ at the beginning of the script to plot: • all trials for a specific unit (‘/DATA/unit_01’, for example). • all trials for a specific unit AND condition (‘/DATA/unit_01/condition_01’, for example). • a specific trial for a specific unit AND condition (‘/DATA/unit_01/condition_01/trial_01’, for example) Example script H5_ISI The script calculates the Inter Spike Intervals (ISIs) and plots the results in a histogram. The proportion of ISIs violating a chosen threshold (normally, 1 ms) is also calculated and indicated in the histogram. Note that the User can change the variable ‘str’ at the beginning of the script to analyze all trials for a specific unit (all conditions), all trials for a specific unit and condition, or a specific trial for a specific unit AND condition combination (Fig. 7 )."
10.1038/s41597-024-03449-z,The current work did not produce any code.
10.1038/s41597-024-03383-0,"The Python code used in this paper was developed in version 3.11.0 and is available for free access. The first code, ‘pixel equivalent’, is designed to calculate the pixel values of linear scales in images. The second code, which consists of ‘aux_info_cal’ and ‘test’, is used to calculate quantitative parameters."
10.1038/s41597-024-03461-3,Te create GIHS code notebooks are available from the GitHub https://github.com/ccmch/industry .
10.1038/s41597-024-03470-2,"The equipment used for laser scanning models is from KathMatic company. The software used for point cloud data visualization is Cloudcompare_ V2.13, which is a very convenient software for processing point cloud data, can run on Windows, MacOS, and Linux systems, and is also open source. The software used for force data processing is Microsoft Excel 2016 and Origin 2018. The microscope used to photograph stylus is the AO-3M180 digital microscope. The press used for mechanical property testing is a 600 kN electro-hydraulic servo testing machine. The image is captured using a digital camera and macro lens."
10.1038/s41597-024-03491-x,"All software utilized in this study is publicly available, and their parameters are explicitly outlined in the Methods section. In instances where specific parameters for the software were not detailed, default settings, as recommended by the developers, were employed."
10.1038/s41597-024-03474-y,"The data pre-processing leading to the developement of CaRDS is based on open source Python software. Jupyter Notebooks with the code to pre-process, transform, and merge the different data sources reported in this article are available on HydroShare 21 at https://doi.org/10.4211/hs.4ec7019fe63944bf87d40d2cdfa0d686 ."
10.1038/s41597-024-03422-w,"In this article, spectral acquisition, calibration, and data export were all completed using Avantes’ Avasoft software, with the software version being 7.8. The algorithms used for data validation were implemented using Python code, and the distribution version of the code can be obtained at the following website: https://github.com/smartLybo/Spectral-data-processing.git ."
10.1038/s41597-024-03482-y,The code to perform the temporal downscaling is available on the CRAN at dynamAedes v2.2.8 and a tutorial illustrating how to apply the temporal dowscaling methodology is available in the article section of the package’s website ( https://mattmar.github.io/dynamAedes/ ). The ovitraps raw observations of a specific stakeholder are available upon request to the contact person indicated in the dataset.
10.1038/s41597-024-03366-1,"The source code employed in this study is accessible on Github at the following repository: https://github.com/mitcriticaldatacolombia/SatelliteBench . The repository contains all the scripts, methods, and essential files necessary to replicate the experiments and analyses presented in our Nature study. Additionally all the pre-trained models used to prepare this article are available in Physionet and can be downloaded without any restrictions. The full set of images used in this research is also available in Physionet under the name A Multi-Modal Satellite Imagery Dataset for Public Health Analysis in Colombia 26 . Repository Structure Overview Readme.md File: This file provides comprehensive instructions for downloading and setting up the environment required for replication. It is strongly recommended to refer to this document before initiating the replication process. Main Directories: • Preprocessing/: Contains Python codes and functions essential for executing the preprocessing steps across all experiments. These codes are fundamental for preparing the data for subsequent analyses. • Models/: Encompasses all Python code related to the creation of models and backbones utilized in the study. Understanding the contents of this directory is crucial for gaining insights into the model architectures employed. Main Files: • Train_selfsupervised.ipynb: A Jupyter notebook encompassing the entire codebase used for training the models to generate embeddings. This notebook is pivotal for understanding the training procedures applied to achieve the embeddings. • Embeddings_generation.ipynb: A Jupyter notebook designed for utilizing the trained model’s backbone to generate a CSV file containing image embeddings. This file is pivotal for comprehending the process of extracting informative embeddings from the trained model. • Time_Series_Fusion_Network_Demo.ipynb: A Jupyter notebook containing the demonstration code to execute the experiments discussed in this paper for the dengue prediction model. This code leverages the proposed time-series multimodal data fusion model and can be executed seamlessly on Google Colab for a simplified implementation. • Access_to_water.ipynb & poverty.ipynb: These Jupyter notebooks encapsulate the codebase for running experiments related to water access and poverty prediction, respectively. Understanding the content of these notebooks is crucial for reproducing and extending the analyses conducted in the study. Demo Dataset: Within the Demo_Data/ directory, researchers can access essential data for quickly testing the predictions presented in the paper. The following files are available: • Embeddings_variational_autoencoder_ResNet50V2_224_1024_12Bands.csv & Embeddings_variational_autoencoder_ResNet50V2_224_1024_3Bands.csv: These files contain embeddings of satellite images for municipalities with the most dengue cases. Extracted using a Variational Autoencoder (VAE) with a ResNet 50 V2 backbone, these embeddings utilize both RGB and all 12 band images. The CSV files include columns such as “Municipality Code” (municipality ID), “Date” (image collection date), and 1024 columns representing vector embeddings. • temperature.csv & precipitation.csv: Two CSV files providing temperature and precipitation values for the aforementioned 10 municipalities. The columns include “LastDayWeek” (date of the last day of the epidemiological week) and 10 columns indicating the respective municipality for the temperature or precipitation values. • Metadata_dengue_tabular.csv: A CSV file encompassing all metadata and labels used for predictions. The “Municipality code” column specifies the municipality from which the data originates. Note on Dataset Format: The dataset employed in this study spans 81 municipalities in Colombia from 2016 to 2018, resulting in a total of 12,636 satellite images. The metadata associated with these images encompasses static sociodemographic variables, indices of poverty, and access to education. Additionally, dynamic epidemiological and climatic metadata were collected, providing a comprehensive spatiotemporal context for the analyses. Researchers are encouraged to refer to the documentation within the repository for detailed guidance on replicating experiments and understanding the nuances of the dataset."
10.1038/s41597-024-03457-z,Code used for the dataset described here is available in Figshare 18 . Source code is archived with each month’s update per NCEI policy.
10.1038/s41597-024-03473-z,"The source code of the baseline, as well as a direct link to the Pulmonary Circulation Vessels Dataset, is available in the GitLab repository 23 . The installation of Python and Jupyter using the virtual environment is recommended, with the necessary technical instruction supplied in the “ReadMe.md” inside the repository. The dataset is publicity available at figshare 24 ."
10.1038/s41597-024-03476-w,"All code used in this paper is available at https://github.com/JLuij/ForametCeTera_scripts . This includes the script to convert a stack of .png cross-section images to an .nrrd file ( stack.py ), the script for segmenting the group-scan volumes ( segment.ipynb ) and the script for performing the technical validation ( technical_validation.ipynb ). As these scripts make use of several packages, an environment.yml file is included to reproduce the conda environment, together with more instructions in the repository’s Readme.md file."
10.1038/s41597-024-03497-5,"In this study, software tools were utilized as described in the Method section. All bash command lines and scripts are available at the GitHub repository: https://github.com/HanLab2018/Mytilus-galloprovincialis-genome-assembly ."
10.1038/s41597-024-03502-x,"All software used in this study is in the public domain and its parameters are clearly described in the methodology. If the detailed parameters of the software are not mentioned, the default parameters are used as recommended by the developer. No custom code or scripts were used for the curation and validation of the dataset."
10.1038/s41597-024-03490-y,"This dataset was created using privately available human mobility data derived through cell phone GPS locations from MPA pings in Cuebiq’s Spectus Clean Room. While access to the original data is restricted, the data product and code underlying the methods is available on Open Science Framework 55 . Python 3.6 and SQL were used to generate the data outputs."
10.1038/s41597-024-03308-x,"The Python Code used to generate the emission inventories is publicly available on GitHub ( https://github.com/lareina678/Embodied-CO2-emissions-of-equity-portfolios-for-Chinese-asset-managers.git ) 29 , and as an archive on Figshare."
10.1038/s41597-024-03440-8,Software used in the generation or processing of our data is stated in the Methods section. Detailed information including versions of software and database are provided in Table 5 .
10.1038/s41597-024-03387-w,The code used for the pre-processing of the images and the training of the different architectures seen in Fig. 5 can be found at https://github.com/imatge-upc/BCN20000 . All code is licensed under the Attribution 4.0 International (CC BY 4.0) License.
10.1038/s41597-024-03487-7,"No custom code was used for this study. All data analyses were conducted using published bioinformatics software with default settings, unless otherwise specified."
10.1038/s41597-024-03444-4,"A file containing multiple Matlab® scripts is available on figshare 49 as a separate .zip file with the suffix Other . These scripts allow simple analysis and visualisation of ground reaction forces and joint angles ( MAIN_KineticsKinematics.m ), metabolic cost and cost of transport ( MAIN_MetabolicCost.m ), and muscle activation ( MAIN_Emg.m ). The same code can also be found on the project’s GitHub website, accompanied by the instructions on how to use the code and description of each of the three main and several auxiliary scripts/functions."
10.1038/s41597-024-03483-x,The LSTM model was customized using Python with Keras-Tensorflow library ( https://github.com/leriomaggio/deep-learning-keras-tensorflow ). The customized Python and figure script can be found at https://github.com/vinhngoctran/datareconstruction .
10.1038/s41597-024-03429-3,"We used ColEmo, our open-source GUI for emotion data collection in this study. The code base of ColEmo is available at https://gitlab.ilabt.imec.be/emowear/colemo with its architecture and usage framework explained in 55 . The code used for data cleaning, synchronization, and technical validation are publicly available at https://gitlab.ilabt.imec.be/emowear/preprocessing ."
10.1038/s41597-024-03427-5,The code to process the data and run baselines can be found in: https://github.com/PaddlePaddle/PaddleSpatial/tree/main/apps/wpf_baseline_gru .
10.1038/s41597-024-03514-7,Softwares for data analyses were mentioned in Methods. The core code and parameters are available at https://github.com/bright-hu/Antennarius_striatus/ .
10.1038/s41597-024-03493-9,"The code structure demonstration is available in the README file in the GitHub repository: https://github.com/athenarc/arcadia-project/tree/master/resource-demand-open-data . The code demonstration contains two folders: data, and notebook. The data directory shows all the original data with the resource demands per scenario and the nodes_allocatable file with records from the experiments analyzed in the section Usage Notes -> Proactive Data Center Sizing and the notebook directory includes a notebook containing the visualizations of data. Python 3 is used for data processing, analysis, visualization, and setup."
10.1038/s41597-024-03503-w,All software and pipelines used in this study were implemented according to the manuals and protocols provided by the software developers. Versions of the software have been described in Methods. No custom code was used in this study.
10.1038/s41597-024-03506-7,"Acquisition and processing of remote sensing images, TSI model construction, and generation of spatial TSI data were done in JavaScript on Google Earth Engine Code Editor and is available at https://github.com/MickyHuu/TSI_inversion.git . The Mann-Kendall-trend was supported by the Python package “pymannkendall” ( https://pypi.org/project/pymannkendall ), and the significance level (p) was set as 0.05. The visualization of TSI mapping was realized by ArcGIS 10.8."
10.1038/s41597-024-03414-w,The analytical codes developed during this project are openly accessible on Zenodo 44 and in our GitHub repository ( https://github.com/OntarioWastewaterSurveillanceConsortium/sars-cov-2-data ) and are shared under the terms of the Creative Commons (CC) BY License.
10.1038/s41597-024-03504-9,"There were no custom software codes developed. The tools used for reads quality control are non-open scripts developed by the Novogene (Beijing, China). All bioinformatics tools and pipelines were performed following the instructions of the manuals and protocols. The versions of the software used, along with their corresponding parameters, have been thoroughly described in the Methods section."
10.1038/s41597-024-03517-4,"Example code for the mortality prediction model developed using INSPIRE is available on GitHub to demonstrate a simple use case of the dataset 24 . Over time, we will work with the community to develop and share additional open source code to support the reuse of the INSPIRE dataset 25 ."
10.1038/s41597-024-03489-5,No specific code or script was used in this work. Commands used for data processing were all executed according to the manuals and protocols of the corresponding software.
10.1038/s41597-024-03515-6,"No custom code was used during this study for the curation and/or validation of the dataset. Table 1 in the Methods section presents the list of programs used in data processing. VLC media player ver. 3.0.19 (Vetinari) extracted frames of a given frequency (recording ratio) from video files. “VLC is a free and open source media player, encoder and streamer created by the VideoLAN volunteer community” 70 . A program by Changchang Wu, VisualSFM – A Visual Structure from Motion System, ver. 0.5.26 was used to process the film frames into a sparse point cloud. “VisualSFM is a GUI application, free for personal, non-profit or academic use for 3D reconstruction using structure from motion” 71 . Using the Yasutaka Furukawa Clustering Views for Multi-view Stereo/Patch-based Multi-view Stereo Software (PMVS/CMVS) algorithm 72 together with VisualSFM enables the generation of a dense point cloud. The program is distributed under the GPL license. Scaling and rectification of the dense model was carried out in MeshLab ver. 2021.05 73 . It is a free, open-source application under the GNU GPL license “for processing and editing 3D triangular meshes” 74 . The filtered data was exported to a text file in another free, open-source program under the GNU GPL license, CloudCompare ver. 2.11.3 (Anoia). It is used for 3D point cloud and mesh processing 75 . Creating DEMs and DTMs, as well as visualizations in the form of maps, was carried out in popular free open-source geoinformation programs under the GNU GPL license, such as QGIS ver. 3.32 Lima and earlier versions 76 and SAGA GIS ver. 9.1.2 and earlier versions 77 ."
10.1038/s41597-024-03374-1,No custom code was generated for this work.
10.1038/s41597-024-03426-6,The source code to compute the dataset and reproduce the figures presented in this paper is available on Github repository https://github.com/vjg65/Length-of-life-inequality-and-polarization .
10.1038/s41597-024-03501-y,"• For the Motor Imagery task, code is available at: https://github.com/txdat/bci-motor-imagery/blob/master/notebooks/eeg_final.ipynb • For the person identification, code is available at: https://github.com/dangkh/VINIF_IdentifyPerson"
10.1038/s41597-024-03509-4,The study did not use any custom code to generate data.
10.1038/s41597-024-03520-9,"The following code can be used to import the dataset into R and to produce basic descriptive plots at the general and specific levels (Fig. 3 ). Since some taxonomic authorities (i.e. the name of the taxon author) include apostrophes, such as Tetragonia crystallina L’Hér., to import data correctly, it might be necessary to specify that these are not quotes (with, quote = ""\"""",) . data < - read.csv(""~path/data.csv"", quote = ""\"""", sep = "";"") #Number of unique species collected per year (Fig. 3a) barplot (rowSums(table(data$year, data$scientificName) >0)) # Number of records on the entire dataset per month (Fig. 3b) barplot (table(data$month)) # Number of records per month for a single species (Fig. 3c) with(subset(data, scientificName == ""Drosophyllum lusitanicum (L.) Link""),barplot(table(factor(month, levels = 1:12)), xlim = c(1, 12)))."
10.1038/s41597-024-03498-4,The codes used for this study are available on GitHub https://github.com/ThenukaRamesh/CN-_Dataset_SUMO_Conversion .
10.1038/s41597-024-03494-8,"The CODC-QC is freely available from GitHub ( https://github.com/zqtzt/CODCQC ) as an Open-Source Python package under the Apache-2.0 License. The XBT, MBT, and Nansen Bottle bias correction schemes are freely available from the IAP ocean website ( http://www.ocean.iap.ac.cn ) under the ‘Data service - New techniques’ label."
10.1038/s41597-024-03537-0,"In the absence of explicitly provided parameters, default configurations were applied to all softwares and tools throughout this study. No specialized code or script was employed in the research methodology."
10.1038/s41597-024-03513-8,"Software was run with standard parameters unless otherwise indicated. Custom code and scripts, to analyze the genome, annotations, or generate figures is available on GitHub ( https://github.com/Bart-Edelbroek/firmibasis_genome ) and also on figshare 36 ."
10.1038/s41597-024-03523-6,"We followed the developers’ instructions for the bioinformatics tools used in this study. The software and code used are publicly accessible, with the version and parameters used specified in the Methods section. No custom code was used during the compilation of the dataset."
10.1038/s41597-024-03511-w,"The complete source code and script files employed for format conversion, refinement, and pre-processing of the MAD dataset, as well as deep learning training, are readily available at https://github.com/kaen2891/military_audio_dataset . The majority of libraries and frameworks employed in the code include Python, PyTorch , TorchAudio , Librosa , and Numpy . To execute the code, please follow the instructions provided on the website. Besides, pretrained model weights for each model based on the MAD dataset are publicly accessible. This facilitates researchers in effortlessly loading and employing the AI model corresponding to the performance presented in the paper."
10.1038/s41597-024-03528-1,All R scripts used in this study and any other analysis files are available at https://github.com/npechl/empclima . Complete documentation and results files for each analysis conducted in this project are also provided at https://github.com/npechl/empclima .
10.1038/s41597-024-03527-2,All software and pipelines were executed according to the manual and protocol of published tools. No custom code was generated for these analyses.
10.1038/s41597-024-03508-5,"JavaScript codes of the Earth Engine repository used to generate the cropland maps and metrics for accuracy evaluation are shared and available from the figshare repository. Python codes for raster pyramid building are also provided. The software and modules used in this study include Origin 2023b, ArcGIS Pro 3.1, Python 3.7, gma 1.1.5, and ArcPy 3.1. The very high spatial resolution (VHR) Google Earth images are accessible by the ArcGIS Web Map Tile Service (WTMS) service."
10.1038/s41597-024-03512-9,"We detail all commands and pipelines employed for data processing in the methods section. For any software where specific parameters were not mentioned, we used the default settings recommended by the software developers. The core code is available at https://github.com/sundongfang/Chromosome-level-genome-of-Panulirus-ornatus ."
10.1038/s41597-024-03480-0,No codes were generated during this study.
10.1038/s41597-024-03450-6,Code for the PADME platform: https://git.rwth-aachen.de/groups/padme-development . Code for the PHT-meDIC platform: https://github.com/PHT-meDIC . The implementation of the ‘Discovery Train’ can be found here: https://doi.org/10.5281/zenodo.11101321 . The security container can be found: https://github.com/PHT-Medic/train-container-library and its explanation: https://difuture.de/wp-content/uploads/2019/10/Marius-Herr_DIFUTURE_Symposium_exported_High_quality.pdf .
10.1038/s41597-024-03495-7,"All the software that was used in this study is publicly available, and the parameters that were used are clearly described in the Methods sections. If no detailed parameters were mentioned for a particular software, the default parameters were used as suggested by the developer. The code for the analysis pipelines was deposited at GitHub: https://github.com/Ckenen/integrated-transcriptome-of-seriola-dumerili ."
10.1038/s41597-024-03477-9,All the methods used to produce the value-added ABL dataset subject of the current study can be reproduced following the description provided in this publication. The procedures were coded by the authors in Matlab. The code is available at Figshare along with the dataset. Additional Quick Look images were produced for each of the methods described in the publication. This additional material can be obtained by contacting the corresponding author.
10.1038/s41597-024-03535-2,Custom codes were used to process the raw data (e.g. for sap flow and canopy CO 2 fluxes). These data were processes by codes writing in R (version 4.3.1). Custom codes for sap flow processes and CO 2 fluxes are provided in the data repository.
10.1038/s41597-024-03510-x,A tutorial on how to read and display HS data is available in a public repository: https://github.com/HIRIS-Lab/HistologyHSI-GB . These tutorials include the use of custom MATLAB and Python functions and some of the most common toolbox/libraries.
10.1038/s41597-024-03347-4,"The data-sets generated during this work are available as multiple stacks of image files, which do not require any additional processing tools for use. The annotation of the data was carried out manually with semi-automatic post-processing, details of which are provided in the main text."
10.1038/s41597-024-03533-4,No propriety codes were used to generate these data.
10.1038/s41597-024-03467-x,No custom code was used in the dataset.
10.1038/s41597-024-03505-8,All original survey datasets used for constructing the SCD are freely available from the data collecting organizations. The websites of these organizations are mentioned in Supplementary Table 1 . The database was constructed from these survey datasets with Stata-16. The Stata Do files with the syntax code used for processing the data and constructing the index are included in the ZIP Archive with the Subnational Corruption Database available from the Figshare repository 55 .
10.1038/s41597-024-03536-1,The analysis methodology associated with this article is available on GitHub ( https://github.com/kts-desilva/UMAI ).
10.1038/s41597-024-03471-1,The workflow for data analysis using R version 4.0.0 is proposed at this address: https://github.com/paescharlotte/early_life_nutrition_rabbit/ 17 .
10.1038/s41597-024-03525-4,We provide the script to facilitate the use of the released data at https://github.com/CmrxRecon/CMRxRecon-SciData . A brief description of the provided package is as follows: a) ‘ReconCode’: contains parallel imaging reconstruction code; b) ‘DemoData’: contain one example data; c) ‘Evaluation’: contains image quality evaluation code.
10.1038/s41597-024-03519-2,"The cervical screening data workflow uses different software components and assigns custom identifiers along the process. However, all data associated with a participant (invitation, sampling, analysis) are linked through the participant’s national personal identification number. This code can be accessed only by researchers and stakeholders authorised by the Swedish Ethical Review Authority. The data can be pseudo-anonymised and aggregated before sharing, depending on the intended use."
10.1038/s41597-024-03507-6,"Sample Python code, FFmpeg commands, and bash source code used to extract features, compute VQA metrics, handle the datasets, statistical analysis and plots is available at https://github.com/cloudmedialab-uv/VideoCodingFeaturesQualityDataset ."
10.1038/s41597-024-03522-7,No custom scripts or code were used in this study.
10.1038/s41597-024-03556-x,No custom code was used to generate or process the data in this study.
10.1038/s41597-024-03557-w,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-024-03543-2,The GEE codes used to produce the annual maximum NDVI dataset is available to the public at https://code.earthengine.google.com/124ae202e3241a9523c0951ae6e4353f?noload=true .
10.1038/s41597-024-03538-z,All of the programs we use in this study for analysis are free and open-access software. No custom code was used during this study for the curation and/or validation of the dataset. Information on the version of the open-source software and its official website: 1. Cytoscape (v3.8.2) https://cytoscape.org/ 2. stringAPP (v2.0.1) https://apps.cytoscape.org/apps/stringapp 3. ClueGO (v2.5.10) https://apps.cytoscape.org/apps/cluego 4. KEGGREST (v1.42.0) https://bioconductor.org/packages/release/bioc/html/KEGGREST.html 5. Limma (v3.18) https://bioconductor.org/packages/release/bioc/html/limma.html
10.1038/s41597-024-03456-0,"The cropland use intensity mapping algorithms were implemented in GEE. The map processing codes and produced cropland use intensity datasets were available at https://doi.org/10.6084/m9.figshare.24603234 . Datasets of cropland use intensity could be automatically generated in China or other countries/regions based on the publicly accessible S1/S2 time series images using the shared processing code, which does not require training samples or human interactions."
10.1038/s41597-024-03496-6,"The code is available in a GitLab repository (available at https://gitlab.com/saviola/rocov2-code , accessed 2023-11-10). The folder “roco-2018” contains scripts and models for the compound figure and radiological figure detection, as taken from the original ROCO pipeline, which are used to filter all extracted images of the PMC Open Access Subset for non-compound radiological images. The folder “baseline” contains code for the training of the baseline models for concept detection and caption prediction, which is described in the Technical Validation section. The folder “ImageCLEF” contains the pre-processing and evaluation scripts for the ImageCLEFmedical Caption 2023 challenge tasks."
10.1038/s41597-024-03542-3,No custom software code was developed for this study. All bioinformatics tools and pipelines are Follow the manuals and protocols provided by the respective software developers. The software and version have been thoroughly described in the Methods section.
10.1038/s41597-024-03550-3,"ICESat-2 photon data can be downloaded from the OpenAltimetry application, available at https://openaltimetry.earthdatacloud.nasa.gov/data/ or the web portal maintained by the National Aeronautics and Space Administration (NASA) National Snow and Ice Data Center (NSIDC) at https://nsidc.org/data/icesat-2 . A Python package providing implementation of the DBSCAN algorithm over ICESat-2 geolocated photons is available at https://doi.org/10.6084/m9.figshare.25991248 . Alternatively, one can use the Create Graph module of ArcGIS software to manually classify/remove the outliers in the tabular data consisting of geolocated photons from ICESat-2. All the maps in the manuscript were compiled using ESRI ArcGIS software. Profile diagrams were generated in the MS Excel using the ICESat-2 photon data. 3D perspective views were generated in the ArcScene module of ESRI ArcGIS ver. 10.8.1 software ( https://www.esri.com/ )."
10.1038/s41597-024-03484-w,Code used in the 40 drought events characterization is available at DIGITAL.CSIC ( https://doi.org/10.20350/digitalCSIC/15446 ). Code was run using the 2023.06.0 RStudio version.
10.1038/s41597-024-03472-0,Data processing can be retraced in the GitHub project https://github.com/nielsfuchs/MOSAiC_inicelight/ . Files containing the derived calibration coefficients for the lightchains and the lightharp are included into the repository.
10.1038/s41597-024-03551-2,The R code used in the analysis of the data is available on figshare ( https://doi.org/10.6084/m9.figshare.24935922.v1 ) 54 .
10.1038/s41597-024-03492-w,Summary statistics and relevant figures and can be reproduced from the open access or protected versions of the UK COVID-19 Vocal Audio Dataset using code found here: https://github.com/alan-turing-institute/Turing-RSS-Health-Data-Lab-Biomedical-Acoustic-Markers/ which is archived 41 under https://doi.org/10.5281/zenodo.11208315 .
10.1038/s41597-024-03555-y,No code was used in this study.
10.1038/s41597-024-03526-3,We used a typical workflow to process the WES data. All software used in this study is freely available: Fastp: https://github.com/OpenGene/fastp . BWA: https://github.com/lh3/bwa . GATK: https://github.com/broadinstitute/gatk/releases . VEP: https://m.ensembl.org/info/docs/tools/vep/script/vep_download.html . No code was used in the generation of the WSIs files. No code is required to access or analyze this dataset.
10.1038/s41597-024-03462-2,"For technical validation, we utilized publicly available code without any restrictions. Specifically, we employed the following functions/scripts: • nk.ecg_peaks.py from the NeuroKit2 package to identify R-peaks in an ECG signal ( https://github.com/neuropsychology/NeuroKit ). • find_peaks.py from the SciPy package to identify R-peaks in an ECG signal ( https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html ). • nk.ecg_delineate.py from the NeuroKit2 package to delineate the QRS complex for morphology features ( https://neuropsychology.github.io/NeuroKit/_modules/neurokit2/ecg/ecg_delineate.html#ecg_delineate ). • nk.hrv.py from the NeuroKit2 package to compute Heart Rate Variability ( https://neuropsychology.github.io/NeuroKit/_modules/neurokit2/hrv/hrv.html#hrv ). • linear_model.LogisticRegression , svm.SVC , ensemble.RandomForestClassifier , neighbors.KNeighborsClassifier , and XGBClassifier from the Scikit-learn package for classification ( https://scikit-learn.org/stable/ ). However, we customized and combined these packages to form our own code for the project. The Python program for ECG denoising and feature extraction is publicly available at https://github.com/pengxiaoting1995/MPPD_MSIMI ."
10.1038/s41597-024-03560-1,"Code files for transcriptome analysis and hierarchical clustering analysis using R, as shown in Fig. 2 , are available in figshare 28 ."
10.1038/s41597-024-03554-z,The code used for the genome annotation and the analysis is available on GitHub ( https://github.com/ylla-lab/Gryllus_longicercus ) and was deposited in Zenodo with the following doi ( https://doi.org/10.5281/zenodo.10628205 ).
10.1038/s41597-024-03561-0,The MATLAB codes for generating the GIMMS FPAR4g product are available on GitHub at https://github.com/WeiqingZhao/GIMMS_FPAR4g .
10.1038/s41597-024-03552-1,All the codes for processing the nitrogen yields were calculated using R version 4.3.1 and archived at GitHub: https://github.com/ymwang4924/tn.git .
10.1038/s41597-024-03468-w,"All code used to generate the reconstruction files are available as Jupyter Notebooks in the associated GitHub repository ( https://github.com/LewisAJones/PhanGrids ). The reconstructions were generated in Python with ‘PyGPlates’ ver. 0.36.0 35 . Two example scripts (in Python and R) for spatiotemporally linking point data (i.e. geographic coordinates) to their reconstructed coordinates (i.e. palaeocoordinates) are also provided within the GitHub repository ( https://github.com/LewisAJones/PhanGrids ). All reconstruction files are archived in the associated Zenodo repository ( https://zenodo.org/doi/10.5281/zenodo.10069221 ). The peer-reviewed version of these files is version 0.0.3, under the following static release: https://doi.org/10.5281/zenodo.11384745 ."
10.1038/s41597-024-03564-x,"No specific code was developed in this work. The parameters of all commands and pipelines used for data processing are described in the Methods section. If no detailed parameters are mentioned for a software, the default parameters were used, as suggested by the developer."
10.1038/s41597-024-03553-0,"The final results of this paper, which utilizes a random hexagonal grid to validate the accuracy of the HDCO, have been published on GitHub and can be accessed at https://github.com/Linjingwu999/HFOCD.git . This includes the code used, the generated hexagonal vector grid, the percentage of area occupied by each grid oasis, and the location of the validation points. Python 3.9 should be used to access and edit the code."
10.1038/s41597-024-03530-7,Code for accessing and processing the data discussed in this study is freely available on Github ( https://github.com/jonathansharp/US-RFR-LMEs ). Code was written in MATLAB version R2022a. Parameters used to generate and validate the current dataset are described throughout the Methods section and are listed in Table 2 .
10.1038/s41597-024-03570-z,"Please find the following codes under the root directory “Elsafty_Codes_for_AI,” 10 : 1. Elsafty_Code_1; segment & localize using a pen. 2. Elsafty_Code_2; train & test a DL-based image classifier using Google Colab."
10.1038/s41597-024-03469-9,No custom code has been made available.
10.1038/s41597-024-03558-9,"Analysis in this study, including data processing and downstream analysis, were conducted according to the standard code of DNBelab C Series scATAC-seq analysis pipeline and ArchR package 24 . The R codes used to analyze the data in this study were available at Figshare 29 ( https://doi.org/10.6084/m9.figshare.24547609.v2 )."
10.1038/s41597-024-03544-1,No custom code was used for the generation or processing of the datasets used in this study.
10.1038/s41597-024-03546-z,The data and source code used in this study are distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License ( https://creativecommons.org/licenses/by/4.0/ ) and are available at Zenodo 25 .
10.1038/s41597-024-03568-7,(1) SOAPnuke v1.5.6: parameters: -n 0.01 -l 20 -q 0.1 -i -Q 2 -G -M 2 -A 0.5 -d (2) Cutadapt v1.16: parameters: -a AGATCGGAAG -q 20 All the other software and pipelines not listed or described in the methods section used the default parameters.
10.1038/s41597-024-03377-y,No custom code has been used in the creation of the Bubona Dataset.
10.1038/s41597-024-03207-1,"The data analyses were performed according to the manuals and protocols provided by the developers of the respective bioinformatics tools and no custom code was used in the execution of this study. All software and codes used in this work are publicly accessible, and their corresponding versions are specified in Methods section."
10.1038/s41597-024-03574-9,"The Python 3.9 scripts used for generating statistics presented in the article, as well as the code for validating the completeness of the dataset (including YOLOv5 and YOLOv10), are available at https://github.com/Daniel00ll/iSOOD-code ."
10.1038/s41597-024-03541-4,"No custom code was generated in this study. The bioinformatics tools implemented in this study and their versions, settings, and parameters were explained in the Methods section. In cases where no specific settings were specified for a particular tool, default parameters were employed."
10.1038/s41597-024-03573-w,Notebooks and scripts for the automated viscous liquid handling and gravimetric analysis to prepare and determine the composition of the liquid formulations can be found at: https://github.com/sustainable-processes/formulations-prep . The code to run the pHbot has been published separately: https://github.com/sustainable-processes/pHbot 31 . And all the software to develop the computer vision phase stability classifier can be found: https://github.com/sustainable-processes/stability-computer-vision .
10.1038/s41597-024-03532-5,"The custom Python and R scripts used to extract CanRCM4 data, calculate the signatures, and generate projected UHI and NBS climate data for building simulations can be found on Github at: https://github.com/henrylu2/Climate-projections-to-support-building-adaptation ."
10.1038/s41597-024-03571-y,Source code for the Datasets command line tooling is in the process of being made available on GitHub at https://github.com/ncbi/datasets . Instructions on how to use the tooling is currently available at that same location. Source code for the Datasets front end interface and backend servers are not provided publicly.
10.1038/s41597-024-03577-6,The R code developed for processing the data (“computed_data.R”) and analysing the forest structure dynamics and the response of tree demography to the silvicultural treatments is available for download in the same archive from CIRAD’s public repository 61 .
10.1038/s41597-024-03486-8,No custom scripts or code were used in this study.
10.1038/s41597-024-03566-9,"We have made our framework available on GitHub via the following link: https://github.com/gergobogacsovics/APACS23 ; it is capable of the automatic segmentation of Pap smear images using the combined architecture from our original paper 10 . We made all the corresponding codes and helper scripts, responsible for data reading, pre-processing, training, and testing, freely available under the GNU General Public License v3.0 in an attempt to facilitate any related research as well as the spread of reliable, modern AI-assisted clinical image-processing systems, and to also ensure the open nature of future research."
10.1038/s41597-024-03594-5,The code developed by Kratzert et al . in the CARAVAN initiative has been used for the calculation of the basin attributes (available at https://zenodo.org/records/6578598 ). The code used for the validation of the meteorological data including the coding of the Témez hydrological model are written in Python and are available at https://zenodo.org/records/10605646 .
10.1038/s41597-024-03518-3,"In this analysis, default parameters or parameters recommended by the developer were used."
10.1038/s41597-024-03567-8,All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in Methods. No custom scripts or code were used.
10.1038/s41597-024-03562-z,This article does not make use of any custom code.
10.1038/s41597-024-03576-7,The code used to calculate SWRC parameters can be found on Github ( https://github.com/TONGYP1116/SoilHydraulicParameter.git ).
10.1038/s41597-024-03529-0,The supplementary information provides access to the pertinent code and data employed for computation and analysis in this study.
10.1038/s41597-024-03521-8,"The initial structure generation was carried out using RDKit 2020.09.5 66 , 67 . Further structure optimization and the creation of conformers were performed by utilizing CREST 13 , 56 and DFTB+ 10 , 77 codes together with ASE 78 . Note that all necessary features regarding the utilized DFTB3+MBD (with and without GBSA implicit solvent) approach are available in the current DFTB+ version 11 . All DFT calculations were carried out using FHI-aims 82 (version 221103). Additional conformer generation experiments were performed with RDKit 2020.09.5, OpenEye’s Omega 4.0.0.4 90 and Schrodinger’s Maestro suite v2020-4 (see SI for detailed procedures)."
10.1038/s41597-024-03596-3,"The source code is available at https://github.com/david-kupas/apacc-smear-cell-db and can be publicly accessed under the GNU General Public License v3.0. The exact details of the usage can also be accessed through the link provided, accompanied by example codes. The code was written in Python language using the NumPy, OpenCV, Pillow, Matplotlib, and Scikit-Image packages."
10.1038/s41597-024-03458-y,This work did not utilise a custom script. Data processing was carried out using the protocols and manuals of the relevant bioinformatics software.
10.1038/s41597-024-03612-6,"In this study, open-source bioinformatics analysis software was employed to analyze the raw sequencing data. Additionally, custom scripts or code were not utilized for dataset management and validation. Details of all software toolkits used are as follows: FastQC (version 0.11.8) was utilized for assessing the quality of the raw sequencing data ( https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ ). Bioinformatics provides an online analysis tool for principal component analysis (PCA), correlation analysis, pie charts, histogram pyramid stacks, and other functionalities ( www.bioinformatics.com ). The seed sequence motif was done using the Hiplot (ORG) online tool ( https://hiplot.cn/ ). Online prediction websites, including miRecords 26 , miRTarBase 27 , and TarBase 28 were employed to predict potential target genes of miRNAs. The potential target gene prediction of tsRNA was performed on the online prediction website Targetscan ( https://www.targetscan.org/vert_50/seedmatch.html ). DAVID online analysis website was employed for Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) functional enrichment analysis ( https://david.ncifcrf.gov/summary.jsp ). R software was used for differential expression analysis of differentially expressed sncRNAs ( https://www.r-project.org/ ). GraphPad Prism (version 8.0.2, GraphPad Software Inc., USA) was used for statistical analyses and data visualization."
10.1038/s41597-024-03583-8,The dataset is available from: https://www.scidb.cn/en/detail?dataSetId=5eb39369148d4f40bb5ec398d6503c46 or https://doi.org/10.57760/sciencedb.17721 . The code to reproduce the analysis is available at Github repository with https://doi.org/10.5281/zenodo.11649590 .
10.1038/s41597-024-03540-5,"All codes used for data set preprocessing and cleaning can be accessed in a specific GitHub repository ( https://github.com/BYO-UPM/CUCO_Database ). Additionally, the aforementioned Jupyter notebook is included as a reference in the same repository. The KARMA algorithm is available as an open-source package in a specific GitHub repository ( https://github.com/BYO-UPM/Formant-Tracking ). The AVCA-ByO toolbox is also available as an open-source package 29 ."
10.1038/s41597-024-03579-4,"In the current study, no custom scripts were used. All the data processing was done using the guidelines standard pipelines and bioinformatics tools given in the Methods section."
10.1038/s41597-024-03524-5,"DANCE source codes are written in C/C++ and Python and are publicly available on GitHub at https://github.com/PhyloSofS-Team/DANCE . This repository also contains a Python wrapper allowing users to seamlessly run DANCE full pipeline. In addition, we provide example input 3D structures."
10.1038/s41597-024-03580-x,"The codes, written in the R language and used for our monitoring system ( https://gitlab.inf.unibz.it/alpenv/Station_Monitoring_System ) (R version 3.6.0) and the DQC script ( https://gitlab.inf.unibz.it/alpenv/ltser_datapaper ) (R version 4.2.2), are freely available on the collaborative platform Gitlab. Furthermore, the DQC script is also provided with a DOI and persistently stored in Zenodo ( https://zenodo.org/records/10255852 )."
10.1038/s41597-024-03575-8,All scripts for summarizing and visualizing the data are available on GitHub under the MIT license agreement 55 .
10.1038/s41597-024-03563-y,No custom code was written for this paper.
10.1038/s41597-024-03584-7,"Serum enzyme activity, antioxidant capacity, and immune parameter were statistically processed and analyzed using R version 3.6.0, default parameters or parameters recommended by the developer were used. A reproducible version of partial microbiome analysis and computer code is available at https://github.com/chaichai9521/goat-rumen-mcirobiome-analysis.git ."
10.1038/s41597-024-03588-3,The database as well as the code and separated data for the predicted values presented in Fig. 5 are available on Dryad 18 .
10.1038/s41597-024-03621-5,"There was no code used in the generation of the data in this work, an only Microsoft Excel is employed to process all the data."
10.1038/s41597-024-03598-1,"Described statistical approaches were performed within the R environment (version 4.0.3). Scripts used for the both outlier correction (“outlier.correction.R”) and estimation of BLUEs (BLUEs.estimation.R) along with input datasets containing original historical data can be found in the e!DAL-Plant Genomics and Phenomics Research Data Repository (PGP).To run the script, input data files (a_spring_wheat.txt; a_winter_wheat.txt) need to be downloaded and proper working directory needs to be set. The script named “outlier.correction.R” shows the outlier handling procedure on the example of plant height data in winter wheat. Output files such as “Var.comp.PH.txt”, “Outliers.PH.txt” and “Data.corrected.PH.txt” corresponding to variance components or regeneration years, list of removed outliers and enhanced (outlier-corrected) data, respectively are generated by the scripts. Script called “BLUEs.estimation.R” uses the outlier corrected data from previous step while generating the BLUEs for individual accessions, as included in the “BLUEs.PH.txt” output. As data for all traits are available in input files, script can easily be altered for other traits and spring wheat following the footnotes within script."
10.1038/s41597-024-03591-8,The code used in this research is available in the Figshare repository ( https://doi.org/10.6084/m9.figshare.24721209.v3 ) 29 .
10.1038/s41597-024-03592-7,"No custom code is necessary for the generation or processing of this dataset. No image pre-processing, normalization, or cropping was applied to the slide section scans. The dataset 11 can be downloaded from CIL using the PHP script provided at https://github.com/CRBS/CIL_RS/wiki/Download_CHOC_dataset ."
10.1038/s41597-024-03603-7,The algorithm is run on Google Earth Engine (GEE) and the code can be found at this link: https://doi.org/10.6084/m9.figshare.24587259 58 .
10.1038/s41597-024-03609-1,"The complete code used in this work is not released along with the dataset because a part of it involves the REST2_v9.1 code, which cannot currently be shared without permission. Meanwhile, the readers could get the code from the home page of the REST2 model ( https://www.solarconsultingservices.com/ )."
10.1038/s41597-024-03590-9,"The open source code ABINIT 26 , 27 , 28 is used throughout this work for calculating the optical properties. ABINIT is distributed under the GNU General Public Licence. The workflows used to run the simulations are implemented using FireWorks ( https://github.com/materialsproject/fireworks ) as workflow manager 47 and specific workflows are available in the Abiflows package ( https://github.com/abinit/abiflows ). The Pymatgen 29 and Abipy ( https://github.com/abinit/abipy ) python packages are used to generate inputs and analyze the results. Pymatgen is released under the MIT (Massachusetts Institute of Technology) License and is open source. AbiPy is released under the GNU GPL license. FireWorks is released under a modified BSD license."
10.1038/s41597-024-03581-w,"No specific code was developed for this study. The data analyses were conducted following the manuals and protocols provided by the developers of the relevant bioinformatics tools, which are described in the Methods section along with the versions used."
10.1038/s41597-024-03436-4,"R code used for producing CFSDS 25 has been made available at the Centre for Open Science OSF data repository at https://osf.io/f48ry/ ( https://doi.org/10.17605/OSF.IO/F48RY ). Four code sections are provided, grouped under one code file “CFSDS_example.R”. Code subsections: 1. ‘1.0 Processing’ - Preprocesses a single NBAC perimeter and FIRMS hotspots 9 . 2. ‘2.0 Interpolation’ - Interpolates fire arrival time for a single NBAC perimeter using kriging. 3. ‘3.0 Covariates - Uses the fire arrival time raster to extract environmental covariates associated with each burning day. 4. ‘4.0 Summarize’ - an example of how an end-user might summarize point-level spread data for use in fire modelling."
10.1038/s41597-024-03500-z,No specifc script was used in this work. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic sofware.
10.1038/s41597-024-03587-4,Data maintenance . UltraMNIST 21 can be downloaded from the official page hosted on the Kaggle platform and accessible at https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/4F4KJS . Information related to alternative download links will also be made available on our official page at https://github.com/transmuteAI/ultramnist . This page will also provide any future updates related to the dataset. Benchmark and code . Code related to all benchmark experiments can be obtained from our repository hosted on GitHub at https://github.com/transmuteAI/ultramnist .
10.1038/s41597-024-03485-9,"All the bioinformatic pipelines described in this manuscript can be found via Github at https://github.com/NeuroGenomicsAndInformatics , and https://github.com/Ibanez-Lab/ . All laboratory protocols can be found via protocols.io at https://www.protocols.io/researchers/neurogenomics-and-informatics . No custom code was used to generate the data described in this manuscript."
10.1038/s41597-024-03604-6,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-024-03447-1,"All calculations described in the paper have been carried out using the Vienna ab-initio Simulation Package (VASP), v 6.3.0, for DFT total energies, forces, and structural relaxations, the Universal Crystal Structure Predictor (USPEX), v 10.5, for crystal structure searches. No custom code was used during this study for the curation of the HEX database 31 . The methods section contains all details needed to reproduce the calculations."
10.1038/s41597-024-03578-5,All codes and fine-tuned language model AgBERT with outputs were publicly available at: https://github.com/guojson/AgCNER.git .
10.1038/s41597-024-03623-3,"The cryosphere-hydrology model (WEB-DHM) is described in detail in our regional study 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 80 . Detailed R codes for comparing and reanalyzing numerous runoff and ET products in this study is available in the Supplement “Part 3”."
10.1038/s41597-024-03565-w,"The version and parameter of all bioinformatics tools used in this work are described in the Methods section. During this study, no custom code was used for the curation or validation of the dataset."
10.1038/s41597-024-03617-1,"The study utilized freely available software to the public, and the parameters are explicitly outlined in the Methods section. All commands and pipelines were executed following the manuals and protocols of the corresponding bioinformatic software. The study did not utilize custom scripts or code."
10.1038/s41597-024-03534-3,The code used to generate the dataset is available at Figshare 16 .
10.1038/s41597-024-03615-3,The code for generating Fig. 1 is provided in our GitHub repository ( https://github.com/nfdi4health/IdentifiedStandards.git ).
10.1038/s41597-024-03549-w,No specific code was developed in this work.
10.1038/s41597-024-03404-y,"All commands and pipelines employed for data processing adhered strictly to the guidelines specified in the manuals of the pertinent bioinformatics software, with the parameters explicitly detailed in the Methods section. In instances where no specific parameters were explicitly stated for a particular software, default parameters were applied. It is noteworthy that no bespoke scripts or custom code were formulated or utilized throughout the course of this study."
10.1038/s41597-024-03572-x,No custom code has been used for this study.
10.1038/s41597-024-03616-2,All the codes used in this study to construct the dataset were written in the Interactive Data Language (IDL) and will be openly available at https://github.com/YuyangXian/TCC.git .
10.1038/s41597-024-03463-1,The code for the “Technical validation” section is publicly available at https://github.com/roberto-roman/mira-mataje_duplicates/tree/main . This code requires and works with large taxonomic data tables that have been mentioned in the “Technical Validation” section and that are available for download at The Knowledge Network for Biocomplexity (KNB) ( https://doi.org/10.5063/F14F1P6H ) 22 . The code for generating the 3D shaded-relief maps is publicly available at https://github.com/PJV-Ecu/3D-shaded-relief-maps-using-Rayshader.git .
10.1038/s41597-024-03607-3,Codes used in this publication are available at GitHub and deposited at Zedono as follows: colour calibration— https://github.com/rdayrell/colour_calibration 22 ; Traitor— https://github.com/TankredO/traitor 23 .
10.1038/s41597-024-03624-2,"The dMRI data processing software employed in this study is publicly available on GitHub. The repositories house the complete processing pipeline and scripts covering stages from preprocessing, CNN-based masking, dMRI harmonization, to whole-brain tractography, white matter parcellation, and visualization. These repositories offer comprehensive documentation, usage instructions, script examples on sample data, and list all dependencies necessary for effective utilization of the software. CHCP dMRI data processing pipeline: https://github.com/pnlbwh/pnlpipe/tree/v2.2.0 ; Convolutional neural network-based dMRI brain segmentation: https://github.com/pnlbwh/CNN-Diffusion-MRIBrain-Segmentation/tree/v0.3 ; dMRI data harmonization: https://github.com/pnlbwh/dMRIharmonization/tree/v2.1 ; UKF whole brain tractography: https://github.com/pnlbwh/ukftractography ; White Matter Analysis: https://github.com/SlicerDMRI/whitematteranalysis ; SlicerDMRI: http://dmri.slicer.org"
10.1038/s41597-024-03619-z,No customized code was produced to prepare or analyze the dataset.
10.1038/s41597-024-03614-4,No specific script was used in this work. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software.
10.1038/s41597-024-03635-z,"The software used in the Methods section was executed with default parameters, with the following exceptions: SOAPnuke v2.1.6, parameters: -lowQual = 20, -nRate = 0.005, -qualRate = 0.5. GenomeScope v2.0, parameters: -k = 21 -m = 10000 3D-DNA v180922, parameters: -s = MboI. RepeatMasker v4.1.5-p1, parameters: -xsmall -gff. CCS v6.4.0, parameters: --min-rq 0.9 -j 60. lima: v2.7.1, parameters: --isoseq -peek-guess. Maker v3.01, parameters, maker_opt.ctl: est2genome = 1 protein2genome = 1 min_protein = 50 run: mpiexec -n 60 maker. eggNOG-mapper v2.1.7, parameters: --ittype proteins -m diamond –cpu 60. BUSCO v5.1.2, parameters: -m = geno, -l = embryophyta_odb10."
10.1038/s41597-024-03595-4,The code implementing the described data collection and analysis methods is accessible at https://github.com/vistalab-technion/pp5 .
10.1038/s41597-024-03606-4,"The software used is at https://github.com/eduardo-aubert/BIDS-Conversion-Code . The codes are: 1) Anomplg.exe: to anonymize the EEG records, all the personal information stored in the EEG recordings, which could facilitate the identification of the participants, was deleted. 2) Plg2bids.exe: to read the original EEG recordings in NEURONIC format and convert them to BIDS structure."
10.1038/s41597-024-03618-0,All the codes for processing the streamflow metrics and changepoints were calculated using R version 4.3.1 and archived at GitHub: https://github.com/ymwang4924/nsd .
10.1038/s41597-024-03634-0,All processing pipeline scripts are openly available. Code to generate pre-processed outputs can be accessed via https://github.com/hongweilibran/MOTUM . The automated segmentation tool can be used by following the instruction in the DockerHub page: https://hub.docker.com/repository/docker/branhongweili/motum_seg/ . Pre-processing scripts for skull-stripping and co-registration are available.
10.1038/s41597-024-03640-2,No custom scripts or code were used in this study.
10.1038/s41597-024-03648-8,"Scripts for Hi-C sequencing data preprocessing, CUT&RUN sequencing data preprocessing and analysis relevant to figures in this study are available at https://github.com/TheHumphreysLab/kidney_anatomy_chromatin . Versions of all required packages are described in the scripts and Methods."
10.1038/s41597-024-03638-w,"All commands and pipelines utilized in the data processing were executed in accordance with the manuals and protocols of the respective bioinformatics software. In instances where detailed parameters were absent, default parameters were employed. The version of the software used is delineated in the Methods section. Notably, no custom programming or coding was incorporated."
10.1038/s41597-024-03632-2,"The data analysis methods as well as software, their versions and parameters used are described in the Methods section. No custom scripts were generated in this work."
10.1038/s41597-024-03637-x,"All code for data processing, continuous trait imputation, and technical validation can be accessed with no restrictions through a Zenodo-archived GitHub Repository 58 ( https://doi.org/10.5281/zenodo.11204431 ) and is linked to the data package in EDI. All steps were performed in R version 4.1.3."
10.1038/s41597-024-03569-6,"All scripts, parameters, and options used in the study are described in the Methods. We used publicly available programs and not custom programs."
10.1038/s41597-024-03642-0,All commands and pipelines used in data processing were executed according to the manuals and protocols of the corresponding bioinformatics software. No specific codes were developed for this study.
10.1038/s41597-024-03633-1,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatics software. There were no custom scripts or code utilized in this study.
10.1038/s41597-024-03610-8,"The scripts used to generate the impervious surface cover data are available and accessible in fighshare 67 . The scripts are available in Google Earth Engine: https://code.earthengine.google.com/?accept_repo=users/korahandrews/WADISC , and on GitHub through this link: https://github.com/Kora0003/WADISC ."
10.1038/s41597-024-03597-2,"Data processing and analysis have been performed in Matlab and Python. Dedicated scripts have been developed by us, both for processing and for analyzing data. We reiterate here that the multiple acoustic features were computed using the Parselmouth 37 , a Python library for the Praat software 38 ; instead, all analysis included in the Technical Validation section has been performed in Matlab and in Python by using classic available functions, for example, shapiro_wilk, vartestn, kruskalwallis, multcompare, violinplot, and boxchart. The developed Python script used to extract all acoustic features from the .wav file is freely available on GitHub at https://github.com/giovannasannino/VOC-ALS.git ."
10.1038/s41597-024-03589-2,No custom code was used to process the current dataset described here.
10.1038/s41597-024-03645-x,"No custom codes were used in this study. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding software. The parameters for different software and tools were specified in the Methods section, and default parameters were used for those without detailed parameters."
10.1038/s41597-024-03653-x,"This paper does not report original code. If no detailed parameters were mentioned for the software, default parameters were used according to the software introduction."
10.1038/s41597-024-03620-6,The code of the CPGV has been uploaded to GitHub: https://github.com/padapeng911/CPGV .
10.1038/s41597-024-03626-0,No custom code was used to generate or process the data described in this manuscript.
10.1038/s41597-024-03593-6,"“The MOTU-to-FHIR Mapping Pipeline code is accessible on GitLab at https://gitlab.com/almahealthdb/ahdb-mapping-service/ . This repository contains all necessary scripts, configuration files, and instructions for transforming the MOTU CSV dataset into the FHIRed dataset based on the HL7 FHIR standard (R4 version). It also provides a Docker Compose file that describes the configuration of Matchbox and the HAPI FHIR Server, facilitating automated deployment of the pipeline. For detailed instructions on running the pipeline, please refer to the README.md file in the repository”."
10.1038/s41597-024-03636-y,All scripts used to create event files and to create figures in this manuscript are available here 32 .
10.1038/s41597-024-03649-7,"In this study, python version of GEE and ArcGIS were used to generate and analyze the result maps. The code used is available on the National Tibetan Plateau/Third Pole Environment Data Center https://doi.org/10.11888/Terre.tpdc.301126 27 ."
10.1038/s41597-024-03647-9,"All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software. The settings and parameters of software were listed below: (1) Fastp v0.23.2: ‘-D’ (drop the duplicated reads), ‘-g’ (tail trimming), ‘-x’ (polymer trimming on 3′ ends), ‘-5’ (move a sliding window from 5′ tail to tail), ‘-u 10’ (unqualified percentage limit), ‘-c’ (overlapped bases correction); (2) Hifiasm v0.19.8: ‘-l2’ (strongly remove haplotig duplications); (3) Minimap2 v2.24: default parameters; (4) NextPolish2 v0.2.0: default parameters; (5) YaHS v1.2: default parameters; (6) Juicer v1.6.2: default parameters; (7) Juicebox v.1.11.08: default parameters; (8) MMseq2 v11: default parameters with ‘--search-type 3’, ‘—min-seq-id 0.8’ for potential contaminants; (9) SAMtools v. 1.9: default parameters; (10) RepeatModeler v2.0.4: ‘-LTRStruct’ LTR discovery pipeline; (11) RepeatMasker v.4.1.4: default parameters; (12) Infernal v1.1.4: default parameters; (13) tRNAscan-SE v2.0.9: ‘EukHighConfidenceFilter’ script with default parameters; (14) MAKER v3.01.03: default parameters; (15) HISAT2 v2.2.1: default parameters; (16) StringTie v2.1.6: default parameters; (17) BRAKER v3.0.3: default parameters; (18) GeneMark-ETP: default parameters; (19) Augustus v3.4.0: default parameters; (20) GeMoMa v1.9: ‘GeMoMa.m = 15000’, ‘ERE.c = false’ with default parameters; (21) Diamond v2.0.11.1: default parameters; (22) eggNOG-mapper v2.1.9: default parameters; (23) InterProScan 5.60–92.0: default parameters."
10.1038/s41597-024-03656-8,"As mentioned earlier, the DsPCBSD+ dataset is accessible on the figshare data repository 24 . Additionally, the Python code for the hash value matching method, utilized to filter highly similar images, is provided alongside the dataset and named Hash.py. Researchers can perform label format conversion from VOC format to YOLO, and YOLO format to COCO format using the resources available at the following links: https://github.com/RapidAI/VOC2YOLO , https://github.com/RapidAI/YOLO2COCO . These links offer the necessary code for label format conversion, accompanied by a README file that serves as a helpful reference. The annotation tool LabelImg is available for download on the official website at https://github.com/heartexlabs/labelImg . For dataset verification using Co-DETR and YOLOv6-L6 codes, the following website links can be visited at: https://github.com/Sense-X/Co-DETR and https://github.com/meituan/YOLOv6 . The site-packages and their corresponding versions used for the aforementioned two networks are provided in Table 6 . The software packages can be obtained by accessing the links specified in the README files of the respective networks and can be easily installed using the Python package installer (pip)."
10.1038/s41597-024-03644-y,All data processing commands and pipelines were carried out in accordance with the instructions and guidelines provided by the relevant bioinformatic sofware. There were no custom scripts or code utilized in this study.
10.1038/s41597-024-03625-1,"No specific script was used in this work. All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software, and default parameters were applied if not mentioned in the Methods described above."
10.1038/s41597-024-03641-1,"Pre-processing code was written in R to delete non otolith artefacts, select meshes without holes, save the mesh as a .ply file, and place 6 landmarks for all 3D otolith meshes. The code is available in the supplementary information of this paper."
10.1038/s41597-024-03409-7,"The dataset is available under CC0 licensing at Figshare 40 , Kaggle 41 , and GitHub 42 and it is accompanied by the anonymized patients’ information in xlsx format. For the data analysis and validation, Python version 3.9 was used. The release of the published dataset is 2023-04-04. The possible database updates will always be published with the release date."
10.1038/s41597-024-03602-8,"No custom code was used in the collection and creation of the data. Scandinavian letters (like “å”, “ä”, and “ö” in Swedish and Finnish) are utilized in place names and descriptive sections. A Python code for merging photographs with artefacts is provided alongside the AADA photos in the Zenodo repository (Supplement 2) 7 . The R-markdown document and tailored R-script (.rmd) provided allow for the replication of spatial distributions of stone tool typologies during the Stone Age (Fig. 4 ), variations in artefacts during the Bronze Age (Fig. 5 ) and Iron Age (Fig. 6 ), and changes in overall activity from the Late Mesolithic to the Middle Neolithic (Fig. 7 ). Users have the capability to selectively query relevant portions of the dataset, facilitating accelerated data exploration and visualization processes (Supplement 3) 8 ."
10.1038/s41597-024-03655-9,All scripts to replicate the presented validation analysis and figures are publicly accessible under Affero General Public License (AGPL) 20 alongside the aggregated datasets based on Scopus and OpenAlex on Zenodo at https://doi.org/10.5281/zenodo.11145735 and this ref. 20 .
10.1038/s41597-024-03599-0,"Each spectra in.mgf format were merged with an R script using the R 3.6.0 language, and it is publicly available on GitHub, https://github.com/Ting1217/Feature-Merge-R ."
10.1038/s41597-024-03631-3,"The codes used in this article are available for free in the dedicated GitHub public repositories https://github.com/ajinkya-kulkarni/PyOrganoIDNet and https://github.com/ajinkya-kulkarni/PyBlendPatches , under the CC-By License v4.0. The pre-trained model weights and the dataset presented are freely accessible from the corresponding Zenodo repository 26 ."
10.1038/s41597-024-03651-z,The authors declare that no custom code was used.
10.1038/s41597-024-03627-z,The code for data processing and visualization can be found in the following GitHub repository: https://github.com/nordlinglab/COVID19TW-Viz .
10.1038/s41597-024-03652-y,No custom code has been used in this study.
10.1038/s41597-024-03646-w,"The computations and visualisations presented in this work were done with licensed software Gravsoft software package developed by the DTU Space at the Technical University of Denmark (DTU), Geosoft by Geosoft Inc; AGR software developed by NYCU (National Yang Ming Chiao Tung University); and freely available software QGIS 43 and GMT 50 . No specific code was used for the creation of this dataset for this research."
10.1038/s41597-024-03622-4,All software and pipelines were executed according to the manual and protocols of the published bioinformatics tools. The version and parameters of software have been described in Methods.
10.1038/s41597-024-03658-6,"In order to enhance accessibility of this dataset for users, we have made it available through a dedicated GitHub repository. This repository hosts Python sample code for data type conversion and data manipulation, aimed at facilitating researchers’ comprehension and utilization of the dataset. The resource can be freely accessed at the following GitHub repository: https://github.com/chycxyzd/LDFC ."
10.1038/s41597-024-03663-9,The R code containing data transformation commands is provided as a supplementary material to the database and can be found in the OSF repository 34 .
10.1038/s41597-024-03662-w,"All recorded data has been processed and evaluated with Python based scripts. Exemplary evaluation codes are available in the uploaded repository, providing information on how specific parameters have been evaluated from the respective raw data files."
10.1038/s41597-024-03661-x,The custom code used for creating the dataset is available online: https://github.com/eic-astound-ai-project/artGenEvalPlatform .
10.1038/s41597-024-03643-z,"The data were generated using the NASA Land Information System (LIS, http://lis.gsfc.nasa.gov ), which is a comprehensive land surface modeling and data assimilation framework that supports modeling over user-specified regional or global domains using an ensemble of land surface models. The code is publicly available on GitHub: https://github.com/NASA-LIS . The datasets used in this study can be found on the following websites: •  ERA5 forcing can be found in https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5 •  IMERG Precipitation: https://gpm.nasa.gov/taxonomy/term/1372 •  CHIRPS Precipitation: https://www.chc.ucsb.edu/data •  SWE reconstruction by Kraaijenbrink et al . 2 : https://zenodo.org/record/4715786#.YqDY0S-B1pI •  MODIS LAI: https://lpdaac.usgs.gov/products/mcd15a2hv006/ •  ESA CCI soil moisture: https://www.esa-soilmoisture-cci.org/data •  GRACE data: https://earth.gsfc.nasa.gov/geo/data/grace-mascons"
10.1038/s41597-024-03639-9,"No code was produced to support this effort. All analyses described in this paper utilized current available tools within ESRI mapping software ArcGIS Pro 3x and required use of the Spatial Analyst extension of this software ( esri.com ). All analyses can also be accomplished within QGIS ( qgis.org ), an open-source mapping software."
10.1038/s41597-024-03660-y,The R codes used in this study are available in GitHub ( https://github.com/CaiYangyangBio/AML ).
10.1038/s41597-024-03671-9,The code of data processing and tree cover mapping algorithm are available on GitHub ( https://github.com/Tamako2024/FNN_forest ).
10.1038/s41597-024-03692-4,No custom code has been used to curate the dataset.
10.1038/s41597-024-03665-7,"The code is publicly available in the following GitHub repository: https://github.com/CMAIBITU/Soul . The repository encompasses python version, which contains how to preprocess the original image, how to extract the ROI region and pre-trained state-of-the-art deep learning models of OCTA image segmentation on ROSE publicic dateset."
10.1038/s41597-024-03667-5,"We refined dcm2niix for converting the source DICOM MRI scans to BIDS format, with improvements incorporated in this open source software ( https://github.com/rordenlab/dcm2niix ). Our defacing method is available from GitHub ( https://github.com/neurolabusc/mydeface ). We provide minimal Matlab and Python scripts to organize, process, and analyze these data using machine learning. These scripts are all stored in a self-contained archive at GitHub ( https://github.com/neurolabusc/StrokeOutcomeOptimizationProjectDemo ), allowing others to replicate and extend the findings we describe in the Technical Validation section."
10.1038/s41597-024-03664-8,"No specific code was developed for this study. The data analyses were conducted following the manuals and protocols provided by the developers of the relevant bioinformatics tools, which are described in the Methods section along with the versions used."
10.1038/s41597-024-03690-6,No novel code was used in the construction of the FPRM dataset.
10.1038/s41597-024-03629-x,"Code for transforming select item-level, raw data to NDA summary/composite score elements is available within the BANDA Resources and Materials repository 51 . Source code for the fMRI tasks is accessible via the BANDA fMRI-tasks repository 52 . Code used to generate the quality assurance and HCP minimally-preprocessed outputs is curated within the BANDAAllFiles collection NDA package 34 (e.g., /fmriresults01/ < subject_MR > /ProcessingInfo )."
10.1038/s41597-024-03685-3,"This work is exclusively based on open software and data. The used object detection architecture (based on PyTorch) is openly available here (Python): https://github.com/ultralytics/yolov5 . CaSSIS image data are openly available here (use the search bar to find specific image IDs or zoom and pan on the map): https://observations.cassis.unibe.ch/ . HiRISE data are openly available here (use the search bar to find specific image IDs or use the “Map” function): https://www.uahirise.org/ . CRISM data are openly available here (zoom and pan across the map to find images in specific regions, query using the “information” tool): http://crism-map.jhuapl.edu/ . The used USGS map products (MOLA topography, TES albedo, geologic map) are openly available here, respectively: (1) https://astrogeology.usgs.gov/search/details/Mars/GlobalSurveyor/MOLA/Mars_MGS_MOLA_DEM_mosaic_global_463m/cub , (2) https://astrogeology.usgs.gov/search/map/Mars/GlobalSurveyor/TES/Mars_MGS_TES_Albedo_mosaic_global_7410m , (3) https://astrogeology.usgs.gov/search/map/Mars/Geology/Mars15MGeologicGISRenovation?p=2&pb=1#downloads . The used CRISM/OMEGA products are openly available here: https://www.cosmos.esa.int/web/psa/mars-maps ."
10.1038/s41597-024-03666-6,The utilization of bioinformatics software and tools for this research was performed in accordance with published protocols and manuals obtained from public databases. The method provided a description of the software version and parameters without employing any specific code or scripts.
10.1038/s41597-024-03678-2,"The code for downloading, reading, pre-processing and applying the reported baseline machine learning algorithms can be found on GITHUB via the following URL ( https://github.com/ounospanas/RadIOCD ). The code was written in Python 3.8 and is provided in a .ipynb format ( https://jupyter.org/ ). In particular, we used Pandas ( https://pandas.pydata.org/ ) for loading the CSV files, Numpy ( https://numpy.org/ ) library for data pre-processing, segmentation and feature extraction, Scikit-learn ( https://scikit-learn.org/ ) for training the common machine learning algorithms and the Tensorflow ( https://tensorflow.org/ ) framework to develop and train the neural networks."
10.1038/s41597-024-03674-6,"The GEE code for classifying CIPs based on Sentinel-2 imagery can be accessed on GitHub at the following link ( https://github.com/Songwm26/CIPs21_ScientificData_main ). The code, written in JavaScript, includes all the steps mentioned in this article, such as feature calculation, random forest training, and so on. Additionally, we provide data on the area of CIPs within different distance ranges for counties and cities along the Yangtze River."
10.1038/s41597-024-03694-2,Genome Association and Prediction Integrated Tool (GAPIT) R package: https://www.maizegenetics.net/gapit 40 . The principle component analysis were performed using MetaboAnalyst 6.0 41 .
10.1038/s41597-024-03680-8,"All programs and pipelines used in this study are open-sourced. Versions and options used for the execution of individual programs are provided in the ‘Method’ section. Unless otherwise specified, default options were employed. No in-house scripts were implemented for this study."
10.1038/s41597-024-03499-3,The precipitation time series at the scan locations and sea state parameters at P0 can be downloaded from the Climate Data Store (CDS) using the Python scripts available at the Nottingham Research Repository ( https://doi.org/10.17639/nott.7308 ) 22 . The scripts require the installation of the CDS Application Programme Interface (API). Guidance is available at https://cds.climate.copernicus.eu/api-how-to . The provided MetOcean data can be visualised and/or analysed using standard releases of any commercial and open- source scientific software/language (such as MATLAB and Python) that can read NetCDF files.
10.1038/s41597-024-03676-4,"The data recording script is written with PsychoPy 2021.2.3 under Python 3.8. The source code and related resource files (e.g., pictures) have been uploaded, and a Readme is also provided for more details. The repository is available at: https://github.com/ypthu/Multimodal-dataset-for-mixed-emotion-recognition-Data-collection . Codes for mixed emotion classification introduced in technical validation are implemented using Matlab and Python and are available at https://github.com/ypthu/Multimodal-dataset-for-mixed-emotion-recognition . The source files in this repository are mainly for raw data formatting, preprocessing, feature extraction, and emotion classification. More details for these source files can be found in README.md in this repository."
10.1038/s41597-024-03691-5,The Python 3.10 code related to this project is available for download at https://github.com/tomoko-mcgaughey/Travel-Times-Canada .
10.1038/s41597-024-03539-y,"All the relevant code for replicating the different variables in the GLocal datasets are publicly available to users at this GitHub repository . We have added README files that explain how to replicate each component of the study. Note that acquiring each underlying input dataset for aggregation involves different processes. Wherever possible, download scripts have been included to simplify the process of downloading each dataset. However, in some cases, the data is not publicly available, and we have provided instructions on how to request access to the data (mentioned in the codebook)."
10.1038/s41597-024-03708-z,"No specific code was used in this study. All analytical processes were executed according to the manuals and protocols of the corresponding bioinformatic tools. The software parameters used in this study are as follows: fastp v0.21.0: -n 0 -f 5 -F 5 -t 5 -T 5 -q 20KMC v3.2.1: -k17 -ci1 -cs1000000GenomeScope v1: defaulthifiasm v0.16.0: defaultBUSCO v4.0.5: -l vertebrata_odb10 -g genomeBWA v0.7.15: defaultminimap2: -x map-hifibowtie2 v2.3.2: -end-to-end --very-sensitive -L 30HiC-Pro v2.8.1: -c confg-hicpro.txt -i -oLACHESIS v1: CLUSTER_MIN_RE_SITES = 100,CLUSTER_MAX_LINK_DENSITY = 2.5, CLUSTER NONINFORMATIVE RATIO = 1.4, ORDER MIN N RES IN TRUNK = 60, ORDER MIN N RES IN SHREDS = 60Juicebox v 1.11.08: defaultTBtools II: defaultGMATA v2.2: defaultTRF v4.07b: 2 7 7 80 10 50 500 -f -d -h -rMITE-hunter: -n 20 -P 0.2 -c 3RepeatModeler v1.0.11: -engine wublastLTR_FINDER: defaultLTR_harvest: defaultLTR_retriver: defaultTEclass v2.1.3: defaultRepeatMasker v1.331: nolow -no_is -gff -norna -engine abblast -lib libGeMoMa v1.6.1: defaultSTAR v2.7.3a: -outWigType bedGraph --outSAMtype BAM SortedByCoordinate--outSAMstrandField intronMotifStringtie v1.3.4d: defaultPASA v2.3.3: -c alignAssembly.config -C -R -g genome.fasta -T -u trans.fasta -t trans.clean.fasta -f fl.acc --CPU 10 --ALIGNERS gmapAugustus v3.3.1: --gff3 = on --hintsfile = hints.gff --extrinsicCfgFile = extrinsic.cfg--allow_hinted_splicesites = gcag, atac --min_intron_len = 30 --softmasking = 1EVM v1.1.1: --segmentSize 1000000 --overlapSize 100000InterProScan v5.32: defaultBlastp v2.7.1: -e 1e-5tRNAscan-SE v2.0: --thread 4 -E -IInfernal v1.1.2: defaultRNAmmer v1.2: -S euk -m lsu, ssu, tsu -gff"
10.1038/s41597-024-03696-0,MISSING
10.1038/s41597-024-03702-5,"The JCDS dataset described in this article was generated using a series of purpose-built programs written in Fortran90 on a MacOS PC. These programs utilize libraries to handle the NetCDF format. Due to the complexity and size of source data, which use several file formats and occupy approximately 7 TB, the programs are not directly distributed. A sample script to read and extract the JCDS data at give locations is available at: https://doi.org/10.5281/zenodo.12597021 ."
10.1038/s41597-024-03689-z,"The toolchain to produce this travel time matrix data set can be found in the following Git repository: https://github.com/DigitalGeographyLab/Helsinki-Travel-Time-Matrices/ , the r5py Python package at https://github.com/r5py/r5py ."
10.1038/s41597-024-03705-2,"The statistical analysis and modeling employed for examples given in the Usage Notes was done using R packages developed within the Pandora & IsoMemo initiatives 38 , 48 , 54 . The source code for all of these is available for download on GitHub: AverageR ( https://github.com/Pandora-IsoMemo/DSSM ), BMSC ( https://github.com/Pandora-IsoMemo/bmsc ), and ReSources ( https://github.com/Pandora-IsoMemo/resources ). These can be run online or locally ( https://github.com/Pandora-IsoMemo/drat ) as Shiny apps 55 . For modeling reproducibility, a full description of model options is available through the IsoMad data community ( https://pandoradata.earth/organization/isomad-isotopic-data-of-madagascar ), which is stored on the Pandora data platform. This platform is based on the CKAN open source data management system ( https://ckan.org/ ) and is hosted by the Max Planck Computing and Data Facility."
10.1038/s41597-024-03695-1,The code used for technical validation is available at our Github repository ( https://github.com/MuSAELab/MSPB ). Other details of hive management can be found at the dataset repository 42 as well as the project webpage ( https://zhu00121.github.io/MSPB-webpage/ ).
10.1038/s41597-024-03711-4,R codes applied in the present study are available at: https://github.com/xnus/PamirSoilMicrobes .
10.1038/s41597-024-03601-9,No custom codes were used for this analysis.
10.1038/s41597-024-03698-y,"Our workflow is built on several open-source toolkits including the RDKit 10 , xTB 48 , and CREST 47 . xTB and CREST are both freely available ( https://github.com/grimme-lab/xtb/releases and https://github.com/grimme-lab/crest/releases ). OpenEye Applications and Toolkits was used for graph reidentification. We provide code with instructions on how to load the data and perform simple CREST runs at https://github.com/Genentech/cremp ."
10.1038/s41597-024-03726-x,Four files with R code are provided together with the data ( https://doi.org/10.3220/DATA20240507093138-0 ). The first R code displays provenances and sites according to their geographic location (location_of_provenances_and_sites.R) and the second merges metadata to phenotypes (merge_metadata_to_phenotypes.R). The third calculates arithmetic trait means per provenance for one individual site and displays these spatially at their corresponding geographic location (mean_growth_by_provenance.R). The fourth illustrates the experimental design and the spatial distribution of single tree-based measurements for each individual site (experimental_design_by_site.R).
10.1038/s41597-024-03709-y,The codes of the MechFinder introduced in this paper are publicly available at https://github.com/snu-micc/MechFinder .
10.1038/s41597-024-03730-1,No specific code was developed in this work.
10.1038/s41597-024-03735-w,No custom code was used to collect and process the data described in this manuscript.
10.1038/s41597-024-03728-9,"All code associated with the Marine Fish Movement data 17 are available as a zip file, “marine_fish_movement_code.zip” hosted by Zenodo 30 ): 1. data-prep folder with two R Markdown scripts: (i) “clean_spp_names.Rmd” to clean up species taxonomy, and (ii) “randomforest_dataprocessing.Rmd” for harmonising and processing model variables. 2. analysis folder with a single R Markdown script, “homerange_randomforest.Rmd” that sets up and implements the random forest regression model. 3. README.md providing further relevant details. Code can be reproduced using R version 4.1.3."
10.1038/s41597-024-03697-z,Detailed code describing the assessment of G0 arrest levels across TCGA cancers is available at the following GitHub repository: https://github.com/secrierlab/CancerG0Arrest .
10.1038/s41597-024-03721-2,A spectral library was generated using the commercial software Spectronaut and evaluated using DIALib-QC. No custom code was generated in this work.
10.1038/s41597-024-03733-y,All commands and pipelines used for data processing were executed according to the manual and protocols of the corresponding bioinformatics software.
10.1038/s41597-024-03717-y,No custom code was developed in this work. Software and pipelines were implemented according to the manuals and protocols of published bioinformatics tools. Software version and parameters were described in Methods.
10.1038/s41597-024-03672-8,The source code to perform the analysis and generate the output reports is publicly available on GitHub ( https://github.com/BioinformaticsPlatformWIV-ISP/BenchmarkingClassifiers ) accompanied by an example dataset showcasing the expected output structure and final output file.
10.1038/s41597-024-03729-8,"The signal process and vigilance estimation scripts employed in this article were provided in “ code /” directory. Specifically, the Matlab scripts in “ code / data_process /” directory were used for loading, preprocessing, segmentation, and feature extraction of the raw data, then, saving the preprocessed data, the trial data, and the feature data. The Python scripts were included in “ code / vigilance_estimation /” directory. main_SVR_EEG.py and main_SVR_Multi_Physiological_Signals.py were used for loading the feature data and conducting vigilance estimation with the SVR 14 method. main_STGCN_EEG.py and main_LSTM_Multi_Physiological_Signals.py were used for loading the feature data and conducting vigilance estimation with the STGCN/LSTM 4 method. The required Python environment was listed in requirements.txt ."
10.1038/s41597-024-03688-0,The MATLAB scripts used for technical validation and data visualizations are accessible under the MIT license through the GitHub repository ( https://github.com/jeelabKIST/Cho2024_MouseEscapeData ). Comprehensive instructions for the installation and execution of these scripts are provided within the same repository.
10.1038/s41597-024-03608-2,"No specifc code was used in this study. The data analyses used standard bioinformatic tools, with parameters being clearly described in Methods. If specific parameters were not provided for the software, default settings recommended by the developer were utilized."
10.1038/s41597-024-03706-1,"The current version of the code used to produce the EStreams dataset and catalogue (v1.0.0) is available at a Zenodo repository 47 at https://doi.org/10.5281/zenodo.13255133 . For the latest version of the code, users are invited to visit the project GitHub repository at https://github.com/thiagovmdon/EStreams . The scripts are organized to enable users to follow a logical sequence during code usage. All data processing scripts are written in Python, while some data retrieval tasks are performed using JavaScript for the Google Earth Engine (GEE) platform. Although all scripts are executable, users must download and preprocess the original data due to redistribution licenses. Detailed instructions regarding the version used, data retrieval, and any required preprocessing are provided within the respective scripts."
10.1038/s41597-024-03670-w,"In this study, all pipelines and software applications were employed for data analysis strictly in accordance with their respective manuals and protocols. Detailed information regarding the versions and parameters of each software is provided in the Methods section. For those instances where specific parameters are not explicitly mentioned, it was ensured that the default settings of the respective software were utilized."
10.1038/s41597-024-03684-4,"The software used to create the dataset were ArcGIS (10.2), Python 3.8, and R 4.3.2. The code is available on GitHub ( https://github.com/kkxiaoqin/electricity_downscaling )."
10.1038/s41597-024-03740-z,No custom codes were created or utilized in this study.
10.1038/s41597-024-03681-7,All software and pipelines were executed in strict accordance with the manuals and protocols provided by the published bioinformatics tools. No custom programming or coding was used.
10.1038/s41597-024-03714-1,We provide the R code on GitHub at https://github.com/JHKim36/DAVIT.git .
10.1038/s41597-024-03605-5,"We automated both phases of the data compilation process presented in this work, using the Python programming language and corresponding libraries that are freely distributed by the Python Package Index (PyPI). The data resources described in this paper, including Python scripts, can be accessed without restrictions at GitHub ( https://github.com/iit-Demokritos/entrant ). Anyone can browse the content of the repository, and everyone is encouraged to fork or create pull requests with improvements and enhancements. The code is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as appropriate credit to the original author(s) and the source are given, a link to the Creative Commons license is provided, and any changes are clearly indicated. By releasing the code for this work, we aim to enable further augmentation of the dataset, as anyone can use it to fetch more reports from EDGAR and transform them in the described format. An additional advantage is that the transformation process alone is a powerful tool, since it allows the transformation of excel files that follow similar table formats to the described JSON format."
10.1038/s41597-024-03683-5,All Python code scripts are publicly accessible on the project’s github repository at https://github.com/oussema-dev/Nacob_walking_surface_classification .
10.1038/s41597-024-03701-6,"Our datasets 16 is public available in Science DB ( http://www.scidb.cn/en ), input link ( https://www.scidb.cn/anonymous/SlpaelFy ) or https://doi.org/10.57760/sciencedb.07008 . The JSON file is in the label folder under the lens dataset. In the label folder, you can also see some visual annotation content after segmentation. If you want to use it, you can directly call the JSON file named “annotations.json” in each dataset. The code for technical validation is public code, the dataset is accessible via the DOI of science DB, and the software EISeg for annotation is open source."
10.1038/s41597-024-03718-x,All software and tools were used following the parameters mentioned in the Methods section or with default parameters.
10.1038/s41597-024-03743-w,"Custom generated code used by all sites for collating the raw data from the ISIC2024 Tile Export Tool, which is described in the Methods section, is available at https://github.com/ISIC-Research/2024-challenge-dataset . All code is written in Python 3.10 and utilizes commonly used open-source packages, all of which are available on pypi.org."
10.1038/s41597-024-03668-4,The code is publicly accessible on Github: https://github.com/burnspat/gedi_gridding .
10.1038/s41597-024-03704-3,The code to conduct quality flagging described in the “Technical validation” section is written in Python and available on Zenodo. The code is available at https://github.com/VolunteeredGI/Lake-in-situ-dataset-code .
10.1038/s41597-024-03719-w,"GlobalMapper is available at this GitHub repository: https://github.com/Arking1995/GlobalMapper . The ALOS, GEDI, Google building footprints, and ESA land cover datasets were obtained through Google Earth Engine, while ICESat-2 data was acquired using the icepyx Python package. Microsoft and OSM building data were downloaded using APIs. ICESat-2 data was processed using PhoREAL ( https://github.com/icesat-2UT/PhoREAL ) and LAStools ( https://lastools.github.io/ ). All the datasets were processed using QGIS for Python version 3.26. The scripts for data downloading and data processing can be found at the UT-GLOBUS GitHub repository ( https://github.com/texuslabut/UT-GLOBUS ). The urban canopy parameters were converted to WRF binary file format using a Fortran code provided in the UT-GLOBUS GitHub repository."
10.1038/s41597-024-03762-7,Custom scripts used to generate or process this dataset were deposited in the figshare ( https://doi.org/10.6084/m9.figshare.26139184.v1 ). Software versions and non-default parameters used have been appropriately specified where required.
10.1038/s41597-024-03727-w,The scripts used for genome assembly and annotation in this article were uploaded to figshare 56 . All commands and pipelines used in data processing were executed according to the manual and protocols of the corresponding bioinformatic software.
10.1038/s41597-024-03768-1,All custom code used for pre-processing and technical validation is available at https://github.com/Fayed-Rsl/RHM_preprocessing .
10.1038/s41597-024-03725-y,"The macro used to calculate the FWHM of the ν 3 band and the IRSF of the ν 4 band of phosphates runs in Macros Basic coupled with OMNIC (including versions of the software older than the one used in this study) and it is available in the online repository 33 with the raw spectra (“IRSF_FWHM.mac”). Given the simple nature of the calculations, similar macros can be easily developed using different spectroscopy programmes."
10.1038/s41597-024-03741-y,"The data was provided in either FASTQ or unmapped BAM format, generated according to the manufacturers’ experimental protocols as detailed in the Methods section. No custom code was developed for data processing. All software and pipelines utilized for data generation are described in the Methods section. Default settings were used where specific parameters were not specified."
10.1038/s41597-024-03731-0,All software packages used in this study were run according to their user manuals. The version and parameters used are listed in the Table 6 . No specific custom codes were used in this study.
10.1038/s41597-024-03703-4,No custom code was used in this study.
10.1038/s41597-024-03744-9,"All files and code for importing, cleaning, and processing the CS-PHOC data described in this manuscript, as well as code for generating the figures and example processing, is available at https://github.com/us-amlr/cs-phoc ."
10.1038/s41597-024-03747-6,All software and tools in this study were used with their default parameters. No specific code or script was used in the study.
10.1038/s41597-024-03753-8,"The data analyses were performed according to the manuals and protocols by the developers of corresponding bioinformatics tools and all software, and codes used in this work are publicly available, with corresponding versions indicated in Methods."
10.1038/s41597-024-03679-1,The codes used to generate and read the dataset are available in the misc folder at https://doi.org/10.20383/103.0935 43 .
10.1038/s41597-024-03754-7,"No customized code was produced to create or analyse this dataset because software that works in a GIS environment is required to open and further process the SvalCryo inventory. Specifically, we used ArcGIS v10.6.1 software to create the dataset, but the SvalCryo inventory can be opened with any GIS software (open-sourced or licensed)."
10.1038/s41597-024-03722-1,"In this study, the codes of the analysis are available and open source ( https://doi.org/10.17605/OSF.IO/R26FH ). We uploaded scripts written in Python to the data repository for basic handling and analysis of the files, such as file opening, sleep stage file conversion, demographics, and sleep parameters analysis."
10.1038/s41597-024-03749-4,"The code developed to derive classification decisions and permeability assignments, as described before, is available at: https://doi.org/10.5281/ZENODO.7845568 40 ."
10.1038/s41597-024-03748-5,No custom script was used to generate datasets in this study. The software with parameters and versions of all the bioinformatics tools used in this study are listed in the “Methods” section.
10.1038/s41597-024-03769-0,All in-house Python codes used in the ‘ Microbial composition analysis and mapped breadth coverage calculation of ARGs ’ section are available through a GitHub repository at https://github.com/hoonjeseong/maricultureMAGs 61 .
10.1038/s41597-024-03699-x,"The code to compute the GW momentum fluxes and to conservatively coarsegrain the fluxes, along with the modified PySpharm functions can be accessed at: https://doi.org/10.17605/OSF.IO/GX32S in the python_scripts folder. Raw climate model output from XNR1K at native grid resolution is available at https://doi.ccs.ornl.gov/ui/doi/475 . A Stanford Redivis project that allows users to work with the data interactively in a Jupyter notebook environment has been staged at: https://doi.org/10.57761/rc6v-hf22 . The default WindSpharm Python package is publicly available at https://ajdawson.github.io/windspharm/latest/ , and the PySpharm Python package is publicly available at: https://pypi.org/project/pyspharm/ . The xESMF package used for conservative coarsegraining is publicly available at: https://xesmf.readthedocs.io/en/stable/ ."
10.1038/s41597-024-03767-2,"The data retrieval pipeline was written in Python (version 3.7.6) and ran in Docker on AWS Fargate (this code is found in https://github.com/wacl-york/quant-scraper ). The post-processing code to upload the data from the daily CSVs into the Postgres database (version 14.10), and then to export the database into NetCDF and CSV for storage into the CEDA repository was written in R (version 4) (found in https://github.com/wacl-york/quant-tools )."
10.1038/s41597-024-03710-5,"The GEE python API version of the SI-x code is available in a publicly accessible repository 11 , facilitating inspection and further calculations (provided that users have completed the free registration process set up by Google) of SI-x products. The code was prepared using the earthengine-api package version 0.2. Besides, this Python package, numpy is needed too. The repository also includes a corresponding requirements file available for their installation via pip package-management system: pip install -r requirements.txt The code was implemented to calculate annual SI-x products. For this, the user needs to go through two main phases: the computation and storage of the LF index and, subsequently, the computation of the BL index. The calculation of these indices, and of the rest of the SI-x products, logically requires loading the input data. This step depends on the geographical scope. The North American dataset used to generate our products, namely the Daily Surface Weather Data version 4 (Daymet V4) 17 is accessible within GEE and can directly be loaded (ee.ImageCollection(“NASA/ORNL/ DAYMET _ V 4”)). In contrast, the European dataset (a downscaled version of the European E-OBS version 3) 18 , demands a distinct procedure, which involves downloading the yearly NetCDF files available from a public FTP repository ( ftp://palantir.boku.ac.at/Public/ClimateData/ ) and transforming them into daily GeoTiffs that contain maximum and minimum temperatures. After that, the GeoTiffs must be ingested into GEE so that our code can load them. An extra difference arising from the geographical scope is that the European dataset does not include a daylength layer. For this reason, the calculation of the SI-x products over Europe required carrying over the day length function available in the MATLAB implementation of the SI-x models."
10.1038/s41597-024-03772-5,All software and packages used in this study are freely distributed and available through their citations brought in the text.
10.1038/s41597-024-03673-7,"The RNA-KG’s project website is at http://RNA-KG.anacleto.di.unimi.it . The code to reproduce results, together with documentation and tutorials, is available in RNA-KG’s GitHub repository at https://github.com/AnacletoLAB/RNA-KG . In addition, the repository contains information and Python scripts to build new versions of RNA-KG as the underlying primary resources get updated and new data become available."
10.1038/s41597-024-03720-3,The code to prepare the inputs of the VASP calculations and performing post-processing statistical analysis of the dataset is open-source and available at the ORNL-GitHub repository https://github.com/ORNL/ScalableWorkflow_VASP_Calculations .
10.1038/s41597-024-03746-7,The spatiotemporal greenspace data cube is generated on the Google Earth Engine (GEE) cloud-computing platform. The code for extracting and displaying greenspace coverage is publicly available at: https://code.earthengine.google.com/33b1f787976f25b33fa1146071ea5c3c .
10.1038/s41597-024-03657-7,"No specific codes or scripts were used in our work. All operations of the data processing were performed according to the manuals and protocols of the corresponding bioinformatics software, some of the parameters are described in the Methods section, and those not mentioned are set as default parameters."
10.1038/s41597-024-03786-z,"The dataset can be used for quantitative and qualitative analysis methods. Researchers who wish to run an already-established quantitative analysis script can find the relevant Python code on Figshare 24 . The analysis code is openly available and free to use under GPL 3.0+. As listed in the requirements file, the script uses NumPy 25 and pandas 26 for statistical analysis and matplotlib 27 with seaborn 28 to visualise the data. However, using these packages is not mandatory as the CSV format allows for statistical analysis with any statistical software."
10.1038/s41597-024-03739-6,"MAPLES-DR annotations must be paired with MESSIDOR fundus images to be exploited to their full potential. To simplify the joint usage of both datasets, we developed a python package named maples_dr , which is available on pipy and GitHub . The key features of this library are: loading and exporting the fundus images with their annotations in formats suited for machine learning applications, preprocessing the fundus images, and providing a pythonic access to all MAPLES-DR additional data (retinologist comments, annotation times, preannotations, etc). Quick-start and code examples are available in the maples_dr documentation , along with an API reference. The repository also contains the code that produced the figures in this paper. The algorithms used to generate the preannotation maps cannot be made public as they were implemented with the Theano library, which is no longer maintained. For transparency purposes, the original preannotations are available in the AdditionalData.zip archive of MAPLES-DR. However, we have released up-to-date Python segmentation models (trained with MAPLES-DR and other public datasets) in two Github repositories: gabriel-lepetitaimon/fundus-vessels-toolkit provides automatic segmentation and graph extraction of the retinal vasculature; ClementPla/fundus-lesions-toolkit provides automatic segmentation and visualization of retinal lesions. Finally the annotation platform code and documentation are freely available to any research team that wishes to use it in the Github repository: LIV4D/AnnotationPlatform ."
10.1038/s41597-024-03750-x,"The source code of the Moringa processing chain is available at https://gitlab.irstea.fr/raffaele.gaetano/moringa.git . It is complemented with custom modules for specific steps (e.g., for computing reasons the calculation of object-based statistics at step 2 makes use an ad-hoc C++ module, “obiatools”) whose source code is available at https://gitlab.irstea.fr/raffaele.gaetano/obiatools ."
10.1038/s41597-024-03782-3,"Raw sequencing data were analyzed using publicly available bioinformatics sofwares. We used common data analysis sofware packages and no custom code was created. Sofware tools used are as follows: TrimGalore (v0.6.8, https://github.com/FelixKrueger/TrimGalore ) STAR (v2.7.10a, https://github.com/alexdobin/STAR ) Kallisto (v0.44.0, https://github.com/pachterlab/kallisto ) R sofware (v4.2.0, https://www.r-project.org/ ) Clusterprofiler (v4.10.0, https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html ) Deseq2 (v1.38.3, https://bioconductor.org/packages/release/bioc/html/DESeq2.html ) Bioinformatics analysis platform ( https://www.bioinformatics.com.cn/ ) Genescloud tools ( https://www.genescloud.cn/home ) GraphPad Prism 8 (GraphPad Sofware Inc., USA) was used for statistical analyses and data visualization."
